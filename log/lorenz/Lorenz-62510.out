Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./Lorenz_tensorboard/PPO_2
Eval num_timesteps=500, episode_reward=-6364830.63 +/- 26206.77
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.008303959 |
|    mean velocity x | -2.51        |
|    mean velocity y | -2.81        |
|    mean velocity z | 25.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.36e+06    |
| time/              |              |
|    total_timesteps | 500          |
-------------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-6352880.24 +/- 32071.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.054123193 |
|    mean velocity x | 1.4         |
|    mean velocity y | 1.67        |
|    mean velocity z | 24.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 1000        |
------------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-6349585.40 +/- 45543.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09282023 |
|    mean velocity x | 0.275       |
|    mean velocity y | 1.1         |
|    mean velocity z | 24.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 1500        |
------------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-6347614.32 +/- 21409.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.05728635 |
|    mean velocity x | 0.414       |
|    mean velocity y | 0.656       |
|    mean velocity z | 26.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 2000        |
------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 1    |
|    time_elapsed    | 82   |
|    total_timesteps | 2048 |
-----------------------------
Eval num_timesteps=2500, episode_reward=-6279803.36 +/- 30132.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05729138  |
|    mean velocity x      | -0.218       |
|    mean velocity y      | -0.305       |
|    mean velocity z      | 25.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.28e+06    |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0016050609 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | -1.32e-05    |
|    learning_rate        | 0.001        |
|    loss                 | 2.38e+08     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0021      |
|    std                  | 1            |
|    value_loss           | 4.96e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=-6305299.89 +/- 29965.34
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.056084853 |
|    mean velocity x | 1.42         |
|    mean velocity y | 1.12         |
|    mean velocity z | 25.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.31e+06    |
| time/              |              |
|    total_timesteps | 3000         |
-------------------------------------
Eval num_timesteps=3500, episode_reward=-6295681.69 +/- 41430.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.039760444 |
|    mean velocity x | 1.91        |
|    mean velocity y | 2.23        |
|    mean velocity z | 25.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.3e+06    |
| time/              |             |
|    total_timesteps | 3500        |
------------------------------------
Eval num_timesteps=4000, episode_reward=-6309931.78 +/- 46308.79
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.018516276 |
|    mean velocity x | -3.97        |
|    mean velocity y | -4.01        |
|    mean velocity z | 25.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.31e+06    |
| time/              |              |
|    total_timesteps | 4000         |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 2    |
|    time_elapsed    | 166  |
|    total_timesteps | 4096 |
-----------------------------
Eval num_timesteps=4500, episode_reward=-6335508.51 +/- 31713.37
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0006414776 |
|    mean velocity x      | -1.77         |
|    mean velocity y      | -1.73         |
|    mean velocity z      | 24.3          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.34e+06     |
| time/                   |               |
|    total_timesteps      | 4500          |
| train/                  |               |
|    approx_kl            | 0.0016727454  |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.26         |
|    explained_variance   | -1.79e-06     |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+08       |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00137      |
|    std                  | 1             |
|    value_loss           | 3.89e+08      |
-------------------------------------------
Eval num_timesteps=5000, episode_reward=-6346662.35 +/- 38900.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.019017097 |
|    mean velocity x | -4.04       |
|    mean velocity y | -3.65       |
|    mean velocity z | 25          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 5000        |
------------------------------------
Eval num_timesteps=5500, episode_reward=-6332092.33 +/- 25521.69
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.010870667 |
|    mean velocity x | -1.01        |
|    mean velocity y | -0.309       |
|    mean velocity z | 24.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.33e+06    |
| time/              |              |
|    total_timesteps | 5500         |
-------------------------------------
Eval num_timesteps=6000, episode_reward=-6351607.08 +/- 29045.85
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.043114588 |
|    mean velocity x | 3.29         |
|    mean velocity y | 3.63         |
|    mean velocity z | 23.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.35e+06    |
| time/              |              |
|    total_timesteps | 6000         |
-------------------------------------
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 3    |
|    time_elapsed    | 249  |
|    total_timesteps | 6144 |
-----------------------------
Eval num_timesteps=6500, episode_reward=-6313873.57 +/- 31269.71
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05102232  |
|    mean velocity x      | -1.97        |
|    mean velocity y      | -1.95        |
|    mean velocity z      | 25.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.31e+06    |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0027363668 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 1.15e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 1.92e+08     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00163     |
|    std                  | 1            |
|    value_loss           | 3.95e+08     |
------------------------------------------
Eval num_timesteps=7000, episode_reward=-6324612.33 +/- 38481.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.043710288 |
|    mean velocity x | -3.29       |
|    mean velocity y | -3.3        |
|    mean velocity z | 25.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.32e+06   |
| time/              |             |
|    total_timesteps | 7000        |
------------------------------------
Eval num_timesteps=7500, episode_reward=-6312192.21 +/- 61676.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11902843 |
|    mean velocity x | -2.25       |
|    mean velocity y | -1.54       |
|    mean velocity z | 26.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.31e+06   |
| time/              |             |
|    total_timesteps | 7500        |
------------------------------------
Eval num_timesteps=8000, episode_reward=-6298989.18 +/- 41082.12
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0036525785 |
|    mean velocity x | -3.58         |
|    mean velocity y | -3.88         |
|    mean velocity z | 24.8          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -6.3e+06      |
| time/              |               |
|    total_timesteps | 8000          |
--------------------------------------
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 4    |
|    time_elapsed    | 333  |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=8500, episode_reward=-6285294.12 +/- 23068.17
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.03424954 |
|    mean velocity x      | 4.79        |
|    mean velocity y      | 5.03        |
|    mean velocity z      | 26.8        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.29e+06   |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.00420539  |
|    clip_fraction        | 0.00874     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 2.23e+08    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00301    |
|    std                  | 1           |
|    value_loss           | 4.44e+08    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-6304870.30 +/- 42345.84
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.032886792 |
|    mean velocity x | 3.71         |
|    mean velocity y | 3.12         |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.3e+06     |
| time/              |              |
|    total_timesteps | 9000         |
-------------------------------------
Eval num_timesteps=9500, episode_reward=-6290946.00 +/- 45024.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.037800286 |
|    mean velocity x | 3.33        |
|    mean velocity y | 3.19        |
|    mean velocity z | 25.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.29e+06   |
| time/              |             |
|    total_timesteps | 9500        |
------------------------------------
Eval num_timesteps=10000, episode_reward=-6317617.54 +/- 17294.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.011053776 |
|    mean velocity x | -0.778      |
|    mean velocity y | -0.294      |
|    mean velocity z | 22.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.32e+06   |
| time/              |             |
|    total_timesteps | 10000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 5     |
|    time_elapsed    | 417   |
|    total_timesteps | 10240 |
------------------------------
Eval num_timesteps=10500, episode_reward=-6283532.25 +/- 43748.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.035950005  |
|    mean velocity x      | -0.161       |
|    mean velocity y      | -0.146       |
|    mean velocity z      | 25.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.28e+06    |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0035086013 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | -5.13e-06    |
|    learning_rate        | 0.001        |
|    loss                 | 2.23e+08     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00295     |
|    std                  | 1            |
|    value_loss           | 4.6e+08      |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-6241855.46 +/- 17699.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.060923453 |
|    mean velocity x | 0.728       |
|    mean velocity y | 0.766       |
|    mean velocity z | 26.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.24e+06   |
| time/              |             |
|    total_timesteps | 11000       |
------------------------------------
New best mean reward!
Eval num_timesteps=11500, episode_reward=-6233627.26 +/- 59880.64
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0045137233 |
|    mean velocity x | 2.48         |
|    mean velocity y | 2.86         |
|    mean velocity z | 24.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.23e+06    |
| time/              |              |
|    total_timesteps | 11500        |
-------------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=-6284647.06 +/- 30616.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.09311835 |
|    mean velocity x | 1.65       |
|    mean velocity y | 1.38       |
|    mean velocity z | 24.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.28e+06  |
| time/              |            |
|    total_timesteps | 12000      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 6     |
|    time_elapsed    | 500   |
|    total_timesteps | 12288 |
------------------------------
Eval num_timesteps=12500, episode_reward=-6283410.67 +/- 38110.98
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.05204654  |
|    mean velocity x      | -2.05       |
|    mean velocity y      | -2.3        |
|    mean velocity z      | 25.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.28e+06   |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.004928314 |
|    clip_fraction        | 0.0158      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 2.98e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 2.01e+08    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00402    |
|    std                  | 1           |
|    value_loss           | 4.44e+08    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-6295362.41 +/- 13254.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.00590019 |
|    mean velocity x | 1.68       |
|    mean velocity y | 1.91       |
|    mean velocity z | 24.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.3e+06   |
| time/              |            |
|    total_timesteps | 13000      |
-----------------------------------
Eval num_timesteps=13500, episode_reward=-6281441.96 +/- 13556.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.014565308 |
|    mean velocity x | 4.42        |
|    mean velocity y | 4.49        |
|    mean velocity z | 25.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+06   |
| time/              |             |
|    total_timesteps | 13500       |
------------------------------------
Eval num_timesteps=14000, episode_reward=-6289653.03 +/- 29582.32
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0029852723 |
|    mean velocity x | 0.815         |
|    mean velocity y | 0.88          |
|    mean velocity z | 25.8          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -6.29e+06     |
| time/              |               |
|    total_timesteps | 14000         |
--------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 7     |
|    time_elapsed    | 584   |
|    total_timesteps | 14336 |
------------------------------
Eval num_timesteps=14500, episode_reward=-6285575.53 +/- 15251.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.056042448 |
|    mean velocity x      | 3.27         |
|    mean velocity y      | 3.81         |
|    mean velocity z      | 26.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.29e+06    |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0030398914 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.4e+08      |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00229     |
|    std                  | 1            |
|    value_loss           | 4.36e+08     |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-6260560.03 +/- 34056.94
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.039651748 |
|    mean velocity x | -0.326       |
|    mean velocity y | -0.00681     |
|    mean velocity z | 25.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.26e+06    |
| time/              |              |
|    total_timesteps | 15000        |
-------------------------------------
Eval num_timesteps=15500, episode_reward=-6263192.20 +/- 49283.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.06772249 |
|    mean velocity x | -3.55      |
|    mean velocity y | -3.49      |
|    mean velocity z | 25.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.26e+06  |
| time/              |            |
|    total_timesteps | 15500      |
-----------------------------------
Eval num_timesteps=16000, episode_reward=-6269762.99 +/- 57827.68
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.098026 |
|    mean velocity x | 4.26      |
|    mean velocity y | 4.72      |
|    mean velocity z | 25.9      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.27e+06 |
| time/              |           |
|    total_timesteps | 16000     |
----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 8     |
|    time_elapsed    | 667   |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=16500, episode_reward=-6317324.90 +/- 37740.21
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05386079  |
|    mean velocity x      | 1.72         |
|    mean velocity y      | 1.92         |
|    mean velocity z      | 25.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.32e+06    |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0035168426 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 2.23e+08     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1            |
|    value_loss           | 4.45e+08     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=-6319788.83 +/- 45908.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10491138 |
|    mean velocity x | 2.84        |
|    mean velocity y | 3.06        |
|    mean velocity z | 24.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.32e+06   |
| time/              |             |
|    total_timesteps | 17000       |
------------------------------------
Eval num_timesteps=17500, episode_reward=-6300932.09 +/- 37986.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07312895 |
|    mean velocity x | 1.57        |
|    mean velocity y | 2.09        |
|    mean velocity z | 25.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.3e+06    |
| time/              |             |
|    total_timesteps | 17500       |
------------------------------------
Eval num_timesteps=18000, episode_reward=-6298187.52 +/- 42185.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1161289 |
|    mean velocity x | 4.75       |
|    mean velocity y | 5.11       |
|    mean velocity z | 25.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.3e+06   |
| time/              |            |
|    total_timesteps | 18000      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 9     |
|    time_elapsed    | 750   |
|    total_timesteps | 18432 |
------------------------------
Eval num_timesteps=18500, episode_reward=-6326236.64 +/- 35326.45
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.011482206 |
|    mean velocity x      | -6.39        |
|    mean velocity y      | -6.04        |
|    mean velocity z      | 25.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.33e+06    |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.003858055  |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.94e+08     |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00215     |
|    std                  | 1            |
|    value_loss           | 4.2e+08      |
------------------------------------------
Eval num_timesteps=19000, episode_reward=-6338223.44 +/- 34058.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13911022 |
|    mean velocity x | 0.797       |
|    mean velocity y | 1.28        |
|    mean velocity z | 25.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.34e+06   |
| time/              |             |
|    total_timesteps | 19000       |
------------------------------------
Eval num_timesteps=19500, episode_reward=-6343764.97 +/- 29238.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0538956 |
|    mean velocity x | -1.94      |
|    mean velocity y | -1.55      |
|    mean velocity z | 23.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.34e+06  |
| time/              |            |
|    total_timesteps | 19500      |
-----------------------------------
Eval num_timesteps=20000, episode_reward=-6351481.49 +/- 45264.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14684956 |
|    mean velocity x | 4.69        |
|    mean velocity y | 4.96        |
|    mean velocity z | 26.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 20000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 10    |
|    time_elapsed    | 834   |
|    total_timesteps | 20480 |
------------------------------
Eval num_timesteps=20500, episode_reward=-6381503.38 +/- 69490.47
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2271643   |
|    mean velocity x      | 1.26         |
|    mean velocity y      | 1.08         |
|    mean velocity z      | 24.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.38e+06    |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0059216125 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+08     |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00508     |
|    std                  | 0.998        |
|    value_loss           | 4.68e+08     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-6339669.05 +/- 40955.14
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.039948344 |
|    mean velocity x | -2.6         |
|    mean velocity y | -2.68        |
|    mean velocity z | 23.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.34e+06    |
| time/              |              |
|    total_timesteps | 21000        |
-------------------------------------
Eval num_timesteps=21500, episode_reward=-6311524.06 +/- 45686.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0655823 |
|    mean velocity x | -0.412     |
|    mean velocity y | -0.466     |
|    mean velocity z | 24.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.31e+06  |
| time/              |            |
|    total_timesteps | 21500      |
-----------------------------------
Eval num_timesteps=22000, episode_reward=-6319085.36 +/- 35595.13
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.080259345 |
|    mean velocity x | 3.06         |
|    mean velocity y | 3.67         |
|    mean velocity z | 26           |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.32e+06    |
| time/              |              |
|    total_timesteps | 22000        |
-------------------------------------
Eval num_timesteps=22500, episode_reward=-6344808.45 +/- 19401.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10754907 |
|    mean velocity x | 3.94        |
|    mean velocity y | 3.66        |
|    mean velocity z | 24.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.34e+06   |
| time/              |             |
|    total_timesteps | 22500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 11    |
|    time_elapsed    | 936   |
|    total_timesteps | 22528 |
------------------------------
Eval num_timesteps=23000, episode_reward=-6365129.80 +/- 56266.06
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.071201526 |
|    mean velocity x      | 1.1         |
|    mean velocity y      | -0.00222    |
|    mean velocity z      | 23.9        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.37e+06   |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.004132258 |
|    clip_fraction        | 0.00894     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.96e+08    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00296    |
|    std                  | 0.998       |
|    value_loss           | 4.2e+08     |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-6369047.43 +/- 42045.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.049575042 |
|    mean velocity x | -0.0195     |
|    mean velocity y | -0.572      |
|    mean velocity z | 23.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.37e+06   |
| time/              |             |
|    total_timesteps | 23500       |
------------------------------------
Eval num_timesteps=24000, episode_reward=-6345509.65 +/- 48415.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10986929 |
|    mean velocity x | 5.36        |
|    mean velocity y | 6.13        |
|    mean velocity z | 25.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 24000       |
------------------------------------
Eval num_timesteps=24500, episode_reward=-6363895.25 +/- 21215.81
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.026247617 |
|    mean velocity x | -2.3         |
|    mean velocity y | -2.71        |
|    mean velocity z | 25.1         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.36e+06    |
| time/              |              |
|    total_timesteps | 24500        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 12    |
|    time_elapsed    | 1018  |
|    total_timesteps | 24576 |
------------------------------
Eval num_timesteps=25000, episode_reward=-6382561.09 +/- 30345.85
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.039348155 |
|    mean velocity x      | -0.351       |
|    mean velocity y      | 0.0938       |
|    mean velocity z      | 23.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.38e+06    |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0034194358 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 7.87e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+08     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00322     |
|    std                  | 0.997        |
|    value_loss           | 3.92e+08     |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-6349306.37 +/- 43286.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.07228668 |
|    mean velocity x | -2.46      |
|    mean velocity y | -2.91      |
|    mean velocity z | 22.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.35e+06  |
| time/              |            |
|    total_timesteps | 25500      |
-----------------------------------
Eval num_timesteps=26000, episode_reward=-6397934.53 +/- 21604.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.041515842 |
|    mean velocity x | 0.992       |
|    mean velocity y | 1.61        |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.4e+06    |
| time/              |             |
|    total_timesteps | 26000       |
------------------------------------
Eval num_timesteps=26500, episode_reward=-6392486.62 +/- 64452.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.05107827 |
|    mean velocity x | 1.68        |
|    mean velocity y | 1.3         |
|    mean velocity z | 24.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.39e+06   |
| time/              |             |
|    total_timesteps | 26500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 13    |
|    time_elapsed    | 1101  |
|    total_timesteps | 26624 |
------------------------------
Eval num_timesteps=27000, episode_reward=-6402112.29 +/- 51737.60
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.020883067  |
|    mean velocity x      | 1.08         |
|    mean velocity y      | 1.14         |
|    mean velocity z      | 26.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.4e+06     |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0026631334 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.02e+08     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.999        |
|    value_loss           | 4.44e+08     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-6351975.73 +/- 54024.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06341221 |
|    mean velocity x | 1.47        |
|    mean velocity y | 1.96        |
|    mean velocity z | 25.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 27500       |
------------------------------------
Eval num_timesteps=28000, episode_reward=-6306553.40 +/- 67212.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.037183188 |
|    mean velocity x | 0.0986      |
|    mean velocity y | -0.652      |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.31e+06   |
| time/              |             |
|    total_timesteps | 28000       |
------------------------------------
Eval num_timesteps=28500, episode_reward=-6333962.56 +/- 47400.99
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.048938505 |
|    mean velocity x | 5.44         |
|    mean velocity y | 4.71         |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.33e+06    |
| time/              |              |
|    total_timesteps | 28500        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 14    |
|    time_elapsed    | 1183  |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=29000, episode_reward=-6384657.48 +/- 40339.72
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.030069971 |
|    mean velocity x      | 2.28         |
|    mean velocity y      | 2.69         |
|    mean velocity z      | 25           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.38e+06    |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0038084816 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.000106     |
|    learning_rate        | 0.001        |
|    loss                 | 2.06e+08     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00249     |
|    std                  | 0.998        |
|    value_loss           | 4.2e+08      |
------------------------------------------
Eval num_timesteps=29500, episode_reward=-6328546.84 +/- 60727.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.12041322 |
|    mean velocity x | -4.77      |
|    mean velocity y | -5.3       |
|    mean velocity z | 25.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.33e+06  |
| time/              |            |
|    total_timesteps | 29500      |
-----------------------------------
Eval num_timesteps=30000, episode_reward=-6359528.20 +/- 31261.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.13870673 |
|    mean velocity x | -3.28      |
|    mean velocity y | -3.88      |
|    mean velocity z | 23.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.36e+06  |
| time/              |            |
|    total_timesteps | 30000      |
-----------------------------------
Eval num_timesteps=30500, episode_reward=-6336610.57 +/- 57807.13
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.020625137 |
|    mean velocity x | -0.471       |
|    mean velocity y | -0.131       |
|    mean velocity z | 24.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.34e+06    |
| time/              |              |
|    total_timesteps | 30500        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 15    |
|    time_elapsed    | 1265  |
|    total_timesteps | 30720 |
------------------------------
Eval num_timesteps=31000, episode_reward=-6333263.67 +/- 19729.68
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.017251637 |
|    mean velocity x      | 0.556        |
|    mean velocity y      | 0.56         |
|    mean velocity z      | 25           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.33e+06    |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0008714299 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.1e+08      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00119     |
|    std                  | 0.998        |
|    value_loss           | 4.13e+08     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-6258495.12 +/- 41770.65
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.015193561 |
|    mean velocity x | -0.826       |
|    mean velocity y | -0.717       |
|    mean velocity z | 25.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.26e+06    |
| time/              |              |
|    total_timesteps | 31500        |
-------------------------------------
Eval num_timesteps=32000, episode_reward=-6323544.60 +/- 78292.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.037925363 |
|    mean velocity x | -3.11       |
|    mean velocity y | -3.27       |
|    mean velocity z | 25.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.32e+06   |
| time/              |             |
|    total_timesteps | 32000       |
------------------------------------
Eval num_timesteps=32500, episode_reward=-6304934.11 +/- 51549.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.086005524 |
|    mean velocity x | -4.99       |
|    mean velocity y | -5.62       |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.3e+06    |
| time/              |             |
|    total_timesteps | 32500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 16    |
|    time_elapsed    | 1347  |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=33000, episode_reward=-6273478.72 +/- 39431.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.013197257  |
|    mean velocity x      | -1.97        |
|    mean velocity y      | -2           |
|    mean velocity z      | 25.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.27e+06    |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0037862337 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.58e+08     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.997        |
|    value_loss           | 4.7e+08      |
------------------------------------------
Eval num_timesteps=33500, episode_reward=-6318454.61 +/- 53522.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.059955087 |
|    mean velocity x | -1.02       |
|    mean velocity y | -1.29       |
|    mean velocity z | 23          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.32e+06   |
| time/              |             |
|    total_timesteps | 33500       |
------------------------------------
Eval num_timesteps=34000, episode_reward=-6320513.44 +/- 30005.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.06240009 |
|    mean velocity x | -5.04      |
|    mean velocity y | -5.92      |
|    mean velocity z | 24.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.32e+06  |
| time/              |            |
|    total_timesteps | 34000      |
-----------------------------------
Eval num_timesteps=34500, episode_reward=-6277963.61 +/- 38968.36
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0059803696 |
|    mean velocity x | 1.31         |
|    mean velocity y | 0.824        |
|    mean velocity z | 24.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.28e+06    |
| time/              |              |
|    total_timesteps | 34500        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 17    |
|    time_elapsed    | 1430  |
|    total_timesteps | 34816 |
------------------------------
Eval num_timesteps=35000, episode_reward=-6278221.74 +/- 20534.13
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.07592981 |
|    mean velocity x      | 2.84        |
|    mean velocity y      | 3.28        |
|    mean velocity z      | 25.2        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.28e+06   |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.002737537 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 2.1e+08     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00204    |
|    std                  | 0.996       |
|    value_loss           | 4.5e+08     |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-6281796.78 +/- 80224.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.050586708 |
|    mean velocity x | -0.616      |
|    mean velocity y | -0.571      |
|    mean velocity z | 25.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+06   |
| time/              |             |
|    total_timesteps | 35500       |
------------------------------------
Eval num_timesteps=36000, episode_reward=-6250750.44 +/- 64660.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06259953 |
|    mean velocity x | 1.65        |
|    mean velocity y | 2.07        |
|    mean velocity z | 25.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+06   |
| time/              |             |
|    total_timesteps | 36000       |
------------------------------------
Eval num_timesteps=36500, episode_reward=-6287538.18 +/- 51240.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.044654325 |
|    mean velocity x | -2.95       |
|    mean velocity y | -2.71       |
|    mean velocity z | 25.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.29e+06   |
| time/              |             |
|    total_timesteps | 36500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 18    |
|    time_elapsed    | 1512  |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=37000, episode_reward=-6204902.87 +/- 54088.70
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14919394  |
|    mean velocity x      | 1.79         |
|    mean velocity y      | 1.5          |
|    mean velocity z      | 22.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.2e+06     |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0032820648 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+08     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00335     |
|    std                  | 0.997        |
|    value_loss           | 4.5e+08      |
------------------------------------------
New best mean reward!
Eval num_timesteps=37500, episode_reward=-6215307.22 +/- 52849.47
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.030379055 |
|    mean velocity x | -0.342       |
|    mean velocity y | -0.27        |
|    mean velocity z | 23.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.22e+06    |
| time/              |              |
|    total_timesteps | 37500        |
-------------------------------------
Eval num_timesteps=38000, episode_reward=-6229711.17 +/- 32252.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10151531 |
|    mean velocity x | 0.127       |
|    mean velocity y | 0.81        |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+06   |
| time/              |             |
|    total_timesteps | 38000       |
------------------------------------
Eval num_timesteps=38500, episode_reward=-6229131.56 +/- 18415.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07040523 |
|    mean velocity x | -1.22       |
|    mean velocity y | -1.67       |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+06   |
| time/              |             |
|    total_timesteps | 38500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 19    |
|    time_elapsed    | 1594  |
|    total_timesteps | 38912 |
------------------------------
Eval num_timesteps=39000, episode_reward=-6225092.19 +/- 31981.05
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07306988  |
|    mean velocity x      | 0.387        |
|    mean velocity y      | 0.771        |
|    mean velocity z      | 24.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.23e+06    |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0011716348 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 6.44e-06     |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+08     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0015      |
|    std                  | 0.996        |
|    value_loss           | 4.05e+08     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-6187462.06 +/- 56199.36
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.055434827 |
|    mean velocity x | -1.41        |
|    mean velocity y | -1.62        |
|    mean velocity z | 22.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.19e+06    |
| time/              |              |
|    total_timesteps | 39500        |
-------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=-6214272.92 +/- 66028.46
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.017259095 |
|    mean velocity x | -0.421       |
|    mean velocity y | -0.328       |
|    mean velocity z | 24.5         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.21e+06    |
| time/              |              |
|    total_timesteps | 40000        |
-------------------------------------
Eval num_timesteps=40500, episode_reward=-6246178.06 +/- 32460.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06026003 |
|    mean velocity x | -5.04       |
|    mean velocity y | -4.73       |
|    mean velocity z | 25.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+06   |
| time/              |             |
|    total_timesteps | 40500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 20    |
|    time_elapsed    | 1677  |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=41000, episode_reward=-6286134.92 +/- 49204.24
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0003701172 |
|    mean velocity x      | -4.06         |
|    mean velocity y      | -3.58         |
|    mean velocity z      | 23.5          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.29e+06     |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 0.003713647   |
|    clip_fraction        | 0.00474       |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.24         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 2.44e+08      |
|    n_updates            | 200           |
|    policy_gradient_loss | -0.00249      |
|    std                  | 0.993         |
|    value_loss           | 4.22e+08      |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=-6273078.66 +/- 36062.57
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.030159142 |
|    mean velocity x | -5.36        |
|    mean velocity y | -5.47        |
|    mean velocity z | 25.5         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.27e+06    |
| time/              |              |
|    total_timesteps | 41500        |
-------------------------------------
Eval num_timesteps=42000, episode_reward=-6222059.69 +/- 61105.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18121909 |
|    mean velocity x | -0.148      |
|    mean velocity y | 0.637       |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.22e+06   |
| time/              |             |
|    total_timesteps | 42000       |
------------------------------------
Eval num_timesteps=42500, episode_reward=-6228717.06 +/- 113174.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17457269 |
|    mean velocity x | 0.245       |
|    mean velocity y | 0.854       |
|    mean velocity z | 23.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+06   |
| time/              |             |
|    total_timesteps | 42500       |
------------------------------------
Eval num_timesteps=43000, episode_reward=-6221568.14 +/- 70595.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06178824 |
|    mean velocity x | -4.49       |
|    mean velocity y | -4.01       |
|    mean velocity z | 24.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.22e+06   |
| time/              |             |
|    total_timesteps | 43000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 21    |
|    time_elapsed    | 1778  |
|    total_timesteps | 43008 |
------------------------------
Eval num_timesteps=43500, episode_reward=-6218064.74 +/- 29713.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.013572513 |
|    mean velocity x      | -2.34        |
|    mean velocity y      | -2.76        |
|    mean velocity z      | 23.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.22e+06    |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0026078536 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+08     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.993        |
|    value_loss           | 4.24e+08     |
------------------------------------------
Eval num_timesteps=44000, episode_reward=-6195569.24 +/- 37576.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07109535 |
|    mean velocity x | 2.06        |
|    mean velocity y | 2.36        |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.2e+06    |
| time/              |             |
|    total_timesteps | 44000       |
------------------------------------
Eval num_timesteps=44500, episode_reward=-6198901.81 +/- 29098.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.10918304 |
|    mean velocity x | -2.73      |
|    mean velocity y | -3.08      |
|    mean velocity z | 24.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.2e+06   |
| time/              |            |
|    total_timesteps | 44500      |
-----------------------------------
Eval num_timesteps=45000, episode_reward=-6246092.65 +/- 14788.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.03868309 |
|    mean velocity x | -2.18       |
|    mean velocity y | -2.09       |
|    mean velocity z | 23.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+06   |
| time/              |             |
|    total_timesteps | 45000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 22    |
|    time_elapsed    | 1860  |
|    total_timesteps | 45056 |
------------------------------
Eval num_timesteps=45500, episode_reward=-6172285.72 +/- 67926.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.009655117  |
|    mean velocity x      | -4.52        |
|    mean velocity y      | -4.42        |
|    mean velocity z      | 24.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.17e+06    |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0034631062 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+08     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00278     |
|    std                  | 0.991        |
|    value_loss           | 4.24e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=46000, episode_reward=-6197928.36 +/- 82352.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07584543 |
|    mean velocity x | -2.85       |
|    mean velocity y | -2.37       |
|    mean velocity z | 24.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.2e+06    |
| time/              |             |
|    total_timesteps | 46000       |
------------------------------------
Eval num_timesteps=46500, episode_reward=-6211287.29 +/- 56440.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1272822 |
|    mean velocity x | 1.13       |
|    mean velocity y | 1.14       |
|    mean velocity z | 25.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.21e+06  |
| time/              |            |
|    total_timesteps | 46500      |
-----------------------------------
Eval num_timesteps=47000, episode_reward=-6212273.04 +/- 15646.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.02811578 |
|    mean velocity x | 1.19        |
|    mean velocity y | 0.885       |
|    mean velocity z | 24.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+06   |
| time/              |             |
|    total_timesteps | 47000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 23    |
|    time_elapsed    | 1942  |
|    total_timesteps | 47104 |
------------------------------
Eval num_timesteps=47500, episode_reward=-6235321.55 +/- 57642.92
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.17900313  |
|    mean velocity x      | 1.21         |
|    mean velocity y      | 1.07         |
|    mean velocity z      | 24.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.24e+06    |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 0.0013310119 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.54e+08     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000998    |
|    std                  | 0.991        |
|    value_loss           | 5.05e+08     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-6247891.27 +/- 33998.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12147729 |
|    mean velocity x | -1.51       |
|    mean velocity y | -0.606      |
|    mean velocity z | 24.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+06   |
| time/              |             |
|    total_timesteps | 48000       |
------------------------------------
Eval num_timesteps=48500, episode_reward=-6241356.42 +/- 15715.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17399023 |
|    mean velocity x | -1.61       |
|    mean velocity y | -0.608      |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.24e+06   |
| time/              |             |
|    total_timesteps | 48500       |
------------------------------------
Eval num_timesteps=49000, episode_reward=-6267562.07 +/- 38580.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21178167 |
|    mean velocity x | 0.00285     |
|    mean velocity y | 0.273       |
|    mean velocity z | 23.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.27e+06   |
| time/              |             |
|    total_timesteps | 49000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 24    |
|    time_elapsed    | 2025  |
|    total_timesteps | 49152 |
------------------------------
Eval num_timesteps=49500, episode_reward=-6186615.40 +/- 40744.08
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21846643  |
|    mean velocity x      | -0.573       |
|    mean velocity y      | -0.188       |
|    mean velocity z      | 24.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.19e+06    |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0037188984 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.29e+08     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00312     |
|    std                  | 0.988        |
|    value_loss           | 4.65e+08     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-6153164.44 +/- 67081.90
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.021037709 |
|    mean velocity x | -5.47        |
|    mean velocity y | -5.85        |
|    mean velocity z | 25.1         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.15e+06    |
| time/              |              |
|    total_timesteps | 50000        |
-------------------------------------
New best mean reward!
Eval num_timesteps=50500, episode_reward=-6209713.97 +/- 31700.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19036095 |
|    mean velocity x | -0.527      |
|    mean velocity y | 0.0462      |
|    mean velocity z | 24.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+06   |
| time/              |             |
|    total_timesteps | 50500       |
------------------------------------
Eval num_timesteps=51000, episode_reward=-6208730.82 +/- 30909.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09171996 |
|    mean velocity x | -3.6        |
|    mean velocity y | -3.36       |
|    mean velocity z | 23.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+06   |
| time/              |             |
|    total_timesteps | 51000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 25    |
|    time_elapsed    | 2107  |
|    total_timesteps | 51200 |
------------------------------
Eval num_timesteps=51500, episode_reward=-6142347.80 +/- 44734.16
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.04201523  |
|    mean velocity x      | -5.29       |
|    mean velocity y      | -5.53       |
|    mean velocity z      | 25.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.14e+06   |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.004341983 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 1.13e-06    |
|    learning_rate        | 0.001       |
|    loss                 | 2.52e+08    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0033     |
|    std                  | 0.988       |
|    value_loss           | 4.39e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=52000, episode_reward=-6122266.21 +/- 17920.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16406052 |
|    mean velocity x | 4.43        |
|    mean velocity y | 5.25        |
|    mean velocity z | 24          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.12e+06   |
| time/              |             |
|    total_timesteps | 52000       |
------------------------------------
New best mean reward!
Eval num_timesteps=52500, episode_reward=-6118723.01 +/- 44708.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0498833 |
|    mean velocity x | -0.981     |
|    mean velocity y | -0.976     |
|    mean velocity z | 24.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.12e+06  |
| time/              |            |
|    total_timesteps | 52500      |
-----------------------------------
New best mean reward!
Eval num_timesteps=53000, episode_reward=-6093288.44 +/- 104062.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12583168 |
|    mean velocity x | -0.00326    |
|    mean velocity y | 0.55        |
|    mean velocity z | 21.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.09e+06   |
| time/              |             |
|    total_timesteps | 53000       |
------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 26    |
|    time_elapsed    | 2189  |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=53500, episode_reward=-6097827.13 +/- 86480.49
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07465956  |
|    mean velocity x      | -0.397       |
|    mean velocity y      | -0.602       |
|    mean velocity z      | 22           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.1e+06     |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0031018564 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 1.25e-06     |
|    learning_rate        | 0.001        |
|    loss                 | 1.97e+08     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00235     |
|    std                  | 0.989        |
|    value_loss           | 4.44e+08     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-6135130.60 +/- 34736.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1698019 |
|    mean velocity x | 1.17       |
|    mean velocity y | 1.5        |
|    mean velocity z | 24.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.14e+06  |
| time/              |            |
|    total_timesteps | 54000      |
-----------------------------------
Eval num_timesteps=54500, episode_reward=-6119990.45 +/- 41968.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27751288 |
|    mean velocity x | 4.8         |
|    mean velocity y | 5.37        |
|    mean velocity z | 26          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.12e+06   |
| time/              |             |
|    total_timesteps | 54500       |
------------------------------------
Eval num_timesteps=55000, episode_reward=-6108919.93 +/- 54629.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15530951 |
|    mean velocity x | 0.317       |
|    mean velocity y | 0.24        |
|    mean velocity z | 23.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.11e+06   |
| time/              |             |
|    total_timesteps | 55000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 27    |
|    time_elapsed    | 2272  |
|    total_timesteps | 55296 |
------------------------------
Eval num_timesteps=55500, episode_reward=-6109873.86 +/- 61773.91
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.11268275  |
|    mean velocity x      | -0.77        |
|    mean velocity y      | -0.903       |
|    mean velocity z      | 24.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.11e+06    |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0035928094 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 2.23e+08     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.988        |
|    value_loss           | 4.71e+08     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-6124342.45 +/- 32355.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21065076 |
|    mean velocity x | -0.425      |
|    mean velocity y | 0.107       |
|    mean velocity z | 23.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.12e+06   |
| time/              |             |
|    total_timesteps | 56000       |
------------------------------------
Eval num_timesteps=56500, episode_reward=-6108618.24 +/- 54621.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21222259 |
|    mean velocity x | 1.5         |
|    mean velocity y | 1.92        |
|    mean velocity z | 24.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.11e+06   |
| time/              |             |
|    total_timesteps | 56500       |
------------------------------------
Eval num_timesteps=57000, episode_reward=-6134391.53 +/- 20187.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29073486 |
|    mean velocity x | 3.5         |
|    mean velocity y | 4.52        |
|    mean velocity z | 25.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.13e+06   |
| time/              |             |
|    total_timesteps | 57000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 28    |
|    time_elapsed    | 2354  |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=57500, episode_reward=-6162054.56 +/- 56427.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3117041   |
|    mean velocity x      | 5.28         |
|    mean velocity y      | 6.13         |
|    mean velocity z      | 25.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.16e+06    |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0047742333 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+08     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00361     |
|    std                  | 0.985        |
|    value_loss           | 4.57e+08     |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-6183962.37 +/- 21098.85
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2570552 |
|    mean velocity x | 2.84       |
|    mean velocity y | 3.2        |
|    mean velocity z | 23.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.18e+06  |
| time/              |            |
|    total_timesteps | 58000      |
-----------------------------------
Eval num_timesteps=58500, episode_reward=-6139691.64 +/- 94947.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14564122 |
|    mean velocity x | 2.47        |
|    mean velocity y | 2.67        |
|    mean velocity z | 23.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.14e+06   |
| time/              |             |
|    total_timesteps | 58500       |
------------------------------------
Eval num_timesteps=59000, episode_reward=-6176587.59 +/- 44533.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19564272 |
|    mean velocity x | 1.59        |
|    mean velocity y | 2.6         |
|    mean velocity z | 24.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+06   |
| time/              |             |
|    total_timesteps | 59000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 29    |
|    time_elapsed    | 2436  |
|    total_timesteps | 59392 |
------------------------------
Eval num_timesteps=59500, episode_reward=-6155602.79 +/- 80090.34
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.09090764 |
|    mean velocity x      | -1.25       |
|    mean velocity y      | -1.27       |
|    mean velocity z      | 24.1        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.16e+06   |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.001553365 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 2.07e+08    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00132    |
|    std                  | 0.988       |
|    value_loss           | 4.07e+08    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-6170514.33 +/- 31226.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21874894 |
|    mean velocity x | 2.22        |
|    mean velocity y | 2.24        |
|    mean velocity z | 24.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+06   |
| time/              |             |
|    total_timesteps | 60000       |
------------------------------------
Eval num_timesteps=60500, episode_reward=-6160355.67 +/- 72049.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16874823 |
|    mean velocity x | -0.961      |
|    mean velocity y | -0.279      |
|    mean velocity z | 24.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.16e+06   |
| time/              |             |
|    total_timesteps | 60500       |
------------------------------------
Eval num_timesteps=61000, episode_reward=-6185104.49 +/- 72375.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18586533 |
|    mean velocity x | 3.21        |
|    mean velocity y | 3.48        |
|    mean velocity z | 24          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+06   |
| time/              |             |
|    total_timesteps | 61000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 30    |
|    time_elapsed    | 2518  |
|    total_timesteps | 61440 |
------------------------------
Eval num_timesteps=61500, episode_reward=-6141095.53 +/- 85321.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.22962746  |
|    mean velocity x      | -0.245       |
|    mean velocity y      | 0.389        |
|    mean velocity z      | 24.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.14e+06    |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0029431973 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.51e+08     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.987        |
|    value_loss           | 4.54e+08     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-6155903.23 +/- 63293.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25326836 |
|    mean velocity x | 1.26        |
|    mean velocity y | 2.27        |
|    mean velocity z | 24.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.16e+06   |
| time/              |             |
|    total_timesteps | 62000       |
------------------------------------
Eval num_timesteps=62500, episode_reward=-6171366.90 +/- 33020.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14125203 |
|    mean velocity x | -4.45       |
|    mean velocity y | -4.25       |
|    mean velocity z | 23          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+06   |
| time/              |             |
|    total_timesteps | 62500       |
------------------------------------
Eval num_timesteps=63000, episode_reward=-6171671.97 +/- 35559.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3446781 |
|    mean velocity x | 2.94       |
|    mean velocity y | 4.05       |
|    mean velocity z | 22.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.17e+06  |
| time/              |            |
|    total_timesteps | 63000      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 31    |
|    time_elapsed    | 2600  |
|    total_timesteps | 63488 |
------------------------------
Eval num_timesteps=63500, episode_reward=-6094612.44 +/- 54325.02
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.23212811 |
|    mean velocity x      | 4.43        |
|    mean velocity y      | 4.86        |
|    mean velocity z      | 24.4        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.09e+06   |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.004382769 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.62e+08    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00378    |
|    std                  | 0.983       |
|    value_loss           | 4e+08       |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=-6095114.10 +/- 17351.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2534575 |
|    mean velocity x | 0.793      |
|    mean velocity y | 0.822      |
|    mean velocity z | 23.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.1e+06   |
| time/              |            |
|    total_timesteps | 64000      |
-----------------------------------
Eval num_timesteps=64500, episode_reward=-6096607.26 +/- 39692.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30600056 |
|    mean velocity x | 3.79        |
|    mean velocity y | 4.31        |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.1e+06    |
| time/              |             |
|    total_timesteps | 64500       |
------------------------------------
Eval num_timesteps=65000, episode_reward=-6021703.97 +/- 49411.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24188983 |
|    mean velocity x | -4.91       |
|    mean velocity y | -4.89       |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.02e+06   |
| time/              |             |
|    total_timesteps | 65000       |
------------------------------------
New best mean reward!
Eval num_timesteps=65500, episode_reward=-6128204.54 +/- 52542.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23941652 |
|    mean velocity x | -0.638      |
|    mean velocity y | -0.615      |
|    mean velocity z | 23.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.13e+06   |
| time/              |             |
|    total_timesteps | 65500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 32    |
|    time_elapsed    | 2702  |
|    total_timesteps | 65536 |
------------------------------
Eval num_timesteps=66000, episode_reward=-6061157.25 +/- 46043.52
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.29551554 |
|    mean velocity x      | -0.611      |
|    mean velocity y      | 0.671       |
|    mean velocity z      | 21.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.06e+06   |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.00201715  |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.77e+08    |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00202    |
|    std                  | 0.986       |
|    value_loss           | 3.95e+08    |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=-5986823.59 +/- 31600.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24097079 |
|    mean velocity x | 1.63        |
|    mean velocity y | 1.83        |
|    mean velocity z | 23          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.99e+06   |
| time/              |             |
|    total_timesteps | 66500       |
------------------------------------
New best mean reward!
Eval num_timesteps=67000, episode_reward=-6001723.45 +/- 55048.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35034087 |
|    mean velocity x | 1.95        |
|    mean velocity y | 3           |
|    mean velocity z | 23.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6e+06      |
| time/              |             |
|    total_timesteps | 67000       |
------------------------------------
Eval num_timesteps=67500, episode_reward=-6009992.36 +/- 69299.07
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.122937664 |
|    mean velocity x | -5.71        |
|    mean velocity y | -5.52        |
|    mean velocity z | 25.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.01e+06    |
| time/              |              |
|    total_timesteps | 67500        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 33    |
|    time_elapsed    | 2785  |
|    total_timesteps | 67584 |
------------------------------
Eval num_timesteps=68000, episode_reward=-5987913.47 +/- 69703.63
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29997143  |
|    mean velocity x      | 1.79         |
|    mean velocity y      | 2.06         |
|    mean velocity z      | 24           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.99e+06    |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0019405865 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.55e+08     |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00196     |
|    std                  | 0.984        |
|    value_loss           | 4.46e+08     |
------------------------------------------
Eval num_timesteps=68500, episode_reward=-5963300.62 +/- 39937.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29320258 |
|    mean velocity x | 0.806       |
|    mean velocity y | 1.64        |
|    mean velocity z | 24.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.96e+06   |
| time/              |             |
|    total_timesteps | 68500       |
------------------------------------
New best mean reward!
Eval num_timesteps=69000, episode_reward=-5964327.24 +/- 38410.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17318149 |
|    mean velocity x | -3.66       |
|    mean velocity y | -3.47       |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.96e+06   |
| time/              |             |
|    total_timesteps | 69000       |
------------------------------------
Eval num_timesteps=69500, episode_reward=-5950311.97 +/- 57129.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22013377 |
|    mean velocity x | -1.3        |
|    mean velocity y | -1.12       |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.95e+06   |
| time/              |             |
|    total_timesteps | 69500       |
------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 34    |
|    time_elapsed    | 2867  |
|    total_timesteps | 69632 |
------------------------------
Eval num_timesteps=70000, episode_reward=-5894433.30 +/- 71402.81
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.014377387 |
|    mean velocity x      | -7.49       |
|    mean velocity y      | -7.77       |
|    mean velocity z      | 25.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.89e+06   |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.003034841 |
|    clip_fraction        | 0.00581     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 2.15e+08    |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00201    |
|    std                  | 0.985       |
|    value_loss           | 3.95e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=70500, episode_reward=-5982969.62 +/- 41555.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22331369 |
|    mean velocity x | -0.71       |
|    mean velocity y | -0.0979     |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.98e+06   |
| time/              |             |
|    total_timesteps | 70500       |
------------------------------------
Eval num_timesteps=71000, episode_reward=-5981083.73 +/- 13968.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36379853 |
|    mean velocity x | 4.5         |
|    mean velocity y | 5.51        |
|    mean velocity z | 24.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.98e+06   |
| time/              |             |
|    total_timesteps | 71000       |
------------------------------------
Eval num_timesteps=71500, episode_reward=-5986595.17 +/- 56242.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18946053 |
|    mean velocity x | -1.74       |
|    mean velocity y | -1.44       |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.99e+06   |
| time/              |             |
|    total_timesteps | 71500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 35    |
|    time_elapsed    | 2949  |
|    total_timesteps | 71680 |
------------------------------
Eval num_timesteps=72000, episode_reward=-5911819.24 +/- 47634.40
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.09317152  |
|    mean velocity x      | -4.83        |
|    mean velocity y      | -4.76        |
|    mean velocity z      | 24.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.91e+06    |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0045212787 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.8e+08      |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00324     |
|    std                  | 0.983        |
|    value_loss           | 3.71e+08     |
------------------------------------------
Eval num_timesteps=72500, episode_reward=-5908751.38 +/- 37314.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33961406 |
|    mean velocity x | 3.51        |
|    mean velocity y | 4.38        |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.91e+06   |
| time/              |             |
|    total_timesteps | 72500       |
------------------------------------
Eval num_timesteps=73000, episode_reward=-5813265.18 +/- 88237.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11089475 |
|    mean velocity x | -5.39       |
|    mean velocity y | -5.8        |
|    mean velocity z | 23.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.81e+06   |
| time/              |             |
|    total_timesteps | 73000       |
------------------------------------
New best mean reward!
Eval num_timesteps=73500, episode_reward=-5919666.10 +/- 10799.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4065435 |
|    mean velocity x | 1.76       |
|    mean velocity y | 2.73       |
|    mean velocity z | 23.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.92e+06  |
| time/              |            |
|    total_timesteps | 73500      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 36    |
|    time_elapsed    | 3031  |
|    total_timesteps | 73728 |
------------------------------
Eval num_timesteps=74000, episode_reward=-5853187.59 +/- 71528.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35159168  |
|    mean velocity x      | 2            |
|    mean velocity y      | 3            |
|    mean velocity z      | 23.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.85e+06    |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0016590793 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+08     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.98         |
|    value_loss           | 3.94e+08     |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-5819812.07 +/- 33743.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25188696 |
|    mean velocity x | -2.24       |
|    mean velocity y | -1.55       |
|    mean velocity z | 23.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.82e+06   |
| time/              |             |
|    total_timesteps | 74500       |
------------------------------------
Eval num_timesteps=75000, episode_reward=-5917036.00 +/- 10562.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2939547 |
|    mean velocity x | 4.09       |
|    mean velocity y | 5.21       |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.92e+06  |
| time/              |            |
|    total_timesteps | 75000      |
-----------------------------------
Eval num_timesteps=75500, episode_reward=-5910996.85 +/- 46073.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3342443 |
|    mean velocity x | -0.195     |
|    mean velocity y | 1.15       |
|    mean velocity z | 22.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.91e+06  |
| time/              |            |
|    total_timesteps | 75500      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 37    |
|    time_elapsed    | 3113  |
|    total_timesteps | 75776 |
------------------------------
Eval num_timesteps=76000, episode_reward=-5818681.96 +/- 146382.17
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.22927031  |
|    mean velocity x      | -2.04        |
|    mean velocity y      | -1.21        |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.82e+06    |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0031721531 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 2.39e+08     |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00231     |
|    std                  | 0.979        |
|    value_loss           | 3.96e+08     |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-5911663.01 +/- 60633.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17497306 |
|    mean velocity x | 1.26        |
|    mean velocity y | 1.72        |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.91e+06   |
| time/              |             |
|    total_timesteps | 76500       |
------------------------------------
Eval num_timesteps=77000, episode_reward=-5808120.67 +/- 91715.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18207225 |
|    mean velocity x | -3.59       |
|    mean velocity y | -3.38       |
|    mean velocity z | 22.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.81e+06   |
| time/              |             |
|    total_timesteps | 77000       |
------------------------------------
New best mean reward!
Eval num_timesteps=77500, episode_reward=-5818534.33 +/- 170578.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33868247 |
|    mean velocity x | 2.08        |
|    mean velocity y | 2.98        |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.82e+06   |
| time/              |             |
|    total_timesteps | 77500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 38    |
|    time_elapsed    | 3196  |
|    total_timesteps | 77824 |
------------------------------
Eval num_timesteps=78000, episode_reward=-5776469.96 +/- 64747.62
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.24219202 |
|    mean velocity x      | -0.968      |
|    mean velocity y      | -0.577      |
|    mean velocity z      | 21.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.78e+06   |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.001978261 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.7e+08     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00237    |
|    std                  | 0.981       |
|    value_loss           | 3.51e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=78500, episode_reward=-5848408.93 +/- 37404.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20898712 |
|    mean velocity x | -2.34       |
|    mean velocity y | -1.93       |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.85e+06   |
| time/              |             |
|    total_timesteps | 78500       |
------------------------------------
Eval num_timesteps=79000, episode_reward=-5799466.62 +/- 65668.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14280385 |
|    mean velocity x | -4.67       |
|    mean velocity y | -3.79       |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.8e+06    |
| time/              |             |
|    total_timesteps | 79000       |
------------------------------------
Eval num_timesteps=79500, episode_reward=-5743789.26 +/- 76573.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2635296 |
|    mean velocity x | -0.249     |
|    mean velocity y | 0.678      |
|    mean velocity z | 23.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.74e+06  |
| time/              |            |
|    total_timesteps | 79500      |
-----------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 39    |
|    time_elapsed    | 3278  |
|    total_timesteps | 79872 |
------------------------------
Eval num_timesteps=80000, episode_reward=-5669689.47 +/- 133620.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35337248   |
|    mean velocity x      | 0.836         |
|    mean velocity y      | 2.15          |
|    mean velocity z      | 24.3          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.67e+06     |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 0.00057509704 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.2          |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+08      |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.000654     |
|    std                  | 0.98          |
|    value_loss           | 4.3e+08       |
-------------------------------------------
New best mean reward!
Eval num_timesteps=80500, episode_reward=-5751702.02 +/- 87655.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25973034 |
|    mean velocity x | -1.37       |
|    mean velocity y | -0.84       |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.75e+06   |
| time/              |             |
|    total_timesteps | 80500       |
------------------------------------
Eval num_timesteps=81000, episode_reward=-5720343.38 +/- 154528.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25685605 |
|    mean velocity x | 0.392       |
|    mean velocity y | 1.68        |
|    mean velocity z | 22.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.72e+06   |
| time/              |             |
|    total_timesteps | 81000       |
------------------------------------
Eval num_timesteps=81500, episode_reward=-5728837.26 +/- 114804.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2433259 |
|    mean velocity x | -0.512     |
|    mean velocity y | 0.578      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.73e+06  |
| time/              |            |
|    total_timesteps | 81500      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 40    |
|    time_elapsed    | 3360  |
|    total_timesteps | 81920 |
------------------------------
Eval num_timesteps=82000, episode_reward=-5551997.53 +/- 146931.97
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.1721915  |
|    mean velocity x      | -2.91       |
|    mean velocity y      | -2.98       |
|    mean velocity z      | 22.1        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.55e+06   |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.004022084 |
|    clip_fraction        | 0.0083      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.95e+08    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00288    |
|    std                  | 0.979       |
|    value_loss           | 3.77e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=82500, episode_reward=-5594350.78 +/- 179024.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21740128 |
|    mean velocity x | -1.78       |
|    mean velocity y | -0.981      |
|    mean velocity z | 21          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.59e+06   |
| time/              |             |
|    total_timesteps | 82500       |
------------------------------------
Eval num_timesteps=83000, episode_reward=-5675954.16 +/- 103269.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10367766 |
|    mean velocity x | -2.29       |
|    mean velocity y | -2.44       |
|    mean velocity z | 21          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.68e+06   |
| time/              |             |
|    total_timesteps | 83000       |
------------------------------------
Eval num_timesteps=83500, episode_reward=-5568933.31 +/- 142256.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38214943 |
|    mean velocity x | 2.56        |
|    mean velocity y | 3.47        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.57e+06   |
| time/              |             |
|    total_timesteps | 83500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 41    |
|    time_elapsed    | 3442  |
|    total_timesteps | 83968 |
------------------------------
Eval num_timesteps=84000, episode_reward=-5695933.39 +/- 56189.73
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.12231141 |
|    mean velocity x      | -2.84       |
|    mean velocity y      | -2.51       |
|    mean velocity z      | 19.1        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.7e+06    |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.002151608 |
|    clip_fraction        | 0.000928    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.6e+08     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00154    |
|    std                  | 0.979       |
|    value_loss           | 3.17e+08    |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=-5767711.83 +/- 47274.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18141714 |
|    mean velocity x | -1.9        |
|    mean velocity y | -1.27       |
|    mean velocity z | 23          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.77e+06   |
| time/              |             |
|    total_timesteps | 84500       |
------------------------------------
Eval num_timesteps=85000, episode_reward=-5719017.56 +/- 75915.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46713373 |
|    mean velocity x | 6.23        |
|    mean velocity y | 7.28        |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.72e+06   |
| time/              |             |
|    total_timesteps | 85000       |
------------------------------------
Eval num_timesteps=85500, episode_reward=-5652674.08 +/- 70255.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35509962 |
|    mean velocity x | 2.01        |
|    mean velocity y | 3.42        |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.65e+06   |
| time/              |             |
|    total_timesteps | 85500       |
------------------------------------
Eval num_timesteps=86000, episode_reward=-5644401.05 +/- 169167.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49817646 |
|    mean velocity x | 7.28        |
|    mean velocity y | 8.98        |
|    mean velocity z | 25.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.64e+06   |
| time/              |             |
|    total_timesteps | 86000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 42    |
|    time_elapsed    | 3544  |
|    total_timesteps | 86016 |
------------------------------
Eval num_timesteps=86500, episode_reward=-5649118.86 +/- 77553.50
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35435063  |
|    mean velocity x      | 2.64         |
|    mean velocity y      | 3.8          |
|    mean velocity z      | 24.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.65e+06    |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0011488895 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+08     |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000996    |
|    std                  | 0.98         |
|    value_loss           | 4.13e+08     |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-5688359.30 +/- 37698.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27729177 |
|    mean velocity x | 2.36        |
|    mean velocity y | 3.16        |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.69e+06   |
| time/              |             |
|    total_timesteps | 87000       |
------------------------------------
Eval num_timesteps=87500, episode_reward=-5604375.67 +/- 150486.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09887525 |
|    mean velocity x | -3.1        |
|    mean velocity y | -2.97       |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.6e+06    |
| time/              |             |
|    total_timesteps | 87500       |
------------------------------------
Eval num_timesteps=88000, episode_reward=-5654870.33 +/- 98453.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25670427 |
|    mean velocity x | -0.861      |
|    mean velocity y | -0.733      |
|    mean velocity z | 23          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.65e+06   |
| time/              |             |
|    total_timesteps | 88000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 43    |
|    time_elapsed    | 3627  |
|    total_timesteps | 88064 |
------------------------------
Eval num_timesteps=88500, episode_reward=-5551793.22 +/- 165739.10
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.10532387  |
|    mean velocity x      | -4.17        |
|    mean velocity y      | -3.56        |
|    mean velocity z      | 22.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.55e+06    |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0012069019 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.01e+08     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00181     |
|    std                  | 0.982        |
|    value_loss           | 4.15e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=89000, episode_reward=-5589181.58 +/- 72809.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48596248 |
|    mean velocity x | 4.87        |
|    mean velocity y | 6.12        |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.59e+06   |
| time/              |             |
|    total_timesteps | 89000       |
------------------------------------
Eval num_timesteps=89500, episode_reward=-5518260.48 +/- 78993.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2931537 |
|    mean velocity x | 1.6        |
|    mean velocity y | 2.04       |
|    mean velocity z | 22.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.52e+06  |
| time/              |            |
|    total_timesteps | 89500      |
-----------------------------------
New best mean reward!
Eval num_timesteps=90000, episode_reward=-5589002.31 +/- 93229.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2894024 |
|    mean velocity x | 2.65       |
|    mean velocity y | 3.29       |
|    mean velocity z | 22.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.59e+06  |
| time/              |            |
|    total_timesteps | 90000      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 44    |
|    time_elapsed    | 3709  |
|    total_timesteps | 90112 |
------------------------------
Eval num_timesteps=90500, episode_reward=-5433587.74 +/- 85186.20
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2772844   |
|    mean velocity x      | 1.56         |
|    mean velocity y      | 1.85         |
|    mean velocity z      | 22.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.43e+06    |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0033598896 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+08     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00299     |
|    std                  | 0.985        |
|    value_loss           | 3.98e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=91000, episode_reward=-5421648.31 +/- 164000.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4656239 |
|    mean velocity x | 4.3        |
|    mean velocity y | 6.01       |
|    mean velocity z | 23.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.42e+06  |
| time/              |            |
|    total_timesteps | 91000      |
-----------------------------------
New best mean reward!
Eval num_timesteps=91500, episode_reward=-5487833.91 +/- 80170.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1833016 |
|    mean velocity x | -0.377     |
|    mean velocity y | 0.37       |
|    mean velocity z | 23         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.49e+06  |
| time/              |            |
|    total_timesteps | 91500      |
-----------------------------------
Eval num_timesteps=92000, episode_reward=-5386335.57 +/- 184623.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18473782 |
|    mean velocity x | -0.408      |
|    mean velocity y | -0.203      |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.39e+06   |
| time/              |             |
|    total_timesteps | 92000       |
------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 45    |
|    time_elapsed    | 3791  |
|    total_timesteps | 92160 |
------------------------------
Eval num_timesteps=92500, episode_reward=-5478340.76 +/- 72430.06
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.2592498  |
|    mean velocity x      | 2.08        |
|    mean velocity y      | 3.22        |
|    mean velocity z      | 22.6        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.48e+06   |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.002532036 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.84e+08    |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00215    |
|    std                  | 0.986       |
|    value_loss           | 3.8e+08     |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=-5469902.31 +/- 67834.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22172174 |
|    mean velocity x | -0.728      |
|    mean velocity y | -0.837      |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.47e+06   |
| time/              |             |
|    total_timesteps | 93000       |
------------------------------------
Eval num_timesteps=93500, episode_reward=-5396944.21 +/- 63110.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28628784 |
|    mean velocity x | 0.0446      |
|    mean velocity y | 0.883       |
|    mean velocity z | 23.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.4e+06    |
| time/              |             |
|    total_timesteps | 93500       |
------------------------------------
Eval num_timesteps=94000, episode_reward=-5394664.53 +/- 144836.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27302992 |
|    mean velocity x | 0.655       |
|    mean velocity y | 1.32        |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.39e+06   |
| time/              |             |
|    total_timesteps | 94000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 46    |
|    time_elapsed    | 3873  |
|    total_timesteps | 94208 |
------------------------------
Eval num_timesteps=94500, episode_reward=-5350782.86 +/- 194354.31
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.15229309 |
|    mean velocity x      | -2.77       |
|    mean velocity y      | -3.07       |
|    mean velocity z      | 22.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.35e+06   |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.004279703 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 2.17e+08    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00363    |
|    std                  | 0.981       |
|    value_loss           | 3.97e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=95000, episode_reward=-5416275.17 +/- 148124.13
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.005081486 |
|    mean velocity x | -3.48        |
|    mean velocity y | -3.05        |
|    mean velocity z | 20.6         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.42e+06    |
| time/              |              |
|    total_timesteps | 95000        |
-------------------------------------
Eval num_timesteps=95500, episode_reward=-5450567.43 +/- 160535.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33384866 |
|    mean velocity x | 1.69        |
|    mean velocity y | 3.18        |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.45e+06   |
| time/              |             |
|    total_timesteps | 95500       |
------------------------------------
Eval num_timesteps=96000, episode_reward=-5346273.71 +/- 142673.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07783322 |
|    mean velocity x | -3.96       |
|    mean velocity y | -4.05       |
|    mean velocity z | 16.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.35e+06   |
| time/              |             |
|    total_timesteps | 96000       |
------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 47    |
|    time_elapsed    | 3955  |
|    total_timesteps | 96256 |
------------------------------
Eval num_timesteps=96500, episode_reward=-5398236.00 +/- 131413.22
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.23441525  |
|    mean velocity x      | 1.56         |
|    mean velocity y      | 2.03         |
|    mean velocity z      | 23.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.4e+06     |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0051512416 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.04e+08     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00437     |
|    std                  | 0.98         |
|    value_loss           | 3.68e+08     |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-5212547.35 +/- 230388.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28521195 |
|    mean velocity x | 2.83        |
|    mean velocity y | 3.65        |
|    mean velocity z | 19          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.21e+06   |
| time/              |             |
|    total_timesteps | 97000       |
------------------------------------
New best mean reward!
Eval num_timesteps=97500, episode_reward=-5192756.96 +/- 310863.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10434831 |
|    mean velocity x | -2.62       |
|    mean velocity y | -2.69       |
|    mean velocity z | 23.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.19e+06   |
| time/              |             |
|    total_timesteps | 97500       |
------------------------------------
New best mean reward!
Eval num_timesteps=98000, episode_reward=-5309971.71 +/- 129402.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40919843 |
|    mean velocity x | 3.94        |
|    mean velocity y | 5.04        |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.31e+06   |
| time/              |             |
|    total_timesteps | 98000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 48    |
|    time_elapsed    | 4038  |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=98500, episode_reward=-5310712.11 +/- 58416.45
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24946699   |
|    mean velocity x      | 2.65          |
|    mean velocity y      | 3.29          |
|    mean velocity z      | 20.9          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.31e+06     |
| time/                   |               |
|    total_timesteps      | 98500         |
| train/                  |               |
|    approx_kl            | 0.00042653823 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.19         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+08      |
|    n_updates            | 480           |
|    policy_gradient_loss | -0.000636     |
|    std                  | 0.979         |
|    value_loss           | 3.56e+08      |
-------------------------------------------
Eval num_timesteps=99000, episode_reward=-5169570.06 +/- 340562.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10441801 |
|    mean velocity x | -2.58       |
|    mean velocity y | -2.44       |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.17e+06   |
| time/              |             |
|    total_timesteps | 99000       |
------------------------------------
New best mean reward!
Eval num_timesteps=99500, episode_reward=-5278782.49 +/- 213159.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29348794 |
|    mean velocity x | 2.14        |
|    mean velocity y | 2.67        |
|    mean velocity z | 23.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.28e+06   |
| time/              |             |
|    total_timesteps | 99500       |
------------------------------------
Eval num_timesteps=100000, episode_reward=-5190527.82 +/- 122326.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30427662 |
|    mean velocity x | 3.75        |
|    mean velocity y | 5.33        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.19e+06   |
| time/              |             |
|    total_timesteps | 100000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 49     |
|    time_elapsed    | 4120   |
|    total_timesteps | 100352 |
-------------------------------
Eval num_timesteps=100500, episode_reward=-3411275.67 +/- 483664.94
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24194406  |
|    mean velocity x      | 0.244        |
|    mean velocity y      | 0.171        |
|    mean velocity z      | 22.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.41e+06    |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0013146768 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+08     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00138     |
|    std                  | 0.976        |
|    value_loss           | 3.47e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=101000, episode_reward=-3693229.97 +/- 1078072.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32535613 |
|    mean velocity x | 2.04        |
|    mean velocity y | 2.62        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.69e+06   |
| time/              |             |
|    total_timesteps | 101000      |
------------------------------------
Eval num_timesteps=101500, episode_reward=-3276887.31 +/- 767689.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19176634 |
|    mean velocity x | -1.38       |
|    mean velocity y | -1.11       |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.28e+06   |
| time/              |             |
|    total_timesteps | 101500      |
------------------------------------
New best mean reward!
Eval num_timesteps=102000, episode_reward=-3813644.29 +/- 978703.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14546838 |
|    mean velocity x | -1.45       |
|    mean velocity y | -0.685      |
|    mean velocity z | 22.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.81e+06   |
| time/              |             |
|    total_timesteps | 102000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 50     |
|    time_elapsed    | 4201   |
|    total_timesteps | 102400 |
-------------------------------
Eval num_timesteps=102500, episode_reward=-4099018.54 +/- 594493.15
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1373066   |
|    mean velocity x      | -0.85        |
|    mean velocity y      | -0.914       |
|    mean velocity z      | 23.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.1e+06     |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0044598076 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.49e+08     |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00483     |
|    std                  | 0.976        |
|    value_loss           | 3.92e+08     |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-4803059.80 +/- 508771.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.485585 |
|    mean velocity x | 4.83      |
|    mean velocity y | 6.2       |
|    mean velocity z | 20.2      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.8e+06  |
| time/              |           |
|    total_timesteps | 103000    |
----------------------------------
Eval num_timesteps=103500, episode_reward=-4884991.73 +/- 375013.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42141047 |
|    mean velocity x | 1.63        |
|    mean velocity y | 2.41        |
|    mean velocity z | 21.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.88e+06   |
| time/              |             |
|    total_timesteps | 103500      |
------------------------------------
Eval num_timesteps=104000, episode_reward=-4652810.25 +/- 663333.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3298707 |
|    mean velocity x | -0.89      |
|    mean velocity y | -0.513     |
|    mean velocity z | 22.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.65e+06  |
| time/              |            |
|    total_timesteps | 104000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 51     |
|    time_elapsed    | 4283   |
|    total_timesteps | 104448 |
-------------------------------
Eval num_timesteps=104500, episode_reward=-3393536.03 +/- 1906198.14
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4748385   |
|    mean velocity x      | 4.54         |
|    mean velocity y      | 6.07         |
|    mean velocity z      | 21.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.39e+06    |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0022183256 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+08     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.976        |
|    value_loss           | 3.6e+08      |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-1493854.91 +/- 1275122.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23424652 |
|    mean velocity x | -0.402      |
|    mean velocity y | 0.163       |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.49e+06   |
| time/              |             |
|    total_timesteps | 105000      |
------------------------------------
New best mean reward!
Eval num_timesteps=105500, episode_reward=-2449311.83 +/- 2292422.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38239655 |
|    mean velocity x | 2.12        |
|    mean velocity y | 3.46        |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.45e+06   |
| time/              |             |
|    total_timesteps | 105500      |
------------------------------------
Eval num_timesteps=106000, episode_reward=-2835233.36 +/- 2274826.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28556427 |
|    mean velocity x | -0.58       |
|    mean velocity y | 0.228       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.84e+06   |
| time/              |             |
|    total_timesteps | 106000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 52     |
|    time_elapsed    | 4364   |
|    total_timesteps | 106496 |
-------------------------------
Eval num_timesteps=106500, episode_reward=-1833921.43 +/- 1236522.92
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2292814   |
|    mean velocity x      | -2.88        |
|    mean velocity y      | -2.5         |
|    mean velocity z      | 21           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.83e+06    |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0028930877 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+08     |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00213     |
|    std                  | 0.974        |
|    value_loss           | 3.78e+08     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-2279949.01 +/- 2375545.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.60349804 |
|    mean velocity x | 4.56        |
|    mean velocity y | 6.45        |
|    mean velocity z | 23.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.28e+06   |
| time/              |             |
|    total_timesteps | 107000      |
------------------------------------
Eval num_timesteps=107500, episode_reward=-2920387.44 +/- 1180041.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22490092 |
|    mean velocity x | -1.96       |
|    mean velocity y | -1.43       |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.92e+06   |
| time/              |             |
|    total_timesteps | 107500      |
------------------------------------
Eval num_timesteps=108000, episode_reward=-1977923.27 +/- 1689380.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24420443 |
|    mean velocity x | -2.6        |
|    mean velocity y | -2.13       |
|    mean velocity z | 22.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.98e+06   |
| time/              |             |
|    total_timesteps | 108000      |
------------------------------------
Eval num_timesteps=108500, episode_reward=-2695264.60 +/- 2019405.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33544374 |
|    mean velocity x | 0.284       |
|    mean velocity y | 1.28        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.7e+06    |
| time/              |             |
|    total_timesteps | 108500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 53     |
|    time_elapsed    | 4465   |
|    total_timesteps | 108544 |
-------------------------------
Eval num_timesteps=109000, episode_reward=-1434860.86 +/- 960651.51
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37779668  |
|    mean velocity x      | 2.17         |
|    mean velocity y      | 3.18         |
|    mean velocity z      | 22.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.43e+06    |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0020571179 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+08      |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.977        |
|    value_loss           | 3.71e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=109500, episode_reward=-806304.85 +/- 436448.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32217297 |
|    mean velocity x | 0.717       |
|    mean velocity y | 2.27        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.06e+05   |
| time/              |             |
|    total_timesteps | 109500      |
------------------------------------
New best mean reward!
Eval num_timesteps=110000, episode_reward=-1412302.24 +/- 1017835.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.04625705 |
|    mean velocity x | -2.62       |
|    mean velocity y | -2.66       |
|    mean velocity z | 12.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.41e+06   |
| time/              |             |
|    total_timesteps | 110000      |
------------------------------------
Eval num_timesteps=110500, episode_reward=-600840.16 +/- 434143.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4229686 |
|    mean velocity x | 1.98       |
|    mean velocity y | 3.42       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.01e+05  |
| time/              |            |
|    total_timesteps | 110500     |
-----------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 54     |
|    time_elapsed    | 4546   |
|    total_timesteps | 110592 |
-------------------------------
Eval num_timesteps=111000, episode_reward=-1255282.37 +/- 1688293.54
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3468745  |
|    mean velocity x      | 2.21        |
|    mean velocity y      | 3.41        |
|    mean velocity z      | 20.9        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.26e+06   |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.004264364 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.77e+08    |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.0041     |
|    std                  | 0.977       |
|    value_loss           | 2.76e+08    |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=-909992.79 +/- 754346.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29165533 |
|    mean velocity x | 0.91        |
|    mean velocity y | 1.56        |
|    mean velocity z | 20          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.1e+05    |
| time/              |             |
|    total_timesteps | 111500      |
------------------------------------
Eval num_timesteps=112000, episode_reward=-1854884.70 +/- 1163422.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35206017 |
|    mean velocity x | 2.11        |
|    mean velocity y | 2.58        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.85e+06   |
| time/              |             |
|    total_timesteps | 112000      |
------------------------------------
Eval num_timesteps=112500, episode_reward=-694883.01 +/- 648953.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27545977 |
|    mean velocity x | 0.579       |
|    mean velocity y | 1.53        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.95e+05   |
| time/              |             |
|    total_timesteps | 112500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 55     |
|    time_elapsed    | 4627   |
|    total_timesteps | 112640 |
-------------------------------
Eval num_timesteps=113000, episode_reward=-737371.88 +/- 302266.58
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.26826832   |
|    mean velocity x      | 0.576         |
|    mean velocity y      | 1.01          |
|    mean velocity z      | 21.1          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.37e+05     |
| time/                   |               |
|    total_timesteps      | 113000        |
| train/                  |               |
|    approx_kl            | 0.00077040563 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.19         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 2.39e+08      |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.00106      |
|    std                  | 0.977         |
|    value_loss           | 3.34e+08      |
-------------------------------------------
Eval num_timesteps=113500, episode_reward=-949781.34 +/- 427517.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19551177 |
|    mean velocity x | -3.22       |
|    mean velocity y | -2.6        |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.5e+05    |
| time/              |             |
|    total_timesteps | 113500      |
------------------------------------
Eval num_timesteps=114000, episode_reward=-431029.11 +/- 151836.16
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.338802 |
|    mean velocity x | 1.86      |
|    mean velocity y | 2.85      |
|    mean velocity z | 22.2      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.31e+05 |
| time/              |           |
|    total_timesteps | 114000    |
----------------------------------
New best mean reward!
Eval num_timesteps=114500, episode_reward=-731950.86 +/- 198709.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3376533 |
|    mean velocity x | 1.11       |
|    mean velocity y | 1.94       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.32e+05  |
| time/              |            |
|    total_timesteps | 114500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 56     |
|    time_elapsed    | 4707   |
|    total_timesteps | 114688 |
-------------------------------
Eval num_timesteps=115000, episode_reward=-360813.45 +/- 149526.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.58786374  |
|    mean velocity x      | 4.13         |
|    mean velocity y      | 5.64         |
|    mean velocity z      | 22.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.61e+05    |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0031891498 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+08     |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.976        |
|    value_loss           | 3.66e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=115500, episode_reward=-463361.05 +/- 206819.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24027115 |
|    mean velocity x | -1.73       |
|    mean velocity y | -0.972      |
|    mean velocity z | 21.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.63e+05   |
| time/              |             |
|    total_timesteps | 115500      |
------------------------------------
Eval num_timesteps=116000, episode_reward=-818782.30 +/- 997778.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3889897 |
|    mean velocity x | 0.495      |
|    mean velocity y | 1.23       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.19e+05  |
| time/              |            |
|    total_timesteps | 116000     |
-----------------------------------
Eval num_timesteps=116500, episode_reward=-555635.23 +/- 443149.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15480411 |
|    mean velocity x | -3.43       |
|    mean velocity y | -3.42       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.56e+05   |
| time/              |             |
|    total_timesteps | 116500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 57     |
|    time_elapsed    | 4788   |
|    total_timesteps | 116736 |
-------------------------------
Eval num_timesteps=117000, episode_reward=-1150799.72 +/- 1608731.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.52411336  |
|    mean velocity x      | 3.09         |
|    mean velocity y      | 4.51         |
|    mean velocity z      | 19.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.15e+06    |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0040261494 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+08     |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00357     |
|    std                  | 0.979        |
|    value_loss           | 3.38e+08     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-501375.03 +/- 388435.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38124532 |
|    mean velocity x | -0.175      |
|    mean velocity y | -0.00616    |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.01e+05   |
| time/              |             |
|    total_timesteps | 117500      |
------------------------------------
Eval num_timesteps=118000, episode_reward=-751000.90 +/- 678479.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.02917403 |
|    mean velocity x | -6.47      |
|    mean velocity y | -6.44      |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.51e+05  |
| time/              |            |
|    total_timesteps | 118000     |
-----------------------------------
Eval num_timesteps=118500, episode_reward=-749584.73 +/- 740310.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10635346 |
|    mean velocity x | -4.44       |
|    mean velocity y | -4.77       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.5e+05    |
| time/              |             |
|    total_timesteps | 118500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 58     |
|    time_elapsed    | 4869   |
|    total_timesteps | 118784 |
-------------------------------
Eval num_timesteps=119000, episode_reward=-876171.41 +/- 697789.57
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.35833558 |
|    mean velocity x      | -0.449      |
|    mean velocity y      | 0.138       |
|    mean velocity z      | 22.2        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.76e+05   |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.002917634 |
|    clip_fraction        | 0.00508     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.46e+08    |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.0023     |
|    std                  | 0.98        |
|    value_loss           | 3e+08       |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=-1425980.53 +/- 1409734.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3812282 |
|    mean velocity x | 2.77       |
|    mean velocity y | 3.7        |
|    mean velocity z | 19         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.43e+06  |
| time/              |            |
|    total_timesteps | 119500     |
-----------------------------------
Eval num_timesteps=120000, episode_reward=-1208498.84 +/- 1020992.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27062947 |
|    mean velocity x | -0.0723     |
|    mean velocity y | 0.662       |
|    mean velocity z | 10.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+06   |
| time/              |             |
|    total_timesteps | 120000      |
------------------------------------
Eval num_timesteps=120500, episode_reward=-554204.67 +/- 523299.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6072105 |
|    mean velocity x | 3.8        |
|    mean velocity y | 5.58       |
|    mean velocity z | 22.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.54e+05  |
| time/              |            |
|    total_timesteps | 120500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 59     |
|    time_elapsed    | 4949   |
|    total_timesteps | 120832 |
-------------------------------
Eval num_timesteps=121000, episode_reward=-583801.74 +/- 244226.57
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40591055  |
|    mean velocity x      | -0.249       |
|    mean velocity y      | 0.685        |
|    mean velocity z      | 21.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.84e+05    |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0013186904 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.34e+08     |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00185     |
|    std                  | 0.979        |
|    value_loss           | 3.13e+08     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-286427.67 +/- 215247.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31802005 |
|    mean velocity x | 0.811       |
|    mean velocity y | 1.72        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.86e+05   |
| time/              |             |
|    total_timesteps | 121500      |
------------------------------------
New best mean reward!
Eval num_timesteps=122000, episode_reward=-385735.20 +/- 221127.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39497262 |
|    mean velocity x | -0.427      |
|    mean velocity y | 0.321       |
|    mean velocity z | 22          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.86e+05   |
| time/              |             |
|    total_timesteps | 122000      |
------------------------------------
Eval num_timesteps=122500, episode_reward=-468990.95 +/- 332194.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4071729 |
|    mean velocity x | 2.7        |
|    mean velocity y | 4.1        |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.69e+05  |
| time/              |            |
|    total_timesteps | 122500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 60     |
|    time_elapsed    | 5030   |
|    total_timesteps | 122880 |
-------------------------------
Eval num_timesteps=123000, episode_reward=-178852.84 +/- 98876.65
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3584175   |
|    mean velocity x      | 1.29         |
|    mean velocity y      | 2.34         |
|    mean velocity z      | 9.07         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.79e+05    |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0021053632 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.61e+08     |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.978        |
|    value_loss           | 3.07e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=123500, episode_reward=-244796.14 +/- 137795.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18500002 |
|    mean velocity x | -1.77       |
|    mean velocity y | -1.52       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.45e+05   |
| time/              |             |
|    total_timesteps | 123500      |
------------------------------------
Eval num_timesteps=124000, episode_reward=-342932.63 +/- 226881.82
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.490554 |
|    mean velocity x | 2.74      |
|    mean velocity y | 3.97      |
|    mean velocity z | 20.8      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.43e+05 |
| time/              |           |
|    total_timesteps | 124000    |
----------------------------------
Eval num_timesteps=124500, episode_reward=-285297.64 +/- 260836.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5333782 |
|    mean velocity x | 1.61       |
|    mean velocity y | 2.67       |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.85e+05  |
| time/              |            |
|    total_timesteps | 124500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 61     |
|    time_elapsed    | 5110   |
|    total_timesteps | 124928 |
-------------------------------
Eval num_timesteps=125000, episode_reward=-349812.18 +/- 397699.96
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24050923  |
|    mean velocity x      | -0.612       |
|    mean velocity y      | 0.344        |
|    mean velocity z      | 21.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.5e+05     |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0048014014 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+08     |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00461     |
|    std                  | 0.98         |
|    value_loss           | 3.12e+08     |
------------------------------------------
Eval num_timesteps=125500, episode_reward=-467375.65 +/- 244346.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16763186 |
|    mean velocity x | 0.0385      |
|    mean velocity y | 0.467       |
|    mean velocity z | 7.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.67e+05   |
| time/              |             |
|    total_timesteps | 125500      |
------------------------------------
Eval num_timesteps=126000, episode_reward=-246153.65 +/- 106115.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42673627 |
|    mean velocity x | 1.27        |
|    mean velocity y | 2.61        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.46e+05   |
| time/              |             |
|    total_timesteps | 126000      |
------------------------------------
Eval num_timesteps=126500, episode_reward=-260972.13 +/- 87327.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31419355 |
|    mean velocity x | 1.49        |
|    mean velocity y | 2.37        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.61e+05   |
| time/              |             |
|    total_timesteps | 126500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 62     |
|    time_elapsed    | 5190   |
|    total_timesteps | 126976 |
-------------------------------
Eval num_timesteps=127000, episode_reward=-257670.21 +/- 72842.74
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.45554766 |
|    mean velocity x      | 1.45        |
|    mean velocity y      | 2.8         |
|    mean velocity z      | 21.2        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -2.58e+05   |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.002913374 |
|    clip_fraction        | 0.00498     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.37e+08    |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00205    |
|    std                  | 0.98        |
|    value_loss           | 2.67e+08    |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=-261176.82 +/- 150207.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3610555 |
|    mean velocity x | 0.424      |
|    mean velocity y | 1.31       |
|    mean velocity z | 11.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.61e+05  |
| time/              |            |
|    total_timesteps | 127500     |
-----------------------------------
Eval num_timesteps=128000, episode_reward=-370882.50 +/- 145165.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39077976 |
|    mean velocity x | 0.384       |
|    mean velocity y | 0.691       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.71e+05   |
| time/              |             |
|    total_timesteps | 128000      |
------------------------------------
Eval num_timesteps=128500, episode_reward=-246852.49 +/- 182162.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0674474 |
|    mean velocity x | -0.598     |
|    mean velocity y | -0.499     |
|    mean velocity z | 9.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.47e+05  |
| time/              |            |
|    total_timesteps | 128500     |
-----------------------------------
Eval num_timesteps=129000, episode_reward=-427769.74 +/- 289281.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.190635 |
|    mean velocity x | -1.47     |
|    mean velocity y | -1.06     |
|    mean velocity z | 16.1      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.28e+05 |
| time/              |           |
|    total_timesteps | 129000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 63     |
|    time_elapsed    | 5290   |
|    total_timesteps | 129024 |
-------------------------------
Eval num_timesteps=129500, episode_reward=-192150.50 +/- 122091.66
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13515027  |
|    mean velocity x      | -0.811       |
|    mean velocity y      | -0.157       |
|    mean velocity z      | 3.42         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.92e+05    |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0021958246 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+08     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.981        |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-193964.24 +/- 109547.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47091672 |
|    mean velocity x | 1.33        |
|    mean velocity y | 2.52        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.94e+05   |
| time/              |             |
|    total_timesteps | 130000      |
------------------------------------
Eval num_timesteps=130500, episode_reward=-194443.93 +/- 98585.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31912267 |
|    mean velocity x | 2.13        |
|    mean velocity y | 2.87        |
|    mean velocity z | 12.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.94e+05   |
| time/              |             |
|    total_timesteps | 130500      |
------------------------------------
Eval num_timesteps=131000, episode_reward=-279534.83 +/- 120193.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4828257 |
|    mean velocity x | 2.31       |
|    mean velocity y | 3.75       |
|    mean velocity z | 12.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.8e+05   |
| time/              |            |
|    total_timesteps | 131000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 64     |
|    time_elapsed    | 5370   |
|    total_timesteps | 131072 |
-------------------------------
Eval num_timesteps=131500, episode_reward=-313908.20 +/- 111344.08
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.45194936 |
|    mean velocity x      | 0.601       |
|    mean velocity y      | 2           |
|    mean velocity z      | 19.8        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -3.14e+05   |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.004248135 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.55e+07    |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00347    |
|    std                  | 0.981       |
|    value_loss           | 1.79e+08    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=-246101.36 +/- 119649.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.72616297 |
|    mean velocity x | 3.52        |
|    mean velocity y | 5.76        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.46e+05   |
| time/              |             |
|    total_timesteps | 132000      |
------------------------------------
Eval num_timesteps=132500, episode_reward=-217581.46 +/- 94143.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06179883 |
|    mean velocity x | -2.5        |
|    mean velocity y | -2.6        |
|    mean velocity z | 12.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.18e+05   |
| time/              |             |
|    total_timesteps | 132500      |
------------------------------------
Eval num_timesteps=133000, episode_reward=-362546.96 +/- 158038.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4372373 |
|    mean velocity x | 1.47       |
|    mean velocity y | 2.98       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.63e+05  |
| time/              |            |
|    total_timesteps | 133000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 65     |
|    time_elapsed    | 5450   |
|    total_timesteps | 133120 |
-------------------------------
Eval num_timesteps=133500, episode_reward=-259841.38 +/- 157713.51
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.12647133 |
|    mean velocity x      | -2.11       |
|    mean velocity y      | -1.9        |
|    mean velocity z      | 18.7        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -2.6e+05    |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.002716078 |
|    clip_fraction        | 0.00386     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.78e+07    |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00169    |
|    std                  | 0.979       |
|    value_loss           | 2.54e+08    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=-178712.08 +/- 143467.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3765162 |
|    mean velocity x | -1.12      |
|    mean velocity y | -0.0519    |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.79e+05  |
| time/              |            |
|    total_timesteps | 134000     |
-----------------------------------
New best mean reward!
Eval num_timesteps=134500, episode_reward=-443473.65 +/- 299338.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22547996 |
|    mean velocity x | 1.33        |
|    mean velocity y | 2.43        |
|    mean velocity z | 8.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.43e+05   |
| time/              |             |
|    total_timesteps | 134500      |
------------------------------------
Eval num_timesteps=135000, episode_reward=-229096.50 +/- 239856.94
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.319076 |
|    mean velocity x | -0.0206   |
|    mean velocity y | 0.929     |
|    mean velocity z | 17        |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.29e+05 |
| time/              |           |
|    total_timesteps | 135000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 66     |
|    time_elapsed    | 5531   |
|    total_timesteps | 135168 |
-------------------------------
Eval num_timesteps=135500, episode_reward=-359494.66 +/- 135364.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24773732  |
|    mean velocity x      | 1.75         |
|    mean velocity y      | 2.73         |
|    mean velocity z      | 7.41         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.59e+05    |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0028021662 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+08     |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00246     |
|    std                  | 0.981        |
|    value_loss           | 2.54e+08     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-158845.13 +/- 37543.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12486861 |
|    mean velocity x | -1.24       |
|    mean velocity y | -0.749      |
|    mean velocity z | 8.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.59e+05   |
| time/              |             |
|    total_timesteps | 136000      |
------------------------------------
New best mean reward!
Eval num_timesteps=136500, episode_reward=-379860.23 +/- 415011.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19845496 |
|    mean velocity x | 0.816       |
|    mean velocity y | 1.22        |
|    mean velocity z | 7.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.8e+05    |
| time/              |             |
|    total_timesteps | 136500      |
------------------------------------
Eval num_timesteps=137000, episode_reward=-229838.91 +/- 79316.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45264247 |
|    mean velocity x | 2.1         |
|    mean velocity y | 3.21        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.3e+05    |
| time/              |             |
|    total_timesteps | 137000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 67     |
|    time_elapsed    | 5611   |
|    total_timesteps | 137216 |
-------------------------------
Eval num_timesteps=137500, episode_reward=-212789.21 +/- 84652.37
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05455638  |
|    mean velocity x      | -0.0253      |
|    mean velocity y      | -0.0935      |
|    mean velocity z      | 4.29         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -2.13e+05    |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0044419924 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.93e+07     |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00637     |
|    std                  | 0.979        |
|    value_loss           | 1.83e+08     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-101857.39 +/- 70972.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29662323 |
|    mean velocity x | 0.995       |
|    mean velocity y | 2.08        |
|    mean velocity z | 14.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 138000      |
------------------------------------
New best mean reward!
Eval num_timesteps=138500, episode_reward=-155943.25 +/- 62203.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32120627 |
|    mean velocity x | 0.45        |
|    mean velocity y | 0.859       |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.56e+05   |
| time/              |             |
|    total_timesteps | 138500      |
------------------------------------
Eval num_timesteps=139000, episode_reward=-275311.97 +/- 103486.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2796431 |
|    mean velocity x | 0.672      |
|    mean velocity y | 1.75       |
|    mean velocity z | 12.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.75e+05  |
| time/              |            |
|    total_timesteps | 139000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 68     |
|    time_elapsed    | 5691   |
|    total_timesteps | 139264 |
-------------------------------
Eval num_timesteps=139500, episode_reward=-257649.30 +/- 72859.55
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1540871   |
|    mean velocity x      | -0.302       |
|    mean velocity y      | 0.0712       |
|    mean velocity z      | 14.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -2.58e+05    |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0037861215 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00329     |
|    std                  | 0.975        |
|    value_loss           | 2.22e+08     |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-117480.59 +/- 42216.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12205591 |
|    mean velocity x | -2.78       |
|    mean velocity y | -2.45       |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.17e+05   |
| time/              |             |
|    total_timesteps | 140000      |
------------------------------------
Eval num_timesteps=140500, episode_reward=-160342.17 +/- 133716.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16833901 |
|    mean velocity x | -0.0484     |
|    mean velocity y | 0.0333      |
|    mean velocity z | 6.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.6e+05    |
| time/              |             |
|    total_timesteps | 140500      |
------------------------------------
Eval num_timesteps=141000, episode_reward=-179860.78 +/- 76312.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08364064 |
|    mean velocity x | 0.0764      |
|    mean velocity y | 0.345       |
|    mean velocity z | 6.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.8e+05    |
| time/              |             |
|    total_timesteps | 141000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 69     |
|    time_elapsed    | 5773   |
|    total_timesteps | 141312 |
-------------------------------
Eval num_timesteps=141500, episode_reward=-122539.94 +/- 67669.30
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.20413491 |
|    mean velocity x      | 0.716       |
|    mean velocity y      | 0.935       |
|    mean velocity z      | 6.95        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.23e+05   |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.004538944 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.89e+07    |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00563    |
|    std                  | 0.983       |
|    value_loss           | 1.46e+08    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=-143895.39 +/- 63329.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.019920563 |
|    mean velocity x | -0.141      |
|    mean velocity y | -0.628      |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.44e+05   |
| time/              |             |
|    total_timesteps | 142000      |
------------------------------------
Eval num_timesteps=142500, episode_reward=-150866.48 +/- 64493.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2205441 |
|    mean velocity x | -0.0325    |
|    mean velocity y | 0.272      |
|    mean velocity z | 10.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.51e+05  |
| time/              |            |
|    total_timesteps | 142500     |
-----------------------------------
Eval num_timesteps=143000, episode_reward=-105698.13 +/- 92155.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.04688133 |
|    mean velocity x | -1.76      |
|    mean velocity y | -1.41      |
|    mean velocity z | 5.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 143000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 70     |
|    time_elapsed    | 5853   |
|    total_timesteps | 143360 |
-------------------------------
Eval num_timesteps=143500, episode_reward=-176987.53 +/- 67754.25
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.17642441 |
|    mean velocity x      | 0.919       |
|    mean velocity y      | 1.44        |
|    mean velocity z      | 6.09        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.77e+05   |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.004437884 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.2e+07     |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00501    |
|    std                  | 0.982       |
|    value_loss           | 1.05e+08    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=-236827.15 +/- 78774.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10321608 |
|    mean velocity x | -0.0867     |
|    mean velocity y | 0.0948      |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.37e+05   |
| time/              |             |
|    total_timesteps | 144000      |
------------------------------------
Eval num_timesteps=144500, episode_reward=-151797.15 +/- 52019.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12907824 |
|    mean velocity x | 0.909       |
|    mean velocity y | 1.46        |
|    mean velocity z | 6.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.52e+05   |
| time/              |             |
|    total_timesteps | 144500      |
------------------------------------
Eval num_timesteps=145000, episode_reward=-149272.83 +/- 59163.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41040364 |
|    mean velocity x | 2.69        |
|    mean velocity y | 3.85        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.49e+05   |
| time/              |             |
|    total_timesteps | 145000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 71     |
|    time_elapsed    | 5933   |
|    total_timesteps | 145408 |
-------------------------------
Eval num_timesteps=145500, episode_reward=-175062.46 +/- 10968.42
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.0010390396 |
|    mean velocity x      | -0.483       |
|    mean velocity y      | -0.786       |
|    mean velocity z      | 3.15         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.75e+05    |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0026310394 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 5.13e+07     |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.0033      |
|    std                  | 0.976        |
|    value_loss           | 1.44e+08     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-90913.41 +/- 57011.82
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.008147087 |
|    mean velocity x | -0.994       |
|    mean velocity y | -1.28        |
|    mean velocity z | 13.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.09e+04    |
| time/              |              |
|    total_timesteps | 146000       |
-------------------------------------
New best mean reward!
Eval num_timesteps=146500, episode_reward=-93066.47 +/- 127156.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10225041 |
|    mean velocity x | 0.191       |
|    mean velocity y | 0.586       |
|    mean velocity z | 10.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.31e+04   |
| time/              |             |
|    total_timesteps | 146500      |
------------------------------------
Eval num_timesteps=147000, episode_reward=-164196.77 +/- 68404.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3791184 |
|    mean velocity x | 1.76       |
|    mean velocity y | 2.35       |
|    mean velocity z | 16.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.64e+05  |
| time/              |            |
|    total_timesteps | 147000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 72     |
|    time_elapsed    | 6014   |
|    total_timesteps | 147456 |
-------------------------------
Eval num_timesteps=147500, episode_reward=-146762.93 +/- 56021.64
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.04509261  |
|    mean velocity x      | 0.0248       |
|    mean velocity y      | 0.137        |
|    mean velocity z      | 3.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.47e+05    |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0024936674 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.71e+07     |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00329     |
|    std                  | 0.979        |
|    value_loss           | 2.09e+08     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-160208.58 +/- 55632.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.08079844 |
|    mean velocity x | -1.49      |
|    mean velocity y | -1.07      |
|    mean velocity z | 3.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.6e+05   |
| time/              |            |
|    total_timesteps | 148000     |
-----------------------------------
Eval num_timesteps=148500, episode_reward=-137511.19 +/- 43296.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10881206 |
|    mean velocity x | 0.832       |
|    mean velocity y | 1.28        |
|    mean velocity z | 5.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.38e+05   |
| time/              |             |
|    total_timesteps | 148500      |
------------------------------------
Eval num_timesteps=149000, episode_reward=-182870.41 +/- 68818.79
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.047454357 |
|    mean velocity x | -0.611       |
|    mean velocity y | 0.0266       |
|    mean velocity z | 2.24         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.83e+05    |
| time/              |              |
|    total_timesteps | 149000       |
-------------------------------------
Eval num_timesteps=149500, episode_reward=-174024.25 +/- 64130.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11690575 |
|    mean velocity x | -0.503      |
|    mean velocity y | -0.12       |
|    mean velocity z | 13.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.74e+05   |
| time/              |             |
|    total_timesteps | 149500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 73     |
|    time_elapsed    | 6113   |
|    total_timesteps | 149504 |
-------------------------------
Eval num_timesteps=150000, episode_reward=-94235.65 +/- 19423.98
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1788925   |
|    mean velocity x      | 1.36         |
|    mean velocity y      | 1.33         |
|    mean velocity z      | 4.51         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.42e+04    |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0039423387 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 2.5e+07      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00536     |
|    std                  | 0.988        |
|    value_loss           | 7.61e+07     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-123726.65 +/- 65176.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18434748 |
|    mean velocity x | 1.03        |
|    mean velocity y | 1.48        |
|    mean velocity z | 7.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.24e+05   |
| time/              |             |
|    total_timesteps | 150500      |
------------------------------------
Eval num_timesteps=151000, episode_reward=-157831.66 +/- 114938.08
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.015819026 |
|    mean velocity x | 0.401        |
|    mean velocity y | 0.677        |
|    mean velocity z | 3.48         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.58e+05    |
| time/              |              |
|    total_timesteps | 151000       |
-------------------------------------
Eval num_timesteps=151500, episode_reward=-131776.09 +/- 69424.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09855709 |
|    mean velocity x | 0.788       |
|    mean velocity y | 0.541       |
|    mean velocity z | 14.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.32e+05   |
| time/              |             |
|    total_timesteps | 151500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 74     |
|    time_elapsed    | 6193   |
|    total_timesteps | 151552 |
-------------------------------
Eval num_timesteps=152000, episode_reward=-203450.92 +/- 95733.20
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.08449831   |
|    mean velocity x      | -0.681       |
|    mean velocity y      | -0.822       |
|    mean velocity z      | 6.55         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -2.03e+05    |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0025724862 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 5.13e+07     |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00379     |
|    std                  | 0.988        |
|    value_loss           | 1.55e+08     |
------------------------------------------
Eval num_timesteps=152500, episode_reward=-150501.40 +/- 41321.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1886964 |
|    mean velocity x | 1.17       |
|    mean velocity y | 1.86       |
|    mean velocity z | 6.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.51e+05  |
| time/              |            |
|    total_timesteps | 152500     |
-----------------------------------
Eval num_timesteps=153000, episode_reward=-122480.42 +/- 41682.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16943912 |
|    mean velocity x | 0.9         |
|    mean velocity y | 1.2         |
|    mean velocity z | 6.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.22e+05   |
| time/              |             |
|    total_timesteps | 153000      |
------------------------------------
Eval num_timesteps=153500, episode_reward=-141324.87 +/- 83355.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.09769026 |
|    mean velocity x | -0.822     |
|    mean velocity y | -1.05      |
|    mean velocity z | 7.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.41e+05  |
| time/              |            |
|    total_timesteps | 153500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 75     |
|    time_elapsed    | 6274   |
|    total_timesteps | 153600 |
-------------------------------
Eval num_timesteps=154000, episode_reward=-190124.63 +/- 31535.30
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.15993847 |
|    mean velocity x      | 1.87        |
|    mean velocity y      | 1.73        |
|    mean velocity z      | 5.9         |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.9e+05    |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.002574849 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 4.12e+07    |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.0031     |
|    std                  | 0.992       |
|    value_loss           | 1.24e+08    |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=-129485.64 +/- 63685.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.14354272 |
|    mean velocity x | -2.07      |
|    mean velocity y | -2.13      |
|    mean velocity z | 6.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.29e+05  |
| time/              |            |
|    total_timesteps | 154500     |
-----------------------------------
Eval num_timesteps=155000, episode_reward=-188571.38 +/- 129464.20
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.078772314 |
|    mean velocity x | 0.158        |
|    mean velocity y | 0.428        |
|    mean velocity z | 3.55         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.89e+05    |
| time/              |              |
|    total_timesteps | 155000       |
-------------------------------------
Eval num_timesteps=155500, episode_reward=-157421.12 +/- 65589.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24138886 |
|    mean velocity x | 0.931       |
|    mean velocity y | 1.89        |
|    mean velocity z | 9.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.57e+05   |
| time/              |             |
|    total_timesteps | 155500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 76     |
|    time_elapsed    | 6354   |
|    total_timesteps | 155648 |
-------------------------------
Eval num_timesteps=156000, episode_reward=-95068.32 +/- 67593.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.04583856   |
|    mean velocity x      | -0.154       |
|    mean velocity y      | -0.291       |
|    mean velocity z      | 3.22         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.51e+04    |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0026904182 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.65e+07     |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.99         |
|    value_loss           | 8.42e+07     |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-155283.58 +/- 46313.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34257358 |
|    mean velocity x | 2.18        |
|    mean velocity y | 3.37        |
|    mean velocity z | 9.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.55e+05   |
| time/              |             |
|    total_timesteps | 156500      |
------------------------------------
Eval num_timesteps=157000, episode_reward=-126973.90 +/- 53925.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06554698 |
|    mean velocity x | 0.0453      |
|    mean velocity y | 0.0659      |
|    mean velocity z | 6.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.27e+05   |
| time/              |             |
|    total_timesteps | 157000      |
------------------------------------
Eval num_timesteps=157500, episode_reward=-96664.51 +/- 76700.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24610844 |
|    mean velocity x | 1.68        |
|    mean velocity y | 2.59        |
|    mean velocity z | 9.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.67e+04   |
| time/              |             |
|    total_timesteps | 157500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 77     |
|    time_elapsed    | 6434   |
|    total_timesteps | 157696 |
-------------------------------
Eval num_timesteps=158000, episode_reward=-153552.99 +/- 78707.99
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.10104242  |
|    mean velocity x      | -0.345       |
|    mean velocity y      | -0.357       |
|    mean velocity z      | 6.1          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.54e+05    |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0025176518 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+08     |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00302     |
|    std                  | 0.986        |
|    value_loss           | 1.14e+08     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-96339.05 +/- 20203.41
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0010595445 |
|    mean velocity x | -0.139        |
|    mean velocity y | -0.183        |
|    mean velocity z | 4.14          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -9.63e+04     |
| time/              |               |
|    total_timesteps | 158500        |
--------------------------------------
Eval num_timesteps=159000, episode_reward=-162356.32 +/- 108332.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1267774 |
|    mean velocity x | 0.0638     |
|    mean velocity y | 0.475      |
|    mean velocity z | 3.4        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.62e+05  |
| time/              |            |
|    total_timesteps | 159000     |
-----------------------------------
Eval num_timesteps=159500, episode_reward=-163219.02 +/- 87323.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19317289 |
|    mean velocity x | 0.336       |
|    mean velocity y | 0.925       |
|    mean velocity z | 3.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.63e+05   |
| time/              |             |
|    total_timesteps | 159500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 78     |
|    time_elapsed    | 6515   |
|    total_timesteps | 159744 |
-------------------------------
Eval num_timesteps=160000, episode_reward=-216926.13 +/- 58268.48
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.24216613 |
|    mean velocity x      | 1.03        |
|    mean velocity y      | 1.55        |
|    mean velocity z      | 6.95        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -2.17e+05   |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.004914126 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.09e+07    |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00573    |
|    std                  | 0.997       |
|    value_loss           | 1.04e+08    |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=-99637.91 +/- 72083.54
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0013727951 |
|    mean velocity x | -0.671       |
|    mean velocity y | -0.377       |
|    mean velocity z | 6.56         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.96e+04    |
| time/              |              |
|    total_timesteps | 160500       |
-------------------------------------
Eval num_timesteps=161000, episode_reward=-218408.75 +/- 46993.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.028453033 |
|    mean velocity x | -0.925      |
|    mean velocity y | -0.792      |
|    mean velocity z | 7.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.18e+05   |
| time/              |             |
|    total_timesteps | 161000      |
------------------------------------
Eval num_timesteps=161500, episode_reward=-201639.17 +/- 8095.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.04173771 |
|    mean velocity x | 0.0319      |
|    mean velocity y | 0.293       |
|    mean velocity z | 3.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.02e+05   |
| time/              |             |
|    total_timesteps | 161500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 79     |
|    time_elapsed    | 6595   |
|    total_timesteps | 161792 |
-------------------------------
Eval num_timesteps=162000, episode_reward=-129398.49 +/- 40486.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.107625    |
|    mean velocity x      | 0.946        |
|    mean velocity y      | 1.08         |
|    mean velocity z      | 3.56         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.29e+05    |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0030893027 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 4.66e+07     |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00646     |
|    std                  | 0.999        |
|    value_loss           | 1.11e+08     |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-79153.84 +/- 20438.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.049498025 |
|    mean velocity x | -0.823      |
|    mean velocity y | -0.577      |
|    mean velocity z | 2.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.92e+04   |
| time/              |             |
|    total_timesteps | 162500      |
------------------------------------
New best mean reward!
Eval num_timesteps=163000, episode_reward=-127633.55 +/- 38082.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1468711 |
|    mean velocity x | 0.0345     |
|    mean velocity y | 0.486      |
|    mean velocity z | 4.23       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.28e+05  |
| time/              |            |
|    total_timesteps | 163000     |
-----------------------------------
Eval num_timesteps=163500, episode_reward=-101277.99 +/- 93077.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29298085 |
|    mean velocity x | 1.03        |
|    mean velocity y | 1.23        |
|    mean velocity z | 9.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 163500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 80     |
|    time_elapsed    | 6675   |
|    total_timesteps | 163840 |
-------------------------------
Eval num_timesteps=164000, episode_reward=-111408.00 +/- 35597.03
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.49524358 |
|    mean velocity x      | 2.03        |
|    mean velocity y      | 3.54        |
|    mean velocity z      | 16.8        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.11e+05   |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.002226333 |
|    clip_fraction        | 0.0112      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.32e+07    |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00383    |
|    std                  | 0.999       |
|    value_loss           | 1.26e+08    |
-----------------------------------------
Eval num_timesteps=164500, episode_reward=-94743.65 +/- 69106.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15684892 |
|    mean velocity x | 1.01        |
|    mean velocity y | 1.62        |
|    mean velocity z | 6.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.47e+04   |
| time/              |             |
|    total_timesteps | 164500      |
------------------------------------
Eval num_timesteps=165000, episode_reward=-111020.49 +/- 12669.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27068415 |
|    mean velocity x | 0.397       |
|    mean velocity y | 1.72        |
|    mean velocity z | 10.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 165000      |
------------------------------------
Eval num_timesteps=165500, episode_reward=-155287.01 +/- 39289.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.007867016 |
|    mean velocity x | -0.356      |
|    mean velocity y | -0.644      |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.55e+05   |
| time/              |             |
|    total_timesteps | 165500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 81     |
|    time_elapsed    | 6755   |
|    total_timesteps | 165888 |
-------------------------------
Eval num_timesteps=166000, episode_reward=-136720.05 +/- 30854.38
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.087921    |
|    mean velocity x      | -0.274       |
|    mean velocity y      | -0.453       |
|    mean velocity z      | 3.3          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.37e+05    |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0013965705 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 5.74e+07     |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00244     |
|    std                  | 1            |
|    value_loss           | 9.92e+07     |
------------------------------------------
Eval num_timesteps=166500, episode_reward=-166184.79 +/- 71612.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07451625 |
|    mean velocity x | 0.286       |
|    mean velocity y | 0.696       |
|    mean velocity z | 6.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.66e+05   |
| time/              |             |
|    total_timesteps | 166500      |
------------------------------------
Eval num_timesteps=167000, episode_reward=-131587.78 +/- 56825.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.032873802 |
|    mean velocity x | 0.099       |
|    mean velocity y | -0.0328     |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.32e+05   |
| time/              |             |
|    total_timesteps | 167000      |
------------------------------------
Eval num_timesteps=167500, episode_reward=-58944.77 +/- 30527.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.12395708 |
|    mean velocity x | -2.56      |
|    mean velocity y | -2.26      |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.89e+04  |
| time/              |            |
|    total_timesteps | 167500     |
-----------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 82     |
|    time_elapsed    | 6836   |
|    total_timesteps | 167936 |
-------------------------------
Eval num_timesteps=168000, episode_reward=-72735.39 +/- 68832.72
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.25056663 |
|    mean velocity x      | 1.9         |
|    mean velocity y      | 2.51        |
|    mean velocity z      | 10.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -7.27e+04   |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.00340578  |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.61e+07    |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00438    |
|    std                  | 1           |
|    value_loss           | 1.72e+08    |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=-121583.43 +/- 64129.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11192358 |
|    mean velocity x | 1.11        |
|    mean velocity y | 1.58        |
|    mean velocity z | 6.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.22e+05   |
| time/              |             |
|    total_timesteps | 168500      |
------------------------------------
Eval num_timesteps=169000, episode_reward=-100887.58 +/- 59362.01
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.072586775 |
|    mean velocity x | -0.0194      |
|    mean velocity y | 0.282        |
|    mean velocity z | 8.41         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.01e+05    |
| time/              |              |
|    total_timesteps | 169000       |
-------------------------------------
Eval num_timesteps=169500, episode_reward=-137692.75 +/- 77211.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.11861163 |
|    mean velocity x | -0.702     |
|    mean velocity y | -1.29      |
|    mean velocity z | 6.18       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.38e+05  |
| time/              |            |
|    total_timesteps | 169500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 83     |
|    time_elapsed    | 6916   |
|    total_timesteps | 169984 |
-------------------------------
Eval num_timesteps=170000, episode_reward=-131958.39 +/- 81616.80
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.042834524 |
|    mean velocity x      | -0.406      |
|    mean velocity y      | -0.11       |
|    mean velocity z      | 1.02        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.32e+05   |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.00397842  |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 3.89e+07    |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00948    |
|    std                  | 1           |
|    value_loss           | 7.69e+07    |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=-172193.43 +/- 91155.22
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | 0.1764476 |
|    mean velocity x | -1.93     |
|    mean velocity y | -2.78     |
|    mean velocity z | 9.34      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.72e+05 |
| time/              |           |
|    total_timesteps | 170500    |
----------------------------------
Eval num_timesteps=171000, episode_reward=-148630.41 +/- 49886.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12778386 |
|    mean velocity x | 0.135       |
|    mean velocity y | 0.712       |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.49e+05   |
| time/              |             |
|    total_timesteps | 171000      |
------------------------------------
Eval num_timesteps=171500, episode_reward=-74327.90 +/- 68656.46
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.064041354 |
|    mean velocity x | 0.0116       |
|    mean velocity y | 0.234        |
|    mean velocity z | 3.53         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -7.43e+04    |
| time/              |              |
|    total_timesteps | 171500       |
-------------------------------------
Eval num_timesteps=172000, episode_reward=-111292.37 +/- 66427.44
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.015154741 |
|    mean velocity x | 0.327        |
|    mean velocity y | -0.0298      |
|    mean velocity z | 6.5          |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.11e+05    |
| time/              |              |
|    total_timesteps | 172000       |
-------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 84     |
|    time_elapsed    | 7015   |
|    total_timesteps | 172032 |
-------------------------------
Eval num_timesteps=172500, episode_reward=-154514.20 +/- 50971.81
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.113556765 |
|    mean velocity x      | -0.0908      |
|    mean velocity y      | 0.207        |
|    mean velocity z      | 3.97         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.55e+05    |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0029480518 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.37e+07     |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00447     |
|    std                  | 0.999        |
|    value_loss           | 1.34e+08     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-120613.17 +/- 47803.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.03993667 |
|    mean velocity x | 0.371       |
|    mean velocity y | 0.713       |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 173000      |
------------------------------------
Eval num_timesteps=173500, episode_reward=-146387.72 +/- 82144.68
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | 0.0836458 |
|    mean velocity x | -1.12     |
|    mean velocity y | -1.53     |
|    mean velocity z | 5.91      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.46e+05 |
| time/              |           |
|    total_timesteps | 173500    |
----------------------------------
Eval num_timesteps=174000, episode_reward=-92506.17 +/- 54377.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.008030159 |
|    mean velocity x | -0.831      |
|    mean velocity y | -1.13       |
|    mean velocity z | 12.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.25e+04   |
| time/              |             |
|    total_timesteps | 174000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 85     |
|    time_elapsed    | 7096   |
|    total_timesteps | 174080 |
-------------------------------
Eval num_timesteps=174500, episode_reward=-116613.36 +/- 99806.13
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.011693152 |
|    mean velocity x      | 0.0659      |
|    mean velocity y      | 0.114       |
|    mean velocity z      | 3.72        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.17e+05   |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.003148708 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.07e+07    |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00367    |
|    std                  | 1           |
|    value_loss           | 1.08e+08    |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=-115600.50 +/- 57601.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36308566 |
|    mean velocity x | 1.33        |
|    mean velocity y | 2.55        |
|    mean velocity z | 6.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 175000      |
------------------------------------
Eval num_timesteps=175500, episode_reward=-92177.27 +/- 31246.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.108120106 |
|    mean velocity x | -0.68       |
|    mean velocity y | -0.924      |
|    mean velocity z | 6.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.22e+04   |
| time/              |             |
|    total_timesteps | 175500      |
------------------------------------
Eval num_timesteps=176000, episode_reward=-101846.35 +/- 28857.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22346456 |
|    mean velocity x | 1.05        |
|    mean velocity y | 1.68        |
|    mean velocity z | 7.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 176000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 86     |
|    time_elapsed    | 7176   |
|    total_timesteps | 176128 |
-------------------------------
Eval num_timesteps=176500, episode_reward=-152160.72 +/- 67800.23
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.24658264 |
|    mean velocity x      | 0.251       |
|    mean velocity y      | 0.803       |
|    mean velocity z      | 12.9        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.52e+05   |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.00354103  |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 3.4e+07     |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00455    |
|    std                  | 1.01        |
|    value_loss           | 1.07e+08    |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=-67112.97 +/- 53966.73
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.067075044 |
|    mean velocity x | -0.226       |
|    mean velocity y | -0.0875      |
|    mean velocity z | 6.85         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.71e+04    |
| time/              |              |
|    total_timesteps | 177000       |
-------------------------------------
Eval num_timesteps=177500, episode_reward=-128609.01 +/- 58139.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1535095 |
|    mean velocity x | -1         |
|    mean velocity y | -0.204     |
|    mean velocity z | 13.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.29e+05  |
| time/              |            |
|    total_timesteps | 177500     |
-----------------------------------
Eval num_timesteps=178000, episode_reward=-114552.60 +/- 81409.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.10293196 |
|    mean velocity x | -0.991     |
|    mean velocity y | -1.57      |
|    mean velocity z | 4.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.15e+05  |
| time/              |            |
|    total_timesteps | 178000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 87     |
|    time_elapsed    | 7258   |
|    total_timesteps | 178176 |
-------------------------------
Eval num_timesteps=178500, episode_reward=-108087.18 +/- 79523.10
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.03957089  |
|    mean velocity x      | 0.353        |
|    mean velocity y      | 0.242        |
|    mean velocity z      | 2.74         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.08e+05    |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0022243042 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.24e+07     |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00372     |
|    std                  | 1.01         |
|    value_loss           | 1.16e+08     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-82287.92 +/- 78994.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.013036186 |
|    mean velocity x | -1.16       |
|    mean velocity y | -0.875      |
|    mean velocity z | 3.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 179000      |
------------------------------------
Eval num_timesteps=179500, episode_reward=-64759.34 +/- 32901.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19856338 |
|    mean velocity x | 1.02        |
|    mean velocity y | 1.67        |
|    mean velocity z | 10.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 179500      |
------------------------------------
Eval num_timesteps=180000, episode_reward=-155235.37 +/- 54942.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10638583 |
|    mean velocity x | 0.598       |
|    mean velocity y | 0.855       |
|    mean velocity z | 3           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.55e+05   |
| time/              |             |
|    total_timesteps | 180000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 88     |
|    time_elapsed    | 7338   |
|    total_timesteps | 180224 |
-------------------------------
Eval num_timesteps=180500, episode_reward=-64368.66 +/- 45511.58
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.065119386 |
|    mean velocity x      | -0.283       |
|    mean velocity y      | 0.0925       |
|    mean velocity z      | 0.619        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.44e+04    |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.003303343  |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.7e+07      |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.0067      |
|    std                  | 1.02         |
|    value_loss           | 5.37e+07     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-130240.95 +/- 38619.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23514448 |
|    mean velocity x | 0.635       |
|    mean velocity y | 1.82        |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.3e+05    |
| time/              |             |
|    total_timesteps | 181000      |
------------------------------------
Eval num_timesteps=181500, episode_reward=-64162.75 +/- 32175.52
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.079921186 |
|    mean velocity x | -0.474       |
|    mean velocity y | -0.26        |
|    mean velocity z | 3.14         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.42e+04    |
| time/              |              |
|    total_timesteps | 181500       |
-------------------------------------
Eval num_timesteps=182000, episode_reward=-99589.57 +/- 61842.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2770342 |
|    mean velocity x | 1.01       |
|    mean velocity y | 2.08       |
|    mean velocity z | 7.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.96e+04  |
| time/              |            |
|    total_timesteps | 182000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 89     |
|    time_elapsed    | 7418   |
|    total_timesteps | 182272 |
-------------------------------
Eval num_timesteps=182500, episode_reward=-116514.03 +/- 68093.72
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.029163294 |
|    mean velocity x      | -0.803       |
|    mean velocity y      | -1.2         |
|    mean velocity z      | 5.49         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.17e+05    |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0023940934 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.31        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.7e+07      |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.007       |
|    std                  | 1.02         |
|    value_loss           | 6.97e+07     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=-146478.21 +/- 60635.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.052704025 |
|    mean velocity x | -0.229      |
|    mean velocity y | -0.705      |
|    mean velocity z | 6.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.46e+05   |
| time/              |             |
|    total_timesteps | 183000      |
------------------------------------
Eval num_timesteps=183500, episode_reward=-115212.11 +/- 24835.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.036926307 |
|    mean velocity x | -1.1        |
|    mean velocity y | -1.57       |
|    mean velocity z | 7.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 183500      |
------------------------------------
Eval num_timesteps=184000, episode_reward=-171615.12 +/- 36326.43
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.043582402 |
|    mean velocity x | 0.129        |
|    mean velocity y | 0.227        |
|    mean velocity z | 4.07         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.72e+05    |
| time/              |              |
|    total_timesteps | 184000       |
-------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 90     |
|    time_elapsed    | 7498   |
|    total_timesteps | 184320 |
-------------------------------
Eval num_timesteps=184500, episode_reward=-125250.50 +/- 67644.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.120734245  |
|    mean velocity x      | -2.25        |
|    mean velocity y      | -2.79        |
|    mean velocity z      | 10.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.25e+05    |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0022606556 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.32        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.65e+07     |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00429     |
|    std                  | 1.02         |
|    value_loss           | 1.27e+08     |
------------------------------------------
Eval num_timesteps=185000, episode_reward=-128712.09 +/- 70339.25
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0015761118 |
|    mean velocity x | -0.836       |
|    mean velocity y | -0.616       |
|    mean velocity z | 2.93         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.29e+05    |
| time/              |              |
|    total_timesteps | 185000       |
-------------------------------------
Eval num_timesteps=185500, episode_reward=-114436.30 +/- 43121.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07397583 |
|    mean velocity x | -0.304      |
|    mean velocity y | 0.0751      |
|    mean velocity z | 2.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 185500      |
------------------------------------
Eval num_timesteps=186000, episode_reward=-94885.79 +/- 70922.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20836526 |
|    mean velocity x | 0.596       |
|    mean velocity y | 1.06        |
|    mean velocity z | 6.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 186000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 91     |
|    time_elapsed    | 7579   |
|    total_timesteps | 186368 |
-------------------------------
Eval num_timesteps=186500, episode_reward=-89083.87 +/- 28433.33
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05903927  |
|    mean velocity x      | -0.101       |
|    mean velocity y      | 0.0917       |
|    mean velocity z      | 4.15         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.91e+04    |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 0.0035985755 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.34        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 3.67e+07     |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00598     |
|    std                  | 1.04         |
|    value_loss           | 6.99e+07     |
------------------------------------------
Eval num_timesteps=187000, episode_reward=-75739.60 +/- 56643.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.15339959 |
|    mean velocity x | -1.04      |
|    mean velocity y | -1.76      |
|    mean velocity z | 7.37       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.57e+04  |
| time/              |            |
|    total_timesteps | 187000     |
-----------------------------------
Eval num_timesteps=187500, episode_reward=-97898.42 +/- 66174.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.054472696 |
|    mean velocity x | -0.786      |
|    mean velocity y | -0.466      |
|    mean velocity z | 2.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.79e+04   |
| time/              |             |
|    total_timesteps | 187500      |
------------------------------------
Eval num_timesteps=188000, episode_reward=-95094.23 +/- 42924.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5922697 |
|    mean velocity x | 1.79       |
|    mean velocity y | 3.19       |
|    mean velocity z | 11.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 188000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 92     |
|    time_elapsed    | 7659   |
|    total_timesteps | 188416 |
-------------------------------
Eval num_timesteps=188500, episode_reward=-117678.97 +/- 18306.06
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14308524  |
|    mean velocity x      | -0.596       |
|    mean velocity y      | 0.12         |
|    mean velocity z      | 3.05         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.18e+05    |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0033014098 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.36        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.89e+07     |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.0051      |
|    std                  | 1.04         |
|    value_loss           | 9.16e+07     |
------------------------------------------
Eval num_timesteps=189000, episode_reward=-109286.27 +/- 27672.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.04485332 |
|    mean velocity x | -0.54       |
|    mean velocity y | -0.947      |
|    mean velocity z | 3.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 189000      |
------------------------------------
Eval num_timesteps=189500, episode_reward=-74016.54 +/- 59914.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.06055815 |
|    mean velocity x | -0.98      |
|    mean velocity y | -0.971     |
|    mean velocity z | 7.49       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.4e+04   |
| time/              |            |
|    total_timesteps | 189500     |
-----------------------------------
Eval num_timesteps=190000, episode_reward=-114852.69 +/- 73181.30
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.506042 |
|    mean velocity x | 1.63      |
|    mean velocity y | 3.05      |
|    mean velocity z | 10.7      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.15e+05 |
| time/              |           |
|    total_timesteps | 190000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 93     |
|    time_elapsed    | 7739   |
|    total_timesteps | 190464 |
-------------------------------
Eval num_timesteps=190500, episode_reward=-88537.25 +/- 60901.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.03854477   |
|    mean velocity x      | -0.911       |
|    mean velocity y      | -1.24        |
|    mean velocity z      | 7.22         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.85e+04    |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0030059544 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.36        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+08     |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00555     |
|    std                  | 1.04         |
|    value_loss           | 1.19e+08     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-67160.34 +/- 33718.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08753894 |
|    mean velocity x | -0.344      |
|    mean velocity y | 0.167       |
|    mean velocity z | 6.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 191000      |
------------------------------------
Eval num_timesteps=191500, episode_reward=-108365.94 +/- 19079.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.12887374 |
|    mean velocity x | -1.72      |
|    mean velocity y | -2.61      |
|    mean velocity z | 7.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 191500     |
-----------------------------------
Eval num_timesteps=192000, episode_reward=-122293.26 +/- 68924.24
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.056826577 |
|    mean velocity x | -0.839       |
|    mean velocity y | -0.804       |
|    mean velocity z | 3.13         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.22e+05    |
| time/              |              |
|    total_timesteps | 192000       |
-------------------------------------
Eval num_timesteps=192500, episode_reward=-131571.72 +/- 72730.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.05707516 |
|    mean velocity x | -0.443     |
|    mean velocity y | -0.759     |
|    mean velocity z | 6.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.32e+05  |
| time/              |            |
|    total_timesteps | 192500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 94     |
|    time_elapsed    | 7838   |
|    total_timesteps | 192512 |
-------------------------------
Eval num_timesteps=193000, episode_reward=-102790.65 +/- 20853.25
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15588775  |
|    mean velocity x      | 0.00221      |
|    mean velocity y      | 0.394        |
|    mean velocity z      | 4.51         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.03e+05    |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0035840068 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.36        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.35e+07     |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00736     |
|    std                  | 1.04         |
|    value_loss           | 9.52e+07     |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-95822.14 +/- 28192.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1107078 |
|    mean velocity x | -0.0725    |
|    mean velocity y | 0.07       |
|    mean velocity z | 1.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.58e+04  |
| time/              |            |
|    total_timesteps | 193500     |
-----------------------------------
Eval num_timesteps=194000, episode_reward=-93757.50 +/- 10190.31
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.038526498 |
|    mean velocity x | -0.0908      |
|    mean velocity y | -0.00611     |
|    mean velocity z | 0.0924       |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.38e+04    |
| time/              |              |
|    total_timesteps | 194000       |
-------------------------------------
Eval num_timesteps=194500, episode_reward=-119123.45 +/- 66630.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12163689 |
|    mean velocity x | 0.0431      |
|    mean velocity y | 0.714       |
|    mean velocity z | 4.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 194500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 95     |
|    time_elapsed    | 7919   |
|    total_timesteps | 194560 |
-------------------------------
Eval num_timesteps=195000, episode_reward=-36641.31 +/- 44497.30
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0028512268 |
|    mean velocity x      | 0.0443        |
|    mean velocity y      | -0.294        |
|    mean velocity z      | 0.477         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.66e+04     |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 0.019658528   |
|    clip_fraction        | 0.177         |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.46         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 9.74e+06      |
|    n_updates            | 950           |
|    policy_gradient_loss | 0.00125       |
|    std                  | 1.08          |
|    value_loss           | 3.52e+07      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=195500, episode_reward=-77852.87 +/- 64615.40
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0063187047 |
|    mean velocity x | -0.185        |
|    mean velocity y | -0.386        |
|    mean velocity z | 3.69          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -7.79e+04     |
| time/              |               |
|    total_timesteps | 195500        |
--------------------------------------
Eval num_timesteps=196000, episode_reward=-95649.29 +/- 65868.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31392223 |
|    mean velocity x | 1.07        |
|    mean velocity y | 2           |
|    mean velocity z | 7.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.56e+04   |
| time/              |             |
|    total_timesteps | 196000      |
------------------------------------
Eval num_timesteps=196500, episode_reward=-80935.87 +/- 44140.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.85543823 |
|    mean velocity x | 3.85        |
|    mean velocity y | 6.46        |
|    mean velocity z | 14.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.09e+04   |
| time/              |             |
|    total_timesteps | 196500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 96     |
|    time_elapsed    | 7999   |
|    total_timesteps | 196608 |
-------------------------------
Eval num_timesteps=197000, episode_reward=-80586.35 +/- 49265.25
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.19757673  |
|    mean velocity x      | 0.257        |
|    mean velocity y      | 0.875        |
|    mean velocity z      | 5.93         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.06e+04    |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0034812994 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.92e+07     |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00449     |
|    std                  | 1.08         |
|    value_loss           | 1.21e+08     |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-79448.88 +/- 47449.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36699992 |
|    mean velocity x | 1.04        |
|    mean velocity y | 2.18        |
|    mean velocity z | 7.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.94e+04   |
| time/              |             |
|    total_timesteps | 197500      |
------------------------------------
Eval num_timesteps=198000, episode_reward=-122695.32 +/- 39730.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28482136 |
|    mean velocity x | 1.43        |
|    mean velocity y | 1.7         |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 198000      |
------------------------------------
Eval num_timesteps=198500, episode_reward=-93287.51 +/- 54458.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.41877776 |
|    mean velocity x | -3.89      |
|    mean velocity y | -4.65      |
|    mean velocity z | 10.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.33e+04  |
| time/              |            |
|    total_timesteps | 198500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 97     |
|    time_elapsed    | 8079   |
|    total_timesteps | 198656 |
-------------------------------
Eval num_timesteps=199000, episode_reward=-104559.68 +/- 57652.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29086205  |
|    mean velocity x      | 1.14         |
|    mean velocity y      | 2.06         |
|    mean velocity z      | 7.35         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0026243129 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.71e+07     |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00404     |
|    std                  | 1.08         |
|    value_loss           | 1.21e+08     |
------------------------------------------
Eval num_timesteps=199500, episode_reward=-111398.92 +/- 33170.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21288326 |
|    mean velocity x | 0.717       |
|    mean velocity y | 1.76        |
|    mean velocity z | 3.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 199500      |
------------------------------------
Eval num_timesteps=200000, episode_reward=-91656.49 +/- 55341.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06636474 |
|    mean velocity x | -0.133      |
|    mean velocity y | -0.255      |
|    mean velocity z | 9.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.17e+04   |
| time/              |             |
|    total_timesteps | 200000      |
------------------------------------
Eval num_timesteps=200500, episode_reward=-101800.10 +/- 66453.69
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.022641847 |
|    mean velocity x | -0.501       |
|    mean velocity y | -0.721       |
|    mean velocity z | 3.15         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.02e+05    |
| time/              |              |
|    total_timesteps | 200500       |
-------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 98     |
|    time_elapsed    | 8160   |
|    total_timesteps | 200704 |
-------------------------------
Eval num_timesteps=201000, episode_reward=-78829.34 +/- 39358.52
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5860804   |
|    mean velocity x      | 2.29         |
|    mean velocity y      | 4.16         |
|    mean velocity z      | 9.82         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.88e+04    |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0013103086 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.48        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.72e+07     |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00234     |
|    std                  | 1.07         |
|    value_loss           | 9.29e+07     |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-114962.21 +/- 32628.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23123118 |
|    mean velocity x | 1.81        |
|    mean velocity y | 2.46        |
|    mean velocity z | 6.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 201500      |
------------------------------------
Eval num_timesteps=202000, episode_reward=-103327.00 +/- 37856.62
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.021897595 |
|    mean velocity x | 0.568        |
|    mean velocity y | 0.579        |
|    mean velocity z | 3.22         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.03e+05    |
| time/              |              |
|    total_timesteps | 202000       |
-------------------------------------
Eval num_timesteps=202500, episode_reward=-98259.77 +/- 27401.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20245178 |
|    mean velocity x | 1.04        |
|    mean velocity y | 1.19        |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.83e+04   |
| time/              |             |
|    total_timesteps | 202500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 99     |
|    time_elapsed    | 8240   |
|    total_timesteps | 202752 |
-------------------------------
Eval num_timesteps=203000, episode_reward=-88996.09 +/- 18518.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1690311   |
|    mean velocity x      | 0.28         |
|    mean velocity y      | 1.23         |
|    mean velocity z      | 3.98         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.9e+04     |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0022752096 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.47        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.43e+07     |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00399     |
|    std                  | 1.08         |
|    value_loss           | 7.14e+07     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-64628.13 +/- 47287.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.13499436 |
|    mean velocity x | -0.938     |
|    mean velocity y | -1.15      |
|    mean velocity z | 2.95       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.46e+04  |
| time/              |            |
|    total_timesteps | 203500     |
-----------------------------------
Eval num_timesteps=204000, episode_reward=-125282.99 +/- 64099.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.16085352 |
|    mean velocity x | -1.86      |
|    mean velocity y | -2.13      |
|    mean velocity z | 5.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.25e+05  |
| time/              |            |
|    total_timesteps | 204000     |
-----------------------------------
Eval num_timesteps=204500, episode_reward=-105841.82 +/- 57514.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.05037046 |
|    mean velocity x | -0.44       |
|    mean velocity y | 0.027       |
|    mean velocity z | 1.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 204500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 100    |
|    time_elapsed    | 8320   |
|    total_timesteps | 204800 |
-------------------------------
Eval num_timesteps=205000, episode_reward=-119157.79 +/- 43056.61
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2623372   |
|    mean velocity x      | 0.758        |
|    mean velocity y      | 1.1          |
|    mean velocity z      | 6.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.19e+05    |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0032333154 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.3e+07      |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00318     |
|    std                  | 1.09         |
|    value_loss           | 5.92e+07     |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-61543.65 +/- 14819.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33465248 |
|    mean velocity x | 0.871       |
|    mean velocity y | 1.32        |
|    mean velocity z | 6.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.15e+04   |
| time/              |             |
|    total_timesteps | 205500      |
------------------------------------
Eval num_timesteps=206000, episode_reward=-86681.97 +/- 30928.65
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.056226924 |
|    mean velocity x | 0.801        |
|    mean velocity y | 0.58         |
|    mean velocity z | 2.64         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -8.67e+04    |
| time/              |              |
|    total_timesteps | 206000       |
-------------------------------------
Eval num_timesteps=206500, episode_reward=-88928.63 +/- 26377.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09650708 |
|    mean velocity x | 0.17        |
|    mean velocity y | -0.0727     |
|    mean velocity z | 0.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.89e+04   |
| time/              |             |
|    total_timesteps | 206500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 101    |
|    time_elapsed    | 8401   |
|    total_timesteps | 206848 |
-------------------------------
Eval num_timesteps=207000, episode_reward=-94715.41 +/- 19535.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.22221172  |
|    mean velocity x      | 0.335        |
|    mean velocity y      | 1.06         |
|    mean velocity z      | 6.14         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.47e+04    |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0023392828 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 6.06e+07     |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00505     |
|    std                  | 1.09         |
|    value_loss           | 7.25e+07     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-57595.34 +/- 33425.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0708435 |
|    mean velocity x | -0.348     |
|    mean velocity y | -0.109     |
|    mean velocity z | 2.88       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.76e+04  |
| time/              |            |
|    total_timesteps | 207500     |
-----------------------------------
Eval num_timesteps=208000, episode_reward=-78336.29 +/- 31190.32
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0065976297 |
|    mean velocity x | 0.0786       |
|    mean velocity y | 0.0465       |
|    mean velocity z | 3.73         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -7.83e+04    |
| time/              |              |
|    total_timesteps | 208000       |
-------------------------------------
Eval num_timesteps=208500, episode_reward=-58780.94 +/- 43898.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12094925 |
|    mean velocity x | -0.0669     |
|    mean velocity y | 0.274       |
|    mean velocity z | 0.0486      |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.88e+04   |
| time/              |             |
|    total_timesteps | 208500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 102    |
|    time_elapsed    | 8481   |
|    total_timesteps | 208896 |
-------------------------------
Eval num_timesteps=209000, episode_reward=-118227.21 +/- 60419.19
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.19387442  |
|    mean velocity x      | 0.108        |
|    mean velocity y      | 0.941        |
|    mean velocity z      | 3.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.18e+05    |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0111223385 |
|    clip_fraction        | 0.0653       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.54        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4e+07        |
|    n_updates            | 1020         |
|    policy_gradient_loss | 0.00289      |
|    std                  | 1.11         |
|    value_loss           | 6.86e+07     |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-66579.90 +/- 35110.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.036547255 |
|    mean velocity x | -0.419      |
|    mean velocity y | -0.0445     |
|    mean velocity z | 0.706       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.66e+04   |
| time/              |             |
|    total_timesteps | 209500      |
------------------------------------
Eval num_timesteps=210000, episode_reward=-141910.00 +/- 30930.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.030235866 |
|    mean velocity x | -0.966      |
|    mean velocity y | -1.17       |
|    mean velocity z | 6.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.42e+05   |
| time/              |             |
|    total_timesteps | 210000      |
------------------------------------
Eval num_timesteps=210500, episode_reward=-88094.41 +/- 15263.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4001758 |
|    mean velocity x | 0.98       |
|    mean velocity y | 2.34       |
|    mean velocity z | 6.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.81e+04  |
| time/              |            |
|    total_timesteps | 210500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 103    |
|    time_elapsed    | 8561   |
|    total_timesteps | 210944 |
-------------------------------
Eval num_timesteps=211000, episode_reward=-84746.85 +/- 85689.99
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.013969132 |
|    mean velocity x      | -0.841       |
|    mean velocity y      | -0.611       |
|    mean velocity z      | 2.69         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.47e+04    |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0019971123 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.57        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+07      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00339     |
|    std                  | 1.11         |
|    value_loss           | 6.63e+07     |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-76195.40 +/- 48048.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.029827064 |
|    mean velocity x | 0.0558      |
|    mean velocity y | 0.031       |
|    mean velocity z | 3.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.62e+04   |
| time/              |             |
|    total_timesteps | 211500      |
------------------------------------
Eval num_timesteps=212000, episode_reward=-56945.19 +/- 44676.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18986362 |
|    mean velocity x | 0.444       |
|    mean velocity y | 1.35        |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 212000      |
------------------------------------
Eval num_timesteps=212500, episode_reward=-71645.55 +/- 47496.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12069742 |
|    mean velocity x | 0.348       |
|    mean velocity y | 1.06        |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.16e+04   |
| time/              |             |
|    total_timesteps | 212500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 104    |
|    time_elapsed    | 8642   |
|    total_timesteps | 212992 |
-------------------------------
Eval num_timesteps=213000, episode_reward=-63214.56 +/- 24310.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05669117  |
|    mean velocity x      | -0.891       |
|    mean velocity y      | -0.343       |
|    mean velocity z      | 2.74         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.32e+04    |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0033939132 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.58        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.53e+07     |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00729     |
|    std                  | 1.12         |
|    value_loss           | 5.96e+07     |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-89048.23 +/- 21966.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12529725 |
|    mean velocity x | 0.0455      |
|    mean velocity y | 0.605       |
|    mean velocity z | 3.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.9e+04    |
| time/              |             |
|    total_timesteps | 213500      |
------------------------------------
Eval num_timesteps=214000, episode_reward=-48323.26 +/- 43296.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31625584 |
|    mean velocity x | 0.972       |
|    mean velocity y | 2.14        |
|    mean velocity z | 6.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.83e+04   |
| time/              |             |
|    total_timesteps | 214000      |
------------------------------------
Eval num_timesteps=214500, episode_reward=-87249.77 +/- 33445.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.30226108 |
|    mean velocity x | -2.1       |
|    mean velocity y | -2.58      |
|    mean velocity z | 6.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.72e+04  |
| time/              |            |
|    total_timesteps | 214500     |
-----------------------------------
Eval num_timesteps=215000, episode_reward=-70928.30 +/- 24668.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.008779859 |
|    mean velocity x | 0.0895      |
|    mean velocity y | -0.288      |
|    mean velocity z | 0.714       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 215000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 105    |
|    time_elapsed    | 8741   |
|    total_timesteps | 215040 |
-------------------------------
Eval num_timesteps=215500, episode_reward=-99837.67 +/- 17175.92
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.065958835 |
|    mean velocity x      | -0.589      |
|    mean velocity y      | -0.898      |
|    mean velocity z      | 6.25        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.98e+04   |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.002565988 |
|    clip_fraction        | 0.00913     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.85e+07    |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00387    |
|    std                  | 1.12        |
|    value_loss           | 7.75e+07    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=-105567.62 +/- 46829.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.03835379 |
|    mean velocity x | -0.0147    |
|    mean velocity y | -0.217     |
|    mean velocity z | 2.55       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 216000     |
-----------------------------------
Eval num_timesteps=216500, episode_reward=-80744.55 +/- 42460.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36441603 |
|    mean velocity x | 0.626       |
|    mean velocity y | 1.7         |
|    mean velocity z | 6.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 216500      |
------------------------------------
Eval num_timesteps=217000, episode_reward=-77319.34 +/- 45710.73
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0060262433 |
|    mean velocity x | -0.185        |
|    mean velocity y | 0.00709       |
|    mean velocity z | -0.0323       |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -7.73e+04     |
| time/              |               |
|    total_timesteps | 217000        |
--------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 106    |
|    time_elapsed    | 8821   |
|    total_timesteps | 217088 |
-------------------------------
Eval num_timesteps=217500, episode_reward=-69419.04 +/- 15109.56
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.04387413 |
|    mean velocity x      | -0.0425     |
|    mean velocity y      | 0.239       |
|    mean velocity z      | 3.93        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.94e+04   |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.002322915 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.6        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.15e+07    |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00316    |
|    std                  | 1.12        |
|    value_loss           | 8.04e+07    |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=-45315.50 +/- 30817.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11078999 |
|    mean velocity x | -0.000111   |
|    mean velocity y | 0.228       |
|    mean velocity z | 3.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.53e+04   |
| time/              |             |
|    total_timesteps | 218000      |
------------------------------------
Eval num_timesteps=218500, episode_reward=-83877.71 +/- 44131.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.112088025 |
|    mean velocity x | 0.12        |
|    mean velocity y | -0.0362     |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.39e+04   |
| time/              |             |
|    total_timesteps | 218500      |
------------------------------------
Eval num_timesteps=219000, episode_reward=-112787.09 +/- 40071.22
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0060022897 |
|    mean velocity x | -0.153       |
|    mean velocity y | -0.346       |
|    mean velocity z | 3.62         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.13e+05    |
| time/              |              |
|    total_timesteps | 219000       |
-------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 107    |
|    time_elapsed    | 8902   |
|    total_timesteps | 219136 |
-------------------------------
Eval num_timesteps=219500, episode_reward=-71655.09 +/- 37619.32
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.101641    |
|    mean velocity x      | -0.146       |
|    mean velocity y      | 0.12         |
|    mean velocity z      | 3.97         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.17e+04    |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 0.0025391765 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.62        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 4.74e+07     |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 1.13         |
|    value_loss           | 1.17e+08     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-61727.69 +/- 37094.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.22744305 |
|    mean velocity x | -0.722     |
|    mean velocity y | -1.92      |
|    mean velocity z | 4.96       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.17e+04  |
| time/              |            |
|    total_timesteps | 220000     |
-----------------------------------
Eval num_timesteps=220500, episode_reward=-65525.51 +/- 42553.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55778056 |
|    mean velocity x | 2.49        |
|    mean velocity y | 3.2         |
|    mean velocity z | 7.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.55e+04   |
| time/              |             |
|    total_timesteps | 220500      |
------------------------------------
Eval num_timesteps=221000, episode_reward=-64177.90 +/- 35332.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13386445 |
|    mean velocity x | -0.0994     |
|    mean velocity y | 0.38        |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 221000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 108    |
|    time_elapsed    | 8982   |
|    total_timesteps | 221184 |
-------------------------------
Eval num_timesteps=221500, episode_reward=-80940.34 +/- 24767.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.044320587 |
|    mean velocity x      | -0.37        |
|    mean velocity y      | 0.322        |
|    mean velocity z      | 0.344        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.09e+04    |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0027642867 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.91e+07     |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00662     |
|    std                  | 1.15         |
|    value_loss           | 5.97e+07     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=-78266.18 +/- 41560.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19258328 |
|    mean velocity x | 0.327       |
|    mean velocity y | 0.325       |
|    mean velocity z | 1.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.83e+04   |
| time/              |             |
|    total_timesteps | 222000      |
------------------------------------
Eval num_timesteps=222500, episode_reward=-103101.09 +/- 18249.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15407793 |
|    mean velocity x | 0.772       |
|    mean velocity y | 1.09        |
|    mean velocity z | 2.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 222500      |
------------------------------------
Eval num_timesteps=223000, episode_reward=-67730.21 +/- 35238.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06669944 |
|    mean velocity x | 0.17        |
|    mean velocity y | 0.222       |
|    mean velocity z | 3.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.77e+04   |
| time/              |             |
|    total_timesteps | 223000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 109    |
|    time_elapsed    | 9064   |
|    total_timesteps | 223232 |
-------------------------------
Eval num_timesteps=223500, episode_reward=-96425.38 +/- 25826.13
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.14093256  |
|    mean velocity x      | -0.715      |
|    mean velocity y      | -1.73       |
|    mean velocity z      | 4.84        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.64e+04   |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.004659299 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.74e+06    |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00575    |
|    std                  | 1.17        |
|    value_loss           | 3.81e+07    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=-58481.10 +/- 43178.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.09201992 |
|    mean velocity x | -0.389     |
|    mean velocity y | -1.07      |
|    mean velocity z | 3.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.85e+04  |
| time/              |            |
|    total_timesteps | 224000     |
-----------------------------------
Eval num_timesteps=224500, episode_reward=-107136.64 +/- 17800.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08262798 |
|    mean velocity x | 0.736       |
|    mean velocity y | 0.923       |
|    mean velocity z | 3.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 224500      |
------------------------------------
Eval num_timesteps=225000, episode_reward=-86318.18 +/- 23703.76
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0023700562 |
|    mean velocity x | 0.00625      |
|    mean velocity y | 0.02         |
|    mean velocity z | 3.84         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -8.63e+04    |
| time/              |              |
|    total_timesteps | 225000       |
-------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 110    |
|    time_elapsed    | 9144   |
|    total_timesteps | 225280 |
-------------------------------
Eval num_timesteps=225500, episode_reward=-117102.96 +/- 53468.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50316125  |
|    mean velocity x      | 1.76         |
|    mean velocity y      | 3.08         |
|    mean velocity z      | 10.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.17e+05    |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0018326999 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+07     |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 1.18         |
|    value_loss           | 9.85e+07     |
------------------------------------------
Eval num_timesteps=226000, episode_reward=-116475.67 +/- 30922.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13781881 |
|    mean velocity x | -0.0534     |
|    mean velocity y | 0.164       |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 226000      |
------------------------------------
Eval num_timesteps=226500, episode_reward=-69112.25 +/- 49588.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.07742354 |
|    mean velocity x | -1.13      |
|    mean velocity y | -1.15      |
|    mean velocity z | 3.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.91e+04  |
| time/              |            |
|    total_timesteps | 226500     |
-----------------------------------
Eval num_timesteps=227000, episode_reward=-77609.30 +/- 37077.88
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.282039 |
|    mean velocity x | 1.01      |
|    mean velocity y | 1.99      |
|    mean velocity z | 5.65      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.76e+04 |
| time/              |           |
|    total_timesteps | 227000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 111    |
|    time_elapsed    | 9225   |
|    total_timesteps | 227328 |
-------------------------------
Eval num_timesteps=227500, episode_reward=-142140.31 +/- 60276.25
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.07476379 |
|    mean velocity x      | -0.1        |
|    mean velocity y      | 0.203       |
|    mean velocity z      | 1.94        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.42e+05   |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.016115036 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.32e+07    |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00358    |
|    std                  | 1.18        |
|    value_loss           | 5.91e+07    |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=-78691.84 +/- 30522.25
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.368453 |
|    mean velocity x | 0.186     |
|    mean velocity y | 1.48      |
|    mean velocity z | 4.14      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.87e+04 |
| time/              |           |
|    total_timesteps | 228000    |
----------------------------------
Eval num_timesteps=228500, episode_reward=-68923.24 +/- 48102.26
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0016594734 |
|    mean velocity x | -0.0874       |
|    mean velocity y | -0.438        |
|    mean velocity z | 1.36          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -6.89e+04     |
| time/              |               |
|    total_timesteps | 228500        |
--------------------------------------
Eval num_timesteps=229000, episode_reward=-66650.58 +/- 27044.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07171582 |
|    mean velocity x | -0.257      |
|    mean velocity y | 0.144       |
|    mean velocity z | 1.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.67e+04   |
| time/              |             |
|    total_timesteps | 229000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 112    |
|    time_elapsed    | 9305   |
|    total_timesteps | 229376 |
-------------------------------
Eval num_timesteps=229500, episode_reward=-109112.23 +/- 28290.97
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.07099768 |
|    mean velocity x      | 0.407       |
|    mean velocity y      | 0.343       |
|    mean velocity z      | 3.44        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.09e+05   |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.03694582  |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 5.25e+06    |
|    n_updates            | 1120        |
|    policy_gradient_loss | 0.0108      |
|    std                  | 1.19        |
|    value_loss           | 3.5e+07     |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-81476.61 +/- 43219.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18595321 |
|    mean velocity x | 0.392       |
|    mean velocity y | 1.27        |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.15e+04   |
| time/              |             |
|    total_timesteps | 230000      |
------------------------------------
Eval num_timesteps=230500, episode_reward=-96624.61 +/- 52205.16
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0075358124 |
|    mean velocity x | -0.134       |
|    mean velocity y | 0.0121       |
|    mean velocity z | 0.278        |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.66e+04    |
| time/              |              |
|    total_timesteps | 230500       |
-------------------------------------
Eval num_timesteps=231000, episode_reward=-86878.55 +/- 46219.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26759958 |
|    mean velocity x | -0.0927     |
|    mean velocity y | 0.507       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 231000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 113    |
|    time_elapsed    | 9385   |
|    total_timesteps | 231424 |
-------------------------------
Eval num_timesteps=231500, episode_reward=-87699.33 +/- 42207.13
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.07881799 |
|    mean velocity x      | 0.456       |
|    mean velocity y      | 0.567       |
|    mean velocity z      | 3.58        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.77e+04   |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.019889984 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 5.7e+06     |
|    n_updates            | 1130        |
|    policy_gradient_loss | 0.0122      |
|    std                  | 1.21        |
|    value_loss           | 7.68e+07    |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=-91326.44 +/- 32344.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06920137 |
|    mean velocity x | -0.016      |
|    mean velocity y | -0.00683    |
|    mean velocity z | 0.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.13e+04   |
| time/              |             |
|    total_timesteps | 232000      |
------------------------------------
Eval num_timesteps=232500, episode_reward=-67480.29 +/- 41946.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24848765 |
|    mean velocity x | -0.0793     |
|    mean velocity y | 0.566       |
|    mean velocity z | 3.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.75e+04   |
| time/              |             |
|    total_timesteps | 232500      |
------------------------------------
Eval num_timesteps=233000, episode_reward=-95363.95 +/- 30865.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21420713 |
|    mean velocity x | 0.0958      |
|    mean velocity y | 0.97        |
|    mean velocity z | 4.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.54e+04   |
| time/              |             |
|    total_timesteps | 233000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 114    |
|    time_elapsed    | 9465   |
|    total_timesteps | 233472 |
-------------------------------
Eval num_timesteps=233500, episode_reward=-75125.81 +/- 36039.06
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.115895994 |
|    mean velocity x      | -0.26        |
|    mean velocity y      | -0.343       |
|    mean velocity z      | 3.66         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.51e+04    |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0008812114 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+07     |
|    n_updates            | 1140         |
|    policy_gradient_loss | 0.000178     |
|    std                  | 1.21         |
|    value_loss           | 6.38e+07     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=-95642.03 +/- 13260.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.003980118 |
|    mean velocity x | 0.17        |
|    mean velocity y | -0.332      |
|    mean velocity z | 3.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.56e+04   |
| time/              |             |
|    total_timesteps | 234000      |
------------------------------------
Eval num_timesteps=234500, episode_reward=-77552.19 +/- 52184.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12034045 |
|    mean velocity x | -0.37       |
|    mean velocity y | -0.482      |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.76e+04   |
| time/              |             |
|    total_timesteps | 234500      |
------------------------------------
Eval num_timesteps=235000, episode_reward=-62885.71 +/- 49884.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28916332 |
|    mean velocity x | 0.325       |
|    mean velocity y | 1.56        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.29e+04   |
| time/              |             |
|    total_timesteps | 235000      |
------------------------------------
Eval num_timesteps=235500, episode_reward=-71260.59 +/- 46846.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.10608979 |
|    mean velocity x | -1.38      |
|    mean velocity y | -1.34      |
|    mean velocity z | 4.11       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.13e+04  |
| time/              |            |
|    total_timesteps | 235500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 115    |
|    time_elapsed    | 9564   |
|    total_timesteps | 235520 |
-------------------------------
Eval num_timesteps=236000, episode_reward=-86379.59 +/- 22769.22
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.06336423   |
|    mean velocity x      | -0.629       |
|    mean velocity y      | -0.481       |
|    mean velocity z      | 2.18         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.64e+04    |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0007013805 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.81        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.56e+07     |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00296     |
|    std                  | 1.21         |
|    value_loss           | 5.59e+07     |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-118448.80 +/- 8829.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.05937901 |
|    mean velocity x | 0.0186     |
|    mean velocity y | -0.454     |
|    mean velocity z | 2.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 236500     |
-----------------------------------
Eval num_timesteps=237000, episode_reward=-56060.85 +/- 45688.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17241049 |
|    mean velocity x | 0.363       |
|    mean velocity y | 0.257       |
|    mean velocity z | 2.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.61e+04   |
| time/              |             |
|    total_timesteps | 237000      |
------------------------------------
Eval num_timesteps=237500, episode_reward=-105470.88 +/- 22249.55
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0018706322 |
|    mean velocity x | 0.00576       |
|    mean velocity y | 0.00414       |
|    mean velocity z | 0.109         |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -1.05e+05     |
| time/              |               |
|    total_timesteps | 237500        |
--------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 116    |
|    time_elapsed    | 9645   |
|    total_timesteps | 237568 |
-------------------------------
Eval num_timesteps=238000, episode_reward=-91533.78 +/- 30196.04
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -1.6894907 |
|    mean velocity x      | 5.14       |
|    mean velocity y      | 8.73       |
|    mean velocity z      | 19.7       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -9.15e+04  |
| time/                   |            |
|    total_timesteps      | 238000     |
| train/                  |            |
|    approx_kl            | 0.0981241  |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.84      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.001      |
|    loss                 | 1.45e+07   |
|    n_updates            | 1160       |
|    policy_gradient_loss | 0.0245     |
|    std                  | 1.23       |
|    value_loss           | 2.11e+07   |
----------------------------------------
Eval num_timesteps=238500, episode_reward=-88676.22 +/- 49789.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.89360094 |
|    mean velocity x | 1.91        |
|    mean velocity y | 3.87        |
|    mean velocity z | 13.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.87e+04   |
| time/              |             |
|    total_timesteps | 238500      |
------------------------------------
Eval num_timesteps=239000, episode_reward=-91953.21 +/- 46094.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5223717 |
|    mean velocity x | 1.07       |
|    mean velocity y | 2.15       |
|    mean velocity z | 4.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.2e+04   |
| time/              |            |
|    total_timesteps | 239000     |
-----------------------------------
Eval num_timesteps=239500, episode_reward=-99597.37 +/- 19169.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17509726 |
|    mean velocity x | 0.419       |
|    mean velocity y | 1.06        |
|    mean velocity z | 5.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.96e+04   |
| time/              |             |
|    total_timesteps | 239500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 117    |
|    time_elapsed    | 9725   |
|    total_timesteps | 239616 |
-------------------------------
Eval num_timesteps=240000, episode_reward=-111055.13 +/- 14151.67
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.11222731 |
|    mean velocity x      | 0.36        |
|    mean velocity y      | 0.473       |
|    mean velocity z      | 4.46        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.11e+05   |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.000881844 |
|    clip_fraction        | 0.00151     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.88       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 8.8e+07     |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.000575   |
|    std                  | 1.23        |
|    value_loss           | 1.39e+08    |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=-94940.92 +/- 39206.93
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.062593184 |
|    mean velocity x | 0.369        |
|    mean velocity y | 0.0532       |
|    mean velocity z | 2.87         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.49e+04    |
| time/              |              |
|    total_timesteps | 240500       |
-------------------------------------
Eval num_timesteps=241000, episode_reward=-86925.11 +/- 20524.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.9548166 |
|    mean velocity x | 2.65       |
|    mean velocity y | 5.25       |
|    mean velocity z | 11.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 241000     |
-----------------------------------
Eval num_timesteps=241500, episode_reward=-67776.99 +/- 45996.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36088836 |
|    mean velocity x | 0.986       |
|    mean velocity y | 1.79        |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.78e+04   |
| time/              |             |
|    total_timesteps | 241500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 118    |
|    time_elapsed    | 9805   |
|    total_timesteps | 241664 |
-------------------------------
Eval num_timesteps=242000, episode_reward=-78373.73 +/- 18779.89
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3591584   |
|    mean velocity x      | 0.627        |
|    mean velocity y      | 1.95         |
|    mean velocity z      | 4.58         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.84e+04    |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0013511395 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.71e+07     |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 1.23         |
|    value_loss           | 6.18e+07     |
------------------------------------------
Eval num_timesteps=242500, episode_reward=-120864.21 +/- 13438.03
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.067493886 |
|    mean velocity x | 0.818        |
|    mean velocity y | 0.864        |
|    mean velocity z | 3.88         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.21e+05    |
| time/              |              |
|    total_timesteps | 242500       |
-------------------------------------
Eval num_timesteps=243000, episode_reward=-101206.79 +/- 28794.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23098317 |
|    mean velocity x | 0.467       |
|    mean velocity y | 1.05        |
|    mean velocity z | 5.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 243000      |
------------------------------------
Eval num_timesteps=243500, episode_reward=-88789.31 +/- 38421.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36435083 |
|    mean velocity x | 0.485       |
|    mean velocity y | 1.03        |
|    mean velocity z | 2.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.88e+04   |
| time/              |             |
|    total_timesteps | 243500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 119    |
|    time_elapsed    | 9885   |
|    total_timesteps | 243712 |
-------------------------------
Eval num_timesteps=244000, episode_reward=-90819.50 +/- 21438.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4907951   |
|    mean velocity x      | 1.6          |
|    mean velocity y      | 3.27         |
|    mean velocity z      | 8.17         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.08e+04    |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0037931432 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 2.82e+07     |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00511     |
|    std                  | 1.23         |
|    value_loss           | 8.32e+07     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-93177.67 +/- 24589.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25888583 |
|    mean velocity x | 0.114       |
|    mean velocity y | 0.914       |
|    mean velocity z | 4.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.32e+04   |
| time/              |             |
|    total_timesteps | 244500      |
------------------------------------
Eval num_timesteps=245000, episode_reward=-90588.91 +/- 16297.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23728131 |
|    mean velocity x | 0.333       |
|    mean velocity y | 0.86        |
|    mean velocity z | 3.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.06e+04   |
| time/              |             |
|    total_timesteps | 245000      |
------------------------------------
Eval num_timesteps=245500, episode_reward=-92087.20 +/- 16585.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1271122 |
|    mean velocity x | 0.106      |
|    mean velocity y | 0.415      |
|    mean velocity z | 4.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.21e+04  |
| time/              |            |
|    total_timesteps | 245500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 120    |
|    time_elapsed    | 9966   |
|    total_timesteps | 245760 |
-------------------------------
Eval num_timesteps=246000, episode_reward=-69555.21 +/- 56736.67
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.35769743  |
|    mean velocity x      | -0.902      |
|    mean velocity y      | -2.12       |
|    mean velocity z      | 4.7         |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.96e+04   |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.006558188 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.88       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.68e+06    |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00303    |
|    std                  | 1.23        |
|    value_loss           | 5.08e+07    |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=-90484.83 +/- 41836.15
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.056609146 |
|    mean velocity x | 0.0794       |
|    mean velocity y | 0.0742       |
|    mean velocity z | 0.36         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.05e+04    |
| time/              |              |
|    total_timesteps | 246500       |
-------------------------------------
Eval num_timesteps=247000, episode_reward=-55921.51 +/- 52823.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16844958 |
|    mean velocity x | 0.0975      |
|    mean velocity y | 0.216       |
|    mean velocity z | 0.496       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.59e+04   |
| time/              |             |
|    total_timesteps | 247000      |
------------------------------------
Eval num_timesteps=247500, episode_reward=-72639.09 +/- 41322.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5863492 |
|    mean velocity x | 1.15       |
|    mean velocity y | 3.06       |
|    mean velocity z | 8.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.26e+04  |
| time/              |            |
|    total_timesteps | 247500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 121    |
|    time_elapsed    | 10046  |
|    total_timesteps | 247808 |
-------------------------------
Eval num_timesteps=248000, episode_reward=-119338.32 +/- 22000.92
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.04422873 |
|    mean velocity x      | -0.148      |
|    mean velocity y      | -0.056      |
|    mean velocity z      | 0.555       |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.19e+05   |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.26722977  |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.87       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 9.31e+06    |
|    n_updates            | 1210        |
|    policy_gradient_loss | 0.0317      |
|    std                  | 1.24        |
|    value_loss           | 3.06e+07    |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=-111758.18 +/- 38009.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12116504 |
|    mean velocity x | -0.0973     |
|    mean velocity y | 0.21        |
|    mean velocity z | 0.616       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 248500      |
------------------------------------
Eval num_timesteps=249000, episode_reward=-96679.97 +/- 44430.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48484698 |
|    mean velocity x | -0.774      |
|    mean velocity y | 0.526       |
|    mean velocity z | 6.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.67e+04   |
| time/              |             |
|    total_timesteps | 249000      |
------------------------------------
Eval num_timesteps=249500, episode_reward=-105913.67 +/- 31319.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29052025 |
|    mean velocity x | -0.203      |
|    mean velocity y | 0.283       |
|    mean velocity z | 1.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 249500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 122    |
|    time_elapsed    | 10126  |
|    total_timesteps | 249856 |
-------------------------------
Eval num_timesteps=250000, episode_reward=-96390.57 +/- 31124.82
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.45060027 |
|    mean velocity x      | -0.692      |
|    mean velocity y      | 0.377       |
|    mean velocity z      | 6.38        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.64e+04   |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.023961492 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.95       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 5.1e+06     |
|    n_updates            | 1220        |
|    policy_gradient_loss | 0.02        |
|    std                  | 1.27        |
|    value_loss           | 4.78e+07    |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=-103473.67 +/- 30509.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24928494 |
|    mean velocity x | -0.173      |
|    mean velocity y | 0.412       |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 250500      |
------------------------------------
Eval num_timesteps=251000, episode_reward=-103286.58 +/- 44606.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.325456 |
|    mean velocity x | -0.159    |
|    mean velocity y | 0.822     |
|    mean velocity z | 5.18      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+05 |
| time/              |           |
|    total_timesteps | 251000    |
----------------------------------
Eval num_timesteps=251500, episode_reward=-80626.36 +/- 60063.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26920465 |
|    mean velocity x | 0.167       |
|    mean velocity y | 0.688       |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 251500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 123    |
|    time_elapsed    | 10207  |
|    total_timesteps | 251904 |
-------------------------------
Eval num_timesteps=252000, episode_reward=-83943.31 +/- 39336.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.00031101608 |
|    mean velocity x      | -0.221        |
|    mean velocity y      | 0.0495        |
|    mean velocity z      | 0.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.39e+04     |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 0.003806415   |
|    clip_fraction        | 0.035         |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.97         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 4.56e+07      |
|    n_updates            | 1230          |
|    policy_gradient_loss | -0.0032       |
|    std                  | 1.28          |
|    value_loss           | 7.3e+07       |
-------------------------------------------
Eval num_timesteps=252500, episode_reward=-98750.03 +/- 19263.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24250239 |
|    mean velocity x | 0.0764      |
|    mean velocity y | 0.919       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.88e+04   |
| time/              |             |
|    total_timesteps | 252500      |
------------------------------------
Eval num_timesteps=253000, episode_reward=-84324.68 +/- 46289.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1602796 |
|    mean velocity x | -0.749     |
|    mean velocity y | -0.992     |
|    mean velocity z | 4.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 253000     |
-----------------------------------
Eval num_timesteps=253500, episode_reward=-66930.38 +/- 40631.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09000198 |
|    mean velocity x | -0.249      |
|    mean velocity y | 0.445       |
|    mean velocity z | 0.381       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.69e+04   |
| time/              |             |
|    total_timesteps | 253500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 124    |
|    time_elapsed    | 10287  |
|    total_timesteps | 253952 |
-------------------------------
Eval num_timesteps=254000, episode_reward=-75917.61 +/- 47994.02
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.29934034 |
|    mean velocity x      | -0.433      |
|    mean velocity y      | 0.172       |
|    mean velocity z      | 5.12        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -7.59e+04   |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.026902083 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.97       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.76e+07    |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.00239     |
|    std                  | 1.27        |
|    value_loss           | 7.14e+07    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-82574.91 +/- 42187.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3406177 |
|    mean velocity x | 0.37       |
|    mean velocity y | 1.23       |
|    mean velocity z | 3.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.26e+04  |
| time/              |            |
|    total_timesteps | 254500     |
-----------------------------------
Eval num_timesteps=255000, episode_reward=-50360.05 +/- 47511.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38419357 |
|    mean velocity x | 0.622       |
|    mean velocity y | 1.37        |
|    mean velocity z | 3.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.04e+04   |
| time/              |             |
|    total_timesteps | 255000      |
------------------------------------
Eval num_timesteps=255500, episode_reward=-51281.39 +/- 45368.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12896623 |
|    mean velocity x | 0.0583      |
|    mean velocity y | 0.207       |
|    mean velocity z | 0.344       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.13e+04   |
| time/              |             |
|    total_timesteps | 255500      |
------------------------------------
Eval num_timesteps=256000, episode_reward=-93905.59 +/- 23320.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37448484 |
|    mean velocity x | 0.662       |
|    mean velocity y | 1.5         |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.39e+04   |
| time/              |             |
|    total_timesteps | 256000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 125    |
|    time_elapsed    | 10386  |
|    total_timesteps | 256000 |
-------------------------------
Eval num_timesteps=256500, episode_reward=-44225.64 +/- 35838.30
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3152494  |
|    mean velocity x      | -0.0993     |
|    mean velocity y      | 0.566       |
|    mean velocity z      | 2.9         |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -4.42e+04   |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.005249521 |
|    clip_fraction        | 0.0581      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.96       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.58e+07    |
|    n_updates            | 1250        |
|    policy_gradient_loss | 0.00233     |
|    std                  | 1.27        |
|    value_loss           | 3.23e+07    |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=-89627.99 +/- 32085.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14215262 |
|    mean velocity x | -0.91       |
|    mean velocity y | -0.587      |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 257000      |
------------------------------------
Eval num_timesteps=257500, episode_reward=-60104.87 +/- 53156.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30467406 |
|    mean velocity x | -0.151      |
|    mean velocity y | 0.453       |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.01e+04   |
| time/              |             |
|    total_timesteps | 257500      |
------------------------------------
Eval num_timesteps=258000, episode_reward=-94595.98 +/- 39307.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29829773 |
|    mean velocity x | -0.573      |
|    mean velocity y | 0.277       |
|    mean velocity z | 4.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.46e+04   |
| time/              |             |
|    total_timesteps | 258000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 126    |
|    time_elapsed    | 10466  |
|    total_timesteps | 258048 |
-------------------------------
Eval num_timesteps=258500, episode_reward=-104794.89 +/- 34595.87
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3347494  |
|    mean velocity x      | -0.555      |
|    mean velocity y      | 0.0978      |
|    mean velocity z      | 4.65        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.05e+05   |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.093273595 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.96       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 2.54e+07    |
|    n_updates            | 1260        |
|    policy_gradient_loss | 0.021       |
|    std                  | 1.27        |
|    value_loss           | 6.78e+07    |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=-102411.01 +/- 60999.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24045587 |
|    mean velocity x | -0.319      |
|    mean velocity y | 0.276       |
|    mean velocity z | 3.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 259000      |
------------------------------------
Eval num_timesteps=259500, episode_reward=-94380.05 +/- 11013.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24846393 |
|    mean velocity x | 0.57        |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.44e+04   |
| time/              |             |
|    total_timesteps | 259500      |
------------------------------------
Eval num_timesteps=260000, episode_reward=-95075.53 +/- 49268.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3110617 |
|    mean velocity x | -0.324     |
|    mean velocity y | 0.344      |
|    mean velocity z | 4.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 260000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 127    |
|    time_elapsed    | 10547  |
|    total_timesteps | 260096 |
-------------------------------
Eval num_timesteps=260500, episode_reward=-76869.05 +/- 35265.76
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.19863969  |
|    mean velocity x      | -0.659       |
|    mean velocity y      | -0.204       |
|    mean velocity z      | 3.22         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.69e+04    |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0010263922 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.96        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.69e+07     |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.27         |
|    value_loss           | 6.05e+07     |
------------------------------------------
Eval num_timesteps=261000, episode_reward=-70440.61 +/- 28044.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4412943 |
|    mean velocity x | 0.265      |
|    mean velocity y | 1.17       |
|    mean velocity z | 2.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.04e+04  |
| time/              |            |
|    total_timesteps | 261000     |
-----------------------------------
Eval num_timesteps=261500, episode_reward=-55902.78 +/- 45003.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2591175 |
|    mean velocity x | 0.324      |
|    mean velocity y | 0.91       |
|    mean velocity z | 4.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.59e+04  |
| time/              |            |
|    total_timesteps | 261500     |
-----------------------------------
Eval num_timesteps=262000, episode_reward=-95115.89 +/- 16585.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3769048 |
|    mean velocity x | 0.06       |
|    mean velocity y | 1.11       |
|    mean velocity z | 4.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 262000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 128    |
|    time_elapsed    | 10627  |
|    total_timesteps | 262144 |
-------------------------------
Eval num_timesteps=262500, episode_reward=-92880.64 +/- 33699.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3253776   |
|    mean velocity x      | 0.176        |
|    mean velocity y      | 0.923        |
|    mean velocity z      | 3.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.29e+04    |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 0.0025211172 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.96        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 3.98e+07     |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00299     |
|    std                  | 1.27         |
|    value_loss           | 7.41e+07     |
------------------------------------------
Eval num_timesteps=263000, episode_reward=-80168.81 +/- 43157.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38760498 |
|    mean velocity x | 0.0552      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.02e+04   |
| time/              |             |
|    total_timesteps | 263000      |
------------------------------------
Eval num_timesteps=263500, episode_reward=-101490.01 +/- 28256.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13570718 |
|    mean velocity x | -0.477      |
|    mean velocity y | -0.394      |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 263500      |
------------------------------------
Eval num_timesteps=264000, episode_reward=-81313.23 +/- 28735.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24873434 |
|    mean velocity x | 0.654       |
|    mean velocity y | 1.31        |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 264000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 129    |
|    time_elapsed    | 10707  |
|    total_timesteps | 264192 |
-------------------------------
Eval num_timesteps=264500, episode_reward=-68070.04 +/- 90196.94
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43344805  |
|    mean velocity x      | -0.111       |
|    mean velocity y      | 0.975        |
|    mean velocity z      | 4.35         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.81e+04    |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0021338235 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.97        |
|    explained_variance   | 1.79e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 5.08e+07     |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00459     |
|    std                  | 1.27         |
|    value_loss           | 9.25e+07     |
------------------------------------------
Eval num_timesteps=265000, episode_reward=-64979.23 +/- 41029.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08201349 |
|    mean velocity x | -0.483      |
|    mean velocity y | -0.574      |
|    mean velocity z | 3.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 265000      |
------------------------------------
Eval num_timesteps=265500, episode_reward=-88344.24 +/- 25321.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2728251 |
|    mean velocity x | 2.55       |
|    mean velocity y | 5.68       |
|    mean velocity z | 12.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.83e+04  |
| time/              |            |
|    total_timesteps | 265500     |
-----------------------------------
Eval num_timesteps=266000, episode_reward=-114587.47 +/- 40221.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.9953357 |
|    mean velocity x | 1.93       |
|    mean velocity y | 3.94       |
|    mean velocity z | 8.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.15e+05  |
| time/              |            |
|    total_timesteps | 266000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 130    |
|    time_elapsed    | 10788  |
|    total_timesteps | 266240 |
-------------------------------
Eval num_timesteps=266500, episode_reward=-165793.74 +/- 102498.25
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15587586  |
|    mean velocity x      | 0.26         |
|    mean velocity y      | 0.18         |
|    mean velocity z      | 2.4          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.66e+05    |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0020595873 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.98        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 2.81e+07     |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 1.27         |
|    value_loss           | 5.55e+07     |
------------------------------------------
Eval num_timesteps=267000, episode_reward=-79937.64 +/- 22570.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51581717 |
|    mean velocity x | -0.131      |
|    mean velocity y | 1.56        |
|    mean velocity z | 7.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.99e+04   |
| time/              |             |
|    total_timesteps | 267000      |
------------------------------------
Eval num_timesteps=267500, episode_reward=-170390.61 +/- 99397.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46855915 |
|    mean velocity x | -0.0604     |
|    mean velocity y | 1.47        |
|    mean velocity z | 5.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.7e+05    |
| time/              |             |
|    total_timesteps | 267500      |
------------------------------------
Eval num_timesteps=268000, episode_reward=-183562.26 +/- 156792.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35355663 |
|    mean velocity x | 0.198       |
|    mean velocity y | 1.12        |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.84e+05   |
| time/              |             |
|    total_timesteps | 268000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 131    |
|    time_elapsed    | 10868  |
|    total_timesteps | 268288 |
-------------------------------
Eval num_timesteps=268500, episode_reward=-84698.61 +/- 46106.35
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.1334656  |
|    mean velocity x      | -0.201      |
|    mean velocity y      | 0.495       |
|    mean velocity z      | 0.49        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.47e+04   |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.001757329 |
|    clip_fraction        | 0.00371     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.98       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.14e+07    |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00365    |
|    std                  | 1.28        |
|    value_loss           | 7.68e+07    |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=-117403.71 +/- 54670.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18601632 |
|    mean velocity x | -0.433      |
|    mean velocity y | 0.498       |
|    mean velocity z | 1.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.17e+05   |
| time/              |             |
|    total_timesteps | 269000      |
------------------------------------
Eval num_timesteps=269500, episode_reward=-163041.78 +/- 100664.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1975093 |
|    mean velocity x | 1          |
|    mean velocity y | 3.82       |
|    mean velocity z | 12.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.63e+05  |
| time/              |            |
|    total_timesteps | 269500     |
-----------------------------------
Eval num_timesteps=270000, episode_reward=-87905.25 +/- 39117.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20370209 |
|    mean velocity x | -0.997      |
|    mean velocity y | -0.425      |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.79e+04   |
| time/              |             |
|    total_timesteps | 270000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 132    |
|    time_elapsed    | 10948  |
|    total_timesteps | 270336 |
-------------------------------
Eval num_timesteps=270500, episode_reward=-92801.99 +/- 17812.56
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.45930701  |
|    mean velocity x      | 0.545        |
|    mean velocity y      | 1.86         |
|    mean velocity z      | 3.59         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.28e+04    |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0022012382 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.98        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 2.89e+07     |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00386     |
|    std                  | 1.28         |
|    value_loss           | 6.06e+07     |
------------------------------------------
Eval num_timesteps=271000, episode_reward=-114427.54 +/- 70847.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25687563 |
|    mean velocity x | 0.169       |
|    mean velocity y | 0.777       |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 271000      |
------------------------------------
Eval num_timesteps=271500, episode_reward=-126091.53 +/- 183888.26
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.094355255 |
|    mean velocity x | 0.0328       |
|    mean velocity y | 0.254        |
|    mean velocity z | 0.263        |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -1.26e+05    |
| time/              |              |
|    total_timesteps | 271500       |
-------------------------------------
Eval num_timesteps=272000, episode_reward=-148415.91 +/- 75397.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28430915 |
|    mean velocity x | 0.0568      |
|    mean velocity y | 0.66        |
|    mean velocity z | 0.449       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.48e+05   |
| time/              |             |
|    total_timesteps | 272000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 133    |
|    time_elapsed    | 11029  |
|    total_timesteps | 272384 |
-------------------------------
Eval num_timesteps=272500, episode_reward=-47304.12 +/- 41178.19
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.6938667   |
|    mean velocity x      | 3.17         |
|    mean velocity y      | 7.27         |
|    mean velocity z      | 21.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.73e+04    |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 0.0019499772 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.98        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.07e+07     |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 1.28         |
|    value_loss           | 8.41e+07     |
------------------------------------------
Eval num_timesteps=273000, episode_reward=-129012.41 +/- 110140.20
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.129694 |
|    mean velocity x | -0.424    |
|    mean velocity y | 0.252     |
|    mean velocity z | 2.12      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.29e+05 |
| time/              |           |
|    total_timesteps | 273000    |
----------------------------------
Eval num_timesteps=273500, episode_reward=-83335.68 +/- 46246.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12338191 |
|    mean velocity x | -1.02       |
|    mean velocity y | -0.428      |
|    mean velocity z | 3.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.33e+04   |
| time/              |             |
|    total_timesteps | 273500      |
------------------------------------
Eval num_timesteps=274000, episode_reward=-80099.28 +/- 36336.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24863742 |
|    mean velocity x | 0.231       |
|    mean velocity y | 0.392       |
|    mean velocity z | 1.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 274000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 134    |
|    time_elapsed    | 11109  |
|    total_timesteps | 274432 |
-------------------------------
Eval num_timesteps=274500, episode_reward=-74264.40 +/- 56981.29
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.513222   |
|    mean velocity x      | -0.182      |
|    mean velocity y      | 1.47        |
|    mean velocity z      | 4.86        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -7.43e+04   |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.018964399 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.98       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.68e+07    |
|    n_updates            | 1340        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 1.27        |
|    value_loss           | 4.26e+07    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=-186145.74 +/- 93121.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3113146 |
|    mean velocity x | -0.0372    |
|    mean velocity y | 0.637      |
|    mean velocity z | 3.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.86e+05  |
| time/              |            |
|    total_timesteps | 275000     |
-----------------------------------
Eval num_timesteps=275500, episode_reward=-121359.99 +/- 112769.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.051594086 |
|    mean velocity x | -0.134      |
|    mean velocity y | -0.659      |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 275500      |
------------------------------------
Eval num_timesteps=276000, episode_reward=-129154.14 +/- 71387.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59009254 |
|    mean velocity x | 0.397       |
|    mean velocity y | 2.37        |
|    mean velocity z | 4.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.29e+05   |
| time/              |             |
|    total_timesteps | 276000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 135    |
|    time_elapsed    | 11189  |
|    total_timesteps | 276480 |
-------------------------------
Eval num_timesteps=276500, episode_reward=-57426.71 +/- 33134.95
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.16736774  |
|    mean velocity x      | 0.242        |
|    mean velocity y      | 0.969        |
|    mean velocity z      | 4.38         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.74e+04    |
| time/                   |              |
|    total_timesteps      | 276500       |
| train/                  |              |
|    approx_kl            | 0.0055124452 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.97        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.59e+07     |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00628     |
|    std                  | 1.27         |
|    value_loss           | 1.02e+08     |
------------------------------------------
Eval num_timesteps=277000, episode_reward=-54767.70 +/- 37007.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2721578 |
|    mean velocity x | 2.2        |
|    mean velocity y | 5.21       |
|    mean velocity z | 16.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.48e+04  |
| time/              |            |
|    total_timesteps | 277000     |
-----------------------------------
Eval num_timesteps=277500, episode_reward=-128640.90 +/- 93770.72
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6025991 |
|    mean velocity x | 0.184      |
|    mean velocity y | 2.16       |
|    mean velocity z | 6.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.29e+05  |
| time/              |            |
|    total_timesteps | 277500     |
-----------------------------------
Eval num_timesteps=278000, episode_reward=-101710.82 +/- 25710.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39241388 |
|    mean velocity x | 0.359       |
|    mean velocity y | 1.54        |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 278000      |
------------------------------------
Eval num_timesteps=278500, episode_reward=-94160.30 +/- 16790.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13082102 |
|    mean velocity x | -0.644      |
|    mean velocity y | -0.877      |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.42e+04   |
| time/              |             |
|    total_timesteps | 278500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 136    |
|    time_elapsed    | 11288  |
|    total_timesteps | 278528 |
-------------------------------
Eval num_timesteps=279000, episode_reward=-92724.77 +/- 16275.99
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43463233  |
|    mean velocity x      | 0.063        |
|    mean velocity y      | 1.34         |
|    mean velocity z      | 3.96         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.27e+04    |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0015532956 |
|    clip_fraction        | 0.00322      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.98        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.74e+07     |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 1.27         |
|    value_loss           | 1.04e+08     |
------------------------------------------
Eval num_timesteps=279500, episode_reward=-85452.37 +/- 9735.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1412221 |
|    mean velocity x | 1.99       |
|    mean velocity y | 5.24       |
|    mean velocity z | 11.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.55e+04  |
| time/              |            |
|    total_timesteps | 279500     |
-----------------------------------
Eval num_timesteps=280000, episode_reward=-53673.49 +/- 26563.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33182237 |
|    mean velocity x | 0.636       |
|    mean velocity y | 1.11        |
|    mean velocity z | 2.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.37e+04   |
| time/              |             |
|    total_timesteps | 280000      |
------------------------------------
Eval num_timesteps=280500, episode_reward=-90017.61 +/- 27554.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32315516 |
|    mean velocity x | 0.163       |
|    mean velocity y | 0.649       |
|    mean velocity z | 2.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9e+04      |
| time/              |             |
|    total_timesteps | 280500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 137    |
|    time_elapsed    | 11371  |
|    total_timesteps | 280576 |
-------------------------------
Eval num_timesteps=281000, episode_reward=-91421.51 +/- 22742.37
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36960936  |
|    mean velocity x      | 0.0387       |
|    mean velocity y      | 1.33         |
|    mean velocity z      | 4.19         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.14e+04    |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0021719662 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.97        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.14e+07     |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00395     |
|    std                  | 1.27         |
|    value_loss           | 6.61e+07     |
------------------------------------------
Eval num_timesteps=281500, episode_reward=-48186.50 +/- 49452.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22660941 |
|    mean velocity x | -0.354      |
|    mean velocity y | 0.131       |
|    mean velocity z | 3.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.82e+04   |
| time/              |             |
|    total_timesteps | 281500      |
------------------------------------
Eval num_timesteps=282000, episode_reward=-81280.36 +/- 43243.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07396824 |
|    mean velocity x | -1.12       |
|    mean velocity y | -0.752      |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 282000      |
------------------------------------
Eval num_timesteps=282500, episode_reward=-80435.10 +/- 36011.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21615332 |
|    mean velocity x | 0.156       |
|    mean velocity y | 0.361       |
|    mean velocity z | 1.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.04e+04   |
| time/              |             |
|    total_timesteps | 282500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 138    |
|    time_elapsed    | 11451  |
|    total_timesteps | 282624 |
-------------------------------
Eval num_timesteps=283000, episode_reward=-79247.69 +/- 21813.91
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.14032678 |
|    mean velocity x      | 0.00243     |
|    mean velocity y      | 0.467       |
|    mean velocity z      | 0.071       |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -7.92e+04   |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.3732249   |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.01       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.48e+07    |
|    n_updates            | 1380        |
|    policy_gradient_loss | 0.0152      |
|    std                  | 1.31        |
|    value_loss           | 2.63e+07    |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=-30129.15 +/- 37723.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18932998 |
|    mean velocity x | -0.246      |
|    mean velocity y | 0.571       |
|    mean velocity z | 0.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.01e+04   |
| time/              |             |
|    total_timesteps | 283500      |
------------------------------------
New best mean reward!
Eval num_timesteps=284000, episode_reward=-92901.39 +/- 54315.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4491081 |
|    mean velocity x | -0.0402    |
|    mean velocity y | 1.21       |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.29e+04  |
| time/              |            |
|    total_timesteps | 284000     |
-----------------------------------
Eval num_timesteps=284500, episode_reward=-106075.85 +/- 56248.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29201728 |
|    mean velocity x | 0.168       |
|    mean velocity y | 0.774       |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 284500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 139    |
|    time_elapsed    | 11531  |
|    total_timesteps | 284672 |
-------------------------------
Eval num_timesteps=285000, episode_reward=-88467.69 +/- 33772.50
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46315747  |
|    mean velocity x      | -0.0303      |
|    mean velocity y      | 1.18         |
|    mean velocity z      | 4.61         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.85e+04    |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0034876917 |
|    clip_fraction        | 0.0492       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.06        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 5.88e+07     |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 1.31         |
|    value_loss           | 7.51e+07     |
------------------------------------------
Eval num_timesteps=285500, episode_reward=-82036.63 +/- 50147.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45728642 |
|    mean velocity x | 0.128       |
|    mean velocity y | 1.36        |
|    mean velocity z | 4.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 285500      |
------------------------------------
Eval num_timesteps=286000, episode_reward=-109326.85 +/- 33520.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41322964 |
|    mean velocity x | 0.578       |
|    mean velocity y | 1.58        |
|    mean velocity z | 3.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 286000      |
------------------------------------
Eval num_timesteps=286500, episode_reward=-71704.76 +/- 60614.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20270908 |
|    mean velocity x | -0.59       |
|    mean velocity y | -0.0388     |
|    mean velocity z | 2.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.17e+04   |
| time/              |             |
|    total_timesteps | 286500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 140    |
|    time_elapsed    | 11611  |
|    total_timesteps | 286720 |
-------------------------------
Eval num_timesteps=287000, episode_reward=-92948.36 +/- 42182.28
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.31388617 |
|    mean velocity x      | 0.0592      |
|    mean velocity y      | 0.838       |
|    mean velocity z      | 5.07        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.29e+04   |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.017546138 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.07       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.88e+06    |
|    n_updates            | 1400        |
|    policy_gradient_loss | 0.00214     |
|    std                  | 1.31        |
|    value_loss           | 8.3e+07     |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=-53595.77 +/- 46741.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40738717 |
|    mean velocity x | 0.92        |
|    mean velocity y | 1.53        |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.36e+04   |
| time/              |             |
|    total_timesteps | 287500      |
------------------------------------
Eval num_timesteps=288000, episode_reward=-86242.13 +/- 13535.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44409657 |
|    mean velocity x | 0.032       |
|    mean velocity y | 1.2         |
|    mean velocity z | 3.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 288000      |
------------------------------------
Eval num_timesteps=288500, episode_reward=-101436.40 +/- 56711.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35818785 |
|    mean velocity x | -0.415      |
|    mean velocity y | 0.0683      |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 288500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 141    |
|    time_elapsed    | 11691  |
|    total_timesteps | 288768 |
-------------------------------
Eval num_timesteps=289000, episode_reward=-109435.35 +/- 52928.76
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -0.2031992 |
|    mean velocity x      | 0.254      |
|    mean velocity y      | 0.348      |
|    mean velocity z      | 0.845      |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -1.09e+05  |
| time/                   |            |
|    total_timesteps      | 289000     |
| train/                  |            |
|    approx_kl            | 0.19198403 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.07      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 2.63e+07   |
|    n_updates            | 1410       |
|    policy_gradient_loss | 0.00746    |
|    std                  | 1.32       |
|    value_loss           | 3.78e+07   |
----------------------------------------
Eval num_timesteps=289500, episode_reward=-98328.95 +/- 15056.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49622568 |
|    mean velocity x | 0.278       |
|    mean velocity y | 1.3         |
|    mean velocity z | 4.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.83e+04   |
| time/              |             |
|    total_timesteps | 289500      |
------------------------------------
Eval num_timesteps=290000, episode_reward=-108377.08 +/- 22545.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51357365 |
|    mean velocity x | -0.443      |
|    mean velocity y | 0.92        |
|    mean velocity z | 5.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 290000      |
------------------------------------
Eval num_timesteps=290500, episode_reward=-78493.74 +/- 58643.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30746916 |
|    mean velocity x | 0.184       |
|    mean velocity y | 0.656       |
|    mean velocity z | 5.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.85e+04   |
| time/              |             |
|    total_timesteps | 290500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 142    |
|    time_elapsed    | 11772  |
|    total_timesteps | 290816 |
-------------------------------
Eval num_timesteps=291000, episode_reward=-95121.07 +/- 49458.51
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5381639   |
|    mean velocity x      | -0.165       |
|    mean velocity y      | 1.52         |
|    mean velocity z      | 6.18         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.51e+04    |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0019912154 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.08        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 4.81e+07     |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.32         |
|    value_loss           | 8.56e+07     |
------------------------------------------
Eval num_timesteps=291500, episode_reward=-83586.85 +/- 41211.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3955758 |
|    mean velocity x | 0.108      |
|    mean velocity y | 1.74       |
|    mean velocity z | 4.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.36e+04  |
| time/              |            |
|    total_timesteps | 291500     |
-----------------------------------
Eval num_timesteps=292000, episode_reward=-109439.33 +/- 32247.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6901224 |
|    mean velocity x | -0.744     |
|    mean velocity y | 1.11       |
|    mean velocity z | 6.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 292000     |
-----------------------------------
Eval num_timesteps=292500, episode_reward=-127322.20 +/- 51956.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45468035 |
|    mean velocity x | -0.0731     |
|    mean velocity y | 1.35        |
|    mean velocity z | 6.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.27e+05   |
| time/              |             |
|    total_timesteps | 292500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 143    |
|    time_elapsed    | 11852  |
|    total_timesteps | 292864 |
-------------------------------
Eval num_timesteps=293000, episode_reward=-113847.91 +/- 53846.48
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.36121026 |
|    mean velocity x      | -0.676      |
|    mean velocity y      | 0.31        |
|    mean velocity z      | 4.57        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.14e+05   |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.00084878  |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.96e+07    |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00288    |
|    std                  | 1.32        |
|    value_loss           | 9.63e+07    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=-98140.68 +/- 25209.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32935053 |
|    mean velocity x | -0.058      |
|    mean velocity y | 0.977       |
|    mean velocity z | 6.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.81e+04   |
| time/              |             |
|    total_timesteps | 293500      |
------------------------------------
Eval num_timesteps=294000, episode_reward=-143422.84 +/- 40850.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52561855 |
|    mean velocity x | -0.401      |
|    mean velocity y | 1.18        |
|    mean velocity z | 6.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.43e+05   |
| time/              |             |
|    total_timesteps | 294000      |
------------------------------------
Eval num_timesteps=294500, episode_reward=-118184.54 +/- 23012.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33901954 |
|    mean velocity x | 0.183       |
|    mean velocity y | 0.562       |
|    mean velocity z | 3.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 294500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 144    |
|    time_elapsed    | 11932  |
|    total_timesteps | 294912 |
-------------------------------
Eval num_timesteps=295000, episode_reward=-111955.54 +/- 29627.56
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.54691607 |
|    mean velocity x      | 0.531       |
|    mean velocity y      | 1.66        |
|    mean velocity z      | 4.96        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.12e+05   |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.004262409 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.08       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 5.34e+07    |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00509    |
|    std                  | 1.32        |
|    value_loss           | 9.37e+07    |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=-77594.37 +/- 57179.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46412262 |
|    mean velocity x | -1.03       |
|    mean velocity y | 0.46        |
|    mean velocity z | 5.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.76e+04   |
| time/              |             |
|    total_timesteps | 295500      |
------------------------------------
Eval num_timesteps=296000, episode_reward=-96801.34 +/- 56659.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5438673 |
|    mean velocity x | 0.366      |
|    mean velocity y | 1.64       |
|    mean velocity z | 4.98       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.68e+04  |
| time/              |            |
|    total_timesteps | 296000     |
-----------------------------------
Eval num_timesteps=296500, episode_reward=-105580.98 +/- 53357.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5767205 |
|    mean velocity x | -0.292     |
|    mean velocity y | 1.21       |
|    mean velocity z | 7.49       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 296500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 145    |
|    time_elapsed    | 12013  |
|    total_timesteps | 296960 |
-------------------------------
Eval num_timesteps=297000, episode_reward=-119722.84 +/- 25930.24
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.41720685 |
|    mean velocity x      | 0.144       |
|    mean velocity y      | 1.32        |
|    mean velocity z      | 5.4         |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.2e+05    |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.05765485  |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.19e+07    |
|    n_updates            | 1450        |
|    policy_gradient_loss | 0.0136      |
|    std                  | 1.33        |
|    value_loss           | 1e+08       |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=-115175.35 +/- 59637.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5310616 |
|    mean velocity x | -0.308     |
|    mean velocity y | 0.77       |
|    mean velocity z | 6.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.15e+05  |
| time/              |            |
|    total_timesteps | 297500     |
-----------------------------------
Eval num_timesteps=298000, episode_reward=-103104.36 +/- 17267.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.65751755 |
|    mean velocity x | -0.627      |
|    mean velocity y | 1.34        |
|    mean velocity z | 6.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 298000      |
------------------------------------
Eval num_timesteps=298500, episode_reward=-106028.18 +/- 61335.42
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.295886 |
|    mean velocity x | -1.26     |
|    mean velocity y | -1.15     |
|    mean velocity z | 7.14      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.06e+05 |
| time/              |           |
|    total_timesteps | 298500    |
----------------------------------
Eval num_timesteps=299000, episode_reward=-75263.01 +/- 27899.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45471522 |
|    mean velocity x | -0.678      |
|    mean velocity y | 0.335       |
|    mean velocity z | 5.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.53e+04   |
| time/              |             |
|    total_timesteps | 299000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 146    |
|    time_elapsed    | 12112  |
|    total_timesteps | 299008 |
-------------------------------
Eval num_timesteps=299500, episode_reward=-116804.10 +/- 50561.98
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47324994  |
|    mean velocity x      | -0.888       |
|    mean velocity y      | -0.142       |
|    mean velocity z      | 5.27         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.17e+05    |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0012626831 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.09        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.93e+07     |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00302     |
|    std                  | 1.33         |
|    value_loss           | 1e+08        |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-90541.66 +/- 74898.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6772137 |
|    mean velocity x | -0.409     |
|    mean velocity y | 1.44       |
|    mean velocity z | 7.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.05e+04  |
| time/              |            |
|    total_timesteps | 300000     |
-----------------------------------
Eval num_timesteps=300500, episode_reward=-98593.44 +/- 52819.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38049716 |
|    mean velocity x | -0.336      |
|    mean velocity y | 0.172       |
|    mean velocity z | 5.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.86e+04   |
| time/              |             |
|    total_timesteps | 300500      |
------------------------------------
Eval num_timesteps=301000, episode_reward=-129198.19 +/- 29106.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44942063 |
|    mean velocity x | -0.169      |
|    mean velocity y | 1.42        |
|    mean velocity z | 5.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.29e+05   |
| time/              |             |
|    total_timesteps | 301000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 147    |
|    time_elapsed    | 12193  |
|    total_timesteps | 301056 |
-------------------------------
Eval num_timesteps=301500, episode_reward=-81243.95 +/- 56279.69
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4012489   |
|    mean velocity x      | -0.209       |
|    mean velocity y      | 0.69         |
|    mean velocity z      | 6.16         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.12e+04    |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 0.0025123213 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.42e+07     |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00574     |
|    std                  | 1.33         |
|    value_loss           | 1.29e+08     |
------------------------------------------
Eval num_timesteps=302000, episode_reward=-125422.81 +/- 58753.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53022695 |
|    mean velocity x | -0.507      |
|    mean velocity y | 0.919       |
|    mean velocity z | 6.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.25e+05   |
| time/              |             |
|    total_timesteps | 302000      |
------------------------------------
Eval num_timesteps=302500, episode_reward=-83090.40 +/- 41748.26
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.582407 |
|    mean velocity x | -0.469    |
|    mean velocity y | 1.65      |
|    mean velocity z | 4.75      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.31e+04 |
| time/              |           |
|    total_timesteps | 302500    |
----------------------------------
Eval num_timesteps=303000, episode_reward=-135809.68 +/- 37317.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37180346 |
|    mean velocity x | 0.0461      |
|    mean velocity y | 0.829       |
|    mean velocity z | 5.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.36e+05   |
| time/              |             |
|    total_timesteps | 303000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 148    |
|    time_elapsed    | 12273  |
|    total_timesteps | 303104 |
-------------------------------
Eval num_timesteps=303500, episode_reward=-64294.54 +/- 40614.72
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5035679   |
|    mean velocity x      | -0.252       |
|    mean velocity y      | 1.06         |
|    mean velocity z      | 6.5          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.43e+04    |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0032011108 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.1         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.66e+07     |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00386     |
|    std                  | 1.33         |
|    value_loss           | 1.25e+08     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=-79310.12 +/- 62437.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45859855 |
|    mean velocity x | 0.0137      |
|    mean velocity y | 1.24        |
|    mean velocity z | 5.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.93e+04   |
| time/              |             |
|    total_timesteps | 304000      |
------------------------------------
Eval num_timesteps=304500, episode_reward=-100585.43 +/- 27194.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2021591 |
|    mean velocity x | -0.0606    |
|    mean velocity y | 0.525      |
|    mean velocity z | 0.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 304500     |
-----------------------------------
Eval num_timesteps=305000, episode_reward=-130653.36 +/- 37502.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34050077 |
|    mean velocity x | 0.373       |
|    mean velocity y | 1.17        |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.31e+05   |
| time/              |             |
|    total_timesteps | 305000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 149    |
|    time_elapsed    | 12353  |
|    total_timesteps | 305152 |
-------------------------------
Eval num_timesteps=305500, episode_reward=-129053.43 +/- 29672.07
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -0.4518464 |
|    mean velocity x      | -0.295     |
|    mean velocity y      | 0.842      |
|    mean velocity z      | 6.13       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -1.29e+05  |
| time/                   |            |
|    total_timesteps      | 305500     |
| train/                  |            |
|    approx_kl            | 0.52518404 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.09      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 3.63e+07   |
|    n_updates            | 1490       |
|    policy_gradient_loss | 0.0185     |
|    std                  | 1.31       |
|    value_loss           | 9.32e+07   |
----------------------------------------
Eval num_timesteps=306000, episode_reward=-145928.12 +/- 13318.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43222937 |
|    mean velocity x | 0.836       |
|    mean velocity y | 1.89        |
|    mean velocity z | 3.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.46e+05   |
| time/              |             |
|    total_timesteps | 306000      |
------------------------------------
Eval num_timesteps=306500, episode_reward=-94784.49 +/- 42226.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20015533 |
|    mean velocity x | -0.637      |
|    mean velocity y | -0.478      |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.48e+04   |
| time/              |             |
|    total_timesteps | 306500      |
------------------------------------
Eval num_timesteps=307000, episode_reward=-86806.56 +/- 47394.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18075372 |
|    mean velocity x | -0.182      |
|    mean velocity y | 0.116       |
|    mean velocity z | 3.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.68e+04   |
| time/              |             |
|    total_timesteps | 307000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 150    |
|    time_elapsed    | 12434  |
|    total_timesteps | 307200 |
-------------------------------
Eval num_timesteps=307500, episode_reward=-46402.04 +/- 36568.37
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3623453  |
|    mean velocity x      | 0.357       |
|    mean velocity y      | 1.12        |
|    mean velocity z      | 4.83        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -4.64e+04   |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.004646777 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 3.6e+07     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00104    |
|    std                  | 1.31        |
|    value_loss           | 5.94e+07    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=-99742.16 +/- 50729.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.9343598 |
|    mean velocity x | 1.34       |
|    mean velocity y | 3.52       |
|    mean velocity z | 10.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.97e+04  |
| time/              |            |
|    total_timesteps | 308000     |
-----------------------------------
Eval num_timesteps=308500, episode_reward=-129110.47 +/- 34318.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19028302 |
|    mean velocity x | 0.0369      |
|    mean velocity y | 0.452       |
|    mean velocity z | 0.172       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.29e+05   |
| time/              |             |
|    total_timesteps | 308500      |
------------------------------------
Eval num_timesteps=309000, episode_reward=-113775.52 +/- 26313.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57669175 |
|    mean velocity x | 0.483       |
|    mean velocity y | 1.82        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 309000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 151    |
|    time_elapsed    | 12514  |
|    total_timesteps | 309248 |
-------------------------------
Eval num_timesteps=309500, episode_reward=-84659.22 +/- 61384.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15360318  |
|    mean velocity x      | -0.209       |
|    mean velocity y      | 0.588        |
|    mean velocity z      | 0.772        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.47e+04    |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 0.0041019954 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.05        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+07     |
|    n_updates            | 1510         |
|    policy_gradient_loss | 0.0066       |
|    std                  | 1.31         |
|    value_loss           | 3.85e+07     |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-129949.46 +/- 61585.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.88914794 |
|    mean velocity x | 0.61        |
|    mean velocity y | 2.28        |
|    mean velocity z | 9.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.3e+05    |
| time/              |             |
|    total_timesteps | 310000      |
------------------------------------
Eval num_timesteps=310500, episode_reward=-92335.08 +/- 26957.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1778565 |
|    mean velocity x | -1.29      |
|    mean velocity y | -1.09      |
|    mean velocity z | 5.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.23e+04  |
| time/              |            |
|    total_timesteps | 310500     |
-----------------------------------
Eval num_timesteps=311000, episode_reward=-117695.91 +/- 32073.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6531271 |
|    mean velocity x | 0.0763     |
|    mean velocity y | 1.98       |
|    mean velocity z | 5.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 311000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 152    |
|    time_elapsed    | 12594  |
|    total_timesteps | 311296 |
-------------------------------
Eval num_timesteps=311500, episode_reward=-95067.99 +/- 31204.38
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44190052  |
|    mean velocity x      | 0.463        |
|    mean velocity y      | 1.89         |
|    mean velocity z      | 5.12         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.51e+04    |
| time/                   |              |
|    total_timesteps      | 311500       |
| train/                  |              |
|    approx_kl            | 0.0033330037 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.06        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 6.21e+07     |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.00389     |
|    std                  | 1.31         |
|    value_loss           | 7.76e+07     |
------------------------------------------
Eval num_timesteps=312000, episode_reward=-111582.08 +/- 28127.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46112657 |
|    mean velocity x | 0.494       |
|    mean velocity y | 1.91        |
|    mean velocity z | 4.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 312000      |
------------------------------------
Eval num_timesteps=312500, episode_reward=-112083.75 +/- 58701.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1029854 |
|    mean velocity x | 0.0866     |
|    mean velocity y | 0.37       |
|    mean velocity z | 0.145      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 312500     |
-----------------------------------
Eval num_timesteps=313000, episode_reward=-124308.86 +/- 28367.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43702042 |
|    mean velocity x | -0.152      |
|    mean velocity y | 0.818       |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.24e+05   |
| time/              |             |
|    total_timesteps | 313000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 153    |
|    time_elapsed    | 12674  |
|    total_timesteps | 313344 |
-------------------------------
Eval num_timesteps=313500, episode_reward=-94796.75 +/- 41877.94
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.4358448  |
|    mean velocity x      | -0.139      |
|    mean velocity y      | 1.35        |
|    mean velocity z      | 4.51        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.48e+04   |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.023096215 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.27e+07    |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.0166      |
|    std                  | 1.31        |
|    value_loss           | 4.43e+07    |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=-119762.02 +/- 20668.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44055942 |
|    mean velocity x | 0.273       |
|    mean velocity y | 1.69        |
|    mean velocity z | 5.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 314000      |
------------------------------------
Eval num_timesteps=314500, episode_reward=-70483.78 +/- 58316.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.65717834 |
|    mean velocity x | -0.108      |
|    mean velocity y | 1.51        |
|    mean velocity z | 6           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 314500      |
------------------------------------
Eval num_timesteps=315000, episode_reward=-93107.90 +/- 73792.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7270804 |
|    mean velocity x | -0.0423    |
|    mean velocity y | 1.51       |
|    mean velocity z | 6.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.31e+04  |
| time/              |            |
|    total_timesteps | 315000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 154    |
|    time_elapsed    | 12755  |
|    total_timesteps | 315392 |
-------------------------------
Eval num_timesteps=315500, episode_reward=-118417.84 +/- 54005.16
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.53523594  |
|    mean velocity x      | 0.285        |
|    mean velocity y      | 2.18         |
|    mean velocity z      | 4.8          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.18e+05    |
| time/                   |              |
|    total_timesteps      | 315500       |
| train/                  |              |
|    approx_kl            | 0.0010120847 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+07     |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00342     |
|    std                  | 1.31         |
|    value_loss           | 1.08e+08     |
------------------------------------------
Eval num_timesteps=316000, episode_reward=-110784.41 +/- 43817.46
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0522715 |
|    mean velocity x | 1.28       |
|    mean velocity y | 4.37       |
|    mean velocity z | 10.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.11e+05  |
| time/              |            |
|    total_timesteps | 316000     |
-----------------------------------
Eval num_timesteps=316500, episode_reward=-133593.15 +/- 20097.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.73370653 |
|    mean velocity x | 0.214       |
|    mean velocity y | 2.37        |
|    mean velocity z | 6.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.34e+05   |
| time/              |             |
|    total_timesteps | 316500      |
------------------------------------
Eval num_timesteps=317000, episode_reward=-104665.98 +/- 56823.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37612554 |
|    mean velocity x | -1.68       |
|    mean velocity y | -0.638      |
|    mean velocity z | 6.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 317000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 155    |
|    time_elapsed    | 12835  |
|    total_timesteps | 317440 |
-------------------------------
Eval num_timesteps=317500, episode_reward=-60291.81 +/- 56997.11
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6440769   |
|    mean velocity x      | 0.347        |
|    mean velocity y      | 2.13         |
|    mean velocity z      | 6.14         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.03e+04    |
| time/                   |              |
|    total_timesteps      | 317500       |
| train/                  |              |
|    approx_kl            | 0.0038550873 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.05        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 4.01e+07     |
|    n_updates            | 1550         |
|    policy_gradient_loss | -0.0053      |
|    std                  | 1.31         |
|    value_loss           | 1.05e+08     |
------------------------------------------
Eval num_timesteps=318000, episode_reward=-81499.63 +/- 36388.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.67381006 |
|    mean velocity x | -0.0934     |
|    mean velocity y | 1.65        |
|    mean velocity z | 4.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.15e+04   |
| time/              |             |
|    total_timesteps | 318000      |
------------------------------------
Eval num_timesteps=318500, episode_reward=-119823.19 +/- 44926.24
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.320458 |
|    mean velocity x | 1.82      |
|    mean velocity y | 5.03      |
|    mean velocity z | 10.7      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.2e+05  |
| time/              |           |
|    total_timesteps | 318500    |
----------------------------------
Eval num_timesteps=319000, episode_reward=-109550.24 +/- 52372.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47516456 |
|    mean velocity x | -0.78       |
|    mean velocity y | 0.249       |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 319000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 156    |
|    time_elapsed    | 12915  |
|    total_timesteps | 319488 |
-------------------------------
Eval num_timesteps=319500, episode_reward=-86517.25 +/- 60971.18
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.39036506  |
|    mean velocity x      | -0.827       |
|    mean velocity y      | 0.0328       |
|    mean velocity z      | 3.63         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.65e+04    |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0013529855 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.05        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.89e+07     |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.31         |
|    value_loss           | 3.83e+07     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-104152.73 +/- 17840.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6549346 |
|    mean velocity x | 0.017      |
|    mean velocity y | 1.94       |
|    mean velocity z | 6.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 320000     |
-----------------------------------
Eval num_timesteps=320500, episode_reward=-60787.44 +/- 63332.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6422852 |
|    mean velocity x | 0.0723     |
|    mean velocity y | 2.03       |
|    mean velocity z | 6.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.08e+04  |
| time/              |            |
|    total_timesteps | 320500     |
-----------------------------------
Eval num_timesteps=321000, episode_reward=-94191.31 +/- 20914.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32887343 |
|    mean velocity x | -0.11       |
|    mean velocity y | 0.654       |
|    mean velocity z | 1.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.42e+04   |
| time/              |             |
|    total_timesteps | 321000      |
------------------------------------
Eval num_timesteps=321500, episode_reward=-121232.50 +/- 38872.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56624836 |
|    mean velocity x | 0.21        |
|    mean velocity y | 1.63        |
|    mean velocity z | 5.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 321500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 157    |
|    time_elapsed    | 13015  |
|    total_timesteps | 321536 |
-------------------------------
Eval num_timesteps=322000, episode_reward=-82642.36 +/- 48527.80
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.69925255 |
|    mean velocity x      | -0.2        |
|    mean velocity y      | 1.65        |
|    mean velocity z      | 4.86        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.26e+04   |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.009033067 |
|    clip_fraction        | 0.0493      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 4.61e+07    |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.00127    |
|    std                  | 1.31        |
|    value_loss           | 1.16e+08    |
-----------------------------------------
Eval num_timesteps=322500, episode_reward=-82933.23 +/- 28511.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51992595 |
|    mean velocity x | 0.048       |
|    mean velocity y | 1.75        |
|    mean velocity z | 4.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 322500      |
------------------------------------
Eval num_timesteps=323000, episode_reward=-94123.72 +/- 60711.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.77533305 |
|    mean velocity x | -0.146      |
|    mean velocity y | 2.07        |
|    mean velocity z | 6.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.41e+04   |
| time/              |             |
|    total_timesteps | 323000      |
------------------------------------
Eval num_timesteps=323500, episode_reward=-93533.28 +/- 52621.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20494674 |
|    mean velocity x | -0.37       |
|    mean velocity y | 0.441       |
|    mean velocity z | 1.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.35e+04   |
| time/              |             |
|    total_timesteps | 323500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 158    |
|    time_elapsed    | 13095  |
|    total_timesteps | 323584 |
-------------------------------
Eval num_timesteps=324000, episode_reward=-155034.43 +/- 16472.04
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.36970365 |
|    mean velocity x      | -0.651      |
|    mean velocity y      | 0.724       |
|    mean velocity z      | 3.59        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.55e+05   |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.007154721 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.06       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.49e+07    |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00522    |
|    std                  | 1.31        |
|    value_loss           | 5.04e+07    |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=-126837.35 +/- 32958.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3682167 |
|    mean velocity x | 0.0206     |
|    mean velocity y | 0.826      |
|    mean velocity z | 1.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.27e+05  |
| time/              |            |
|    total_timesteps | 324500     |
-----------------------------------
Eval num_timesteps=325000, episode_reward=-141761.50 +/- 25919.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49849042 |
|    mean velocity x | 0.673       |
|    mean velocity y | 1.68        |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.42e+05   |
| time/              |             |
|    total_timesteps | 325000      |
------------------------------------
Eval num_timesteps=325500, episode_reward=-88472.58 +/- 44536.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36505175 |
|    mean velocity x | -1.23       |
|    mean velocity y | -0.0986     |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 325500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 159    |
|    time_elapsed    | 13175  |
|    total_timesteps | 325632 |
-------------------------------
Eval num_timesteps=326000, episode_reward=-82884.20 +/- 46754.53
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -0.6693425 |
|    mean velocity x      | 0.613      |
|    mean velocity y      | 2.2        |
|    mean velocity z      | 4.75       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -8.29e+04  |
| time/                   |            |
|    total_timesteps      | 326000     |
| train/                  |            |
|    approx_kl            | 0.01357673 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.08      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 2.17e+07   |
|    n_updates            | 1590       |
|    policy_gradient_loss | 0.00464    |
|    std                  | 1.33       |
|    value_loss           | 3.3e+07    |
----------------------------------------
Eval num_timesteps=326500, episode_reward=-92460.53 +/- 32801.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.98598665 |
|    mean velocity x | 0.951       |
|    mean velocity y | 3.49        |
|    mean velocity z | 10.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.25e+04   |
| time/              |             |
|    total_timesteps | 326500      |
------------------------------------
Eval num_timesteps=327000, episode_reward=-87927.69 +/- 18386.30
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.732801 |
|    mean velocity x | 3         |
|    mean velocity y | 7.6       |
|    mean velocity z | 18.9      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.79e+04 |
| time/              |           |
|    total_timesteps | 327000    |
----------------------------------
Eval num_timesteps=327500, episode_reward=-105548.77 +/- 19260.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49138188 |
|    mean velocity x | 0.55        |
|    mean velocity y | 1.85        |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 327500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 160    |
|    time_elapsed    | 13255  |
|    total_timesteps | 327680 |
-------------------------------
Eval num_timesteps=328000, episode_reward=-70207.89 +/- 50721.20
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.13879545 |
|    mean velocity x      | 0.018       |
|    mean velocity y      | 0.14        |
|    mean velocity z      | 0.384       |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -7.02e+04   |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.004194922 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.1        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.88e+07    |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00591    |
|    std                  | 1.33        |
|    value_loss           | 7.91e+07    |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=-91031.15 +/- 43121.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6948595 |
|    mean velocity x | 0.58       |
|    mean velocity y | 2.91       |
|    mean velocity z | 6.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.1e+04   |
| time/              |            |
|    total_timesteps | 328500     |
-----------------------------------
Eval num_timesteps=329000, episode_reward=-122821.03 +/- 30780.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39975342 |
|    mean velocity x | -0.686      |
|    mean velocity y | 0.346       |
|    mean velocity z | 3.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 329000      |
------------------------------------
Eval num_timesteps=329500, episode_reward=-56726.00 +/- 36596.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4650811 |
|    mean velocity x | 1.51       |
|    mean velocity y | 4.94       |
|    mean velocity z | 13.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.67e+04  |
| time/              |            |
|    total_timesteps | 329500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 161    |
|    time_elapsed    | 13336  |
|    total_timesteps | 329728 |
-------------------------------
Eval num_timesteps=330000, episode_reward=-99656.35 +/- 25572.08
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.2994158   |
|    mean velocity x      | 1.91         |
|    mean velocity y      | 5.24         |
|    mean velocity z      | 14.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.97e+04    |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0028546804 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.1         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.87e+07     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00341     |
|    std                  | 1.33         |
|    value_loss           | 8.43e+07     |
------------------------------------------
Eval num_timesteps=330500, episode_reward=-121007.22 +/- 54716.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35033894 |
|    mean velocity x | 0.229       |
|    mean velocity y | 0.932       |
|    mean velocity z | 4.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 330500      |
------------------------------------
Eval num_timesteps=331000, episode_reward=-101225.72 +/- 20275.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6491938 |
|    mean velocity x | 3.22       |
|    mean velocity y | 6.85       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 331000     |
-----------------------------------
Eval num_timesteps=331500, episode_reward=-114013.69 +/- 20781.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46612555 |
|    mean velocity x | 0.723       |
|    mean velocity y | 1.69        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 331500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 162    |
|    time_elapsed    | 13416  |
|    total_timesteps | 331776 |
-------------------------------
Eval num_timesteps=332000, episode_reward=-75588.97 +/- 40303.47
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42823938  |
|    mean velocity x      | -0.97        |
|    mean velocity y      | -0.108       |
|    mean velocity z      | 4.95         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.56e+04    |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0028560397 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.11        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 5.16e+07     |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00371     |
|    std                  | 1.34         |
|    value_loss           | 9.54e+07     |
------------------------------------------
Eval num_timesteps=332500, episode_reward=-92157.54 +/- 49887.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5607045 |
|    mean velocity x | 0.228      |
|    mean velocity y | 1.44       |
|    mean velocity z | 5.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.22e+04  |
| time/              |            |
|    total_timesteps | 332500     |
-----------------------------------
Eval num_timesteps=333000, episode_reward=-75192.75 +/- 43706.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21882814 |
|    mean velocity x | 0.0994      |
|    mean velocity y | 0.402       |
|    mean velocity z | 0.763       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.52e+04   |
| time/              |             |
|    total_timesteps | 333000      |
------------------------------------
Eval num_timesteps=333500, episode_reward=-96472.40 +/- 43517.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6390127 |
|    mean velocity x | 0.353      |
|    mean velocity y | 2.59       |
|    mean velocity z | 5.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.65e+04  |
| time/              |            |
|    total_timesteps | 333500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 163    |
|    time_elapsed    | 13496  |
|    total_timesteps | 333824 |
-------------------------------
Eval num_timesteps=334000, episode_reward=-174625.41 +/- 74091.69
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.27216348 |
|    mean velocity x      | -0.0576     |
|    mean velocity y      | 0.623       |
|    mean velocity z      | 0.722       |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.75e+05   |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.02317982  |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.16       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.94e+07    |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.000481   |
|    std                  | 1.36        |
|    value_loss           | 3.82e+07    |
-----------------------------------------
Eval num_timesteps=334500, episode_reward=-155446.32 +/- 74717.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7740669 |
|    mean velocity x | 2.93       |
|    mean velocity y | 7.75       |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.55e+05  |
| time/              |            |
|    total_timesteps | 334500     |
-----------------------------------
Eval num_timesteps=335000, episode_reward=-162988.21 +/- 56142.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7596055 |
|    mean velocity x | 3.7        |
|    mean velocity y | 8.17       |
|    mean velocity z | 22         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.63e+05  |
| time/              |            |
|    total_timesteps | 335000     |
-----------------------------------
Eval num_timesteps=335500, episode_reward=-168305.67 +/- 86030.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43862638 |
|    mean velocity x | -0.0652     |
|    mean velocity y | 0.722       |
|    mean velocity z | 1.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.68e+05   |
| time/              |             |
|    total_timesteps | 335500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 164    |
|    time_elapsed    | 13577  |
|    total_timesteps | 335872 |
-------------------------------
Eval num_timesteps=336000, episode_reward=-1888810.83 +/- 1516525.87
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.7548738   |
|    mean velocity x      | 3.33         |
|    mean velocity y      | 7.87         |
|    mean velocity z      | 22.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.89e+06    |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0020538827 |
|    clip_fraction        | 0.0755       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.33e+07     |
|    n_updates            | 1640         |
|    policy_gradient_loss | 0.00316      |
|    std                  | 1.36         |
|    value_loss           | 1.43e+08     |
------------------------------------------
Eval num_timesteps=336500, episode_reward=-2582889.01 +/- 1212527.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6988013 |
|    mean velocity x | 2.92       |
|    mean velocity y | 7.34       |
|    mean velocity z | 23         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.58e+06  |
| time/              |            |
|    total_timesteps | 336500     |
-----------------------------------
Eval num_timesteps=337000, episode_reward=-1935360.92 +/- 1476822.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6569971 |
|    mean velocity x | 2.55       |
|    mean velocity y | 6.7        |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.94e+06  |
| time/              |            |
|    total_timesteps | 337000     |
-----------------------------------
Eval num_timesteps=337500, episode_reward=-1965413.17 +/- 1488238.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7243857 |
|    mean velocity x | 4.28       |
|    mean velocity y | 8.66       |
|    mean velocity z | 22.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.97e+06  |
| time/              |            |
|    total_timesteps | 337500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 165    |
|    time_elapsed    | 13658  |
|    total_timesteps | 337920 |
-------------------------------
Eval num_timesteps=338000, episode_reward=-195313.66 +/- 26235.61
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.77265    |
|    mean velocity x      | 4.03        |
|    mean velocity y      | 8.61        |
|    mean velocity z      | 23.1        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.95e+05   |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.002654433 |
|    clip_fraction        | 0.000928    |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.17       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+08    |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 1.36        |
|    value_loss           | 2.06e+08    |
-----------------------------------------
Eval num_timesteps=338500, episode_reward=-133736.42 +/- 37078.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7398733 |
|    mean velocity x | 4.26       |
|    mean velocity y | 8.24       |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.34e+05  |
| time/              |            |
|    total_timesteps | 338500     |
-----------------------------------
Eval num_timesteps=339000, episode_reward=-148000.82 +/- 37179.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7811183 |
|    mean velocity x | 3.57       |
|    mean velocity y | 7.77       |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.48e+05  |
| time/              |            |
|    total_timesteps | 339000     |
-----------------------------------
Eval num_timesteps=339500, episode_reward=-174297.20 +/- 19245.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7244495 |
|    mean velocity x | 3.06       |
|    mean velocity y | 7.53       |
|    mean velocity z | 22.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.74e+05  |
| time/              |            |
|    total_timesteps | 339500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 166    |
|    time_elapsed    | 13739  |
|    total_timesteps | 339968 |
-------------------------------
Eval num_timesteps=340000, episode_reward=-119663.82 +/- 70342.17
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.7794329  |
|    mean velocity x      | 3.53        |
|    mean velocity y      | 8.03        |
|    mean velocity z      | 23.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.2e+05    |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.000689292 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.17       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+08    |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.000862   |
|    std                  | 1.36        |
|    value_loss           | 2.25e+08    |
-----------------------------------------
Eval num_timesteps=340500, episode_reward=-145021.15 +/- 34331.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52261597 |
|    mean velocity x | 0.291       |
|    mean velocity y | 2.11        |
|    mean velocity z | 5.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.45e+05   |
| time/              |             |
|    total_timesteps | 340500      |
------------------------------------
Eval num_timesteps=341000, episode_reward=-157788.74 +/- 54731.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2872834 |
|    mean velocity x | 1.13       |
|    mean velocity y | 4.41       |
|    mean velocity z | 14.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.58e+05  |
| time/              |            |
|    total_timesteps | 341000     |
-----------------------------------
Eval num_timesteps=341500, episode_reward=-132175.78 +/- 64941.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4509726 |
|    mean velocity x | -0.189     |
|    mean velocity y | 0.966      |
|    mean velocity z | 1.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.32e+05  |
| time/              |            |
|    total_timesteps | 341500     |
-----------------------------------
Eval num_timesteps=342000, episode_reward=-148241.70 +/- 80267.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8500177 |
|    mean velocity x | -0.0661    |
|    mean velocity y | 2.32       |
|    mean velocity z | 8.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.48e+05  |
| time/              |            |
|    total_timesteps | 342000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 167    |
|    time_elapsed    | 13838  |
|    total_timesteps | 342016 |
-------------------------------
Eval num_timesteps=342500, episode_reward=-140187.85 +/- 40040.42
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.96215457  |
|    mean velocity x      | 0.156        |
|    mean velocity y      | 2.84         |
|    mean velocity z      | 9.06         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.4e+05     |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0027405152 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.4e+07      |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.00425     |
|    std                  | 1.36         |
|    value_loss           | 1.42e+08     |
------------------------------------------
Eval num_timesteps=343000, episode_reward=-143394.58 +/- 25939.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38422737 |
|    mean velocity x | -0.216      |
|    mean velocity y | 0.724       |
|    mean velocity z | 1.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.43e+05   |
| time/              |             |
|    total_timesteps | 343000      |
------------------------------------
Eval num_timesteps=343500, episode_reward=-124104.69 +/- 67444.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7418371 |
|    mean velocity x | 2.87       |
|    mean velocity y | 7.33       |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.24e+05  |
| time/              |            |
|    total_timesteps | 343500     |
-----------------------------------
Eval num_timesteps=344000, episode_reward=-70789.46 +/- 51549.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1722699 |
|    mean velocity x | 0.572      |
|    mean velocity y | 3.55       |
|    mean velocity z | 11.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.08e+04  |
| time/              |            |
|    total_timesteps | 344000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 168    |
|    time_elapsed    | 13918  |
|    total_timesteps | 344064 |
-------------------------------
Eval num_timesteps=344500, episode_reward=-110358.93 +/- 34776.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.93332505  |
|    mean velocity x      | -0.14        |
|    mean velocity y      | 2.13         |
|    mean velocity z      | 8.78         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.1e+05     |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 0.0025257864 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.89e+07     |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00406     |
|    std                  | 1.36         |
|    value_loss           | 1.41e+08     |
------------------------------------------
Eval num_timesteps=345000, episode_reward=-123874.07 +/- 36326.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7157775 |
|    mean velocity x | 0.473      |
|    mean velocity y | 2.41       |
|    mean velocity z | 6.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.24e+05  |
| time/              |            |
|    total_timesteps | 345000     |
-----------------------------------
Eval num_timesteps=345500, episode_reward=-133895.90 +/- 65845.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5837937 |
|    mean velocity x | -0.385     |
|    mean velocity y | 0.977      |
|    mean velocity z | 4.55       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.34e+05  |
| time/              |            |
|    total_timesteps | 345500     |
-----------------------------------
Eval num_timesteps=346000, episode_reward=-90063.72 +/- 59904.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.89140624 |
|    mean velocity x | 0.768       |
|    mean velocity y | 2.61        |
|    mean velocity z | 6.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.01e+04   |
| time/              |             |
|    total_timesteps | 346000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 169    |
|    time_elapsed    | 13999  |
|    total_timesteps | 346112 |
-------------------------------
Eval num_timesteps=346500, episode_reward=-124997.53 +/- 23813.78
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.7665749  |
|    mean velocity x      | 0.158       |
|    mean velocity y      | 2.02        |
|    mean velocity z      | 7.37        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.25e+05   |
| time/                   |             |
|    total_timesteps      | 346500      |
| train/                  |             |
|    approx_kl            | 0.003537159 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.17       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 2.95e+07    |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00492    |
|    std                  | 1.37        |
|    value_loss           | 6.09e+07    |
-----------------------------------------
Eval num_timesteps=347000, episode_reward=-88348.63 +/- 44004.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7371604 |
|    mean velocity x | -0.805     |
|    mean velocity y | 1.24       |
|    mean velocity z | 6.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.83e+04  |
| time/              |            |
|    total_timesteps | 347000     |
-----------------------------------
Eval num_timesteps=347500, episode_reward=-145776.08 +/- 41047.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8986935 |
|    mean velocity x | 0.308      |
|    mean velocity y | 2.88       |
|    mean velocity z | 6.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.46e+05  |
| time/              |            |
|    total_timesteps | 347500     |
-----------------------------------
Eval num_timesteps=348000, episode_reward=-109566.20 +/- 65244.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.90532094 |
|    mean velocity x | -0.317      |
|    mean velocity y | 2.46        |
|    mean velocity z | 7.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 348000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 170    |
|    time_elapsed    | 14079  |
|    total_timesteps | 348160 |
-------------------------------
Eval num_timesteps=348500, episode_reward=-103141.44 +/- 47352.89
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5626531   |
|    mean velocity x      | 0.176        |
|    mean velocity y      | 1.6          |
|    mean velocity z      | 6.76         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.03e+05    |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0028246413 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.19        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 4.63e+07     |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00397     |
|    std                  | 1.37         |
|    value_loss           | 9.68e+07     |
------------------------------------------
Eval num_timesteps=349000, episode_reward=-144369.66 +/- 22448.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37725154 |
|    mean velocity x | -0.855      |
|    mean velocity y | -0.187      |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.44e+05   |
| time/              |             |
|    total_timesteps | 349000      |
------------------------------------
Eval num_timesteps=349500, episode_reward=-119477.89 +/- 44551.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46575436 |
|    mean velocity x | -0.233      |
|    mean velocity y | 0.856       |
|    mean velocity z | 2.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 349500      |
------------------------------------
Eval num_timesteps=350000, episode_reward=-68670.28 +/- 69830.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8007697 |
|    mean velocity x | -0.192     |
|    mean velocity y | 1.79       |
|    mean velocity z | 8.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.87e+04  |
| time/              |            |
|    total_timesteps | 350000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 171    |
|    time_elapsed    | 14160  |
|    total_timesteps | 350208 |
-------------------------------
Eval num_timesteps=350500, episode_reward=-10029168.22 +/- 23261.91
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -1.115882  |
|    mean velocity x      | 1.83       |
|    mean velocity y      | 4.88       |
|    mean velocity z      | 18.2       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -1e+07     |
| time/                   |            |
|    total_timesteps      | 350500     |
| train/                  |            |
|    approx_kl            | 0.58826035 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.26      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 3.68e+07   |
|    n_updates            | 1710       |
|    policy_gradient_loss | 0.0743     |
|    std                  | 1.41       |
|    value_loss           | 5.5e+07    |
----------------------------------------
Eval num_timesteps=351000, episode_reward=-10069097.44 +/- 29088.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2368954 |
|    mean velocity x | 5.64       |
|    mean velocity y | 8.7        |
|    mean velocity z | 32.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 351000     |
-----------------------------------
Eval num_timesteps=351500, episode_reward=-10067836.55 +/- 63351.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2174735 |
|    mean velocity x | 6.1        |
|    mean velocity y | 9.38       |
|    mean velocity z | 32.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 351500     |
-----------------------------------
Eval num_timesteps=352000, episode_reward=-10040349.92 +/- 55568.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2261469 |
|    mean velocity x | 5.14       |
|    mean velocity y | 8.53       |
|    mean velocity z | 31.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 352000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 172    |
|    time_elapsed    | 14244  |
|    total_timesteps | 352256 |
-------------------------------
Eval num_timesteps=352500, episode_reward=-10025395.94 +/- 7545.34
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.2043623   |
|    mean velocity x      | 6.04         |
|    mean velocity y      | 9.29         |
|    mean velocity z      | 32.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1e+07       |
| time/                   |              |
|    total_timesteps      | 352500       |
| train/                  |              |
|    approx_kl            | 0.0003190926 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.07e+08     |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.000373    |
|    std                  | 1.41         |
|    value_loss           | 6.09e+08     |
------------------------------------------
Eval num_timesteps=353000, episode_reward=-9999971.89 +/- 68220.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0214154 |
|    mean velocity x | 5.05       |
|    mean velocity y | 7.85       |
|    mean velocity z | 25.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 353000     |
-----------------------------------
Eval num_timesteps=353500, episode_reward=-10067861.97 +/- 16006.51
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2555387 |
|    mean velocity x | 5.88       |
|    mean velocity y | 9.2        |
|    mean velocity z | 33         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 353500     |
-----------------------------------
Eval num_timesteps=354000, episode_reward=-10048491.67 +/- 21009.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2682602 |
|    mean velocity x | 5.94       |
|    mean velocity y | 9.19       |
|    mean velocity z | 33.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 354000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 173    |
|    time_elapsed    | 14326  |
|    total_timesteps | 354304 |
-------------------------------
Eval num_timesteps=354500, episode_reward=-10048120.53 +/- 44963.03
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -1.1539764     |
|    mean velocity x      | 5.79           |
|    mean velocity y      | 9.02           |
|    mean velocity z      | 31.1           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1e+07         |
| time/                   |                |
|    total_timesteps      | 354500         |
| train/                  |                |
|    approx_kl            | 0.000102310965 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.27          |
|    explained_variance   | 0              |
|    learning_rate        | 0.001          |
|    loss                 | 2.71e+08       |
|    n_updates            | 1730           |
|    policy_gradient_loss | -0.000338      |
|    std                  | 1.41           |
|    value_loss           | 6.29e+08       |
--------------------------------------------
Eval num_timesteps=355000, episode_reward=-10036106.55 +/- 43525.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2761931 |
|    mean velocity x | 5.82       |
|    mean velocity y | 9.14       |
|    mean velocity z | 33.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 355000     |
-----------------------------------
Eval num_timesteps=355500, episode_reward=-10007929.46 +/- 86287.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2521404 |
|    mean velocity x | 6.26       |
|    mean velocity y | 9.37       |
|    mean velocity z | 30.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 355500     |
-----------------------------------
Eval num_timesteps=356000, episode_reward=-10016227.19 +/- 43674.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1525121 |
|    mean velocity x | 5.89       |
|    mean velocity y | 8.76       |
|    mean velocity z | 31         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 356000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 174    |
|    time_elapsed    | 14409  |
|    total_timesteps | 356352 |
-------------------------------
Eval num_timesteps=356500, episode_reward=-10070144.27 +/- 30956.06
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.2612805    |
|    mean velocity x      | 5.7           |
|    mean velocity y      | 9.03          |
|    mean velocity z      | 33.3          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+07     |
| time/                   |               |
|    total_timesteps      | 356500        |
| train/                  |               |
|    approx_kl            | 8.4824715e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.27         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 3.27e+08      |
|    n_updates            | 1740          |
|    policy_gradient_loss | -0.00047      |
|    std                  | 1.41          |
|    value_loss           | 6.81e+08      |
-------------------------------------------
Eval num_timesteps=357000, episode_reward=-10046856.91 +/- 38827.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1834849 |
|    mean velocity x | 5.95       |
|    mean velocity y | 8.69       |
|    mean velocity z | 32.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 357000     |
-----------------------------------
Eval num_timesteps=357500, episode_reward=-10044849.15 +/- 25720.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2163926 |
|    mean velocity x | 6.15       |
|    mean velocity y | 9.06       |
|    mean velocity z | 31.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+07     |
| time/              |            |
|    total_timesteps | 357500     |
-----------------------------------
Eval num_timesteps=358000, episode_reward=-9993286.57 +/- 41529.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1855202 |
|    mean velocity x | 5.99       |
|    mean velocity y | 9.09       |
|    mean velocity z | 33.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.99e+06  |
| time/              |            |
|    total_timesteps | 358000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 175    |
|    time_elapsed    | 14491  |
|    total_timesteps | 358400 |
-------------------------------
Eval num_timesteps=358500, episode_reward=-10110659.92 +/- 24738.03
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.135428    |
|    mean velocity x      | 5.46         |
|    mean velocity y      | 7.92         |
|    mean velocity z      | 31.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.01e+07    |
| time/                   |              |
|    total_timesteps      | 358500       |
| train/                  |              |
|    approx_kl            | 0.0012097103 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.5e+08      |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.41         |
|    value_loss           | 6.54e+08     |
------------------------------------------
Eval num_timesteps=359000, episode_reward=-10041693.85 +/- 127086.41
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.225163 |
|    mean velocity x | 5.99      |
|    mean velocity y | 9.27      |
|    mean velocity z | 32.5      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1e+07    |
| time/              |           |
|    total_timesteps | 359000    |
----------------------------------
Eval num_timesteps=359500, episode_reward=-10069752.58 +/- 80120.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2396733 |
|    mean velocity x | 4.76       |
|    mean velocity y | 8.11       |
|    mean velocity z | 30.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 359500     |
-----------------------------------
Eval num_timesteps=360000, episode_reward=-10085453.64 +/- 24543.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2276119 |
|    mean velocity x | 5.19       |
|    mean velocity y | 7.86       |
|    mean velocity z | 31.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 360000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 176    |
|    time_elapsed    | 14573  |
|    total_timesteps | 360448 |
-------------------------------
Eval num_timesteps=360500, episode_reward=-10107306.73 +/- 29230.04
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.2983099   |
|    mean velocity x      | 5.7          |
|    mean velocity y      | 9.29         |
|    mean velocity z      | 32.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.01e+07    |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0005041479 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.05e+08     |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.000951    |
|    std                  | 1.41         |
|    value_loss           | 6.2e+08      |
------------------------------------------
Eval num_timesteps=361000, episode_reward=-10092597.93 +/- 37257.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2842363 |
|    mean velocity x | 5.65       |
|    mean velocity y | 9.39       |
|    mean velocity z | 32.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 361000     |
-----------------------------------
Eval num_timesteps=361500, episode_reward=-10079757.67 +/- 67933.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2596157 |
|    mean velocity x | 6.25       |
|    mean velocity y | 9.42       |
|    mean velocity z | 30.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 361500     |
-----------------------------------
Eval num_timesteps=362000, episode_reward=-8051941.28 +/- 4017334.81
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.210835 |
|    mean velocity x | 5.32      |
|    mean velocity y | 7.89      |
|    mean velocity z | 26.5      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.05e+06 |
| time/              |           |
|    total_timesteps | 362000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 177    |
|    time_elapsed    | 14655  |
|    total_timesteps | 362496 |
-------------------------------
Eval num_timesteps=362500, episode_reward=-10086552.31 +/- 59907.61
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.3521084   |
|    mean velocity x      | 6.18         |
|    mean velocity y      | 9.72         |
|    mean velocity z      | 31.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.01e+07    |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0009573623 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 2.74e+08     |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.41         |
|    value_loss           | 5.71e+08     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=-10069974.88 +/- 77202.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2533358 |
|    mean velocity x | 5.63       |
|    mean velocity y | 9.42       |
|    mean velocity z | 32.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 363000     |
-----------------------------------
Eval num_timesteps=363500, episode_reward=-10078967.06 +/- 54000.38
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.284394 |
|    mean velocity x | 5.97      |
|    mean velocity y | 9.23      |
|    mean velocity z | 32.7      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+07 |
| time/              |           |
|    total_timesteps | 363500    |
----------------------------------
Eval num_timesteps=364000, episode_reward=-10108639.90 +/- 31328.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3362296 |
|    mean velocity x | 5.2        |
|    mean velocity y | 8.73       |
|    mean velocity z | 31.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 364000     |
-----------------------------------
Eval num_timesteps=364500, episode_reward=-10052388.37 +/- 69232.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3228569 |
|    mean velocity x | 5.74       |
|    mean velocity y | 9.66       |
|    mean velocity z | 29.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 364500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 178    |
|    time_elapsed    | 14757  |
|    total_timesteps | 364544 |
-------------------------------
Eval num_timesteps=365000, episode_reward=-10081336.77 +/- 61298.44
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.3500276    |
|    mean velocity x      | 5.7           |
|    mean velocity y      | 9.3           |
|    mean velocity z      | 32.1          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+07     |
| time/                   |               |
|    total_timesteps      | 365000        |
| train/                  |               |
|    approx_kl            | 0.00062629156 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.28         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+08      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -0.001        |
|    std                  | 1.41          |
|    value_loss           | 6.44e+08      |
-------------------------------------------
Eval num_timesteps=365500, episode_reward=-10061299.19 +/- 15854.37
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.230452 |
|    mean velocity x | 4.1       |
|    mean velocity y | 7.57      |
|    mean velocity z | 29.7      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+07 |
| time/              |           |
|    total_timesteps | 365500    |
----------------------------------
Eval num_timesteps=366000, episode_reward=-10120925.41 +/- 31843.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2564499 |
|    mean velocity x | 5.81       |
|    mean velocity y | 8.81       |
|    mean velocity z | 32.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 366000     |
-----------------------------------
Eval num_timesteps=366500, episode_reward=-8123392.46 +/- 4052819.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3564031 |
|    mean velocity x | 6.25       |
|    mean velocity y | 9.63       |
|    mean velocity z | 31.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.12e+06  |
| time/              |            |
|    total_timesteps | 366500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 179    |
|    time_elapsed    | 14839  |
|    total_timesteps | 366592 |
-------------------------------
Eval num_timesteps=367000, episode_reward=-10104938.96 +/- 74176.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.3138134    |
|    mean velocity x      | 5.79          |
|    mean velocity y      | 9.19          |
|    mean velocity z      | 32.9          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+07     |
| time/                   |               |
|    total_timesteps      | 367000        |
| train/                  |               |
|    approx_kl            | 0.00047821325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.28         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 2.98e+08      |
|    n_updates            | 1790          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 1.41          |
|    value_loss           | 6.19e+08      |
-------------------------------------------
Eval num_timesteps=367500, episode_reward=-10112274.98 +/- 73255.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2987452 |
|    mean velocity x | 6.13       |
|    mean velocity y | 9.47       |
|    mean velocity z | 30.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 367500     |
-----------------------------------
Eval num_timesteps=368000, episode_reward=-10103204.90 +/- 25886.43
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.367601 |
|    mean velocity x | 5.72      |
|    mean velocity y | 9.33      |
|    mean velocity z | 32.6      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+07 |
| time/              |           |
|    total_timesteps | 368000    |
----------------------------------
Eval num_timesteps=368500, episode_reward=-10080678.12 +/- 51328.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2705078 |
|    mean velocity x | 5.92       |
|    mean velocity y | 9.16       |
|    mean velocity z | 32.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 368500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 180    |
|    time_elapsed    | 14921  |
|    total_timesteps | 368640 |
-------------------------------
Eval num_timesteps=369000, episode_reward=-10089790.72 +/- 37283.08
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.374717     |
|    mean velocity x      | 6.09          |
|    mean velocity y      | 9.87          |
|    mean velocity z      | 31.2          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+07     |
| time/                   |               |
|    total_timesteps      | 369000        |
| train/                  |               |
|    approx_kl            | 3.1277566e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.28         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+08      |
|    n_updates            | 1800          |
|    policy_gradient_loss | -0.000265     |
|    std                  | 1.41          |
|    value_loss           | 6.18e+08      |
-------------------------------------------
Eval num_timesteps=369500, episode_reward=-10100190.84 +/- 38513.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3415749 |
|    mean velocity x | 5.32       |
|    mean velocity y | 8.69       |
|    mean velocity z | 31.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 369500     |
-----------------------------------
Eval num_timesteps=370000, episode_reward=-10084028.55 +/- 6121.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2639002 |
|    mean velocity x | 4.78       |
|    mean velocity y | 7.86       |
|    mean velocity z | 30.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 370000     |
-----------------------------------
Eval num_timesteps=370500, episode_reward=-10137217.31 +/- 29409.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3023353 |
|    mean velocity x | 5.33       |
|    mean velocity y | 8.41       |
|    mean velocity z | 32.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+07  |
| time/              |            |
|    total_timesteps | 370500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 181    |
|    time_elapsed    | 15003  |
|    total_timesteps | 370688 |
-------------------------------
Eval num_timesteps=371000, episode_reward=-9970546.39 +/- 62186.63
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.3599744   |
|    mean velocity x      | 5.85         |
|    mean velocity y      | 9.35         |
|    mean velocity z      | 32.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.97e+06    |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 0.0029170876 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.19e+08     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 1.41         |
|    value_loss           | 6.34e+08     |
------------------------------------------
Eval num_timesteps=371500, episode_reward=-9944479.33 +/- 75826.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.361393 |
|    mean velocity x | 5.73      |
|    mean velocity y | 9.32      |
|    mean velocity z | 32.3      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.94e+06 |
| time/              |           |
|    total_timesteps | 371500    |
----------------------------------
Eval num_timesteps=372000, episode_reward=-9965395.15 +/- 30217.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2750647 |
|    mean velocity x | 5.67       |
|    mean velocity y | 9.08       |
|    mean velocity z | 32.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.97e+06  |
| time/              |            |
|    total_timesteps | 372000     |
-----------------------------------
Eval num_timesteps=372500, episode_reward=-9981391.05 +/- 42302.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3563267 |
|    mean velocity x | 6.31       |
|    mean velocity y | 9.4        |
|    mean velocity z | 30.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.98e+06  |
| time/              |            |
|    total_timesteps | 372500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 182    |
|    time_elapsed    | 15086  |
|    total_timesteps | 372736 |
-------------------------------
Eval num_timesteps=373000, episode_reward=-9713882.29 +/- 40145.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.1620402   |
|    mean velocity x      | 4.07         |
|    mean velocity y      | 6.83         |
|    mean velocity z      | 28.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.71e+06    |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0033544647 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.74e+08     |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 1.41         |
|    value_loss           | 5.9e+08      |
------------------------------------------
Eval num_timesteps=373500, episode_reward=-9662228.71 +/- 102899.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4406725 |
|    mean velocity x | 6.08       |
|    mean velocity y | 9.45       |
|    mean velocity z | 31.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.66e+06  |
| time/              |            |
|    total_timesteps | 373500     |
-----------------------------------
Eval num_timesteps=374000, episode_reward=-9728562.71 +/- 34516.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0434536 |
|    mean velocity x | 4.84       |
|    mean velocity y | 7.81       |
|    mean velocity z | 23.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.73e+06  |
| time/              |            |
|    total_timesteps | 374000     |
-----------------------------------
Eval num_timesteps=374500, episode_reward=-9654133.01 +/- 86147.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3939927 |
|    mean velocity x | 5.69       |
|    mean velocity y | 9.34       |
|    mean velocity z | 32.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.65e+06  |
| time/              |            |
|    total_timesteps | 374500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 183    |
|    time_elapsed    | 15168  |
|    total_timesteps | 374784 |
-------------------------------
Eval num_timesteps=375000, episode_reward=-9574771.76 +/- 35336.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.3346813   |
|    mean velocity x      | 5.78         |
|    mean velocity y      | 9.34         |
|    mean velocity z      | 30.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.57e+06    |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0011588562 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.79e+08     |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.41         |
|    value_loss           | 5.48e+08     |
------------------------------------------
Eval num_timesteps=375500, episode_reward=-9593934.63 +/- 32884.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4905913 |
|    mean velocity x | 5.58       |
|    mean velocity y | 9.36       |
|    mean velocity z | 32         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.59e+06  |
| time/              |            |
|    total_timesteps | 375500     |
-----------------------------------
Eval num_timesteps=376000, episode_reward=-9624288.59 +/- 38088.46
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4828577 |
|    mean velocity x | 6.32       |
|    mean velocity y | 9.52       |
|    mean velocity z | 31.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.62e+06  |
| time/              |            |
|    total_timesteps | 376000     |
-----------------------------------
Eval num_timesteps=376500, episode_reward=-9567812.45 +/- 36768.10
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3210489 |
|    mean velocity x | 5.73       |
|    mean velocity y | 9.13       |
|    mean velocity z | 30.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.57e+06  |
| time/              |            |
|    total_timesteps | 376500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 184    |
|    time_elapsed    | 15250  |
|    total_timesteps | 376832 |
-------------------------------
Eval num_timesteps=377000, episode_reward=-9452931.53 +/- 72750.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.4411572    |
|    mean velocity x      | 5.73          |
|    mean velocity y      | 9.57          |
|    mean velocity z      | 32            |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.45e+06     |
| time/                   |               |
|    total_timesteps      | 377000        |
| train/                  |               |
|    approx_kl            | 0.00084955397 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.28         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.001         |
|    loss                 | 2.57e+08      |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.00111      |
|    std                  | 1.41          |
|    value_loss           | 5.88e+08      |
-------------------------------------------
Eval num_timesteps=377500, episode_reward=-9492715.87 +/- 22885.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4138932 |
|    mean velocity x | 5.83       |
|    mean velocity y | 9.43       |
|    mean velocity z | 29.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.49e+06  |
| time/              |            |
|    total_timesteps | 377500     |
-----------------------------------
Eval num_timesteps=378000, episode_reward=-9474242.57 +/- 60505.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3266369 |
|    mean velocity x | 4.9        |
|    mean velocity y | 8.52       |
|    mean velocity z | 25.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.47e+06  |
| time/              |            |
|    total_timesteps | 378000     |
-----------------------------------
Eval num_timesteps=378500, episode_reward=-9472289.91 +/- 52670.05
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.322226 |
|    mean velocity x | 5.65      |
|    mean velocity y | 8.5       |
|    mean velocity z | 29.5      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.47e+06 |
| time/              |           |
|    total_timesteps | 378500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 185    |
|    time_elapsed    | 15332  |
|    total_timesteps | 378880 |
-------------------------------
Eval num_timesteps=379000, episode_reward=-9313359.77 +/- 35550.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.3767334   |
|    mean velocity x      | 6.31         |
|    mean velocity y      | 9.65         |
|    mean velocity z      | 30.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.31e+06    |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0019696283 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.3e+08      |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 1.41         |
|    value_loss           | 4.83e+08     |
------------------------------------------
Eval num_timesteps=379500, episode_reward=-9278330.60 +/- 69812.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3380146 |
|    mean velocity x | 5.6        |
|    mean velocity y | 9.36       |
|    mean velocity z | 30.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.28e+06  |
| time/              |            |
|    total_timesteps | 379500     |
-----------------------------------
Eval num_timesteps=380000, episode_reward=-9306470.38 +/- 32928.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5055873 |
|    mean velocity x | 5.82       |
|    mean velocity y | 9.48       |
|    mean velocity z | 30.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.31e+06  |
| time/              |            |
|    total_timesteps | 380000     |
-----------------------------------
Eval num_timesteps=380500, episode_reward=-9303225.12 +/- 79485.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5041127 |
|    mean velocity x | 5.25       |
|    mean velocity y | 9.14       |
|    mean velocity z | 32.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.3e+06   |
| time/              |            |
|    total_timesteps | 380500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 186    |
|    time_elapsed    | 15415  |
|    total_timesteps | 380928 |
-------------------------------
Eval num_timesteps=381000, episode_reward=-9210777.88 +/- 30353.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4391665   |
|    mean velocity x      | 4.98         |
|    mean velocity y      | 8.84         |
|    mean velocity z      | 31.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.21e+06    |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0031041454 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 3.51e+08     |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.41         |
|    value_loss           | 5.89e+08     |
------------------------------------------
Eval num_timesteps=381500, episode_reward=-9233447.61 +/- 22619.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5302414 |
|    mean velocity x | 5.63       |
|    mean velocity y | 9.78       |
|    mean velocity z | 31.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.23e+06  |
| time/              |            |
|    total_timesteps | 381500     |
-----------------------------------
Eval num_timesteps=382000, episode_reward=-9203617.53 +/- 51451.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5241941 |
|    mean velocity x | 5.94       |
|    mean velocity y | 9.78       |
|    mean velocity z | 30.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.2e+06   |
| time/              |            |
|    total_timesteps | 382000     |
-----------------------------------
Eval num_timesteps=382500, episode_reward=-9133418.45 +/- 151740.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4920417 |
|    mean velocity x | 5.6        |
|    mean velocity y | 9.11       |
|    mean velocity z | 31.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.13e+06  |
| time/              |            |
|    total_timesteps | 382500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 187    |
|    time_elapsed    | 15497  |
|    total_timesteps | 382976 |
-------------------------------
Eval num_timesteps=383000, episode_reward=-8777599.23 +/- 120454.97
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.4698821  |
|    mean velocity x      | 5.13        |
|    mean velocity y      | 9.04        |
|    mean velocity z      | 31.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.78e+06   |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.003989619 |
|    clip_fraction        | 0.00674     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.27       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 3.02e+08    |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00345    |
|    std                  | 1.41        |
|    value_loss           | 5.64e+08    |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=-8826595.82 +/- 55753.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4073195 |
|    mean velocity x | 4.59       |
|    mean velocity y | 8.19       |
|    mean velocity z | 29.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.83e+06  |
| time/              |            |
|    total_timesteps | 383500     |
-----------------------------------
Eval num_timesteps=384000, episode_reward=-8773548.45 +/- 88236.72
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4431863 |
|    mean velocity x | 5.51       |
|    mean velocity y | 9.05       |
|    mean velocity z | 29.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.77e+06  |
| time/              |            |
|    total_timesteps | 384000     |
-----------------------------------
Eval num_timesteps=384500, episode_reward=-8851008.41 +/- 48174.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4763558 |
|    mean velocity x | 5.96       |
|    mean velocity y | 9.83       |
|    mean velocity z | 29.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.85e+06  |
| time/              |            |
|    total_timesteps | 384500     |
-----------------------------------
Eval num_timesteps=385000, episode_reward=-8869981.26 +/- 44084.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3613296 |
|    mean velocity x | 4.38       |
|    mean velocity y | 7.93       |
|    mean velocity z | 29.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.87e+06  |
| time/              |            |
|    total_timesteps | 385000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 188    |
|    time_elapsed    | 15599  |
|    total_timesteps | 385024 |
-------------------------------
Eval num_timesteps=385500, episode_reward=-8601388.27 +/- 53534.68
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5017874   |
|    mean velocity x      | 5.82         |
|    mean velocity y      | 9.99         |
|    mean velocity z      | 30.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.6e+06     |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 0.0042609484 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.28e+08     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00335     |
|    std                  | 1.41         |
|    value_loss           | 4.65e+08     |
------------------------------------------
Eval num_timesteps=386000, episode_reward=-8554940.11 +/- 130432.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4742509 |
|    mean velocity x | 5.96       |
|    mean velocity y | 9.67       |
|    mean velocity z | 29.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.55e+06  |
| time/              |            |
|    total_timesteps | 386000     |
-----------------------------------
Eval num_timesteps=386500, episode_reward=-8626349.39 +/- 25793.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4559902 |
|    mean velocity x | 5.48       |
|    mean velocity y | 9.53       |
|    mean velocity z | 31         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.63e+06  |
| time/              |            |
|    total_timesteps | 386500     |
-----------------------------------
Eval num_timesteps=387000, episode_reward=-8577779.62 +/- 51969.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5629907 |
|    mean velocity x | 5.68       |
|    mean velocity y | 9.79       |
|    mean velocity z | 30.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.58e+06  |
| time/              |            |
|    total_timesteps | 387000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 189    |
|    time_elapsed    | 15681  |
|    total_timesteps | 387072 |
-------------------------------
Eval num_timesteps=387500, episode_reward=-8445409.83 +/- 35188.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4508275   |
|    mean velocity x      | 4.83         |
|    mean velocity y      | 8.86         |
|    mean velocity z      | 30.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.45e+06    |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0012389976 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.97e+08     |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.41         |
|    value_loss           | 5.35e+08     |
------------------------------------------
Eval num_timesteps=388000, episode_reward=-8447445.36 +/- 56378.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5070733 |
|    mean velocity x | 5.93       |
|    mean velocity y | 9.76       |
|    mean velocity z | 29.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.45e+06  |
| time/              |            |
|    total_timesteps | 388000     |
-----------------------------------
Eval num_timesteps=388500, episode_reward=-8456626.25 +/- 36045.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4762996 |
|    mean velocity x | 5.42       |
|    mean velocity y | 9.4        |
|    mean velocity z | 30.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.46e+06  |
| time/              |            |
|    total_timesteps | 388500     |
-----------------------------------
Eval num_timesteps=389000, episode_reward=-8429771.87 +/- 20279.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5151985 |
|    mean velocity x | 5.69       |
|    mean velocity y | 9.93       |
|    mean velocity z | 30.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.43e+06  |
| time/              |            |
|    total_timesteps | 389000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 190    |
|    time_elapsed    | 15763  |
|    total_timesteps | 389120 |
-------------------------------
Eval num_timesteps=389500, episode_reward=-8095828.29 +/- 33696.49
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4775437   |
|    mean velocity x      | 5.67         |
|    mean velocity y      | 9.34         |
|    mean velocity z      | 27.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.1e+06     |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0043087937 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+08     |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00388     |
|    std                  | 1.41         |
|    value_loss           | 4.91e+08     |
------------------------------------------
Eval num_timesteps=390000, episode_reward=-8133294.50 +/- 10790.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5016379 |
|    mean velocity x | 5.88       |
|    mean velocity y | 9.79       |
|    mean velocity z | 28.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.13e+06  |
| time/              |            |
|    total_timesteps | 390000     |
-----------------------------------
Eval num_timesteps=390500, episode_reward=-8109688.40 +/- 39119.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4773142 |
|    mean velocity x | 5.91       |
|    mean velocity y | 9.7        |
|    mean velocity z | 28.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.11e+06  |
| time/              |            |
|    total_timesteps | 390500     |
-----------------------------------
Eval num_timesteps=391000, episode_reward=-8103758.12 +/- 36272.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5464944 |
|    mean velocity x | 5.66       |
|    mean velocity y | 9.97       |
|    mean velocity z | 29.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.1e+06   |
| time/              |            |
|    total_timesteps | 391000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 191    |
|    time_elapsed    | 15845  |
|    total_timesteps | 391168 |
-------------------------------
Eval num_timesteps=391500, episode_reward=-7652887.30 +/- 65146.51
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.519153    |
|    mean velocity x      | 5.17         |
|    mean velocity y      | 9.56         |
|    mean velocity z      | 30           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.65e+06    |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0066978894 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.43e+08     |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00627     |
|    std                  | 1.41         |
|    value_loss           | 4.55e+08     |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-7694234.54 +/- 31655.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5807402 |
|    mean velocity x | 5.75       |
|    mean velocity y | 9.52       |
|    mean velocity z | 28.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.69e+06  |
| time/              |            |
|    total_timesteps | 392000     |
-----------------------------------
Eval num_timesteps=392500, episode_reward=-7688098.58 +/- 46710.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4743088 |
|    mean velocity x | 5.03       |
|    mean velocity y | 8.69       |
|    mean velocity z | 28.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.69e+06  |
| time/              |            |
|    total_timesteps | 392500     |
-----------------------------------
Eval num_timesteps=393000, episode_reward=-7680680.31 +/- 34142.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5649458 |
|    mean velocity x | 5.76       |
|    mean velocity y | 9.53       |
|    mean velocity z | 28.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.68e+06  |
| time/              |            |
|    total_timesteps | 393000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 192    |
|    time_elapsed    | 15927  |
|    total_timesteps | 393216 |
-------------------------------
Eval num_timesteps=393500, episode_reward=-7281636.33 +/- 36995.47
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4365121   |
|    mean velocity x      | 5.37         |
|    mean velocity y      | 9.6          |
|    mean velocity z      | 28.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.28e+06    |
| time/                   |              |
|    total_timesteps      | 393500       |
| train/                  |              |
|    approx_kl            | 0.0072418177 |
|    clip_fraction        | 0.0457       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+08     |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00837     |
|    std                  | 1.41         |
|    value_loss           | 4.07e+08     |
------------------------------------------
Eval num_timesteps=394000, episode_reward=-7310007.37 +/- 33880.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1949317 |
|    mean velocity x | 4.69       |
|    mean velocity y | 7.69       |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.31e+06  |
| time/              |            |
|    total_timesteps | 394000     |
-----------------------------------
Eval num_timesteps=394500, episode_reward=-7291259.44 +/- 36425.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4456438 |
|    mean velocity x | 5.12       |
|    mean velocity y | 8.85       |
|    mean velocity z | 23.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.29e+06  |
| time/              |            |
|    total_timesteps | 394500     |
-----------------------------------
Eval num_timesteps=395000, episode_reward=-7293847.65 +/- 58098.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4438212 |
|    mean velocity x | 4.28       |
|    mean velocity y | 8.1        |
|    mean velocity z | 26.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.29e+06  |
| time/              |            |
|    total_timesteps | 395000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 193    |
|    time_elapsed    | 16010  |
|    total_timesteps | 395264 |
-------------------------------
Eval num_timesteps=395500, episode_reward=-7116728.71 +/- 54176.90
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4859405   |
|    mean velocity x      | 5.29         |
|    mean velocity y      | 9.36         |
|    mean velocity z      | 29.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.12e+06    |
| time/                   |              |
|    total_timesteps      | 395500       |
| train/                  |              |
|    approx_kl            | 0.0016639765 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+08     |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.41         |
|    value_loss           | 3.75e+08     |
------------------------------------------
Eval num_timesteps=396000, episode_reward=-7052830.74 +/- 107772.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4975635 |
|    mean velocity x | 5.39       |
|    mean velocity y | 9.84       |
|    mean velocity z | 28.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.05e+06  |
| time/              |            |
|    total_timesteps | 396000     |
-----------------------------------
Eval num_timesteps=396500, episode_reward=-7153466.48 +/- 40083.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5379466 |
|    mean velocity x | 5.78       |
|    mean velocity y | 9.63       |
|    mean velocity z | 28.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.15e+06  |
| time/              |            |
|    total_timesteps | 396500     |
-----------------------------------
Eval num_timesteps=397000, episode_reward=-7163812.55 +/- 29515.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4259777 |
|    mean velocity x | 4.43       |
|    mean velocity y | 8.12       |
|    mean velocity z | 27.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.16e+06  |
| time/              |            |
|    total_timesteps | 397000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 194    |
|    time_elapsed    | 16092  |
|    total_timesteps | 397312 |
-------------------------------
Eval num_timesteps=397500, episode_reward=-6940032.75 +/- 36439.73
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.1130989   |
|    mean velocity x      | 3.96         |
|    mean velocity y      | 6.85         |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.94e+06    |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0023664525 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.83e+08     |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00248     |
|    std                  | 1.42         |
|    value_loss           | 3.66e+08     |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-6942258.04 +/- 31751.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5019834 |
|    mean velocity x | 5.3        |
|    mean velocity y | 9.53       |
|    mean velocity z | 29.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.94e+06  |
| time/              |            |
|    total_timesteps | 398000     |
-----------------------------------
Eval num_timesteps=398500, episode_reward=-6887719.58 +/- 101851.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5826087 |
|    mean velocity x | 5.62       |
|    mean velocity y | 9.61       |
|    mean velocity z | 26.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.89e+06  |
| time/              |            |
|    total_timesteps | 398500     |
-----------------------------------
Eval num_timesteps=399000, episode_reward=-6920053.48 +/- 13831.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4683732 |
|    mean velocity x | 5.38       |
|    mean velocity y | 9.36       |
|    mean velocity z | 29         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.92e+06  |
| time/              |            |
|    total_timesteps | 399000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 195    |
|    time_elapsed    | 16174  |
|    total_timesteps | 399360 |
-------------------------------
Eval num_timesteps=399500, episode_reward=-6778519.24 +/- 35075.97
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5087595   |
|    mean velocity x      | 5.33         |
|    mean velocity y      | 9.23         |
|    mean velocity z      | 27.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.78e+06    |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0026674701 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+08     |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 1.42         |
|    value_loss           | 4.52e+08     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-6813129.66 +/- 35653.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4700373 |
|    mean velocity x | 5.28       |
|    mean velocity y | 9.05       |
|    mean velocity z | 29         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.81e+06  |
| time/              |            |
|    total_timesteps | 400000     |
-----------------------------------
Eval num_timesteps=400500, episode_reward=-6737420.23 +/- 62993.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4638612 |
|    mean velocity x | 3.83       |
|    mean velocity y | 8.21       |
|    mean velocity z | 25.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.74e+06  |
| time/              |            |
|    total_timesteps | 400500     |
-----------------------------------
Eval num_timesteps=401000, episode_reward=-6726620.59 +/- 171418.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5855445 |
|    mean velocity x | 5.43       |
|    mean velocity y | 9.56       |
|    mean velocity z | 27.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.73e+06  |
| time/              |            |
|    total_timesteps | 401000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 196    |
|    time_elapsed    | 16256  |
|    total_timesteps | 401408 |
-------------------------------
Eval num_timesteps=401500, episode_reward=-6424768.86 +/- 36149.27
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5492283   |
|    mean velocity x      | 5.21         |
|    mean velocity y      | 9.37         |
|    mean velocity z      | 28.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.42e+06    |
| time/                   |              |
|    total_timesteps      | 401500       |
| train/                  |              |
|    approx_kl            | 0.0067467215 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.23e+08     |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00613     |
|    std                  | 1.42         |
|    value_loss           | 3.75e+08     |
------------------------------------------
Eval num_timesteps=402000, episode_reward=-6442710.84 +/- 16204.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5304308 |
|    mean velocity x | 4.63       |
|    mean velocity y | 8.74       |
|    mean velocity z | 27.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.44e+06  |
| time/              |            |
|    total_timesteps | 402000     |
-----------------------------------
Eval num_timesteps=402500, episode_reward=-6441982.11 +/- 36643.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4855101 |
|    mean velocity x | 4.5        |
|    mean velocity y | 8.03       |
|    mean velocity z | 27.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.44e+06  |
| time/              |            |
|    total_timesteps | 402500     |
-----------------------------------
Eval num_timesteps=403000, episode_reward=-6442130.10 +/- 27954.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5252547 |
|    mean velocity x | 5.42       |
|    mean velocity y | 8.99       |
|    mean velocity z | 26.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.44e+06  |
| time/              |            |
|    total_timesteps | 403000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 197    |
|    time_elapsed    | 16338  |
|    total_timesteps | 403456 |
-------------------------------
Eval num_timesteps=403500, episode_reward=-6065551.08 +/- 280394.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5309544   |
|    mean velocity x      | 5.06         |
|    mean velocity y      | 8.99         |
|    mean velocity z      | 28.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.07e+06    |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0029640417 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.26e+08     |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 1.42         |
|    value_loss           | 3.92e+08     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-6203814.09 +/- 34614.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5597019 |
|    mean velocity x | 5.35       |
|    mean velocity y | 9.34       |
|    mean velocity z | 27.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.2e+06   |
| time/              |            |
|    total_timesteps | 404000     |
-----------------------------------
Eval num_timesteps=404500, episode_reward=-6215327.04 +/- 33816.13
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.580272 |
|    mean velocity x | 5.04      |
|    mean velocity y | 9.15      |
|    mean velocity z | 27.6      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.22e+06 |
| time/              |           |
|    total_timesteps | 404500    |
----------------------------------
Eval num_timesteps=405000, episode_reward=-6222093.68 +/- 26722.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5524812 |
|    mean velocity x | 4.3        |
|    mean velocity y | 8.47       |
|    mean velocity z | 26.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.22e+06  |
| time/              |            |
|    total_timesteps | 405000     |
-----------------------------------
Eval num_timesteps=405500, episode_reward=-6176800.37 +/- 121591.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5472906 |
|    mean velocity x | 5.34       |
|    mean velocity y | 9.45       |
|    mean velocity z | 27.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.18e+06  |
| time/              |            |
|    total_timesteps | 405500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 198    |
|    time_elapsed    | 16440  |
|    total_timesteps | 405504 |
-------------------------------
Eval num_timesteps=406000, episode_reward=-5861060.32 +/- 30252.51
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.2999455  |
|    mean velocity x      | 2.31        |
|    mean velocity y      | 5.77        |
|    mean velocity z      | 24.1        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.86e+06   |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.005963769 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.3        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 2.06e+08    |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00477    |
|    std                  | 1.42        |
|    value_loss           | 3.97e+08    |
-----------------------------------------
Eval num_timesteps=406500, episode_reward=-5864528.54 +/- 35303.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5123445 |
|    mean velocity x | 5.13       |
|    mean velocity y | 8.91       |
|    mean velocity z | 27.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.86e+06  |
| time/              |            |
|    total_timesteps | 406500     |
-----------------------------------
Eval num_timesteps=407000, episode_reward=-5864241.99 +/- 34982.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5911177 |
|    mean velocity x | 4.97       |
|    mean velocity y | 9.08       |
|    mean velocity z | 27.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.86e+06  |
| time/              |            |
|    total_timesteps | 407000     |
-----------------------------------
Eval num_timesteps=407500, episode_reward=-5830337.26 +/- 54456.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5865879 |
|    mean velocity x | 5.48       |
|    mean velocity y | 9.72       |
|    mean velocity z | 25.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.83e+06  |
| time/              |            |
|    total_timesteps | 407500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 199    |
|    time_elapsed    | 16522  |
|    total_timesteps | 407552 |
-------------------------------
Eval num_timesteps=408000, episode_reward=-5652673.33 +/- 9800.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4868335   |
|    mean velocity x      | 5.4          |
|    mean velocity y      | 9.16         |
|    mean velocity z      | 26.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.65e+06    |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0027175648 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+08     |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.42         |
|    value_loss           | 3.8e+08      |
------------------------------------------
Eval num_timesteps=408500, episode_reward=-5650018.75 +/- 15202.53
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.455074 |
|    mean velocity x | 4.28      |
|    mean velocity y | 8.21      |
|    mean velocity z | 20.4      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.65e+06 |
| time/              |           |
|    total_timesteps | 408500    |
----------------------------------
Eval num_timesteps=409000, episode_reward=-5650135.90 +/- 37375.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4958925 |
|    mean velocity x | 4.85       |
|    mean velocity y | 9.18       |
|    mean velocity z | 26.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.65e+06  |
| time/              |            |
|    total_timesteps | 409000     |
-----------------------------------
Eval num_timesteps=409500, episode_reward=-5665654.22 +/- 29253.10
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2390754 |
|    mean velocity x | 2.18       |
|    mean velocity y | 5.01       |
|    mean velocity z | 24.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.67e+06  |
| time/              |            |
|    total_timesteps | 409500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 200    |
|    time_elapsed    | 16605  |
|    total_timesteps | 409600 |
-------------------------------
Eval num_timesteps=410000, episode_reward=-5553095.63 +/- 11416.65
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5069063   |
|    mean velocity x      | 5.15         |
|    mean velocity y      | 9.2          |
|    mean velocity z      | 27.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.55e+06    |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0006062254 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.8e+08      |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.42         |
|    value_loss           | 3.07e+08     |
------------------------------------------
Eval num_timesteps=410500, episode_reward=-5591328.14 +/- 26302.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4984481 |
|    mean velocity x | 4.68       |
|    mean velocity y | 8.82       |
|    mean velocity z | 26.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.59e+06  |
| time/              |            |
|    total_timesteps | 410500     |
-----------------------------------
Eval num_timesteps=411000, episode_reward=-5572879.61 +/- 25762.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5157931 |
|    mean velocity x | 5.39       |
|    mean velocity y | 9.24       |
|    mean velocity z | 25.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.57e+06  |
| time/              |            |
|    total_timesteps | 411000     |
-----------------------------------
Eval num_timesteps=411500, episode_reward=-5577781.75 +/- 49595.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5714233 |
|    mean velocity x | 5.6        |
|    mean velocity y | 9.01       |
|    mean velocity z | 24.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.58e+06  |
| time/              |            |
|    total_timesteps | 411500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 201    |
|    time_elapsed    | 16687  |
|    total_timesteps | 411648 |
-------------------------------
Eval num_timesteps=412000, episode_reward=-4161688.16 +/- 2048464.08
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.5237918  |
|    mean velocity x      | 5.29        |
|    mean velocity y      | 9.13        |
|    mean velocity z      | 26.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -4.16e+06   |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.005921284 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.29       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 1.51e+08    |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00451    |
|    std                  | 1.42        |
|    value_loss           | 3.04e+08    |
-----------------------------------------
Eval num_timesteps=412500, episode_reward=-4173210.87 +/- 2055601.28
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.52503  |
|    mean velocity x | 5.1       |
|    mean velocity y | 9.16      |
|    mean velocity z | 25.5      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.17e+06 |
| time/              |           |
|    total_timesteps | 412500    |
----------------------------------
Eval num_timesteps=413000, episode_reward=-5179729.41 +/- 41646.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4956464 |
|    mean velocity x | 4.29       |
|    mean velocity y | 8.1        |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.18e+06  |
| time/              |            |
|    total_timesteps | 413000     |
-----------------------------------
Eval num_timesteps=413500, episode_reward=-4164832.95 +/- 2033598.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5521091 |
|    mean velocity x | 5.03       |
|    mean velocity y | 9.52       |
|    mean velocity z | 26.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.16e+06  |
| time/              |            |
|    total_timesteps | 413500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 202    |
|    time_elapsed    | 16769  |
|    total_timesteps | 413696 |
-------------------------------
Eval num_timesteps=414000, episode_reward=-4985983.34 +/- 23703.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5436364   |
|    mean velocity x      | 5.62         |
|    mean velocity y      | 9.33         |
|    mean velocity z      | 24.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.99e+06    |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0038610357 |
|    clip_fraction        | 0.00635      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+08      |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.43         |
|    value_loss           | 2.76e+08     |
------------------------------------------
Eval num_timesteps=414500, episode_reward=-5020141.58 +/- 26489.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6272976 |
|    mean velocity x | 5.35       |
|    mean velocity y | 9.1        |
|    mean velocity z | 24.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.02e+06  |
| time/              |            |
|    total_timesteps | 414500     |
-----------------------------------
Eval num_timesteps=415000, episode_reward=-5011382.12 +/- 22302.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5985525 |
|    mean velocity x | 5.31       |
|    mean velocity y | 9.38       |
|    mean velocity z | 24.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.01e+06  |
| time/              |            |
|    total_timesteps | 415000     |
-----------------------------------
Eval num_timesteps=415500, episode_reward=-5015059.45 +/- 36114.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4946578 |
|    mean velocity x | 4.62       |
|    mean velocity y | 8.47       |
|    mean velocity z | 24.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.02e+06  |
| time/              |            |
|    total_timesteps | 415500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 203    |
|    time_elapsed    | 16851  |
|    total_timesteps | 415744 |
-------------------------------
Eval num_timesteps=416000, episode_reward=-3689544.72 +/- 1815494.66
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5698116   |
|    mean velocity x      | 4.59         |
|    mean velocity y      | 8.32         |
|    mean velocity z      | 25.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.69e+06    |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0056890817 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+08     |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00436     |
|    std                  | 1.42         |
|    value_loss           | 2.72e+08     |
------------------------------------------
Eval num_timesteps=416500, episode_reward=-4613639.03 +/- 34752.17
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.606315 |
|    mean velocity x | 5.12      |
|    mean velocity y | 9.28      |
|    mean velocity z | 24        |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.61e+06 |
| time/              |           |
|    total_timesteps | 416500    |
----------------------------------
Eval num_timesteps=417000, episode_reward=-4590662.37 +/- 30420.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5273242 |
|    mean velocity x | 4.63       |
|    mean velocity y | 8.48       |
|    mean velocity z | 25.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.59e+06  |
| time/              |            |
|    total_timesteps | 417000     |
-----------------------------------
Eval num_timesteps=417500, episode_reward=-2794314.28 +/- 2195277.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6068001 |
|    mean velocity x | 4.1        |
|    mean velocity y | 8.13       |
|    mean velocity z | 25.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.79e+06  |
| time/              |            |
|    total_timesteps | 417500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 204    |
|    time_elapsed    | 16933  |
|    total_timesteps | 417792 |
-------------------------------
Eval num_timesteps=418000, episode_reward=-3326330.09 +/- 1632554.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.6108986   |
|    mean velocity x      | 4.21         |
|    mean velocity y      | 8.01         |
|    mean velocity z      | 24.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.33e+06    |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0049706893 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+08     |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 1.42         |
|    value_loss           | 3.14e+08     |
------------------------------------------
Eval num_timesteps=418500, episode_reward=-4158524.13 +/- 40928.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5625405 |
|    mean velocity x | 4.28       |
|    mean velocity y | 8.72       |
|    mean velocity z | 23.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.16e+06  |
| time/              |            |
|    total_timesteps | 418500     |
-----------------------------------
Eval num_timesteps=419000, episode_reward=-3316126.21 +/- 1635663.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6544375 |
|    mean velocity x | 4.29       |
|    mean velocity y | 8.63       |
|    mean velocity z | 24         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.32e+06  |
| time/              |            |
|    total_timesteps | 419000     |
-----------------------------------
Eval num_timesteps=419500, episode_reward=-4174091.80 +/- 34368.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6610994 |
|    mean velocity x | 4.5        |
|    mean velocity y | 8.73       |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.17e+06  |
| time/              |            |
|    total_timesteps | 419500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 205    |
|    time_elapsed    | 17016  |
|    total_timesteps | 419840 |
-------------------------------
Eval num_timesteps=420000, episode_reward=-4044497.47 +/- 41972.77
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5671797   |
|    mean velocity x      | 4.7          |
|    mean velocity y      | 8.64         |
|    mean velocity z      | 24.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.04e+06    |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0041660657 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+08     |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.00334     |
|    std                  | 1.42         |
|    value_loss           | 2.56e+08     |
------------------------------------------
Eval num_timesteps=420500, episode_reward=-3199307.46 +/- 1583347.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6500204 |
|    mean velocity x | 4.52       |
|    mean velocity y | 8.8        |
|    mean velocity z | 24.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.2e+06   |
| time/              |            |
|    total_timesteps | 420500     |
-----------------------------------
Eval num_timesteps=421000, episode_reward=-3215377.81 +/- 1590153.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6265868 |
|    mean velocity x | 4.68       |
|    mean velocity y | 8.68       |
|    mean velocity z | 23.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.22e+06  |
| time/              |            |
|    total_timesteps | 421000     |
-----------------------------------
Eval num_timesteps=421500, episode_reward=-4041495.09 +/- 31132.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6572778 |
|    mean velocity x | 4.98       |
|    mean velocity y | 9.26       |
|    mean velocity z | 22.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.04e+06  |
| time/              |            |
|    total_timesteps | 421500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 206    |
|    time_elapsed    | 17098  |
|    total_timesteps | 421888 |
-------------------------------
Eval num_timesteps=422000, episode_reward=-3087947.63 +/- 1525073.89
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5673145   |
|    mean velocity x      | 4.31         |
|    mean velocity y      | 8.32         |
|    mean velocity z      | 24.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.09e+06    |
| time/                   |              |
|    total_timesteps      | 422000       |
| train/                  |              |
|    approx_kl            | 0.0047711795 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+08     |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.0038      |
|    std                  | 1.41         |
|    value_loss           | 2.53e+08     |
------------------------------------------
Eval num_timesteps=422500, episode_reward=-3095016.41 +/- 1507783.33
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6346599 |
|    mean velocity x | 4.15       |
|    mean velocity y | 8.26       |
|    mean velocity z | 24.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.1e+06   |
| time/              |            |
|    total_timesteps | 422500     |
-----------------------------------
Eval num_timesteps=423000, episode_reward=-3834061.21 +/- 78072.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6272017 |
|    mean velocity x | 4.36       |
|    mean velocity y | 8.52       |
|    mean velocity z | 24.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.83e+06  |
| time/              |            |
|    total_timesteps | 423000     |
-----------------------------------
Eval num_timesteps=423500, episode_reward=-3862696.46 +/- 29645.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6027143 |
|    mean velocity x | 3.42       |
|    mean velocity y | 7.62       |
|    mean velocity z | 22.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.86e+06  |
| time/              |            |
|    total_timesteps | 423500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 207    |
|    time_elapsed    | 17180  |
|    total_timesteps | 423936 |
-------------------------------
Eval num_timesteps=424000, episode_reward=-3597710.70 +/- 42691.85
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.5645261  |
|    mean velocity x      | 3.52        |
|    mean velocity y      | 7.65        |
|    mean velocity z      | 22.9        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -3.6e+06    |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.005013141 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.27       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+08    |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00472    |
|    std                  | 1.41        |
|    value_loss           | 2.59e+08    |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=-2934286.47 +/- 1383661.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5276814 |
|    mean velocity x | 3.12       |
|    mean velocity y | 7.16       |
|    mean velocity z | 21.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.93e+06  |
| time/              |            |
|    total_timesteps | 424500     |
-----------------------------------
Eval num_timesteps=425000, episode_reward=-2909931.18 +/- 1390019.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4759946 |
|    mean velocity x | 4.24       |
|    mean velocity y | 7.61       |
|    mean velocity z | 21         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.91e+06  |
| time/              |            |
|    total_timesteps | 425000     |
-----------------------------------
Eval num_timesteps=425500, episode_reward=-3635473.38 +/- 38872.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.74674976 |
|    mean velocity x | 0.145       |
|    mean velocity y | 1.98        |
|    mean velocity z | 2.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.64e+06   |
| time/              |             |
|    total_timesteps | 425500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 208    |
|    time_elapsed    | 17262  |
|    total_timesteps | 425984 |
-------------------------------
Eval num_timesteps=426000, episode_reward=-1888589.32 +/- 1423484.70
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -1.5153984 |
|    mean velocity x      | 4.54       |
|    mean velocity y      | 8.27       |
|    mean velocity z      | 23.9       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -1.89e+06  |
| time/                   |            |
|    total_timesteps      | 426000     |
| train/                  |            |
|    approx_kl            | 0.00296647 |
|    clip_fraction        | 0.00288    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.26      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 6.38e+07   |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.00174   |
|    std                  | 1.4        |
|    value_loss           | 1.53e+08   |
----------------------------------------
Eval num_timesteps=426500, episode_reward=-2466743.66 +/- 1216737.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5396267 |
|    mean velocity x | 4.82       |
|    mean velocity y | 8.98       |
|    mean velocity z | 22.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.47e+06  |
| time/              |            |
|    total_timesteps | 426500     |
-----------------------------------
Eval num_timesteps=427000, episode_reward=-3065370.52 +/- 20125.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6274735 |
|    mean velocity x | 4.45       |
|    mean velocity y | 8.52       |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.07e+06  |
| time/              |            |
|    total_timesteps | 427000     |
-----------------------------------
Eval num_timesteps=427500, episode_reward=-2460562.37 +/- 1207427.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6057564 |
|    mean velocity x | 3.68       |
|    mean velocity y | 7.85       |
|    mean velocity z | 22.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.46e+06  |
| time/              |            |
|    total_timesteps | 427500     |
-----------------------------------
Eval num_timesteps=428000, episode_reward=-2447169.02 +/- 1208226.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49997228 |
|    mean velocity x | 0.0299      |
|    mean velocity y | 1.24        |
|    mean velocity z | 1.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.45e+06   |
| time/              |             |
|    total_timesteps | 428000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 209    |
|    time_elapsed    | 17364  |
|    total_timesteps | 428032 |
-------------------------------
Eval num_timesteps=428500, episode_reward=-2595483.80 +/- 32290.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.567059    |
|    mean velocity x      | 3.43         |
|    mean velocity y      | 7.74         |
|    mean velocity z      | 21.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -2.6e+06     |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 0.0035040989 |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.26        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+08     |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 1.41         |
|    value_loss           | 1.66e+08     |
------------------------------------------
Eval num_timesteps=429000, episode_reward=-2572957.84 +/- 28509.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5752873 |
|    mean velocity x | 4.7        |
|    mean velocity y | 8.66       |
|    mean velocity z | 21         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.57e+06  |
| time/              |            |
|    total_timesteps | 429000     |
-----------------------------------
Eval num_timesteps=429500, episode_reward=-2083470.88 +/- 1026338.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6137831 |
|    mean velocity x | 3.88       |
|    mean velocity y | 8.23       |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.08e+06  |
| time/              |            |
|    total_timesteps | 429500     |
-----------------------------------
Eval num_timesteps=430000, episode_reward=-2597310.67 +/- 31291.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5886416 |
|    mean velocity x | 3.86       |
|    mean velocity y | 8.12       |
|    mean velocity z | 23.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.6e+06   |
| time/              |            |
|    total_timesteps | 430000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 210    |
|    time_elapsed    | 17447  |
|    total_timesteps | 430080 |
-------------------------------
Eval num_timesteps=430500, episode_reward=-164437.78 +/- 23852.12
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.586949    |
|    mean velocity x      | 3.78         |
|    mean velocity y      | 7.54         |
|    mean velocity z      | 22.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.64e+05    |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0038876585 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+08     |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00222     |
|    std                  | 1.41         |
|    value_loss           | 2.24e+08     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=-186072.87 +/- 55132.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5729423 |
|    mean velocity x | 4.05       |
|    mean velocity y | 7.94       |
|    mean velocity z | 22.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.86e+05  |
| time/              |            |
|    total_timesteps | 431000     |
-----------------------------------
Eval num_timesteps=431500, episode_reward=-166198.48 +/- 41249.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5693082 |
|    mean velocity x | 3.91       |
|    mean velocity y | 7.61       |
|    mean velocity z | 22.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.66e+05  |
| time/              |            |
|    total_timesteps | 431500     |
-----------------------------------
Eval num_timesteps=432000, episode_reward=-175397.69 +/- 40437.08
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.161642 |
|    mean velocity x | 0.446     |
|    mean velocity y | 3.14      |
|    mean velocity z | 4.81      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.75e+05 |
| time/              |           |
|    total_timesteps | 432000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 211    |
|    time_elapsed    | 17529  |
|    total_timesteps | 432128 |
-------------------------------
Eval num_timesteps=432500, episode_reward=-1795757.24 +/- 1371777.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5959405   |
|    mean velocity x      | 4.54         |
|    mean velocity y      | 8.41         |
|    mean velocity z      | 21.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.8e+06     |
| time/                   |              |
|    total_timesteps      | 432500       |
| train/                  |              |
|    approx_kl            | 0.0056306906 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.05e+07     |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00461     |
|    std                  | 1.42         |
|    value_loss           | 1.76e+08     |
------------------------------------------
Eval num_timesteps=433000, episode_reward=-2346584.12 +/- 1118272.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5239092 |
|    mean velocity x | 2.51       |
|    mean velocity y | 6.4        |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.35e+06  |
| time/              |            |
|    total_timesteps | 433000     |
-----------------------------------
Eval num_timesteps=433500, episode_reward=-2912298.47 +/- 24274.74
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.68194  |
|    mean velocity x | 4.12      |
|    mean velocity y | 8.82      |
|    mean velocity z | 21.1      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.91e+06 |
| time/              |           |
|    total_timesteps | 433500    |
----------------------------------
Eval num_timesteps=434000, episode_reward=-2892749.30 +/- 41150.55
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.672753 |
|    mean velocity x | 4.27      |
|    mean velocity y | 8.78      |
|    mean velocity z | 22.4      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.89e+06 |
| time/              |           |
|    total_timesteps | 434000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 212    |
|    time_elapsed    | 17611  |
|    total_timesteps | 434176 |
-------------------------------
Eval num_timesteps=434500, episode_reward=-165416.39 +/- 50509.89
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.6316489   |
|    mean velocity x      | 3.26         |
|    mean velocity y      | 7.83         |
|    mean velocity z      | 21.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.65e+05    |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0034840852 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 7.16e+07     |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 1.41         |
|    value_loss           | 1.58e+08     |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-167856.72 +/- 68439.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7030042 |
|    mean velocity x | 3.96       |
|    mean velocity y | 8.28       |
|    mean velocity z | 21.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.68e+05  |
| time/              |            |
|    total_timesteps | 435000     |
-----------------------------------
Eval num_timesteps=435500, episode_reward=-155957.74 +/- 82681.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6477402 |
|    mean velocity x | 3.75       |
|    mean velocity y | 7.55       |
|    mean velocity z | 22         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.56e+05  |
| time/              |            |
|    total_timesteps | 435500     |
-----------------------------------
Eval num_timesteps=436000, episode_reward=-146930.64 +/- 68840.46
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6164424 |
|    mean velocity x | 4.21       |
|    mean velocity y | 8.38       |
|    mean velocity z | 22         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.47e+05  |
| time/              |            |
|    total_timesteps | 436000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 213    |
|    time_elapsed    | 17693  |
|    total_timesteps | 436224 |
-------------------------------
Eval num_timesteps=436500, episode_reward=-113776.41 +/- 51989.16
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.7139016  |
|    mean velocity x      | 4.77        |
|    mean velocity y      | 8.89        |
|    mean velocity z      | 20.4        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.14e+05   |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.004905207 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.28       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 7.54e+07    |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00391    |
|    std                  | 1.41        |
|    value_loss           | 1.74e+08    |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=-167034.22 +/- 30728.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6447968 |
|    mean velocity x | 3.95       |
|    mean velocity y | 8.01       |
|    mean velocity z | 21.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.67e+05  |
| time/              |            |
|    total_timesteps | 437000     |
-----------------------------------
Eval num_timesteps=437500, episode_reward=-123433.78 +/- 57700.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.70829445 |
|    mean velocity x | -0.0572     |
|    mean velocity y | 1.39        |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 437500      |
------------------------------------
Eval num_timesteps=438000, episode_reward=-118342.21 +/- 51346.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6862042 |
|    mean velocity x | 4.26       |
|    mean velocity y | 8.55       |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 438000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 214    |
|    time_elapsed    | 17776  |
|    total_timesteps | 438272 |
-------------------------------
Eval num_timesteps=438500, episode_reward=-135659.26 +/- 41307.03
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.2165533   |
|    mean velocity x      | 1.22         |
|    mean velocity y      | 4.01         |
|    mean velocity z      | 12.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.36e+05    |
| time/                   |              |
|    total_timesteps      | 438500       |
| train/                  |              |
|    approx_kl            | 0.0016780277 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 7.94e+07     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.42         |
|    value_loss           | 1.71e+08     |
------------------------------------------
Eval num_timesteps=439000, episode_reward=-136517.76 +/- 35138.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1417694 |
|    mean velocity x | 1.72       |
|    mean velocity y | 4.26       |
|    mean velocity z | 9.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.37e+05  |
| time/              |            |
|    total_timesteps | 439000     |
-----------------------------------
Eval num_timesteps=439500, episode_reward=-142134.39 +/- 45148.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6602532 |
|    mean velocity x | 3.54       |
|    mean velocity y | 7.55       |
|    mean velocity z | 20         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.42e+05  |
| time/              |            |
|    total_timesteps | 439500     |
-----------------------------------
Eval num_timesteps=440000, episode_reward=-121104.05 +/- 59251.33
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6850613 |
|    mean velocity x | 4.41       |
|    mean velocity y | 8.34       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.21e+05  |
| time/              |            |
|    total_timesteps | 440000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 215    |
|    time_elapsed    | 17858  |
|    total_timesteps | 440320 |
-------------------------------
Eval num_timesteps=440500, episode_reward=-137474.31 +/- 31285.44
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.7105699   |
|    mean velocity x      | 4.25         |
|    mean velocity y      | 8.93         |
|    mean velocity z      | 21.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.37e+05    |
| time/                   |              |
|    total_timesteps      | 440500       |
| train/                  |              |
|    approx_kl            | 0.0042825164 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.24e+07     |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00351     |
|    std                  | 1.42         |
|    value_loss           | 1.24e+08     |
------------------------------------------
Eval num_timesteps=441000, episode_reward=-108395.94 +/- 35260.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6308435 |
|    mean velocity x | 3.16       |
|    mean velocity y | 7.01       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 441000     |
-----------------------------------
Eval num_timesteps=441500, episode_reward=-114890.61 +/- 21075.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36059654 |
|    mean velocity x | -0.0199     |
|    mean velocity y | 1.02        |
|    mean velocity z | 0.951       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 441500      |
------------------------------------
Eval num_timesteps=442000, episode_reward=-99835.94 +/- 66380.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0075327 |
|    mean velocity x | 0.871      |
|    mean velocity y | 3.27       |
|    mean velocity z | 6.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.98e+04  |
| time/              |            |
|    total_timesteps | 442000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 216    |
|    time_elapsed    | 17943  |
|    total_timesteps | 442368 |
-------------------------------
Eval num_timesteps=442500, episode_reward=-120006.85 +/- 32458.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.6864525   |
|    mean velocity x      | 3.41         |
|    mean velocity y      | 7.32         |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.2e+05     |
| time/                   |              |
|    total_timesteps      | 442500       |
| train/                  |              |
|    approx_kl            | 0.0027889507 |
|    clip_fraction        | 0.00474      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 3.02e+07     |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.42         |
|    value_loss           | 1.02e+08     |
------------------------------------------
Eval num_timesteps=443000, episode_reward=-87155.25 +/- 49902.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7066708 |
|    mean velocity x | 4.1        |
|    mean velocity y | 8.52       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.72e+04  |
| time/              |            |
|    total_timesteps | 443000     |
-----------------------------------
Eval num_timesteps=443500, episode_reward=-147089.27 +/- 8486.72
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.556752 |
|    mean velocity x | 1.88      |
|    mean velocity y | 5.9       |
|    mean velocity z | 15.5      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.47e+05 |
| time/              |           |
|    total_timesteps | 443500    |
----------------------------------
Eval num_timesteps=444000, episode_reward=-80780.41 +/- 28310.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4554505 |
|    mean velocity x | 0.321      |
|    mean velocity y | 0.945      |
|    mean velocity z | 2.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.08e+04  |
| time/              |            |
|    total_timesteps | 444000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 217    |
|    time_elapsed    | 18024  |
|    total_timesteps | 444416 |
-------------------------------
Eval num_timesteps=444500, episode_reward=-103664.61 +/- 23321.94
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.5733658  |
|    mean velocity x      | 0.499       |
|    mean velocity y      | 2.2         |
|    mean velocity z      | 4.89        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.04e+05   |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.006247411 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.3        |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 6.98e+07    |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00786    |
|    std                  | 1.42        |
|    value_loss           | 8.74e+07    |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=-132085.50 +/- 53048.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5466523 |
|    mean velocity x | -0.628     |
|    mean velocity y | 1.36       |
|    mean velocity z | 2.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.32e+05  |
| time/              |            |
|    total_timesteps | 445000     |
-----------------------------------
Eval num_timesteps=445500, episode_reward=-95106.19 +/- 28581.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8618581 |
|    mean velocity x | 0.413      |
|    mean velocity y | 2.87       |
|    mean velocity z | 7.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 445500     |
-----------------------------------
Eval num_timesteps=446000, episode_reward=-110579.49 +/- 35916.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7628666 |
|    mean velocity x | 0.291      |
|    mean velocity y | 2.26       |
|    mean velocity z | 6.55       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.11e+05  |
| time/              |            |
|    total_timesteps | 446000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 218    |
|    time_elapsed    | 18104  |
|    total_timesteps | 446464 |
-------------------------------
Eval num_timesteps=446500, episode_reward=-4594269.37 +/- 27164.85
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.51259315 |
|    mean velocity x      | 0.529       |
|    mean velocity y      | 1.24        |
|    mean velocity z      | 2.88        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -4.59e+06   |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.47319874  |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.33       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 3.58e+07    |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.0134      |
|    std                  | 1.44        |
|    value_loss           | 7.48e+07    |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=-3695877.40 +/- 1847792.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7012035 |
|    mean velocity x | -3.53      |
|    mean velocity y | 0.732      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.7e+06   |
| time/              |            |
|    total_timesteps | 447000     |
-----------------------------------
Eval num_timesteps=447500, episode_reward=-4611717.84 +/- 25045.09
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.76071  |
|    mean velocity x | -4.55     |
|    mean velocity y | -0.918    |
|    mean velocity z | 18        |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.61e+06 |
| time/              |           |
|    total_timesteps | 447500    |
----------------------------------
Eval num_timesteps=448000, episode_reward=-4605596.95 +/- 26351.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7340009 |
|    mean velocity x | -3.68      |
|    mean velocity y | 0.181      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.61e+06  |
| time/              |            |
|    total_timesteps | 448000     |
-----------------------------------
Eval num_timesteps=448500, episode_reward=-3681086.93 +/- 1839677.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7293992 |
|    mean velocity x | -3.74      |
|    mean velocity y | 0.267      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.68e+06  |
| time/              |            |
|    total_timesteps | 448500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 219    |
|    time_elapsed    | 18206  |
|    total_timesteps | 448512 |
-------------------------------
Eval num_timesteps=449000, episode_reward=-4655691.49 +/- 28163.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.7577202    |
|    mean velocity x      | -5.5          |
|    mean velocity y      | -1.54         |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.66e+06     |
| time/                   |               |
|    total_timesteps      | 449000        |
| train/                  |               |
|    approx_kl            | 0.00089016533 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.33         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 8.32e+07      |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.00129      |
|    std                  | 1.44          |
|    value_loss           | 2.56e+08      |
-------------------------------------------
Eval num_timesteps=449500, episode_reward=-4654606.31 +/- 6512.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7477871 |
|    mean velocity x | -3.57      |
|    mean velocity y | 0.771      |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.65e+06  |
| time/              |            |
|    total_timesteps | 449500     |
-----------------------------------
Eval num_timesteps=450000, episode_reward=-4674727.38 +/- 32708.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7285327 |
|    mean velocity x | -4.39      |
|    mean velocity y | -0.596     |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.67e+06  |
| time/              |            |
|    total_timesteps | 450000     |
-----------------------------------
Eval num_timesteps=450500, episode_reward=-4660267.14 +/- 52297.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7211426 |
|    mean velocity x | -3.96      |
|    mean velocity y | -0.161     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.66e+06  |
| time/              |            |
|    total_timesteps | 450500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 220    |
|    time_elapsed    | 18288  |
|    total_timesteps | 450560 |
-------------------------------
Eval num_timesteps=451000, episode_reward=-3655388.04 +/- 1825705.77
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.6702192  |
|    mean velocity x      | -4.59       |
|    mean velocity y      | -0.343      |
|    mean velocity z      | 18.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -3.66e+06   |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.000575613 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.33       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+08    |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00101    |
|    std                  | 1.44        |
|    value_loss           | 2.85e+08    |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=-4600400.93 +/- 23013.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7133536 |
|    mean velocity x | -5         |
|    mean velocity y | -0.753     |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.6e+06   |
| time/              |            |
|    total_timesteps | 451500     |
-----------------------------------
Eval num_timesteps=452000, episode_reward=-4601371.35 +/- 37274.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6960075 |
|    mean velocity x | -4.07      |
|    mean velocity y | 0.4        |
|    mean velocity z | 19         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.6e+06   |
| time/              |            |
|    total_timesteps | 452000     |
-----------------------------------
Eval num_timesteps=452500, episode_reward=-4571078.24 +/- 23477.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7550703 |
|    mean velocity x | -3.63      |
|    mean velocity y | 0.978      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.57e+06  |
| time/              |            |
|    total_timesteps | 452500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 221    |
|    time_elapsed    | 18370  |
|    total_timesteps | 452608 |
-------------------------------
Eval num_timesteps=453000, episode_reward=-4667051.49 +/- 6539.92
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.7253813    |
|    mean velocity x      | -3.56         |
|    mean velocity y      | 0.263         |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.67e+06     |
| time/                   |               |
|    total_timesteps      | 453000        |
| train/                  |               |
|    approx_kl            | 0.00067127135 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.33         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+08      |
|    n_updates            | 2210          |
|    policy_gradient_loss | -0.00127      |
|    std                  | 1.44          |
|    value_loss           | 2.63e+08      |
-------------------------------------------
Eval num_timesteps=453500, episode_reward=-3729180.27 +/- 1864669.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7892336 |
|    mean velocity x | -4.19      |
|    mean velocity y | 0.202      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.73e+06  |
| time/              |            |
|    total_timesteps | 453500     |
-----------------------------------
Eval num_timesteps=454000, episode_reward=-4648581.44 +/- 27397.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7434727 |
|    mean velocity x | -6.23      |
|    mean velocity y | -1.38      |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.65e+06  |
| time/              |            |
|    total_timesteps | 454000     |
-----------------------------------
Eval num_timesteps=454500, episode_reward=-4646663.18 +/- 37558.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18549743 |
|    mean velocity x | -0.14       |
|    mean velocity y | 0.298       |
|    mean velocity z | 0.348       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.65e+06   |
| time/              |             |
|    total_timesteps | 454500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 222    |
|    time_elapsed    | 18452  |
|    total_timesteps | 454656 |
-------------------------------
Eval num_timesteps=455000, episode_reward=-4524122.98 +/- 14848.46
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.7230455   |
|    mean velocity x      | -4.33        |
|    mean velocity y      | -0.19        |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.52e+06    |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0010753286 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.33        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1e+08        |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 1.44         |
|    value_loss           | 1.83e+08     |
------------------------------------------
Eval num_timesteps=455500, episode_reward=-4547957.56 +/- 24262.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4951093 |
|    mean velocity x | -3.77      |
|    mean velocity y | 0.225      |
|    mean velocity z | 15.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.55e+06  |
| time/              |            |
|    total_timesteps | 455500     |
-----------------------------------
Eval num_timesteps=456000, episode_reward=-4571689.89 +/- 25592.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7041166 |
|    mean velocity x | -4.94      |
|    mean velocity y | -0.7       |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.57e+06  |
| time/              |            |
|    total_timesteps | 456000     |
-----------------------------------
Eval num_timesteps=456500, episode_reward=-4565086.84 +/- 35092.71
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.694874 |
|    mean velocity x | -4        |
|    mean velocity y | 0.0679    |
|    mean velocity z | 17.4      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.57e+06 |
| time/              |           |
|    total_timesteps | 456500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 223    |
|    time_elapsed    | 18534  |
|    total_timesteps | 456704 |
-------------------------------
Eval num_timesteps=457000, episode_reward=-4471118.18 +/- 14589.76
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.6885766   |
|    mean velocity x      | -5.08        |
|    mean velocity y      | -0.792       |
|    mean velocity z      | 18           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.47e+06    |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0012009761 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.33        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+08     |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.44         |
|    value_loss           | 2.33e+08     |
------------------------------------------
Eval num_timesteps=457500, episode_reward=-4474502.01 +/- 25731.09
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.642499 |
|    mean velocity x | -4.79     |
|    mean velocity y | -0.514    |
|    mean velocity z | 15.8      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.47e+06 |
| time/              |           |
|    total_timesteps | 457500    |
----------------------------------
Eval num_timesteps=458000, episode_reward=-4461950.58 +/- 18217.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7008005 |
|    mean velocity x | -4.65      |
|    mean velocity y | -0.743     |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.46e+06  |
| time/              |            |
|    total_timesteps | 458000     |
-----------------------------------
Eval num_timesteps=458500, episode_reward=-4458780.77 +/- 34542.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7609534 |
|    mean velocity x | -4.32      |
|    mean velocity y | -0.214     |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.46e+06  |
| time/              |            |
|    total_timesteps | 458500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 224    |
|    time_elapsed    | 18617  |
|    total_timesteps | 458752 |
-------------------------------
Eval num_timesteps=459000, episode_reward=-4483051.11 +/- 15539.27
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.6352003   |
|    mean velocity x      | -4.52        |
|    mean velocity y      | -0.507       |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.48e+06    |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0010072932 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.33        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+08     |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.44         |
|    value_loss           | 2.41e+08     |
------------------------------------------
Eval num_timesteps=459500, episode_reward=-4488913.67 +/- 14281.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7721426 |
|    mean velocity x | -4.4       |
|    mean velocity y | 0.132      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.49e+06  |
| time/              |            |
|    total_timesteps | 459500     |
-----------------------------------
Eval num_timesteps=460000, episode_reward=-3609207.20 +/- 1804758.06
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.803976 |
|    mean velocity x | -1.28     |
|    mean velocity y | 1.09      |
|    mean velocity z | 8.71      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.61e+06 |
| time/              |           |
|    total_timesteps | 460000    |
----------------------------------
Eval num_timesteps=460500, episode_reward=-4516617.36 +/- 35165.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7099687 |
|    mean velocity x | -4.42      |
|    mean velocity y | -0.324     |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.52e+06  |
| time/              |            |
|    total_timesteps | 460500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 225    |
|    time_elapsed    | 18699  |
|    total_timesteps | 460800 |
-------------------------------
Eval num_timesteps=461000, episode_reward=-3705985.34 +/- 1852532.96
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.2550243   |
|    mean velocity x      | -2.97        |
|    mean velocity y      | 0.034        |
|    mean velocity z      | 11.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.71e+06    |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0033624093 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.33        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.59e+07     |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.0034      |
|    std                  | 1.44         |
|    value_loss           | 2.46e+08     |
------------------------------------------
Eval num_timesteps=461500, episode_reward=-4644601.34 +/- 38831.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7273375 |
|    mean velocity x | -4.04      |
|    mean velocity y | 0.276      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.64e+06  |
| time/              |            |
|    total_timesteps | 461500     |
-----------------------------------
Eval num_timesteps=462000, episode_reward=-4629007.41 +/- 46272.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6163485 |
|    mean velocity x | -4.16      |
|    mean velocity y | -0.634     |
|    mean velocity z | 16         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.63e+06  |
| time/              |            |
|    total_timesteps | 462000     |
-----------------------------------
Eval num_timesteps=462500, episode_reward=-4641445.02 +/- 38377.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6460485 |
|    mean velocity x | -3.83      |
|    mean velocity y | -0.289     |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.64e+06  |
| time/              |            |
|    total_timesteps | 462500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 226    |
|    time_elapsed    | 18781  |
|    total_timesteps | 462848 |
-------------------------------
Eval num_timesteps=463000, episode_reward=-4715608.44 +/- 24505.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.7352436   |
|    mean velocity x      | -3.75        |
|    mean velocity y      | 0.208        |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.72e+06    |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0020190184 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.33        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+08     |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.44         |
|    value_loss           | 2.71e+08     |
------------------------------------------
Eval num_timesteps=463500, episode_reward=-4660365.25 +/- 28403.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7228377 |
|    mean velocity x | -0.509     |
|    mean velocity y | 1.83       |
|    mean velocity z | 6.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.66e+06  |
| time/              |            |
|    total_timesteps | 463500     |
-----------------------------------
Eval num_timesteps=464000, episode_reward=-3752296.45 +/- 1876242.51
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.681571 |
|    mean velocity x | -3.61     |
|    mean velocity y | 0.42      |
|    mean velocity z | 17.9      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.75e+06 |
| time/              |           |
|    total_timesteps | 464000    |
----------------------------------
Eval num_timesteps=464500, episode_reward=-3738993.63 +/- 1868208.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7071211 |
|    mean velocity x | -4.57      |
|    mean velocity y | -0.616     |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.74e+06  |
| time/              |            |
|    total_timesteps | 464500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 227    |
|    time_elapsed    | 18863  |
|    total_timesteps | 464896 |
-------------------------------
Eval num_timesteps=465000, episode_reward=-4708121.59 +/- 34156.83
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.7497392   |
|    mean velocity x      | -3.48        |
|    mean velocity y      | 0.706        |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.71e+06    |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0021710119 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.33        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 9.43e+07     |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 1.44         |
|    value_loss           | 2.15e+08     |
------------------------------------------
Eval num_timesteps=465500, episode_reward=-4671336.69 +/- 13917.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.7181256 |
|    mean velocity x | -3.72      |
|    mean velocity y | -0.00127   |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.67e+06  |
| time/              |            |
|    total_timesteps | 465500     |
-----------------------------------
Eval num_timesteps=466000, episode_reward=-4708445.22 +/- 38002.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5894938 |
|    mean velocity x | -3.65      |
|    mean velocity y | -0.107     |
|    mean velocity z | 16.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.71e+06  |
| time/              |            |
|    total_timesteps | 466000     |
-----------------------------------
Eval num_timesteps=466500, episode_reward=-4709423.84 +/- 26563.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6493552 |
|    mean velocity x | -3.69      |
|    mean velocity y | 0.605      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.71e+06  |
| time/              |            |
|    total_timesteps | 466500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 228    |
|    time_elapsed    | 18945  |
|    total_timesteps | 466944 |
-------------------------------
Eval num_timesteps=467000, episode_reward=-4740510.15 +/- 23897.32
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.7316312    |
|    mean velocity x      | -4.04         |
|    mean velocity y      | 0.224         |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.74e+06     |
| time/                   |               |
|    total_timesteps      | 467000        |
| train/                  |               |
|    approx_kl            | 0.00034033888 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.33         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+08      |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.000781     |
|    std                  | 1.44          |
|    value_loss           | 3.03e+08      |
-------------------------------------------
Eval num_timesteps=467500, episode_reward=-4745700.98 +/- 32479.04
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.686309 |
|    mean velocity x | -3.73     |
|    mean velocity y | -0.0272   |
|    mean velocity z | 18.4      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.75e+06 |
| time/              |           |
|    total_timesteps | 467500    |
----------------------------------
Eval num_timesteps=468000, episode_reward=-3803394.77 +/- 1901725.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6641119 |
|    mean velocity x | -4.35      |
|    mean velocity y | 0.114      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.8e+06   |
| time/              |            |
|    total_timesteps | 468000     |
-----------------------------------
Eval num_timesteps=468500, episode_reward=-4747243.18 +/- 34496.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6202722 |
|    mean velocity x | -3.52      |
|    mean velocity y | 0.577      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.75e+06  |
| time/              |            |
|    total_timesteps | 468500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 229    |
|    time_elapsed    | 19027  |
|    total_timesteps | 468992 |
-------------------------------
Eval num_timesteps=469000, episode_reward=-4554196.26 +/- 28632.76
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.7067788  |
|    mean velocity x      | -3.79       |
|    mean velocity y      | 0.941       |
|    mean velocity z      | 19.1        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -4.55e+06   |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.003907768 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.33       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.6e+08     |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00506    |
|    std                  | 1.44        |
|    value_loss           | 3.21e+08    |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=-4543679.81 +/- 34226.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4108888 |
|    mean velocity x | -3.7       |
|    mean velocity y | -0.0667    |
|    mean velocity z | 15.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.54e+06  |
| time/              |            |
|    total_timesteps | 469500     |
-----------------------------------
Eval num_timesteps=470000, episode_reward=-4552654.82 +/- 17712.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6292964 |
|    mean velocity x | -3.77      |
|    mean velocity y | 0.421      |
|    mean velocity z | 17         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.55e+06  |
| time/              |            |
|    total_timesteps | 470000     |
-----------------------------------
Eval num_timesteps=470500, episode_reward=-2727624.76 +/- 2225652.59
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.615696 |
|    mean velocity x | -4.18     |
|    mean velocity y | -0.0989   |
|    mean velocity z | 17.4      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -2.73e+06 |
| time/              |           |
|    total_timesteps | 470500    |
----------------------------------
Eval num_timesteps=471000, episode_reward=-4531013.70 +/- 18593.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6446265 |
|    mean velocity x | -3.52      |
|    mean velocity y | 0.413      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.53e+06  |
| time/              |            |
|    total_timesteps | 471000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 230    |
|    time_elapsed    | 19129  |
|    total_timesteps | 471040 |
-------------------------------
Eval num_timesteps=471500, episode_reward=-4389209.69 +/- 21870.86
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -0.5490062 |
|    mean velocity x      | -1.33      |
|    mean velocity y      | 0.153      |
|    mean velocity z      | 4.04       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -4.39e+06  |
| time/                   |            |
|    total_timesteps      | 471500     |
| train/                  |            |
|    approx_kl            | 0.00370605 |
|    clip_fraction        | 0.00742    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.33      |
|    explained_variance   | 1.19e-07   |
|    learning_rate        | 0.001      |
|    loss                 | 1.14e+08   |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0028    |
|    std                  | 1.44       |
|    value_loss           | 2.51e+08   |
----------------------------------------
Eval num_timesteps=472000, episode_reward=-4391513.80 +/- 41410.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6437635 |
|    mean velocity x | -3.87      |
|    mean velocity y | 0.316      |
|    mean velocity z | 16.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.39e+06  |
| time/              |            |
|    total_timesteps | 472000     |
-----------------------------------
Eval num_timesteps=472500, episode_reward=-4375741.02 +/- 47462.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.6075959 |
|    mean velocity x | -4.42      |
|    mean velocity y | -0.459     |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.38e+06  |
| time/              |            |
|    total_timesteps | 472500     |
-----------------------------------
Eval num_timesteps=473000, episode_reward=-4393061.99 +/- 43792.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5658729 |
|    mean velocity x | -4.27      |
|    mean velocity y | -1.31      |
|    mean velocity z | 16.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.39e+06  |
| time/              |            |
|    total_timesteps | 473000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 231    |
|    time_elapsed    | 19211  |
|    total_timesteps | 473088 |
-------------------------------
Eval num_timesteps=473500, episode_reward=-4444919.84 +/- 27186.75
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.0623534   |
|    mean velocity x      | -1.8         |
|    mean velocity y      | 0.581        |
|    mean velocity z      | 10.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.44e+06    |
| time/                   |              |
|    total_timesteps      | 473500       |
| train/                  |              |
|    approx_kl            | 0.0026346953 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.34        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 6.06e+07     |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 1.45         |
|    value_loss           | 1.94e+08     |
------------------------------------------
Eval num_timesteps=474000, episode_reward=-3561075.23 +/- 1780454.07
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.592111 |
|    mean velocity x | -4.28     |
|    mean velocity y | -0.289    |
|    mean velocity z | 18.2      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.56e+06 |
| time/              |           |
|    total_timesteps | 474000    |
----------------------------------
Eval num_timesteps=474500, episode_reward=-4427621.24 +/- 12908.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43616682 |
|    mean velocity x | -0.341      |
|    mean velocity y | 0.457       |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.43e+06   |
| time/              |             |
|    total_timesteps | 474500      |
------------------------------------
Eval num_timesteps=475000, episode_reward=-4481812.35 +/- 9310.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5543127 |
|    mean velocity x | -4.74      |
|    mean velocity y | -0.645     |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.48e+06  |
| time/              |            |
|    total_timesteps | 475000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 232    |
|    time_elapsed    | 19293  |
|    total_timesteps | 475136 |
-------------------------------
Eval num_timesteps=475500, episode_reward=-4442666.48 +/- 31451.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.6136011    |
|    mean velocity x      | -4.83         |
|    mean velocity y      | -0.766        |
|    mean velocity z      | 17.1          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.44e+06     |
| time/                   |               |
|    total_timesteps      | 475500        |
| train/                  |               |
|    approx_kl            | 0.00078551564 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.35         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.001         |
|    loss                 | 8.08e+07      |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.00134      |
|    std                  | 1.45          |
|    value_loss           | 1.92e+08      |
-------------------------------------------
Eval num_timesteps=476000, episode_reward=-4426881.64 +/- 24701.32
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.169042 |
|    mean velocity x | -2.61     |
|    mean velocity y | -0.101    |
|    mean velocity z | 12.4      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.43e+06 |
| time/              |           |
|    total_timesteps | 476000    |
----------------------------------
Eval num_timesteps=476500, episode_reward=-4435020.60 +/- 24369.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2378542 |
|    mean velocity x | -2.92      |
|    mean velocity y | 0.179      |
|    mean velocity z | 13.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.44e+06  |
| time/              |            |
|    total_timesteps | 476500     |
-----------------------------------
Eval num_timesteps=477000, episode_reward=-4433382.36 +/- 30328.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4605638 |
|    mean velocity x | -4.65      |
|    mean velocity y | -0.854     |
|    mean velocity z | 14.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.43e+06  |
| time/              |            |
|    total_timesteps | 477000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 233    |
|    time_elapsed    | 19375  |
|    total_timesteps | 477184 |
-------------------------------
Eval num_timesteps=477500, episode_reward=-4231505.65 +/- 20097.12
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.4745647   |
|    mean velocity x      | -4.08        |
|    mean velocity y      | -0.914       |
|    mean velocity z      | 16.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.23e+06    |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0018898709 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+08     |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 1.45         |
|    value_loss           | 2.31e+08     |
------------------------------------------
Eval num_timesteps=478000, episode_reward=-4233939.60 +/- 5492.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4933101 |
|    mean velocity x | -4.81      |
|    mean velocity y | -1.28      |
|    mean velocity z | 16.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.23e+06  |
| time/              |            |
|    total_timesteps | 478000     |
-----------------------------------
Eval num_timesteps=478500, episode_reward=-3384481.51 +/- 1685612.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09252155 |
|    mean velocity x | 0.0616      |
|    mean velocity y | 0.272       |
|    mean velocity z | 0.255       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.38e+06   |
| time/              |             |
|    total_timesteps | 478500      |
------------------------------------
Eval num_timesteps=479000, episode_reward=-4235702.49 +/- 24499.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5449027 |
|    mean velocity x | -3.07      |
|    mean velocity y | -0.0532    |
|    mean velocity z | 15.9       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.24e+06  |
| time/              |            |
|    total_timesteps | 479000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 234    |
|    time_elapsed    | 19457  |
|    total_timesteps | 479232 |
-------------------------------
Eval num_timesteps=479500, episode_reward=-116187.66 +/- 9194.16
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.5755804   |
|    mean velocity x      | -4.06        |
|    mean velocity y      | -0.202       |
|    mean velocity z      | 18           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.16e+05    |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0040401043 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.22e+07     |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00428     |
|    std                  | 1.45         |
|    value_loss           | 1.8e+08      |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-98758.23 +/- 28661.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1161565 |
|    mean velocity x | -2.64      |
|    mean velocity y | 0.273      |
|    mean velocity z | 11.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.88e+04  |
| time/              |            |
|    total_timesteps | 480000     |
-----------------------------------
Eval num_timesteps=480500, episode_reward=-81145.06 +/- 43310.46
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2292798 |
|    mean velocity x | -1.62      |
|    mean velocity y | 1.23       |
|    mean velocity z | 12.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.11e+04  |
| time/              |            |
|    total_timesteps | 480500     |
-----------------------------------
Eval num_timesteps=481000, episode_reward=-112780.39 +/- 19564.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5651806 |
|    mean velocity x | -2.24      |
|    mean velocity y | -0.907     |
|    mean velocity z | 6.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.13e+05  |
| time/              |            |
|    total_timesteps | 481000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 235    |
|    time_elapsed    | 19538  |
|    total_timesteps | 481280 |
-------------------------------
Eval num_timesteps=481500, episode_reward=-99531.24 +/- 17617.04
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.59137833 |
|    mean velocity x      | -1.84       |
|    mean velocity y      | -0.804      |
|    mean velocity z      | 7.08        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.95e+04   |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.004513209 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 7.05e+07    |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00394    |
|    std                  | 1.45        |
|    value_loss           | 1.46e+08    |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=-90393.30 +/- 11856.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.97573054 |
|    mean velocity x | -2.04       |
|    mean velocity y | 0.206       |
|    mean velocity z | 12.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.04e+04   |
| time/              |             |
|    total_timesteps | 482000      |
------------------------------------
Eval num_timesteps=482500, episode_reward=-91673.43 +/- 49690.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.70815414 |
|    mean velocity x | -1.42       |
|    mean velocity y | 0.226       |
|    mean velocity z | 7.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.17e+04   |
| time/              |             |
|    total_timesteps | 482500      |
------------------------------------
Eval num_timesteps=483000, episode_reward=-99556.32 +/- 47274.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4221255 |
|    mean velocity x | -3.49      |
|    mean velocity y | -0.416     |
|    mean velocity z | 15.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.96e+04  |
| time/              |            |
|    total_timesteps | 483000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 236    |
|    time_elapsed    | 19618  |
|    total_timesteps | 483328 |
-------------------------------
Eval num_timesteps=483500, episode_reward=-95679.32 +/- 42966.98
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.73900086  |
|    mean velocity x      | -1.33        |
|    mean velocity y      | 0.514        |
|    mean velocity z      | 7.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.57e+04    |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 0.0017344509 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 9.34e+07     |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 1.45         |
|    value_loss           | 1.6e+08      |
------------------------------------------
Eval num_timesteps=484000, episode_reward=-85299.28 +/- 38611.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.430943 |
|    mean velocity x | -4.3      |
|    mean velocity y | -0.837    |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.53e+04 |
| time/              |           |
|    total_timesteps | 484000    |
----------------------------------
Eval num_timesteps=484500, episode_reward=-88224.54 +/- 33767.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.9283827 |
|    mean velocity x | -3.03      |
|    mean velocity y | -0.668     |
|    mean velocity z | 10         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.82e+04  |
| time/              |            |
|    total_timesteps | 484500     |
-----------------------------------
Eval num_timesteps=485000, episode_reward=-101418.67 +/- 43923.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.5019829 |
|    mean velocity x | -2.22      |
|    mean velocity y | 1.72       |
|    mean velocity z | 15.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 485000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 237    |
|    time_elapsed    | 19699  |
|    total_timesteps | 485376 |
-------------------------------
Eval num_timesteps=485500, episode_reward=-99469.91 +/- 69430.95
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -1.3498892    |
|    mean velocity x      | -3.56         |
|    mean velocity y      | -0.331        |
|    mean velocity z      | 15.4          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.95e+04     |
| time/                   |               |
|    total_timesteps      | 485500        |
| train/                  |               |
|    approx_kl            | 0.00065723027 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.35         |
|    explained_variance   | 0             |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+08      |
|    n_updates            | 2370          |
|    policy_gradient_loss | -0.00109      |
|    std                  | 1.45          |
|    value_loss           | 2.16e+08      |
-------------------------------------------
Eval num_timesteps=486000, episode_reward=-80704.38 +/- 42170.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35907352 |
|    mean velocity x | -0.853      |
|    mean velocity y | -0.0813     |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 486000      |
------------------------------------
Eval num_timesteps=486500, episode_reward=-97882.30 +/- 34648.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.4615378 |
|    mean velocity x | -3.99      |
|    mean velocity y | 0.0238     |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.79e+04  |
| time/              |            |
|    total_timesteps | 486500     |
-----------------------------------
Eval num_timesteps=487000, episode_reward=-58647.35 +/- 39672.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.68010366 |
|    mean velocity x | -1.14       |
|    mean velocity y | 0.716       |
|    mean velocity z | 9.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.86e+04   |
| time/              |             |
|    total_timesteps | 487000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 238    |
|    time_elapsed    | 19779  |
|    total_timesteps | 487424 |
-------------------------------
Eval num_timesteps=487500, episode_reward=-102460.23 +/- 46438.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.3987368   |
|    mean velocity x      | -4.06        |
|    mean velocity y      | -0.47        |
|    mean velocity z      | 16.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.02e+05    |
| time/                   |              |
|    total_timesteps      | 487500       |
| train/                  |              |
|    approx_kl            | 0.0017708838 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+08     |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 1.45         |
|    value_loss           | 1.98e+08     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=-119525.93 +/- 30084.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1881675 |
|    mean velocity x | -3.45      |
|    mean velocity y | -0.363     |
|    mean velocity z | 15.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.2e+05   |
| time/              |            |
|    total_timesteps | 488000     |
-----------------------------------
Eval num_timesteps=488500, episode_reward=-103466.64 +/- 26804.43
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.650349 |
|    mean velocity x | -0.433    |
|    mean velocity y | 0.913     |
|    mean velocity z | 6.99      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+05 |
| time/              |           |
|    total_timesteps | 488500    |
----------------------------------
Eval num_timesteps=489000, episode_reward=-69946.70 +/- 51348.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.2039502 |
|    mean velocity x | -3.21      |
|    mean velocity y | -0.502     |
|    mean velocity z | 15         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.99e+04  |
| time/              |            |
|    total_timesteps | 489000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 239    |
|    time_elapsed    | 19859  |
|    total_timesteps | 489472 |
-------------------------------
Eval num_timesteps=489500, episode_reward=-80707.44 +/- 45852.63
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.6330673  |
|    mean velocity x      | -1.56       |
|    mean velocity y      | 0.142       |
|    mean velocity z      | 5.49        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.07e+04   |
| time/                   |             |
|    total_timesteps      | 489500      |
| train/                  |             |
|    approx_kl            | 0.003714758 |
|    clip_fraction        | 0.0158      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.35       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+08    |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 1.45        |
|    value_loss           | 1.57e+08    |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=-103315.67 +/- 35972.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0734227 |
|    mean velocity x | -2.7       |
|    mean velocity y | 0.0915     |
|    mean velocity z | 12         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 490000     |
-----------------------------------
Eval num_timesteps=490500, episode_reward=-106947.80 +/- 55197.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0638709 |
|    mean velocity x | -2.25      |
|    mean velocity y | 0.876      |
|    mean velocity z | 12.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 490500     |
-----------------------------------
Eval num_timesteps=491000, episode_reward=-62885.68 +/- 50163.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3575887 |
|    mean velocity x | 0.125      |
|    mean velocity y | 1.05       |
|    mean velocity z | 4.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.29e+04  |
| time/              |            |
|    total_timesteps | 491000     |
-----------------------------------
Eval num_timesteps=491500, episode_reward=-61057.67 +/- 53826.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28455937 |
|    mean velocity x | -0.563      |
|    mean velocity y | 0.714       |
|    mean velocity z | 1.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.11e+04   |
| time/              |             |
|    total_timesteps | 491500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 240    |
|    time_elapsed    | 19959  |
|    total_timesteps | 491520 |
-------------------------------
Eval num_timesteps=492000, episode_reward=-4161962.70 +/- 20881.25
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.297669   |
|    mean velocity x      | -3.04       |
|    mean velocity y      | 0.376       |
|    mean velocity z      | 16.3        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -4.16e+06   |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.002592734 |
|    clip_fraction        | 0.00972     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.36       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.001       |
|    loss                 | 7.68e+07    |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00365    |
|    std                  | 1.46        |
|    value_loss           | 1.39e+08    |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=-4154473.32 +/- 28971.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.82674223 |
|    mean velocity x | -1.11       |
|    mean velocity y | 1.46        |
|    mean velocity z | 7.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.15e+06   |
| time/              |             |
|    total_timesteps | 492500      |
------------------------------------
Eval num_timesteps=493000, episode_reward=-4169832.11 +/- 26563.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.98260325 |
|    mean velocity x | -1.92       |
|    mean velocity y | 0.44        |
|    mean velocity z | 11.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.17e+06   |
| time/              |             |
|    total_timesteps | 493000      |
------------------------------------
Eval num_timesteps=493500, episode_reward=-4185608.58 +/- 32154.48
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -1.158777 |
|    mean velocity x | -3.79     |
|    mean velocity y | -1.42     |
|    mean velocity z | 13.9      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.19e+06 |
| time/              |           |
|    total_timesteps | 493500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 241    |
|    time_elapsed    | 20041  |
|    total_timesteps | 493568 |
-------------------------------
Eval num_timesteps=494000, episode_reward=-69784.12 +/- 50973.15
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -1.3979875   |
|    mean velocity x      | -3.15        |
|    mean velocity y      | 0.449        |
|    mean velocity z      | 16.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.98e+04    |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0043873135 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.37        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 8.5e+07      |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.004       |
|    std                  | 1.46         |
|    value_loss           | 1.71e+08     |
------------------------------------------
Eval num_timesteps=494500, episode_reward=-99482.13 +/- 39416.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.72446793 |
|    mean velocity x | -1.03       |
|    mean velocity y | 0.883       |
|    mean velocity z | 8.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 494500      |
------------------------------------
Eval num_timesteps=495000, episode_reward=-100953.17 +/- 36796.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0964204 |
|    mean velocity x | -1.89      |
|    mean velocity y | 0.708      |
|    mean velocity z | 13.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 495000     |
-----------------------------------
Eval num_timesteps=495500, episode_reward=-78661.14 +/- 48971.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4274995 |
|    mean velocity x | -0.513     |
|    mean velocity y | 1          |
|    mean velocity z | 4.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.87e+04  |
| time/              |            |
|    total_timesteps | 495500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 242    |
|    time_elapsed    | 20121  |
|    total_timesteps | 495616 |
-------------------------------
Eval num_timesteps=496000, episode_reward=-110312.37 +/- 30549.13
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.44004118 |
|    mean velocity x      | -0.453      |
|    mean velocity y      | 0.622       |
|    mean velocity z      | 3.22        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.1e+05    |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.004221758 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.37       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+08    |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00503    |
|    std                  | 1.46        |
|    value_loss           | 1.56e+08    |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=-78797.54 +/- 36536.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29927117 |
|    mean velocity x | 0.401       |
|    mean velocity y | 0.667       |
|    mean velocity z | 1.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.88e+04   |
| time/              |             |
|    total_timesteps | 496500      |
------------------------------------
Eval num_timesteps=497000, episode_reward=-115022.77 +/- 23875.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.72231466 |
|    mean velocity x | -1.54       |
|    mean velocity y | 0.207       |
|    mean velocity z | 8.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 497000      |
------------------------------------
Eval num_timesteps=497500, episode_reward=-66027.55 +/- 36397.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.64113635 |
|    mean velocity x | -0.975      |
|    mean velocity y | 0.924       |
|    mean velocity z | 6.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.6e+04    |
| time/              |             |
|    total_timesteps | 497500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 243    |
|    time_elapsed    | 20202  |
|    total_timesteps | 497664 |
-------------------------------
Eval num_timesteps=498000, episode_reward=-82728.84 +/- 40762.74
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.7400054  |
|    mean velocity x      | -1.46       |
|    mean velocity y      | 0.387       |
|    mean velocity z      | 8.61        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.27e+04   |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.004223288 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.38       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 9.43e+07    |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00564    |
|    std                  | 1.47        |
|    value_loss           | 9.84e+07    |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=-82041.91 +/- 26874.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5440418 |
|    mean velocity x | -1.16      |
|    mean velocity y | 0.567      |
|    mean velocity z | 8.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.2e+04   |
| time/              |            |
|    total_timesteps | 498500     |
-----------------------------------
Eval num_timesteps=499000, episode_reward=-117307.46 +/- 20043.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.1977273 |
|    mean velocity x | -4.82      |
|    mean velocity y | -1.65      |
|    mean velocity z | 15.2       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.17e+05  |
| time/              |            |
|    total_timesteps | 499000     |
-----------------------------------
Eval num_timesteps=499500, episode_reward=-107571.29 +/- 33851.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33591235 |
|    mean velocity x | -0.158      |
|    mean velocity y | 0.63        |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 499500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 244    |
|    time_elapsed    | 20282  |
|    total_timesteps | 499712 |
-------------------------------
Eval num_timesteps=500000, episode_reward=-87535.19 +/- 28796.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38542017  |
|    mean velocity x      | -0.35        |
|    mean velocity y      | 0.796        |
|    mean velocity z      | 4.32         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.75e+04    |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0038929633 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.41        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+08     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00592     |
|    std                  | 1.48         |
|    value_loss           | 1.5e+08      |
------------------------------------------
Eval num_timesteps=500500, episode_reward=-124149.90 +/- 7790.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38945937 |
|    mean velocity x | -0.783      |
|    mean velocity y | 0.525       |
|    mean velocity z | 3.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.24e+05   |
| time/              |             |
|    total_timesteps | 500500      |
------------------------------------
Eval num_timesteps=501000, episode_reward=-93494.96 +/- 31512.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.3524477 |
|    mean velocity x | -2.89      |
|    mean velocity y | 0.516      |
|    mean velocity z | 15.3       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.35e+04  |
| time/              |            |
|    total_timesteps | 501000     |
-----------------------------------
Eval num_timesteps=501500, episode_reward=-50349.48 +/- 26165.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0128152 |
|    mean velocity x | -2.23      |
|    mean velocity y | 0.448      |
|    mean velocity z | 12.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.03e+04  |
| time/              |            |
|    total_timesteps | 501500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 245    |
|    time_elapsed    | 20362  |
|    total_timesteps | 501760 |
-------------------------------
Eval num_timesteps=502000, episode_reward=-109193.76 +/- 24724.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3634342   |
|    mean velocity x      | -1.06        |
|    mean velocity y      | 0.0773       |
|    mean velocity z      | 3.3          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.09e+05    |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0036094086 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.42        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 6.39e+07     |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00397     |
|    std                  | 1.48         |
|    value_loss           | 1.09e+08     |
------------------------------------------
Eval num_timesteps=502500, episode_reward=-106110.52 +/- 29784.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20040764 |
|    mean velocity x | 0.0711      |
|    mean velocity y | 0.207       |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 502500      |
------------------------------------
Eval num_timesteps=503000, episode_reward=-79772.00 +/- 53185.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33748522 |
|    mean velocity x | -0.591      |
|    mean velocity y | -0.0725     |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.98e+04   |
| time/              |             |
|    total_timesteps | 503000      |
------------------------------------
Eval num_timesteps=503500, episode_reward=-125988.33 +/- 17141.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17072526 |
|    mean velocity x | -0.125      |
|    mean velocity y | 0.392       |
|    mean velocity z | 0.337       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.26e+05   |
| time/              |             |
|    total_timesteps | 503500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 246    |
|    time_elapsed    | 20443  |
|    total_timesteps | 503808 |
-------------------------------
Eval num_timesteps=504000, episode_reward=-105888.56 +/- 22940.74
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.40574086 |
|    mean velocity x      | -0.898      |
|    mean velocity y      | -0.243      |
|    mean velocity z      | 3.59        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.06e+05   |
| time/                   |             |
|    total_timesteps      | 504000      |
| train/                  |             |
|    approx_kl            | 0.013168979 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.49       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.34e+07    |
|    n_updates            | 2460        |
|    policy_gradient_loss | 0.000265    |
|    std                  | 1.52        |
|    value_loss           | 4.31e+07    |
-----------------------------------------
Eval num_timesteps=504500, episode_reward=-68140.58 +/- 48470.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45075604 |
|    mean velocity x | -0.225      |
|    mean velocity y | 0.894       |
|    mean velocity z | 5.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 504500      |
------------------------------------
Eval num_timesteps=505000, episode_reward=-92149.77 +/- 13906.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46708715 |
|    mean velocity x | 0.06        |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.21e+04   |
| time/              |             |
|    total_timesteps | 505000      |
------------------------------------
Eval num_timesteps=505500, episode_reward=-68657.87 +/- 57034.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40903935 |
|    mean velocity x | 0.105       |
|    mean velocity y | 1.08        |
|    mean velocity z | 4.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.87e+04   |
| time/              |             |
|    total_timesteps | 505500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 247    |
|    time_elapsed    | 20523  |
|    total_timesteps | 505856 |
-------------------------------
Eval num_timesteps=506000, episode_reward=-109363.44 +/- 44709.78
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3561144    |
|    mean velocity x      | -0.301        |
|    mean velocity y      | 0.765         |
|    mean velocity z      | 0.69          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.09e+05     |
| time/                   |               |
|    total_timesteps      | 506000        |
| train/                  |               |
|    approx_kl            | 0.00043044548 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.5          |
|    explained_variance   | 0.0107        |
|    learning_rate        | 0.001         |
|    loss                 | 5.78e+07      |
|    n_updates            | 2470          |
|    policy_gradient_loss | -0.00146      |
|    std                  | 1.52          |
|    value_loss           | 7.07e+07      |
-------------------------------------------
Eval num_timesteps=506500, episode_reward=-101447.37 +/- 35827.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6240204 |
|    mean velocity x | -1.44      |
|    mean velocity y | -0.378     |
|    mean velocity z | 7.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 506500     |
-----------------------------------
Eval num_timesteps=507000, episode_reward=-92120.44 +/- 50280.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5370492 |
|    mean velocity x | -0.392     |
|    mean velocity y | 1.47       |
|    mean velocity z | 5.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.21e+04  |
| time/              |            |
|    total_timesteps | 507000     |
-----------------------------------
Eval num_timesteps=507500, episode_reward=-107218.09 +/- 32679.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5295837 |
|    mean velocity x | -0.497     |
|    mean velocity y | 0.991      |
|    mean velocity z | 3.95       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 507500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 248    |
|    time_elapsed    | 20603  |
|    total_timesteps | 507904 |
-------------------------------
Eval num_timesteps=508000, episode_reward=-85965.25 +/- 41606.52
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.734151    |
|    mean velocity x      | -0.647       |
|    mean velocity y      | 1.27         |
|    mean velocity z      | 8.21         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.6e+04     |
| time/                   |              |
|    total_timesteps      | 508000       |
| train/                  |              |
|    approx_kl            | 2.441724e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.5         |
|    explained_variance   | 0.0133       |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.000286    |
|    std                  | 1.52         |
|    value_loss           | 9.93e+07     |
------------------------------------------
Eval num_timesteps=508500, episode_reward=-114220.86 +/- 20130.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45263562 |
|    mean velocity x | 0.371       |
|    mean velocity y | 1.48        |
|    mean velocity z | 3.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 508500      |
------------------------------------
Eval num_timesteps=509000, episode_reward=-112333.42 +/- 56389.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45961535 |
|    mean velocity x | -0.159      |
|    mean velocity y | 1.35        |
|    mean velocity z | 4.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 509000      |
------------------------------------
Eval num_timesteps=509500, episode_reward=-114689.80 +/- 20824.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52252907 |
|    mean velocity x | -1.18       |
|    mean velocity y | -0.302      |
|    mean velocity z | 4.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 509500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 249    |
|    time_elapsed    | 20684  |
|    total_timesteps | 509952 |
-------------------------------
Eval num_timesteps=510000, episode_reward=-115575.43 +/- 23860.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4315329    |
|    mean velocity x      | -0.633        |
|    mean velocity y      | -0.182        |
|    mean velocity z      | 4.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.16e+05     |
| time/                   |               |
|    total_timesteps      | 510000        |
| train/                  |               |
|    approx_kl            | 0.00022539913 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.5          |
|    explained_variance   | 0.0149        |
|    learning_rate        | 0.001         |
|    loss                 | 4.65e+07      |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.00107      |
|    std                  | 1.52          |
|    value_loss           | 5.58e+07      |
-------------------------------------------
Eval num_timesteps=510500, episode_reward=-76628.38 +/- 46988.33
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7033835 |
|    mean velocity x | -0.588     |
|    mean velocity y | 0.654      |
|    mean velocity z | 6.49       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.66e+04  |
| time/              |            |
|    total_timesteps | 510500     |
-----------------------------------
Eval num_timesteps=511000, episode_reward=-94096.51 +/- 37246.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.72067946 |
|    mean velocity x | -1.53       |
|    mean velocity y | 0.433       |
|    mean velocity z | 8.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.41e+04   |
| time/              |             |
|    total_timesteps | 511000      |
------------------------------------
Eval num_timesteps=511500, episode_reward=-110492.65 +/- 53772.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7530253 |
|    mean velocity x | -1.42      |
|    mean velocity y | 0.154      |
|    mean velocity z | 6.78       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 511500     |
-----------------------------------
Eval num_timesteps=512000, episode_reward=-67361.61 +/- 32962.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15955988 |
|    mean velocity x | 0.0878      |
|    mean velocity y | 0.424       |
|    mean velocity z | 0.512       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 512000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 250    |
|    time_elapsed    | 20783  |
|    total_timesteps | 512000 |
-------------------------------
Eval num_timesteps=512500, episode_reward=-97021.84 +/- 16002.47
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5420735    |
|    mean velocity x      | -0.832        |
|    mean velocity y      | 0.783         |
|    mean velocity z      | 3.53          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.7e+04      |
| time/                   |               |
|    total_timesteps      | 512500        |
| train/                  |               |
|    approx_kl            | 0.00024826877 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.5          |
|    explained_variance   | 0.0311        |
|    learning_rate        | 0.001         |
|    loss                 | 2.71e+07      |
|    n_updates            | 2500          |
|    policy_gradient_loss | -0.00132      |
|    std                  | 1.52          |
|    value_loss           | 6.99e+07      |
-------------------------------------------
Eval num_timesteps=513000, episode_reward=-86397.71 +/- 41089.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5658046 |
|    mean velocity x | -0.929     |
|    mean velocity y | 0.199      |
|    mean velocity z | 5.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.64e+04  |
| time/              |            |
|    total_timesteps | 513000     |
-----------------------------------
Eval num_timesteps=513500, episode_reward=-86784.10 +/- 38061.89
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.452943 |
|    mean velocity x | -0.665    |
|    mean velocity y | 0.734     |
|    mean velocity z | 3.58      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.68e+04 |
| time/              |           |
|    total_timesteps | 513500    |
----------------------------------
Eval num_timesteps=514000, episode_reward=-85495.04 +/- 46019.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7890156 |
|    mean velocity x | -2.13      |
|    mean velocity y | -0.326     |
|    mean velocity z | 7.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.55e+04  |
| time/              |            |
|    total_timesteps | 514000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 251    |
|    time_elapsed    | 20863  |
|    total_timesteps | 514048 |
-------------------------------
Eval num_timesteps=514500, episode_reward=-91513.20 +/- 18659.06
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48776263   |
|    mean velocity x      | 0.283         |
|    mean velocity y      | 1.31          |
|    mean velocity z      | 3.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.15e+04     |
| time/                   |               |
|    total_timesteps      | 514500        |
| train/                  |               |
|    approx_kl            | 0.00023976198 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.49         |
|    explained_variance   | 0.0623        |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+07      |
|    n_updates            | 2510          |
|    policy_gradient_loss | -0.00104      |
|    std                  | 1.52          |
|    value_loss           | 5.8e+07       |
-------------------------------------------
Eval num_timesteps=515000, episode_reward=-115232.64 +/- 69521.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4940124 |
|    mean velocity x | -0.525     |
|    mean velocity y | 1.04       |
|    mean velocity z | 3.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.15e+05  |
| time/              |            |
|    total_timesteps | 515000     |
-----------------------------------
Eval num_timesteps=515500, episode_reward=-71388.34 +/- 39347.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3201533 |
|    mean velocity x | -0.245     |
|    mean velocity y | 0.733      |
|    mean velocity z | 1.3        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.14e+04  |
| time/              |            |
|    total_timesteps | 515500     |
-----------------------------------
Eval num_timesteps=516000, episode_reward=-99943.87 +/- 29304.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5057349 |
|    mean velocity x | -1.52      |
|    mean velocity y | -0.277     |
|    mean velocity z | 4.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.99e+04  |
| time/              |            |
|    total_timesteps | 516000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 252    |
|    time_elapsed    | 20943  |
|    total_timesteps | 516096 |
-------------------------------
Eval num_timesteps=516500, episode_reward=-94142.32 +/- 50442.49
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.7075868    |
|    mean velocity x      | -1.48         |
|    mean velocity y      | 0.208         |
|    mean velocity z      | 5.96          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.41e+04     |
| time/                   |               |
|    total_timesteps      | 516500        |
| train/                  |               |
|    approx_kl            | 0.00044233448 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.49         |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.001         |
|    loss                 | 3.39e+06      |
|    n_updates            | 2520          |
|    policy_gradient_loss | -0.00138      |
|    std                  | 1.52          |
|    value_loss           | 1.93e+07      |
-------------------------------------------
Eval num_timesteps=517000, episode_reward=-87188.69 +/- 49457.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58854234 |
|    mean velocity x | -0.622      |
|    mean velocity y | 0.873       |
|    mean velocity z | 4.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.72e+04   |
| time/              |             |
|    total_timesteps | 517000      |
------------------------------------
Eval num_timesteps=517500, episode_reward=-84310.00 +/- 22862.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51711804 |
|    mean velocity x | -0.421      |
|    mean velocity y | 0.979       |
|    mean velocity z | 5.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.43e+04   |
| time/              |             |
|    total_timesteps | 517500      |
------------------------------------
Eval num_timesteps=518000, episode_reward=-96668.64 +/- 51874.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47684968 |
|    mean velocity x | -0.324      |
|    mean velocity y | 0.737       |
|    mean velocity z | 5.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.67e+04   |
| time/              |             |
|    total_timesteps | 518000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 253    |
|    time_elapsed    | 21024  |
|    total_timesteps | 518144 |
-------------------------------
Eval num_timesteps=518500, episode_reward=-87528.49 +/- 35788.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.59188384   |
|    mean velocity x      | -1.53         |
|    mean velocity y      | 0.0718        |
|    mean velocity z      | 5.31          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.75e+04     |
| time/                   |               |
|    total_timesteps      | 518500        |
| train/                  |               |
|    approx_kl            | 0.00044157173 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.49         |
|    explained_variance   | 0.0629        |
|    learning_rate        | 0.001         |
|    loss                 | 6.39e+07      |
|    n_updates            | 2530          |
|    policy_gradient_loss | -0.00152      |
|    std                  | 1.52          |
|    value_loss           | 1.22e+08      |
-------------------------------------------
Eval num_timesteps=519000, episode_reward=-85468.57 +/- 49429.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5440248 |
|    mean velocity x | -0.146     |
|    mean velocity y | 1.81       |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.55e+04  |
| time/              |            |
|    total_timesteps | 519000     |
-----------------------------------
Eval num_timesteps=519500, episode_reward=-76677.47 +/- 34304.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49511144 |
|    mean velocity x | -1.16       |
|    mean velocity y | 0.487       |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.67e+04   |
| time/              |             |
|    total_timesteps | 519500      |
------------------------------------
Eval num_timesteps=520000, episode_reward=-91519.13 +/- 40230.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45107913 |
|    mean velocity x | -0.268      |
|    mean velocity y | 0.922       |
|    mean velocity z | 3.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.15e+04   |
| time/              |             |
|    total_timesteps | 520000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 254    |
|    time_elapsed    | 21104  |
|    total_timesteps | 520192 |
-------------------------------
Eval num_timesteps=520500, episode_reward=-108805.80 +/- 26998.98
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4447652   |
|    mean velocity x      | -0.32        |
|    mean velocity y      | 0.617        |
|    mean velocity z      | 4.25         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.09e+05    |
| time/                   |              |
|    total_timesteps      | 520500       |
| train/                  |              |
|    approx_kl            | 8.405041e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.0711       |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+07     |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.000798    |
|    std                  | 1.52         |
|    value_loss           | 4.5e+07      |
------------------------------------------
Eval num_timesteps=521000, episode_reward=-120024.88 +/- 34387.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33049032 |
|    mean velocity x | 0.264       |
|    mean velocity y | 1.18        |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 521000      |
------------------------------------
Eval num_timesteps=521500, episode_reward=-67087.53 +/- 18662.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45072815 |
|    mean velocity x | -0.207      |
|    mean velocity y | 0.999       |
|    mean velocity z | 4.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 521500      |
------------------------------------
Eval num_timesteps=522000, episode_reward=-73034.25 +/- 31801.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6603006 |
|    mean velocity x | -1.94      |
|    mean velocity y | -0.653     |
|    mean velocity z | 7.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.3e+04   |
| time/              |            |
|    total_timesteps | 522000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 255    |
|    time_elapsed    | 21184  |
|    total_timesteps | 522240 |
-------------------------------
Eval num_timesteps=522500, episode_reward=-82716.72 +/- 35565.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40004858  |
|    mean velocity x      | -0.722       |
|    mean velocity y      | 0.809        |
|    mean velocity z      | 2.24         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.27e+04    |
| time/                   |              |
|    total_timesteps      | 522500       |
| train/                  |              |
|    approx_kl            | 0.0003661405 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.0725       |
|    learning_rate        | 0.001        |
|    loss                 | 8.47e+07     |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 1.51         |
|    value_loss           | 7.98e+07     |
------------------------------------------
Eval num_timesteps=523000, episode_reward=-60001.09 +/- 54976.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7887712 |
|    mean velocity x | -0.802     |
|    mean velocity y | 0.817      |
|    mean velocity z | 8.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6e+04     |
| time/              |            |
|    total_timesteps | 523000     |
-----------------------------------
Eval num_timesteps=523500, episode_reward=-77888.10 +/- 42469.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4350086 |
|    mean velocity x | -0.367     |
|    mean velocity y | 0.831      |
|    mean velocity z | 4.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.79e+04  |
| time/              |            |
|    total_timesteps | 523500     |
-----------------------------------
Eval num_timesteps=524000, episode_reward=-82118.78 +/- 46862.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.71868527 |
|    mean velocity x | -1.41       |
|    mean velocity y | 0.383       |
|    mean velocity z | 7.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.21e+04   |
| time/              |             |
|    total_timesteps | 524000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 256    |
|    time_elapsed    | 21264  |
|    total_timesteps | 524288 |
-------------------------------
Eval num_timesteps=524500, episode_reward=-75391.72 +/- 52855.97
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.73242193  |
|    mean velocity x      | -1.74        |
|    mean velocity y      | -0.276       |
|    mean velocity z      | 7.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.54e+04    |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 4.647352e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.48        |
|    explained_variance   | 0.0712       |
|    learning_rate        | 0.001        |
|    loss                 | 9.83e+07     |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.000377    |
|    std                  | 1.51         |
|    value_loss           | 1.19e+08     |
------------------------------------------
Eval num_timesteps=525000, episode_reward=-96059.25 +/- 18113.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54139686 |
|    mean velocity x | 0.156       |
|    mean velocity y | 1.97        |
|    mean velocity z | 4.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.61e+04   |
| time/              |             |
|    total_timesteps | 525000      |
------------------------------------
Eval num_timesteps=525500, episode_reward=-77869.82 +/- 28592.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45511913 |
|    mean velocity x | 0.129       |
|    mean velocity y | 0.802       |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.79e+04   |
| time/              |             |
|    total_timesteps | 525500      |
------------------------------------
Eval num_timesteps=526000, episode_reward=-78057.72 +/- 21445.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58869785 |
|    mean velocity x | -0.37       |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.81e+04   |
| time/              |             |
|    total_timesteps | 526000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 257    |
|    time_elapsed    | 21345  |
|    total_timesteps | 526336 |
-------------------------------
Eval num_timesteps=526500, episode_reward=-101483.03 +/- 38247.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5198671    |
|    mean velocity x      | -0.934        |
|    mean velocity y      | 0.524         |
|    mean velocity z      | 4.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+05     |
| time/                   |               |
|    total_timesteps      | 526500        |
| train/                  |               |
|    approx_kl            | 0.00038371253 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.48         |
|    explained_variance   | 0.0436        |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+07       |
|    n_updates            | 2570          |
|    policy_gradient_loss | -0.0015       |
|    std                  | 1.51          |
|    value_loss           | 5.96e+07      |
-------------------------------------------
Eval num_timesteps=527000, episode_reward=-108469.91 +/- 31467.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40840462 |
|    mean velocity x | -0.831      |
|    mean velocity y | 0.246       |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 527000      |
------------------------------------
Eval num_timesteps=527500, episode_reward=-95477.39 +/- 51186.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43223438 |
|    mean velocity x | -0.393      |
|    mean velocity y | 0.464       |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.55e+04   |
| time/              |             |
|    total_timesteps | 527500      |
------------------------------------
Eval num_timesteps=528000, episode_reward=-104398.91 +/- 35474.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5909531 |
|    mean velocity x | -0.567     |
|    mean velocity y | 0.92       |
|    mean velocity z | 5.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 528000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 258    |
|    time_elapsed    | 21425  |
|    total_timesteps | 528384 |
-------------------------------
Eval num_timesteps=528500, episode_reward=-107864.33 +/- 51220.15
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37142664  |
|    mean velocity x      | 0.114        |
|    mean velocity y      | 1.1          |
|    mean velocity z      | 2.55         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.08e+05    |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0014733479 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.0556       |
|    learning_rate        | 0.001        |
|    loss                 | 5.25e+07     |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.00414     |
|    std                  | 1.52         |
|    value_loss           | 6.33e+07     |
------------------------------------------
Eval num_timesteps=529000, episode_reward=-58098.01 +/- 44873.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48260722 |
|    mean velocity x | -0.789      |
|    mean velocity y | 0.366       |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.81e+04   |
| time/              |             |
|    total_timesteps | 529000      |
------------------------------------
Eval num_timesteps=529500, episode_reward=-85585.24 +/- 28958.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5226288 |
|    mean velocity x | -0.028     |
|    mean velocity y | 1.37       |
|    mean velocity z | 4.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.56e+04  |
| time/              |            |
|    total_timesteps | 529500     |
-----------------------------------
Eval num_timesteps=530000, episode_reward=-74514.60 +/- 45822.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4381408 |
|    mean velocity x | 0.281      |
|    mean velocity y | 0.978      |
|    mean velocity z | 3.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.45e+04  |
| time/              |            |
|    total_timesteps | 530000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 259    |
|    time_elapsed    | 21505  |
|    total_timesteps | 530432 |
-------------------------------
Eval num_timesteps=530500, episode_reward=-106089.33 +/- 32770.26
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47506076  |
|    mean velocity x      | -0.159       |
|    mean velocity y      | 1.17         |
|    mean velocity z      | 5.3          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.06e+05    |
| time/                   |              |
|    total_timesteps      | 530500       |
| train/                  |              |
|    approx_kl            | 0.0006985238 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.48        |
|    explained_variance   | 0.0583       |
|    learning_rate        | 0.001        |
|    loss                 | 1.75e+07     |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.51         |
|    value_loss           | 8.04e+07     |
------------------------------------------
Eval num_timesteps=531000, episode_reward=-97933.68 +/- 50215.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36684278 |
|    mean velocity x | 0.133       |
|    mean velocity y | 0.439       |
|    mean velocity z | 1.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.79e+04   |
| time/              |             |
|    total_timesteps | 531000      |
------------------------------------
Eval num_timesteps=531500, episode_reward=-91841.65 +/- 43910.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41107795 |
|    mean velocity x | -0.182      |
|    mean velocity y | 0.741       |
|    mean velocity z | 4.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.18e+04   |
| time/              |             |
|    total_timesteps | 531500      |
------------------------------------
Eval num_timesteps=532000, episode_reward=-111569.30 +/- 26918.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58159643 |
|    mean velocity x | 0.2         |
|    mean velocity y | 1.58        |
|    mean velocity z | 4.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 532000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 260    |
|    time_elapsed    | 21586  |
|    total_timesteps | 532480 |
-------------------------------
Eval num_timesteps=532500, episode_reward=-124897.29 +/- 21229.55
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35876516  |
|    mean velocity x      | -0.297       |
|    mean velocity y      | 0.575        |
|    mean velocity z      | 0.953        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.25e+05    |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0022288347 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.48        |
|    explained_variance   | 0.058        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+07     |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00623     |
|    std                  | 1.5          |
|    value_loss           | 4.38e+07     |
------------------------------------------
Eval num_timesteps=533000, episode_reward=-47039.29 +/- 35232.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37674004 |
|    mean velocity x | -0.061      |
|    mean velocity y | 0.594       |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.7e+04    |
| time/              |             |
|    total_timesteps | 533000      |
------------------------------------
Eval num_timesteps=533500, episode_reward=-84334.50 +/- 48084.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4401654 |
|    mean velocity x | -0.791     |
|    mean velocity y | -0.229     |
|    mean velocity z | 3.88       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 533500     |
-----------------------------------
Eval num_timesteps=534000, episode_reward=-102889.34 +/- 23260.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.451958 |
|    mean velocity x | -1.73     |
|    mean velocity y | -0.573    |
|    mean velocity z | 4.91      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.03e+05 |
| time/              |           |
|    total_timesteps | 534000    |
----------------------------------
Eval num_timesteps=534500, episode_reward=-48957.44 +/- 36845.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4060719 |
|    mean velocity x | -0.0378    |
|    mean velocity y | 0.896      |
|    mean velocity z | 5.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.9e+04   |
| time/              |            |
|    total_timesteps | 534500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 261    |
|    time_elapsed    | 21685  |
|    total_timesteps | 534528 |
-------------------------------
Eval num_timesteps=535000, episode_reward=-64397.27 +/- 47110.32
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.53788847 |
|    mean velocity x      | -0.892      |
|    mean velocity y      | 0.459       |
|    mean velocity z      | 4.92        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.44e+04   |
| time/                   |             |
|    total_timesteps      | 535000      |
| train/                  |             |
|    approx_kl            | 0.001903005 |
|    clip_fraction        | 0.00552     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.47       |
|    explained_variance   | 0.0697      |
|    learning_rate        | 0.001       |
|    loss                 | 3.46e+07    |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00387    |
|    std                  | 1.5         |
|    value_loss           | 7.84e+07    |
-----------------------------------------
Eval num_timesteps=535500, episode_reward=-74564.06 +/- 61583.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52534914 |
|    mean velocity x | -1.21       |
|    mean velocity y | 0.418       |
|    mean velocity z | 4.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 535500      |
------------------------------------
Eval num_timesteps=536000, episode_reward=-102002.19 +/- 56799.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49568447 |
|    mean velocity x | -0.187      |
|    mean velocity y | 0.843       |
|    mean velocity z | 4.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 536000      |
------------------------------------
Eval num_timesteps=536500, episode_reward=-94414.78 +/- 14679.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12289027 |
|    mean velocity x | -0.000377   |
|    mean velocity y | 0.452       |
|    mean velocity z | 0.343       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.44e+04   |
| time/              |             |
|    total_timesteps | 536500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 262    |
|    time_elapsed    | 21765  |
|    total_timesteps | 536576 |
-------------------------------
Eval num_timesteps=537000, episode_reward=-113704.79 +/- 44908.01
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50747126  |
|    mean velocity x      | -0.803       |
|    mean velocity y      | 0.308        |
|    mean velocity z      | 4.14         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.14e+05    |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0025088356 |
|    clip_fraction        | 0.0083       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.47        |
|    explained_variance   | 0.0746       |
|    learning_rate        | 0.001        |
|    loss                 | 3.97e+06     |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00328     |
|    std                  | 1.51         |
|    value_loss           | 5.74e+07     |
------------------------------------------
Eval num_timesteps=537500, episode_reward=-111876.00 +/- 37603.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.74318594 |
|    mean velocity x | -1.35       |
|    mean velocity y | 0.418       |
|    mean velocity z | 6.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 537500      |
------------------------------------
Eval num_timesteps=538000, episode_reward=-67098.13 +/- 46069.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48811194 |
|    mean velocity x | -0.731      |
|    mean velocity y | 0.811       |
|    mean velocity z | 3.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 538000      |
------------------------------------
Eval num_timesteps=538500, episode_reward=-108360.42 +/- 40026.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28181985 |
|    mean velocity x | 0.179       |
|    mean velocity y | 0.933       |
|    mean velocity z | 0.823       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 538500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 263    |
|    time_elapsed    | 21846  |
|    total_timesteps | 538624 |
-------------------------------
Eval num_timesteps=539000, episode_reward=-95467.81 +/- 36443.82
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41901878  |
|    mean velocity x      | 0.171        |
|    mean velocity y      | 0.831        |
|    mean velocity z      | 3.37         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.55e+04    |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0018663614 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.48        |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.001        |
|    loss                 | 7.15e+06     |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.0032      |
|    std                  | 1.51         |
|    value_loss           | 3.01e+07     |
------------------------------------------
Eval num_timesteps=539500, episode_reward=-99621.96 +/- 36389.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51602113 |
|    mean velocity x | -0.759      |
|    mean velocity y | 0.586       |
|    mean velocity z | 4.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.96e+04   |
| time/              |             |
|    total_timesteps | 539500      |
------------------------------------
Eval num_timesteps=540000, episode_reward=-82547.94 +/- 33326.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47222131 |
|    mean velocity x | -1.11       |
|    mean velocity y | 0.403       |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.25e+04   |
| time/              |             |
|    total_timesteps | 540000      |
------------------------------------
Eval num_timesteps=540500, episode_reward=-90703.16 +/- 37859.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7666959 |
|    mean velocity x | -1.15      |
|    mean velocity y | 0.572      |
|    mean velocity z | 6.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.07e+04  |
| time/              |            |
|    total_timesteps | 540500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 264    |
|    time_elapsed    | 21926  |
|    total_timesteps | 540672 |
-------------------------------
Eval num_timesteps=541000, episode_reward=-86485.85 +/- 13204.43
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4199072    |
|    mean velocity x      | -0.39         |
|    mean velocity y      | 0.744         |
|    mean velocity z      | 1.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.65e+04     |
| time/                   |               |
|    total_timesteps      | 541000        |
| train/                  |               |
|    approx_kl            | 0.00034362837 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.47         |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+07      |
|    n_updates            | 2640          |
|    policy_gradient_loss | -0.000991     |
|    std                  | 1.5           |
|    value_loss           | 3.46e+07      |
-------------------------------------------
Eval num_timesteps=541500, episode_reward=-111404.82 +/- 15676.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47624084 |
|    mean velocity x | -0.445      |
|    mean velocity y | 0.358       |
|    mean velocity z | 4.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 541500      |
------------------------------------
Eval num_timesteps=542000, episode_reward=-55381.54 +/- 30403.54
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.504671 |
|    mean velocity x | -1.05     |
|    mean velocity y | 0.264     |
|    mean velocity z | 4.06      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.54e+04 |
| time/              |           |
|    total_timesteps | 542000    |
----------------------------------
Eval num_timesteps=542500, episode_reward=-97853.63 +/- 41468.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5164605 |
|    mean velocity x | 0.348      |
|    mean velocity y | 1.77       |
|    mean velocity z | 4.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.79e+04  |
| time/              |            |
|    total_timesteps | 542500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 265    |
|    time_elapsed    | 22006  |
|    total_timesteps | 542720 |
-------------------------------
Eval num_timesteps=543000, episode_reward=-60014.64 +/- 41949.18
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50222296  |
|    mean velocity x      | -0.538       |
|    mean velocity y      | 0.751        |
|    mean velocity z      | 5.31         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6e+04       |
| time/                   |              |
|    total_timesteps      | 543000       |
| train/                  |              |
|    approx_kl            | 0.0018003916 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.46        |
|    explained_variance   | 0.0703       |
|    learning_rate        | 0.001        |
|    loss                 | 3.97e+07     |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.0042      |
|    std                  | 1.5          |
|    value_loss           | 7.72e+07     |
------------------------------------------
Eval num_timesteps=543500, episode_reward=-109461.37 +/- 38551.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32501277 |
|    mean velocity x | -0.0476     |
|    mean velocity y | 0.729       |
|    mean velocity z | 0.755       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 543500      |
------------------------------------
Eval num_timesteps=544000, episode_reward=-81256.10 +/- 59325.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39365184 |
|    mean velocity x | -1.73       |
|    mean velocity y | -0.914      |
|    mean velocity z | 5.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 544000      |
------------------------------------
Eval num_timesteps=544500, episode_reward=-108396.10 +/- 13921.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4265458 |
|    mean velocity x | 0.219      |
|    mean velocity y | 1.51       |
|    mean velocity z | 4.23       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 544500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 266    |
|    time_elapsed    | 22087  |
|    total_timesteps | 544768 |
-------------------------------
Eval num_timesteps=545000, episode_reward=-94562.70 +/- 53720.72
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4881325   |
|    mean velocity x      | 0.0726       |
|    mean velocity y      | 0.923        |
|    mean velocity z      | 3.65         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.46e+04    |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0013812633 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.47        |
|    explained_variance   | 0.113        |
|    learning_rate        | 0.001        |
|    loss                 | 5.91e+06     |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00441     |
|    std                  | 1.51         |
|    value_loss           | 2.82e+07     |
------------------------------------------
Eval num_timesteps=545500, episode_reward=-113700.56 +/- 28035.02
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.54416  |
|    mean velocity x | -0.384    |
|    mean velocity y | 0.709     |
|    mean velocity z | 4.62      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.14e+05 |
| time/              |           |
|    total_timesteps | 545500    |
----------------------------------
Eval num_timesteps=546000, episode_reward=-103565.29 +/- 14583.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44666034 |
|    mean velocity x | -0.775      |
|    mean velocity y | 0.308       |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 546000      |
------------------------------------
Eval num_timesteps=546500, episode_reward=-96542.28 +/- 49067.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38688555 |
|    mean velocity x | -1.03       |
|    mean velocity y | 0.25        |
|    mean velocity z | 2.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.65e+04   |
| time/              |             |
|    total_timesteps | 546500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 267    |
|    time_elapsed    | 22167  |
|    total_timesteps | 546816 |
-------------------------------
Eval num_timesteps=547000, episode_reward=-119584.35 +/- 16283.84
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42050314  |
|    mean velocity x      | 0.0871       |
|    mean velocity y      | 1.05         |
|    mean velocity z      | 3.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.2e+05     |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 0.0023284066 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.48        |
|    explained_variance   | 0.078        |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+07     |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.0039      |
|    std                  | 1.51         |
|    value_loss           | 5.76e+07     |
------------------------------------------
Eval num_timesteps=547500, episode_reward=-102481.51 +/- 53530.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42752972 |
|    mean velocity x | -1.06       |
|    mean velocity y | 0.16        |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 547500      |
------------------------------------
Eval num_timesteps=548000, episode_reward=-84208.23 +/- 48450.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53692734 |
|    mean velocity x | 0.598       |
|    mean velocity y | 1.83        |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 548000      |
------------------------------------
Eval num_timesteps=548500, episode_reward=-102997.83 +/- 44148.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54639363 |
|    mean velocity x | -0.624      |
|    mean velocity y | 1.01        |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 548500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 268    |
|    time_elapsed    | 22247  |
|    total_timesteps | 548864 |
-------------------------------
Eval num_timesteps=549000, episode_reward=-83488.37 +/- 59760.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5221455    |
|    mean velocity x      | -0.113        |
|    mean velocity y      | 1.53          |
|    mean velocity z      | 5             |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.35e+04     |
| time/                   |               |
|    total_timesteps      | 549000        |
| train/                  |               |
|    approx_kl            | 0.00073482667 |
|    clip_fraction        | 0.0022        |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.49         |
|    explained_variance   | 0.094         |
|    learning_rate        | 0.001         |
|    loss                 | 3.59e+07      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -0.00291      |
|    std                  | 1.52          |
|    value_loss           | 5.21e+07      |
-------------------------------------------
Eval num_timesteps=549500, episode_reward=-86279.20 +/- 44739.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45914927 |
|    mean velocity x | -0.382      |
|    mean velocity y | 0.743       |
|    mean velocity z | 3.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.63e+04   |
| time/              |             |
|    total_timesteps | 549500      |
------------------------------------
Eval num_timesteps=550000, episode_reward=-80109.44 +/- 36499.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48226112 |
|    mean velocity x | -0.652      |
|    mean velocity y | 0.537       |
|    mean velocity z | 3           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 550000      |
------------------------------------
Eval num_timesteps=550500, episode_reward=-54006.83 +/- 51449.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5273477 |
|    mean velocity x | -0.798     |
|    mean velocity y | 0.754      |
|    mean velocity z | 4.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.4e+04   |
| time/              |            |
|    total_timesteps | 550500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 269    |
|    time_elapsed    | 22328  |
|    total_timesteps | 550912 |
-------------------------------
Eval num_timesteps=551000, episode_reward=-78385.51 +/- 41469.36
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21182425  |
|    mean velocity x      | 0.113        |
|    mean velocity y      | 0.358        |
|    mean velocity z      | 0.659        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.84e+04    |
| time/                   |              |
|    total_timesteps      | 551000       |
| train/                  |              |
|    approx_kl            | 0.0055822353 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.0909       |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+07     |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.54         |
|    value_loss           | 2.19e+07     |
------------------------------------------
Eval num_timesteps=551500, episode_reward=-95580.53 +/- 28421.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51515675 |
|    mean velocity x | -1.62       |
|    mean velocity y | -0.591      |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.56e+04   |
| time/              |             |
|    total_timesteps | 551500      |
------------------------------------
Eval num_timesteps=552000, episode_reward=-114664.69 +/- 17194.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5072881 |
|    mean velocity x | -0.446     |
|    mean velocity y | 0.703      |
|    mean velocity z | 5.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.15e+05  |
| time/              |            |
|    total_timesteps | 552000     |
-----------------------------------
Eval num_timesteps=552500, episode_reward=-94913.30 +/- 30918.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52984107 |
|    mean velocity x | -0.321      |
|    mean velocity y | 1.05        |
|    mean velocity z | 4.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 552500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 270    |
|    time_elapsed    | 22412  |
|    total_timesteps | 552960 |
-------------------------------
Eval num_timesteps=553000, episode_reward=-61693.49 +/- 66357.40
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.525586    |
|    mean velocity x      | -0.0927      |
|    mean velocity y      | 1.34         |
|    mean velocity z      | 3.8          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.17e+04    |
| time/                   |              |
|    total_timesteps      | 553000       |
| train/                  |              |
|    approx_kl            | 0.0018168932 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0797       |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+07     |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 1.55         |
|    value_loss           | 6.59e+07     |
------------------------------------------
Eval num_timesteps=553500, episode_reward=-50296.17 +/- 40443.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56890035 |
|    mean velocity x | -0.272      |
|    mean velocity y | 1.41        |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.03e+04   |
| time/              |             |
|    total_timesteps | 553500      |
------------------------------------
Eval num_timesteps=554000, episode_reward=-111338.14 +/- 19176.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53655565 |
|    mean velocity x | 0.409       |
|    mean velocity y | 1.43        |
|    mean velocity z | 3.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 554000      |
------------------------------------
Eval num_timesteps=554500, episode_reward=-120704.52 +/- 25220.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36498037 |
|    mean velocity x | -1.63       |
|    mean velocity y | -0.116      |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 554500      |
------------------------------------
Eval num_timesteps=555000, episode_reward=-78327.57 +/- 31690.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48166406 |
|    mean velocity x | -0.481      |
|    mean velocity y | 0.412       |
|    mean velocity z | 5.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.83e+04   |
| time/              |             |
|    total_timesteps | 555000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 271    |
|    time_elapsed    | 22511  |
|    total_timesteps | 555008 |
-------------------------------
Eval num_timesteps=555500, episode_reward=-108840.76 +/- 31277.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5346913    |
|    mean velocity x      | -0.379        |
|    mean velocity y      | 0.788         |
|    mean velocity z      | 5.25          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.09e+05     |
| time/                   |               |
|    total_timesteps      | 555500        |
| train/                  |               |
|    approx_kl            | 0.00033447007 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0901        |
|    learning_rate        | 0.001         |
|    loss                 | 3.81e+07      |
|    n_updates            | 2710          |
|    policy_gradient_loss | -0.00141      |
|    std                  | 1.55          |
|    value_loss           | 6.05e+07      |
-------------------------------------------
Eval num_timesteps=556000, episode_reward=-75115.83 +/- 44200.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5881596 |
|    mean velocity x | -0.102     |
|    mean velocity y | 1.42       |
|    mean velocity z | 4.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.51e+04  |
| time/              |            |
|    total_timesteps | 556000     |
-----------------------------------
Eval num_timesteps=556500, episode_reward=-124930.25 +/- 34882.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5707334 |
|    mean velocity x | -0.131     |
|    mean velocity y | 1.32       |
|    mean velocity z | 4.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.25e+05  |
| time/              |            |
|    total_timesteps | 556500     |
-----------------------------------
Eval num_timesteps=557000, episode_reward=-116789.30 +/- 18687.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4915935 |
|    mean velocity x | -1.29      |
|    mean velocity y | 0.179      |
|    mean velocity z | 3.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.17e+05  |
| time/              |            |
|    total_timesteps | 557000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 272    |
|    time_elapsed    | 22592  |
|    total_timesteps | 557056 |
-------------------------------
Eval num_timesteps=557500, episode_reward=-95127.90 +/- 24136.16
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6144621   |
|    mean velocity x      | -0.714       |
|    mean velocity y      | 0.866        |
|    mean velocity z      | 5.38         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.51e+04    |
| time/                   |              |
|    total_timesteps      | 557500       |
| train/                  |              |
|    approx_kl            | 0.0008414781 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0694       |
|    learning_rate        | 0.001        |
|    loss                 | 2.93e+07     |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.000882    |
|    std                  | 1.56         |
|    value_loss           | 8.68e+07     |
------------------------------------------
Eval num_timesteps=558000, episode_reward=-57393.00 +/- 42842.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5535169 |
|    mean velocity x | -0.438     |
|    mean velocity y | 1.12       |
|    mean velocity z | 3.88       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.74e+04  |
| time/              |            |
|    total_timesteps | 558000     |
-----------------------------------
Eval num_timesteps=558500, episode_reward=-48769.72 +/- 37947.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41288757 |
|    mean velocity x | -1.63       |
|    mean velocity y | -0.0144     |
|    mean velocity z | 3.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.88e+04   |
| time/              |             |
|    total_timesteps | 558500      |
------------------------------------
Eval num_timesteps=559000, episode_reward=-93204.02 +/- 60296.11
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5073956 |
|    mean velocity x | -0.449     |
|    mean velocity y | 1.2        |
|    mean velocity z | 4.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.32e+04  |
| time/              |            |
|    total_timesteps | 559000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 273    |
|    time_elapsed    | 22672  |
|    total_timesteps | 559104 |
-------------------------------
Eval num_timesteps=559500, episode_reward=-89112.27 +/- 40995.43
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5758933   |
|    mean velocity x      | -0.446       |
|    mean velocity y      | 0.922        |
|    mean velocity z      | 5.2          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.91e+04    |
| time/                   |              |
|    total_timesteps      | 559500       |
| train/                  |              |
|    approx_kl            | 0.0008815822 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.068        |
|    learning_rate        | 0.001        |
|    loss                 | 4.93e+07     |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.56         |
|    value_loss           | 7.27e+07     |
------------------------------------------
Eval num_timesteps=560000, episode_reward=-71168.37 +/- 40887.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5817238 |
|    mean velocity x | -0.452     |
|    mean velocity y | 0.972      |
|    mean velocity z | 5.11       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.12e+04  |
| time/              |            |
|    total_timesteps | 560000     |
-----------------------------------
Eval num_timesteps=560500, episode_reward=-117962.00 +/- 22488.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50698316 |
|    mean velocity x | -0.335      |
|    mean velocity y | 0.935       |
|    mean velocity z | 5.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 560500      |
------------------------------------
Eval num_timesteps=561000, episode_reward=-80416.18 +/- 56602.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4948206 |
|    mean velocity x | -0.0163    |
|    mean velocity y | 1.25       |
|    mean velocity z | 4.15       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 561000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 274    |
|    time_elapsed    | 22752  |
|    total_timesteps | 561152 |
-------------------------------
Eval num_timesteps=561500, episode_reward=-89344.46 +/- 55302.45
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.58158803   |
|    mean velocity x      | -0.793        |
|    mean velocity y      | 0.773         |
|    mean velocity z      | 4.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.93e+04     |
| time/                   |               |
|    total_timesteps      | 561500        |
| train/                  |               |
|    approx_kl            | 0.00075849285 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.0508        |
|    learning_rate        | 0.001         |
|    loss                 | 9.25e+07      |
|    n_updates            | 2740          |
|    policy_gradient_loss | -0.00284      |
|    std                  | 1.56          |
|    value_loss           | 1.14e+08      |
-------------------------------------------
Eval num_timesteps=562000, episode_reward=-119692.55 +/- 34252.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5052906 |
|    mean velocity x | -0.412     |
|    mean velocity y | 1          |
|    mean velocity z | 4.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.2e+05   |
| time/              |            |
|    total_timesteps | 562000     |
-----------------------------------
Eval num_timesteps=562500, episode_reward=-70582.93 +/- 43948.73
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.522083 |
|    mean velocity x | -0.478    |
|    mean velocity y | 0.769     |
|    mean velocity z | 5         |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.06e+04 |
| time/              |           |
|    total_timesteps | 562500    |
----------------------------------
Eval num_timesteps=563000, episode_reward=-108760.81 +/- 29132.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5941613 |
|    mean velocity x | -0.619     |
|    mean velocity y | 0.687      |
|    mean velocity z | 5.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 563000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 275    |
|    time_elapsed    | 22833  |
|    total_timesteps | 563200 |
-------------------------------
Eval num_timesteps=563500, episode_reward=-111540.88 +/- 45290.71
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5376395   |
|    mean velocity x      | -1.34        |
|    mean velocity y      | 0.442        |
|    mean velocity z      | 3.79         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.12e+05    |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0016437317 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.0551       |
|    learning_rate        | 0.001        |
|    loss                 | 4.97e+07     |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.56         |
|    value_loss           | 7.71e+07     |
------------------------------------------
Eval num_timesteps=564000, episode_reward=-88090.96 +/- 48088.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6499347 |
|    mean velocity x | -0.696     |
|    mean velocity y | 0.825      |
|    mean velocity z | 5.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.81e+04  |
| time/              |            |
|    total_timesteps | 564000     |
-----------------------------------
Eval num_timesteps=564500, episode_reward=-62576.61 +/- 57638.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4966074 |
|    mean velocity x | -0.413     |
|    mean velocity y | 0.946      |
|    mean velocity z | 4.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.26e+04  |
| time/              |            |
|    total_timesteps | 564500     |
-----------------------------------
Eval num_timesteps=565000, episode_reward=-65429.60 +/- 19950.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5133633 |
|    mean velocity x | -0.255     |
|    mean velocity y | 1.35       |
|    mean velocity z | 5.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.54e+04  |
| time/              |            |
|    total_timesteps | 565000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 276    |
|    time_elapsed    | 22913  |
|    total_timesteps | 565248 |
-------------------------------
Eval num_timesteps=565500, episode_reward=-83857.41 +/- 50355.75
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.45953956  |
|    mean velocity x      | -0.427       |
|    mean velocity y      | 0.445        |
|    mean velocity z      | 4.82         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.39e+04    |
| time/                   |              |
|    total_timesteps      | 565500       |
| train/                  |              |
|    approx_kl            | 0.0011846593 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0516       |
|    learning_rate        | 0.001        |
|    loss                 | 8e+07        |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 1.56         |
|    value_loss           | 1.26e+08     |
------------------------------------------
Eval num_timesteps=566000, episode_reward=-76787.35 +/- 52297.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48865628 |
|    mean velocity x | -2.78       |
|    mean velocity y | -1.21       |
|    mean velocity z | 7.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.68e+04   |
| time/              |             |
|    total_timesteps | 566000      |
------------------------------------
Eval num_timesteps=566500, episode_reward=-106550.77 +/- 22179.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34652182 |
|    mean velocity x | -1.34       |
|    mean velocity y | -0.67       |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 566500      |
------------------------------------
Eval num_timesteps=567000, episode_reward=-99765.26 +/- 38519.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2919773 |
|    mean velocity x | -0.512     |
|    mean velocity y | 0.752      |
|    mean velocity z | 1.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.98e+04  |
| time/              |            |
|    total_timesteps | 567000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 277    |
|    time_elapsed    | 22993  |
|    total_timesteps | 567296 |
-------------------------------
Eval num_timesteps=567500, episode_reward=-97601.97 +/- 43291.84
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.4510515  |
|    mean velocity x      | -0.18       |
|    mean velocity y      | 1.06        |
|    mean velocity z      | 5.43        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.76e+04   |
| time/                   |             |
|    total_timesteps      | 567500      |
| train/                  |             |
|    approx_kl            | 0.000283653 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.0865      |
|    learning_rate        | 0.001       |
|    loss                 | 4.46e+07    |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00095    |
|    std                  | 1.56        |
|    value_loss           | 6.21e+07    |
-----------------------------------------
Eval num_timesteps=568000, episode_reward=-99818.41 +/- 32312.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4218422 |
|    mean velocity x | -0.24      |
|    mean velocity y | 1.05       |
|    mean velocity z | 4.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.98e+04  |
| time/              |            |
|    total_timesteps | 568000     |
-----------------------------------
Eval num_timesteps=568500, episode_reward=-78441.07 +/- 62512.93
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.522979 |
|    mean velocity x | -0.217    |
|    mean velocity y | 1.36      |
|    mean velocity z | 4.94      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.84e+04 |
| time/              |           |
|    total_timesteps | 568500    |
----------------------------------
Eval num_timesteps=569000, episode_reward=-62834.18 +/- 38771.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36100355 |
|    mean velocity x | 0.132       |
|    mean velocity y | 0.978       |
|    mean velocity z | 3.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 569000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 278    |
|    time_elapsed    | 23074  |
|    total_timesteps | 569344 |
-------------------------------
Eval num_timesteps=569500, episode_reward=-86026.06 +/- 47028.16
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.50536346   |
|    mean velocity x      | -0.0399       |
|    mean velocity y      | 0.959         |
|    mean velocity z      | 4.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.6e+04      |
| time/                   |               |
|    total_timesteps      | 569500        |
| train/                  |               |
|    approx_kl            | 0.00044302593 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0586        |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+07       |
|    n_updates            | 2780          |
|    policy_gradient_loss | -0.00288      |
|    std                  | 1.56          |
|    value_loss           | 1e+08         |
-------------------------------------------
Eval num_timesteps=570000, episode_reward=-80596.36 +/- 45469.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52945775 |
|    mean velocity x | -0.171      |
|    mean velocity y | 1.05        |
|    mean velocity z | 5.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 570000      |
------------------------------------
Eval num_timesteps=570500, episode_reward=-92604.24 +/- 35158.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39706096 |
|    mean velocity x | -0.0803     |
|    mean velocity y | 0.601       |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.26e+04   |
| time/              |             |
|    total_timesteps | 570500      |
------------------------------------
Eval num_timesteps=571000, episode_reward=-100412.27 +/- 32100.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5782416 |
|    mean velocity x | -0.457     |
|    mean velocity y | 0.82       |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+05     |
| time/              |            |
|    total_timesteps | 571000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 279    |
|    time_elapsed    | 23154  |
|    total_timesteps | 571392 |
-------------------------------
Eval num_timesteps=571500, episode_reward=-91068.75 +/- 22157.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5044984   |
|    mean velocity x      | -0.0691      |
|    mean velocity y      | 1.37         |
|    mean velocity z      | 4.74         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.11e+04    |
| time/                   |              |
|    total_timesteps      | 571500       |
| train/                  |              |
|    approx_kl            | 0.0012220821 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0563       |
|    learning_rate        | 0.001        |
|    loss                 | 8.46e+07     |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.00372     |
|    std                  | 1.56         |
|    value_loss           | 1.03e+08     |
------------------------------------------
Eval num_timesteps=572000, episode_reward=-116116.47 +/- 65676.60
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.470957 |
|    mean velocity x | -1.33     |
|    mean velocity y | -0.115    |
|    mean velocity z | 4.05      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.16e+05 |
| time/              |           |
|    total_timesteps | 572000    |
----------------------------------
Eval num_timesteps=572500, episode_reward=-50031.79 +/- 48243.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42318103 |
|    mean velocity x | 0.0887      |
|    mean velocity y | 0.777       |
|    mean velocity z | 2.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5e+04      |
| time/              |             |
|    total_timesteps | 572500      |
------------------------------------
Eval num_timesteps=573000, episode_reward=-68287.71 +/- 40997.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4045017 |
|    mean velocity x | -0.127     |
|    mean velocity y | 0.809      |
|    mean velocity z | 1.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.83e+04  |
| time/              |            |
|    total_timesteps | 573000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 280    |
|    time_elapsed    | 23234  |
|    total_timesteps | 573440 |
-------------------------------
Eval num_timesteps=573500, episode_reward=-95641.86 +/- 32326.80
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.60863626  |
|    mean velocity x      | -0.0805      |
|    mean velocity y      | 1.43         |
|    mean velocity z      | 3.52         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.56e+04    |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0015023169 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.001        |
|    loss                 | 2.78e+06     |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 1.55         |
|    value_loss           | 1.39e+07     |
------------------------------------------
Eval num_timesteps=574000, episode_reward=-100869.95 +/- 41445.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48457915 |
|    mean velocity x | -0.528      |
|    mean velocity y | 0.768       |
|    mean velocity z | 5.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 574000      |
------------------------------------
Eval num_timesteps=574500, episode_reward=-58848.90 +/- 48348.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4304024 |
|    mean velocity x | 0.333      |
|    mean velocity y | 1.23       |
|    mean velocity z | 3.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.88e+04  |
| time/              |            |
|    total_timesteps | 574500     |
-----------------------------------
Eval num_timesteps=575000, episode_reward=-103366.34 +/- 30296.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42710817 |
|    mean velocity x | -0.271      |
|    mean velocity y | 0.859       |
|    mean velocity z | 1.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 575000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 281    |
|    time_elapsed    | 23314  |
|    total_timesteps | 575488 |
-------------------------------
Eval num_timesteps=575500, episode_reward=-79506.06 +/- 36972.58
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5282078    |
|    mean velocity x      | -0.261        |
|    mean velocity y      | 0.953         |
|    mean velocity z      | 4.17          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.95e+04     |
| time/                   |               |
|    total_timesteps      | 575500        |
| train/                  |               |
|    approx_kl            | 0.00025059882 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0943        |
|    learning_rate        | 0.001         |
|    loss                 | 9.55e+07      |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.00136      |
|    std                  | 1.55          |
|    value_loss           | 5.72e+07      |
-------------------------------------------
Eval num_timesteps=576000, episode_reward=-96526.61 +/- 26389.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3017291 |
|    mean velocity x | -0.0671    |
|    mean velocity y | 0.521      |
|    mean velocity z | 0.664      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.65e+04  |
| time/              |            |
|    total_timesteps | 576000     |
-----------------------------------
Eval num_timesteps=576500, episode_reward=-94462.20 +/- 47617.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50436825 |
|    mean velocity x | -0.00682    |
|    mean velocity y | 1.84        |
|    mean velocity z | 4.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.45e+04   |
| time/              |             |
|    total_timesteps | 576500      |
------------------------------------
Eval num_timesteps=577000, episode_reward=-105795.50 +/- 43163.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5321534 |
|    mean velocity x | -1.27      |
|    mean velocity y | -0.237     |
|    mean velocity z | 8.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 577000     |
-----------------------------------
Eval num_timesteps=577500, episode_reward=-73697.74 +/- 47140.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44277087 |
|    mean velocity x | -0.00196    |
|    mean velocity y | 0.803       |
|    mean velocity z | 3.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 577500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 282    |
|    time_elapsed    | 23414  |
|    total_timesteps | 577536 |
-------------------------------
Eval num_timesteps=578000, episode_reward=-61861.07 +/- 50064.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5120951    |
|    mean velocity x      | -1.05         |
|    mean velocity y      | 0.186         |
|    mean velocity z      | 3.76          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.19e+04     |
| time/                   |               |
|    total_timesteps      | 578000        |
| train/                  |               |
|    approx_kl            | 0.00023632459 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.001         |
|    loss                 | 7e+07         |
|    n_updates            | 2820          |
|    policy_gradient_loss | -0.001        |
|    std                  | 1.55          |
|    value_loss           | 6.87e+07      |
-------------------------------------------
Eval num_timesteps=578500, episode_reward=-102147.40 +/- 19276.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47479516 |
|    mean velocity x | -0.796      |
|    mean velocity y | 0.534       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 578500      |
------------------------------------
Eval num_timesteps=579000, episode_reward=-82746.51 +/- 53739.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40672046 |
|    mean velocity x | -2.05       |
|    mean velocity y | -0.54       |
|    mean velocity z | 5.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.27e+04   |
| time/              |             |
|    total_timesteps | 579000      |
------------------------------------
Eval num_timesteps=579500, episode_reward=-92030.03 +/- 29111.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58553386 |
|    mean velocity x | 0.276       |
|    mean velocity y | 2.16        |
|    mean velocity z | 4.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.2e+04    |
| time/              |             |
|    total_timesteps | 579500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 283    |
|    time_elapsed    | 23494  |
|    total_timesteps | 579584 |
-------------------------------
Eval num_timesteps=580000, episode_reward=-74405.75 +/- 57747.07
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.55266005   |
|    mean velocity x      | -0.35         |
|    mean velocity y      | 1.18          |
|    mean velocity z      | 4.36          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.44e+04     |
| time/                   |               |
|    total_timesteps      | 580000        |
| train/                  |               |
|    approx_kl            | 0.00021601276 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.001         |
|    loss                 | 2.24e+07      |
|    n_updates            | 2830          |
|    policy_gradient_loss | -0.000838     |
|    std                  | 1.55          |
|    value_loss           | 5.11e+07      |
-------------------------------------------
Eval num_timesteps=580500, episode_reward=-70115.93 +/- 54902.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30407825 |
|    mean velocity x | -0.142      |
|    mean velocity y | 0.728       |
|    mean velocity z | 1.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.01e+04   |
| time/              |             |
|    total_timesteps | 580500      |
------------------------------------
Eval num_timesteps=581000, episode_reward=-113635.36 +/- 70242.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41062644 |
|    mean velocity x | -0.687      |
|    mean velocity y | 0.805       |
|    mean velocity z | 1.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 581000      |
------------------------------------
Eval num_timesteps=581500, episode_reward=-68992.42 +/- 48134.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5392248 |
|    mean velocity x | -0.466     |
|    mean velocity y | 0.746      |
|    mean velocity z | 4.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.9e+04   |
| time/              |            |
|    total_timesteps | 581500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 284    |
|    time_elapsed    | 23574  |
|    total_timesteps | 581632 |
-------------------------------
Eval num_timesteps=582000, episode_reward=-82766.18 +/- 24033.47
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3556338   |
|    mean velocity x      | 0.118        |
|    mean velocity y      | 0.766        |
|    mean velocity z      | 4.32         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.28e+04    |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0003587793 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0704       |
|    learning_rate        | 0.001        |
|    loss                 | 2.11e+07     |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.55         |
|    value_loss           | 5.37e+07     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-124791.70 +/- 26352.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54410195 |
|    mean velocity x | -0.64       |
|    mean velocity y | 0.817       |
|    mean velocity z | 4.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.25e+05   |
| time/              |             |
|    total_timesteps | 582500      |
------------------------------------
Eval num_timesteps=583000, episode_reward=-88250.26 +/- 47469.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5682339 |
|    mean velocity x | -0.315     |
|    mean velocity y | 0.995      |
|    mean velocity z | 4.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.83e+04  |
| time/              |            |
|    total_timesteps | 583000     |
-----------------------------------
Eval num_timesteps=583500, episode_reward=-106341.33 +/- 39480.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5029994 |
|    mean velocity x | 0.274      |
|    mean velocity y | 1.02       |
|    mean velocity z | 3.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 583500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 285    |
|    time_elapsed    | 23655  |
|    total_timesteps | 583680 |
-------------------------------
Eval num_timesteps=584000, episode_reward=-84330.03 +/- 18254.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39990532   |
|    mean velocity x      | -0.439        |
|    mean velocity y      | 0.639         |
|    mean velocity z      | 4.35          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.43e+04     |
| time/                   |               |
|    total_timesteps      | 584000        |
| train/                  |               |
|    approx_kl            | 0.00012859158 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0823        |
|    learning_rate        | 0.001         |
|    loss                 | 5.37e+07      |
|    n_updates            | 2850          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 1.55          |
|    value_loss           | 8.05e+07      |
-------------------------------------------
Eval num_timesteps=584500, episode_reward=-131685.01 +/- 17090.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45912898 |
|    mean velocity x | -0.882      |
|    mean velocity y | -0.0691     |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.32e+05   |
| time/              |             |
|    total_timesteps | 584500      |
------------------------------------
Eval num_timesteps=585000, episode_reward=-86343.78 +/- 27113.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46630228 |
|    mean velocity x | -0.213      |
|    mean velocity y | 1.2         |
|    mean velocity z | 4.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.63e+04   |
| time/              |             |
|    total_timesteps | 585000      |
------------------------------------
Eval num_timesteps=585500, episode_reward=-70857.04 +/- 11463.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41702795 |
|    mean velocity x | 0.0577      |
|    mean velocity y | 0.998       |
|    mean velocity z | 4.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 585500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 286    |
|    time_elapsed    | 23735  |
|    total_timesteps | 585728 |
-------------------------------
Eval num_timesteps=586000, episode_reward=-73702.15 +/- 37550.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6109244   |
|    mean velocity x      | -0.331       |
|    mean velocity y      | 1.76         |
|    mean velocity z      | 4.59         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.37e+04    |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0009313802 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.0809       |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+07      |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.55         |
|    value_loss           | 8.2e+07      |
------------------------------------------
Eval num_timesteps=586500, episode_reward=-68194.81 +/- 49031.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51269275 |
|    mean velocity x | -0.666      |
|    mean velocity y | 0.821       |
|    mean velocity z | 4.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.82e+04   |
| time/              |             |
|    total_timesteps | 586500      |
------------------------------------
Eval num_timesteps=587000, episode_reward=-82289.24 +/- 29869.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56836146 |
|    mean velocity x | -2.6        |
|    mean velocity y | -0.526      |
|    mean velocity z | 6.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 587000      |
------------------------------------
Eval num_timesteps=587500, episode_reward=-70488.79 +/- 28051.70
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.509659 |
|    mean velocity x | -0.2      |
|    mean velocity y | 1.16      |
|    mean velocity z | 4.99      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.05e+04 |
| time/              |           |
|    total_timesteps | 587500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 287    |
|    time_elapsed    | 23815  |
|    total_timesteps | 587776 |
-------------------------------
Eval num_timesteps=588000, episode_reward=-78956.23 +/- 45583.56
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.53649914  |
|    mean velocity x      | 0.0396       |
|    mean velocity y      | 1.11         |
|    mean velocity z      | 4.13         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.9e+04     |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0003844387 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0911       |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+07     |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 1.56         |
|    value_loss           | 7.96e+07     |
------------------------------------------
Eval num_timesteps=588500, episode_reward=-44440.50 +/- 31470.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5490984 |
|    mean velocity x | -0.809     |
|    mean velocity y | 0.429      |
|    mean velocity z | 5.21       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.44e+04  |
| time/              |            |
|    total_timesteps | 588500     |
-----------------------------------
Eval num_timesteps=589000, episode_reward=-90968.74 +/- 22789.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51381123 |
|    mean velocity x | -0.237      |
|    mean velocity y | 1.12        |
|    mean velocity z | 5.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 589000      |
------------------------------------
Eval num_timesteps=589500, episode_reward=-89794.96 +/- 45828.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4314282 |
|    mean velocity x | -0.49      |
|    mean velocity y | 0.948      |
|    mean velocity z | 2.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.98e+04  |
| time/              |            |
|    total_timesteps | 589500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 288    |
|    time_elapsed    | 23896  |
|    total_timesteps | 589824 |
-------------------------------
Eval num_timesteps=590000, episode_reward=-71433.08 +/- 61443.95
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6327679    |
|    mean velocity x      | 0.274         |
|    mean velocity y      | 2.19          |
|    mean velocity z      | 4.06          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.14e+04     |
| time/                   |               |
|    total_timesteps      | 590000        |
| train/                  |               |
|    approx_kl            | 0.00015369517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0842        |
|    learning_rate        | 0.001         |
|    loss                 | 8.19e+06      |
|    n_updates            | 2880          |
|    policy_gradient_loss | -0.000771     |
|    std                  | 1.56          |
|    value_loss           | 6.89e+07      |
-------------------------------------------
Eval num_timesteps=590500, episode_reward=-75316.86 +/- 47846.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59296477 |
|    mean velocity x | -1.01       |
|    mean velocity y | 0.448       |
|    mean velocity z | 4.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.53e+04   |
| time/              |             |
|    total_timesteps | 590500      |
------------------------------------
Eval num_timesteps=591000, episode_reward=-109558.48 +/- 34493.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44358438 |
|    mean velocity x | -0.134      |
|    mean velocity y | 1.12        |
|    mean velocity z | 4.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 591000      |
------------------------------------
Eval num_timesteps=591500, episode_reward=-98870.30 +/- 49269.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41562167 |
|    mean velocity x | -0.249      |
|    mean velocity y | 0.947       |
|    mean velocity z | 4.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.89e+04   |
| time/              |             |
|    total_timesteps | 591500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 289    |
|    time_elapsed    | 23976  |
|    total_timesteps | 591872 |
-------------------------------
Eval num_timesteps=592000, episode_reward=-79101.52 +/- 35494.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3105891   |
|    mean velocity x      | -0.204       |
|    mean velocity y      | 0.127        |
|    mean velocity z      | 4.19         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.91e+04    |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0003670146 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0702       |
|    learning_rate        | 0.001        |
|    loss                 | 4.22e+07     |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.56         |
|    value_loss           | 1.03e+08     |
------------------------------------------
Eval num_timesteps=592500, episode_reward=-63746.16 +/- 49289.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39795977 |
|    mean velocity x | -0.539      |
|    mean velocity y | 0.943       |
|    mean velocity z | 1.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.37e+04   |
| time/              |             |
|    total_timesteps | 592500      |
------------------------------------
Eval num_timesteps=593000, episode_reward=-53325.71 +/- 39530.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41223744 |
|    mean velocity x | -0.88       |
|    mean velocity y | -0.0973     |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.33e+04   |
| time/              |             |
|    total_timesteps | 593000      |
------------------------------------
Eval num_timesteps=593500, episode_reward=-80229.54 +/- 46325.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46023706 |
|    mean velocity x | -0.201      |
|    mean velocity y | 0.51        |
|    mean velocity z | 3.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.02e+04   |
| time/              |             |
|    total_timesteps | 593500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 290    |
|    time_elapsed    | 24056  |
|    total_timesteps | 593920 |
-------------------------------
Eval num_timesteps=594000, episode_reward=-55259.32 +/- 42509.41
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4418439    |
|    mean velocity x      | -0.922        |
|    mean velocity y      | -0.345        |
|    mean velocity z      | 3.98          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.53e+04     |
| time/                   |               |
|    total_timesteps      | 594000        |
| train/                  |               |
|    approx_kl            | 0.00012820464 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.001         |
|    loss                 | 1.92e+07      |
|    n_updates            | 2900          |
|    policy_gradient_loss | -0.000572     |
|    std                  | 1.56          |
|    value_loss           | 3.15e+07      |
-------------------------------------------
Eval num_timesteps=594500, episode_reward=-92102.34 +/- 41251.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57148707 |
|    mean velocity x | -0.258      |
|    mean velocity y | 1.16        |
|    mean velocity z | 5.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.21e+04   |
| time/              |             |
|    total_timesteps | 594500      |
------------------------------------
Eval num_timesteps=595000, episode_reward=-79301.70 +/- 38039.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4752882 |
|    mean velocity x | -0.151     |
|    mean velocity y | 0.92       |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.93e+04  |
| time/              |            |
|    total_timesteps | 595000     |
-----------------------------------
Eval num_timesteps=595500, episode_reward=-65278.90 +/- 31818.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49311998 |
|    mean velocity x | -0.334      |
|    mean velocity y | 0.758       |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.53e+04   |
| time/              |             |
|    total_timesteps | 595500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 291    |
|    time_elapsed    | 24136  |
|    total_timesteps | 595968 |
-------------------------------
Eval num_timesteps=596000, episode_reward=-98408.30 +/- 37723.64
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5375267   |
|    mean velocity x      | 0.321        |
|    mean velocity y      | 1.62         |
|    mean velocity z      | 3.94         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.84e+04    |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0004509062 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0688       |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+07     |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.0027      |
|    std                  | 1.56         |
|    value_loss           | 1e+08        |
------------------------------------------
Eval num_timesteps=596500, episode_reward=-78197.04 +/- 48946.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50720435 |
|    mean velocity x | -0.263      |
|    mean velocity y | 1.52        |
|    mean velocity z | 4.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 596500      |
------------------------------------
Eval num_timesteps=597000, episode_reward=-69757.53 +/- 56793.11
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3353749 |
|    mean velocity x | -0.173     |
|    mean velocity y | 0.565      |
|    mean velocity z | 0.668      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.98e+04  |
| time/              |            |
|    total_timesteps | 597000     |
-----------------------------------
Eval num_timesteps=597500, episode_reward=-92526.96 +/- 21782.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4052294 |
|    mean velocity x | -0.399     |
|    mean velocity y | 0.974      |
|    mean velocity z | 4.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.25e+04  |
| time/              |            |
|    total_timesteps | 597500     |
-----------------------------------
Eval num_timesteps=598000, episode_reward=-72988.69 +/- 43787.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4345808 |
|    mean velocity x | -0.463     |
|    mean velocity y | 0.0917     |
|    mean velocity z | 4.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.3e+04   |
| time/              |            |
|    total_timesteps | 598000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 292    |
|    time_elapsed    | 24236  |
|    total_timesteps | 598016 |
-------------------------------
Eval num_timesteps=598500, episode_reward=-94678.54 +/- 54086.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43710387   |
|    mean velocity x      | -0.414        |
|    mean velocity y      | 0.656         |
|    mean velocity z      | 4.62          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.47e+04     |
| time/                   |               |
|    total_timesteps      | 598500        |
| train/                  |               |
|    approx_kl            | 6.4808904e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0687        |
|    learning_rate        | 0.001         |
|    loss                 | 6.12e+07      |
|    n_updates            | 2920          |
|    policy_gradient_loss | -0.000497     |
|    std                  | 1.56          |
|    value_loss           | 7.88e+07      |
-------------------------------------------
Eval num_timesteps=599000, episode_reward=-122536.72 +/- 36047.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4547196 |
|    mean velocity x | 0.0636     |
|    mean velocity y | 1.16       |
|    mean velocity z | 3.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.23e+05  |
| time/              |            |
|    total_timesteps | 599000     |
-----------------------------------
Eval num_timesteps=599500, episode_reward=-67159.27 +/- 25642.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44072938 |
|    mean velocity x | -0.156      |
|    mean velocity y | 0.817       |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 599500      |
------------------------------------
Eval num_timesteps=600000, episode_reward=-95628.57 +/- 35778.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5271048 |
|    mean velocity x | -0.263     |
|    mean velocity y | 1.13       |
|    mean velocity z | 3.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.56e+04  |
| time/              |            |
|    total_timesteps | 600000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 293    |
|    time_elapsed    | 24316  |
|    total_timesteps | 600064 |
-------------------------------
Eval num_timesteps=600500, episode_reward=-113692.11 +/- 12726.64
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.45677882  |
|    mean velocity x      | 0.545        |
|    mean velocity y      | 1.52         |
|    mean velocity z      | 3.84         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.14e+05    |
| time/                   |              |
|    total_timesteps      | 600500       |
| train/                  |              |
|    approx_kl            | 0.0011638164 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0791       |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+07     |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 1.56         |
|    value_loss           | 5.59e+07     |
------------------------------------------
Eval num_timesteps=601000, episode_reward=-104613.73 +/- 28413.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47665545 |
|    mean velocity x | 0.378       |
|    mean velocity y | 0.922       |
|    mean velocity z | 2.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 601000      |
------------------------------------
Eval num_timesteps=601500, episode_reward=-90475.66 +/- 52022.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46982574 |
|    mean velocity x | -0.0804     |
|    mean velocity y | 0.95        |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.05e+04   |
| time/              |             |
|    total_timesteps | 601500      |
------------------------------------
Eval num_timesteps=602000, episode_reward=-88138.31 +/- 45386.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39175868 |
|    mean velocity x | -0.744      |
|    mean velocity y | 0.29        |
|    mean velocity z | 2.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.81e+04   |
| time/              |             |
|    total_timesteps | 602000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 294    |
|    time_elapsed    | 24396  |
|    total_timesteps | 602112 |
-------------------------------
Eval num_timesteps=602500, episode_reward=-79919.94 +/- 32106.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5122968   |
|    mean velocity x      | -0.349       |
|    mean velocity y      | 1.26         |
|    mean velocity z      | 4.73         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.99e+04    |
| time/                   |              |
|    total_timesteps      | 602500       |
| train/                  |              |
|    approx_kl            | 0.0002152923 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0684       |
|    learning_rate        | 0.001        |
|    loss                 | 2.16e+07     |
|    n_updates            | 2940         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 1.56         |
|    value_loss           | 5.99e+07     |
------------------------------------------
Eval num_timesteps=603000, episode_reward=-116409.31 +/- 18038.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47809944 |
|    mean velocity x | -0.0571     |
|    mean velocity y | 1.75        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 603000      |
------------------------------------
Eval num_timesteps=603500, episode_reward=-101523.87 +/- 31750.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4446279 |
|    mean velocity x | -0.322     |
|    mean velocity y | 0.704      |
|    mean velocity z | 4.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 603500     |
-----------------------------------
Eval num_timesteps=604000, episode_reward=-83827.39 +/- 22161.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5521291 |
|    mean velocity x | 0.736      |
|    mean velocity y | 1.93       |
|    mean velocity z | 4.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.38e+04  |
| time/              |            |
|    total_timesteps | 604000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 295    |
|    time_elapsed    | 24477  |
|    total_timesteps | 604160 |
-------------------------------
Eval num_timesteps=604500, episode_reward=-96117.92 +/- 42625.03
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39651337   |
|    mean velocity x      | -0.304        |
|    mean velocity y      | 1.04          |
|    mean velocity z      | 4.92          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.61e+04     |
| time/                   |               |
|    total_timesteps      | 604500        |
| train/                  |               |
|    approx_kl            | 0.00013534108 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0806        |
|    learning_rate        | 0.001         |
|    loss                 | 4.22e+07      |
|    n_updates            | 2950          |
|    policy_gradient_loss | -0.00146      |
|    std                  | 1.56          |
|    value_loss           | 8.22e+07      |
-------------------------------------------
Eval num_timesteps=605000, episode_reward=-114041.82 +/- 41922.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43783155 |
|    mean velocity x | -0.71       |
|    mean velocity y | -0.216      |
|    mean velocity z | 3.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 605000      |
------------------------------------
Eval num_timesteps=605500, episode_reward=-66493.93 +/- 56404.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23088205 |
|    mean velocity x | 0.18        |
|    mean velocity y | 0.326       |
|    mean velocity z | 0.686       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.65e+04   |
| time/              |             |
|    total_timesteps | 605500      |
------------------------------------
Eval num_timesteps=606000, episode_reward=-81142.83 +/- 43053.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37048987 |
|    mean velocity x | -0.46       |
|    mean velocity y | 0.647       |
|    mean velocity z | 1.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.11e+04   |
| time/              |             |
|    total_timesteps | 606000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 296    |
|    time_elapsed    | 24557  |
|    total_timesteps | 606208 |
-------------------------------
Eval num_timesteps=606500, episode_reward=-79727.94 +/- 41642.60
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5380995    |
|    mean velocity x      | -0.252        |
|    mean velocity y      | 0.927         |
|    mean velocity z      | 4.41          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.97e+04     |
| time/                   |               |
|    total_timesteps      | 606500        |
| train/                  |               |
|    approx_kl            | 0.00029195863 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0716        |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+06      |
|    n_updates            | 2960          |
|    policy_gradient_loss | -0.00145      |
|    std                  | 1.56          |
|    value_loss           | 3.99e+07      |
-------------------------------------------
Eval num_timesteps=607000, episode_reward=-84325.21 +/- 37604.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47893006 |
|    mean velocity x | -0.261      |
|    mean velocity y | 1.3         |
|    mean velocity z | 4.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.43e+04   |
| time/              |             |
|    total_timesteps | 607000      |
------------------------------------
Eval num_timesteps=607500, episode_reward=-73724.56 +/- 36799.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56178975 |
|    mean velocity x | 0.454       |
|    mean velocity y | 1.75        |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 607500      |
------------------------------------
Eval num_timesteps=608000, episode_reward=-78478.68 +/- 40659.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41552737 |
|    mean velocity x | 0.29        |
|    mean velocity y | 1.09        |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.85e+04   |
| time/              |             |
|    total_timesteps | 608000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 297    |
|    time_elapsed    | 24637  |
|    total_timesteps | 608256 |
-------------------------------
Eval num_timesteps=608500, episode_reward=-161138.68 +/- 77275.11
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26975936  |
|    mean velocity x      | -0.159       |
|    mean velocity y      | 0.635        |
|    mean velocity z      | 0.481        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.61e+05    |
| time/                   |              |
|    total_timesteps      | 608500       |
| train/                  |              |
|    approx_kl            | 9.787778e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.0951       |
|    learning_rate        | 0.001        |
|    loss                 | 4.16e+07     |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.000824    |
|    std                  | 1.56         |
|    value_loss           | 5.41e+07     |
------------------------------------------
Eval num_timesteps=609000, episode_reward=-82856.35 +/- 58010.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47523865 |
|    mean velocity x | 0.0044      |
|    mean velocity y | 1.24        |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 609000      |
------------------------------------
Eval num_timesteps=609500, episode_reward=-46999.53 +/- 53405.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26499602 |
|    mean velocity x | -1.43       |
|    mean velocity y | -0.763      |
|    mean velocity z | 4.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.7e+04    |
| time/              |             |
|    total_timesteps | 609500      |
------------------------------------
Eval num_timesteps=610000, episode_reward=-113137.68 +/- 23412.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36563966 |
|    mean velocity x | -0.173      |
|    mean velocity y | 0.49        |
|    mean velocity z | 1.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 610000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 298    |
|    time_elapsed    | 24718  |
|    total_timesteps | 610304 |
-------------------------------
Eval num_timesteps=610500, episode_reward=-90746.51 +/- 34639.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36449128   |
|    mean velocity x      | -0.289        |
|    mean velocity y      | 0.891         |
|    mean velocity z      | 1.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.07e+04     |
| time/                   |               |
|    total_timesteps      | 610500        |
| train/                  |               |
|    approx_kl            | 0.00021328896 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.001         |
|    loss                 | 2.49e+07      |
|    n_updates            | 2980          |
|    policy_gradient_loss | -0.00102      |
|    std                  | 1.56          |
|    value_loss           | 2.85e+07      |
-------------------------------------------
Eval num_timesteps=611000, episode_reward=-106170.27 +/- 23166.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4481316 |
|    mean velocity x | -0.221     |
|    mean velocity y | 0.534      |
|    mean velocity z | 2.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 611000     |
-----------------------------------
Eval num_timesteps=611500, episode_reward=-80244.37 +/- 52025.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5415505 |
|    mean velocity x | -0.562     |
|    mean velocity y | 1          |
|    mean velocity z | 4.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.02e+04  |
| time/              |            |
|    total_timesteps | 611500     |
-----------------------------------
Eval num_timesteps=612000, episode_reward=-78592.35 +/- 50122.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5871536 |
|    mean velocity x | -0.557     |
|    mean velocity y | 0.693      |
|    mean velocity z | 5.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.86e+04  |
| time/              |            |
|    total_timesteps | 612000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 299    |
|    time_elapsed    | 24798  |
|    total_timesteps | 612352 |
-------------------------------
Eval num_timesteps=612500, episode_reward=-117568.16 +/- 29386.15
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40420395   |
|    mean velocity x      | -0.239        |
|    mean velocity y      | 0.437         |
|    mean velocity z      | 3.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.18e+05     |
| time/                   |               |
|    total_timesteps      | 612500        |
| train/                  |               |
|    approx_kl            | 0.00039541483 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0947        |
|    learning_rate        | 0.001         |
|    loss                 | 7.6e+06       |
|    n_updates            | 2990          |
|    policy_gradient_loss | -0.00205      |
|    std                  | 1.56          |
|    value_loss           | 6.99e+07      |
-------------------------------------------
Eval num_timesteps=613000, episode_reward=-102879.62 +/- 40568.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50935894 |
|    mean velocity x | 0.248       |
|    mean velocity y | 2.15        |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 613000      |
------------------------------------
Eval num_timesteps=613500, episode_reward=-89170.45 +/- 30627.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44088283 |
|    mean velocity x | -1.07       |
|    mean velocity y | 0.177       |
|    mean velocity z | 3.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.92e+04   |
| time/              |             |
|    total_timesteps | 613500      |
------------------------------------
Eval num_timesteps=614000, episode_reward=-80424.14 +/- 46953.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5853216 |
|    mean velocity x | -0.978     |
|    mean velocity y | 0.391      |
|    mean velocity z | 4.06       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 614000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 300    |
|    time_elapsed    | 24879  |
|    total_timesteps | 614400 |
-------------------------------
Eval num_timesteps=614500, episode_reward=-99365.77 +/- 23788.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3518068    |
|    mean velocity x      | 0.176         |
|    mean velocity y      | 0.529         |
|    mean velocity z      | 1.11          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.94e+04     |
| time/                   |               |
|    total_timesteps      | 614500        |
| train/                  |               |
|    approx_kl            | 0.00061589363 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 2.06e+07      |
|    n_updates            | 3000          |
|    policy_gradient_loss | -0.00241      |
|    std                  | 1.55          |
|    value_loss           | 2.81e+07      |
-------------------------------------------
Eval num_timesteps=615000, episode_reward=-96758.99 +/- 48544.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5026078 |
|    mean velocity x | -0.408     |
|    mean velocity y | 1.34       |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.68e+04  |
| time/              |            |
|    total_timesteps | 615000     |
-----------------------------------
Eval num_timesteps=615500, episode_reward=-96647.37 +/- 27009.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29125017 |
|    mean velocity x | -0.339      |
|    mean velocity y | 0.212       |
|    mean velocity z | 2.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.66e+04   |
| time/              |             |
|    total_timesteps | 615500      |
------------------------------------
Eval num_timesteps=616000, episode_reward=-81648.97 +/- 30080.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44319662 |
|    mean velocity x | -0.483      |
|    mean velocity y | 0.403       |
|    mean velocity z | 4.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 616000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 301    |
|    time_elapsed    | 24959  |
|    total_timesteps | 616448 |
-------------------------------
Eval num_timesteps=616500, episode_reward=-105862.04 +/- 55139.20
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5294758    |
|    mean velocity x      | -0.301        |
|    mean velocity y      | 1.05          |
|    mean velocity z      | 5.09          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.06e+05     |
| time/                   |               |
|    total_timesteps      | 616500        |
| train/                  |               |
|    approx_kl            | 0.00014913903 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0866        |
|    learning_rate        | 0.001         |
|    loss                 | 5.01e+07      |
|    n_updates            | 3010          |
|    policy_gradient_loss | -0.00124      |
|    std                  | 1.55          |
|    value_loss           | 8.41e+07      |
-------------------------------------------
Eval num_timesteps=617000, episode_reward=-60755.55 +/- 38292.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46452937 |
|    mean velocity x | -0.223      |
|    mean velocity y | 0.95        |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.08e+04   |
| time/              |             |
|    total_timesteps | 617000      |
------------------------------------
Eval num_timesteps=617500, episode_reward=-104115.06 +/- 52036.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35583737 |
|    mean velocity x | -0.523      |
|    mean velocity y | 0.323       |
|    mean velocity z | 2.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 617500      |
------------------------------------
Eval num_timesteps=618000, episode_reward=-95920.77 +/- 20610.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5892235 |
|    mean velocity x | -0.619     |
|    mean velocity y | 0.695      |
|    mean velocity z | 5.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.59e+04  |
| time/              |            |
|    total_timesteps | 618000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 302    |
|    time_elapsed    | 25039  |
|    total_timesteps | 618496 |
-------------------------------
Eval num_timesteps=618500, episode_reward=-99458.97 +/- 16404.96
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.54467505  |
|    mean velocity x      | -0.339       |
|    mean velocity y      | 1.05         |
|    mean velocity z      | 5.24         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.95e+04    |
| time/                   |              |
|    total_timesteps      | 618500       |
| train/                  |              |
|    approx_kl            | 0.0003382676 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.0756       |
|    learning_rate        | 0.001        |
|    loss                 | 3.76e+07     |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 1.56         |
|    value_loss           | 9.38e+07     |
------------------------------------------
Eval num_timesteps=619000, episode_reward=-72258.38 +/- 36635.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44703794 |
|    mean velocity x | -0.066      |
|    mean velocity y | 1.19        |
|    mean velocity z | 4.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 619000      |
------------------------------------
Eval num_timesteps=619500, episode_reward=-86344.32 +/- 43260.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47067857 |
|    mean velocity x | -0.241      |
|    mean velocity y | 0.74        |
|    mean velocity z | 2.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.63e+04   |
| time/              |             |
|    total_timesteps | 619500      |
------------------------------------
Eval num_timesteps=620000, episode_reward=-68605.22 +/- 62737.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5422862 |
|    mean velocity x | -0.417     |
|    mean velocity y | 1.31       |
|    mean velocity z | 5.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.86e+04  |
| time/              |            |
|    total_timesteps | 620000     |
-----------------------------------
Eval num_timesteps=620500, episode_reward=-114674.53 +/- 29409.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41802496 |
|    mean velocity x | -0.196      |
|    mean velocity y | 0.609       |
|    mean velocity z | 1.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 620500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 303    |
|    time_elapsed    | 25139  |
|    total_timesteps | 620544 |
-------------------------------
Eval num_timesteps=621000, episode_reward=-99213.94 +/- 34361.56
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47294384   |
|    mean velocity x      | -0.319        |
|    mean velocity y      | 1.23          |
|    mean velocity z      | 4.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.92e+04     |
| time/                   |               |
|    total_timesteps      | 621000        |
| train/                  |               |
|    approx_kl            | 0.00040763672 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.088         |
|    learning_rate        | 0.001         |
|    loss                 | 2.73e+07      |
|    n_updates            | 3030          |
|    policy_gradient_loss | -0.000954     |
|    std                  | 1.56          |
|    value_loss           | 7.49e+07      |
-------------------------------------------
Eval num_timesteps=621500, episode_reward=-103989.51 +/- 51254.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56447875 |
|    mean velocity x | -0.182      |
|    mean velocity y | 1.28        |
|    mean velocity z | 4.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 621500      |
------------------------------------
Eval num_timesteps=622000, episode_reward=-92146.21 +/- 29227.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5962048 |
|    mean velocity x | -0.174     |
|    mean velocity y | 1.23       |
|    mean velocity z | 4.78       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.21e+04  |
| time/              |            |
|    total_timesteps | 622000     |
-----------------------------------
Eval num_timesteps=622500, episode_reward=-86850.37 +/- 35660.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33832335 |
|    mean velocity x | -1.34       |
|    mean velocity y | -0.027      |
|    mean velocity z | 3.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 622500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 304    |
|    time_elapsed    | 25219  |
|    total_timesteps | 622592 |
-------------------------------
Eval num_timesteps=623000, episode_reward=-89955.05 +/- 49079.61
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4791831    |
|    mean velocity x      | -1.3          |
|    mean velocity y      | 0.553         |
|    mean velocity z      | 5.24          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9e+04        |
| time/                   |               |
|    total_timesteps      | 623000        |
| train/                  |               |
|    approx_kl            | 0.00042331923 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.0942        |
|    learning_rate        | 0.001         |
|    loss                 | 3.02e+07      |
|    n_updates            | 3040          |
|    policy_gradient_loss | -0.00116      |
|    std                  | 1.56          |
|    value_loss           | 7.05e+07      |
-------------------------------------------
Eval num_timesteps=623500, episode_reward=-99011.59 +/- 23303.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50063056 |
|    mean velocity x | -0.215      |
|    mean velocity y | 0.861       |
|    mean velocity z | 3.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.9e+04    |
| time/              |             |
|    total_timesteps | 623500      |
------------------------------------
Eval num_timesteps=624000, episode_reward=-108483.13 +/- 19710.85
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3959243 |
|    mean velocity x | -0.862     |
|    mean velocity y | 0.27       |
|    mean velocity z | 3.06       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 624000     |
-----------------------------------
Eval num_timesteps=624500, episode_reward=-82755.31 +/- 34246.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33128083 |
|    mean velocity x | -2.19       |
|    mean velocity y | -0.741      |
|    mean velocity z | 4.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.28e+04   |
| time/              |             |
|    total_timesteps | 624500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 305    |
|    time_elapsed    | 25299  |
|    total_timesteps | 624640 |
-------------------------------
Eval num_timesteps=625000, episode_reward=-66842.26 +/- 47538.73
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3158407    |
|    mean velocity x      | -0.196        |
|    mean velocity y      | 0.636         |
|    mean velocity z      | 0.465         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.68e+04     |
| time/                   |               |
|    total_timesteps      | 625000        |
| train/                  |               |
|    approx_kl            | 0.00051435374 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.001         |
|    loss                 | 8.72e+06      |
|    n_updates            | 3050          |
|    policy_gradient_loss | -0.00193      |
|    std                  | 1.55          |
|    value_loss           | 2.62e+07      |
-------------------------------------------
Eval num_timesteps=625500, episode_reward=-82148.12 +/- 18819.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37911025 |
|    mean velocity x | -0.195      |
|    mean velocity y | 0.824       |
|    mean velocity z | 1.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.21e+04   |
| time/              |             |
|    total_timesteps | 625500      |
------------------------------------
Eval num_timesteps=626000, episode_reward=-80710.52 +/- 43719.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6091809 |
|    mean velocity x | -0.499     |
|    mean velocity y | 1.19       |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 626000     |
-----------------------------------
Eval num_timesteps=626500, episode_reward=-60596.19 +/- 37488.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46612453 |
|    mean velocity x | -0.0321     |
|    mean velocity y | 1.35        |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.06e+04   |
| time/              |             |
|    total_timesteps | 626500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 306    |
|    time_elapsed    | 25380  |
|    total_timesteps | 626688 |
-------------------------------
Eval num_timesteps=627000, episode_reward=-68171.95 +/- 29275.44
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.14682025   |
|    mean velocity x      | 0.0214        |
|    mean velocity y      | 0.428         |
|    mean velocity z      | 0.319         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.82e+04     |
| time/                   |               |
|    total_timesteps      | 627000        |
| train/                  |               |
|    approx_kl            | 0.00014182748 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.088         |
|    learning_rate        | 0.001         |
|    loss                 | 2.49e+07      |
|    n_updates            | 3060          |
|    policy_gradient_loss | -0.00166      |
|    std                  | 1.55          |
|    value_loss           | 4.67e+07      |
-------------------------------------------
Eval num_timesteps=627500, episode_reward=-115671.41 +/- 19494.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37222365 |
|    mean velocity x | -0.36       |
|    mean velocity y | 0.305       |
|    mean velocity z | 3.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 627500      |
------------------------------------
Eval num_timesteps=628000, episode_reward=-89212.41 +/- 37184.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6283605 |
|    mean velocity x | -0.283     |
|    mean velocity y | 1.83       |
|    mean velocity z | 5.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.92e+04  |
| time/              |            |
|    total_timesteps | 628000     |
-----------------------------------
Eval num_timesteps=628500, episode_reward=-96101.30 +/- 18663.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5455339 |
|    mean velocity x | -0.735     |
|    mean velocity y | 0.874      |
|    mean velocity z | 3.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.61e+04  |
| time/              |            |
|    total_timesteps | 628500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 307    |
|    time_elapsed    | 25460  |
|    total_timesteps | 628736 |
-------------------------------
Eval num_timesteps=629000, episode_reward=-96594.20 +/- 20644.90
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.381038    |
|    mean velocity x      | -0.245       |
|    mean velocity y      | 0.241        |
|    mean velocity z      | 3.49         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.66e+04    |
| time/                   |              |
|    total_timesteps      | 629000       |
| train/                  |              |
|    approx_kl            | 0.0003230404 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.128        |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+07     |
|    n_updates            | 3070         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.55         |
|    value_loss           | 3.55e+07     |
------------------------------------------
Eval num_timesteps=629500, episode_reward=-91569.51 +/- 20739.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50487435 |
|    mean velocity x | -0.245      |
|    mean velocity y | 1.1         |
|    mean velocity z | 5.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.16e+04   |
| time/              |             |
|    total_timesteps | 629500      |
------------------------------------
Eval num_timesteps=630000, episode_reward=-72354.88 +/- 42762.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5021119 |
|    mean velocity x | 0.00852    |
|    mean velocity y | 1.04       |
|    mean velocity z | 4.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.24e+04  |
| time/              |            |
|    total_timesteps | 630000     |
-----------------------------------
Eval num_timesteps=630500, episode_reward=-80360.29 +/- 48775.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6511911 |
|    mean velocity x | 0.502      |
|    mean velocity y | 1.94       |
|    mean velocity z | 4.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 630500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 308    |
|    time_elapsed    | 25540  |
|    total_timesteps | 630784 |
-------------------------------
Eval num_timesteps=631000, episode_reward=-78852.65 +/- 46154.10
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46452636  |
|    mean velocity x      | -0.201       |
|    mean velocity y      | 1.26         |
|    mean velocity z      | 4.94         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.89e+04    |
| time/                   |              |
|    total_timesteps      | 631000       |
| train/                  |              |
|    approx_kl            | 0.0005794759 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.0814       |
|    learning_rate        | 0.001        |
|    loss                 | 6.59e+07     |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 1.55         |
|    value_loss           | 9.4e+07      |
------------------------------------------
Eval num_timesteps=631500, episode_reward=-88351.16 +/- 56310.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51182246 |
|    mean velocity x | -0.214      |
|    mean velocity y | 1.07        |
|    mean velocity z | 3.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 631500      |
------------------------------------
Eval num_timesteps=632000, episode_reward=-39960.89 +/- 50078.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47616675 |
|    mean velocity x | -0.279      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4e+04      |
| time/              |             |
|    total_timesteps | 632000      |
------------------------------------
Eval num_timesteps=632500, episode_reward=-104888.80 +/- 20824.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43577772 |
|    mean velocity x | -0.712      |
|    mean velocity y | 0.118       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 632500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 309    |
|    time_elapsed    | 25621  |
|    total_timesteps | 632832 |
-------------------------------
Eval num_timesteps=633000, episode_reward=-105194.43 +/- 29602.98
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3156176   |
|    mean velocity x      | -0.117       |
|    mean velocity y      | 0.306        |
|    mean velocity z      | 2.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 633000       |
| train/                  |              |
|    approx_kl            | 0.0009643336 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0963       |
|    learning_rate        | 0.001        |
|    loss                 | 2.69e+07     |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00308     |
|    std                  | 1.55         |
|    value_loss           | 6.39e+07     |
------------------------------------------
Eval num_timesteps=633500, episode_reward=-105477.26 +/- 67815.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45095998 |
|    mean velocity x | -0.663      |
|    mean velocity y | 0.491       |
|    mean velocity z | 3.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 633500      |
------------------------------------
Eval num_timesteps=634000, episode_reward=-84279.54 +/- 39624.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58496773 |
|    mean velocity x | 0.288       |
|    mean velocity y | 1.73        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.43e+04   |
| time/              |             |
|    total_timesteps | 634000      |
------------------------------------
Eval num_timesteps=634500, episode_reward=-70437.45 +/- 37428.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6538499 |
|    mean velocity x | 0.497      |
|    mean velocity y | 2.04       |
|    mean velocity z | 4.23       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.04e+04  |
| time/              |            |
|    total_timesteps | 634500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 310    |
|    time_elapsed    | 25701  |
|    total_timesteps | 634880 |
-------------------------------
Eval num_timesteps=635000, episode_reward=-114968.50 +/- 32833.13
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20429908  |
|    mean velocity x      | -0.0126      |
|    mean velocity y      | 0.217        |
|    mean velocity z      | 1.14         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.15e+05    |
| time/                   |              |
|    total_timesteps      | 635000       |
| train/                  |              |
|    approx_kl            | 0.0002493348 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+07     |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.000664    |
|    std                  | 1.55         |
|    value_loss           | 2.97e+07     |
------------------------------------------
Eval num_timesteps=635500, episode_reward=-64605.33 +/- 21413.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50713503 |
|    mean velocity x | -0.11       |
|    mean velocity y | 1.55        |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.46e+04   |
| time/              |             |
|    total_timesteps | 635500      |
------------------------------------
Eval num_timesteps=636000, episode_reward=-70497.58 +/- 51718.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6507304 |
|    mean velocity x | 0.396      |
|    mean velocity y | 1.69       |
|    mean velocity z | 4.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.05e+04  |
| time/              |            |
|    total_timesteps | 636000     |
-----------------------------------
Eval num_timesteps=636500, episode_reward=-62767.64 +/- 34548.94
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.537311 |
|    mean velocity x | -0.229    |
|    mean velocity y | 1.31      |
|    mean velocity z | 5.89      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.28e+04 |
| time/              |           |
|    total_timesteps | 636500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 311    |
|    time_elapsed    | 25781  |
|    total_timesteps | 636928 |
-------------------------------
Eval num_timesteps=637000, episode_reward=-100018.04 +/- 24819.95
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5055795   |
|    mean velocity x      | -0.6         |
|    mean velocity y      | 0.101        |
|    mean velocity z      | 4.25         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1e+05       |
| time/                   |              |
|    total_timesteps      | 637000       |
| train/                  |              |
|    approx_kl            | 0.0001072202 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0999       |
|    learning_rate        | 0.001        |
|    loss                 | 7.11e+07     |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.55         |
|    value_loss           | 9.32e+07     |
------------------------------------------
Eval num_timesteps=637500, episode_reward=-104734.75 +/- 23775.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5157381 |
|    mean velocity x | 0.0857     |
|    mean velocity y | 1.03       |
|    mean velocity z | 3.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.05e+05  |
| time/              |            |
|    total_timesteps | 637500     |
-----------------------------------
Eval num_timesteps=638000, episode_reward=-92812.15 +/- 45303.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09302388 |
|    mean velocity x | -0.218      |
|    mean velocity y | 0.396       |
|    mean velocity z | 0.238       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.28e+04   |
| time/              |             |
|    total_timesteps | 638000      |
------------------------------------
Eval num_timesteps=638500, episode_reward=-109799.53 +/- 47572.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5638102 |
|    mean velocity x | 0.424      |
|    mean velocity y | 1.56       |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 638500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 312    |
|    time_elapsed    | 25862  |
|    total_timesteps | 638976 |
-------------------------------
Eval num_timesteps=639000, episode_reward=-93134.13 +/- 12150.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49611717  |
|    mean velocity x      | -1.1         |
|    mean velocity y      | -0.461       |
|    mean velocity z      | 5.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.31e+04    |
| time/                   |              |
|    total_timesteps      | 639000       |
| train/                  |              |
|    approx_kl            | 6.016961e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+07     |
|    n_updates            | 3120         |
|    policy_gradient_loss | -0.000507    |
|    std                  | 1.55         |
|    value_loss           | 2.7e+07      |
------------------------------------------
Eval num_timesteps=639500, episode_reward=-53633.44 +/- 44315.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39567894 |
|    mean velocity x | -0.0697     |
|    mean velocity y | 0.936       |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.36e+04   |
| time/              |             |
|    total_timesteps | 639500      |
------------------------------------
Eval num_timesteps=640000, episode_reward=-83661.72 +/- 33611.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5125798 |
|    mean velocity x | -0.0721    |
|    mean velocity y | 1.53       |
|    mean velocity z | 4.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.37e+04  |
| time/              |            |
|    total_timesteps | 640000     |
-----------------------------------
Eval num_timesteps=640500, episode_reward=-94111.52 +/- 47432.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4719898 |
|    mean velocity x | -1.01      |
|    mean velocity y | -0.366     |
|    mean velocity z | 5.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.41e+04  |
| time/              |            |
|    total_timesteps | 640500     |
-----------------------------------
Eval num_timesteps=641000, episode_reward=-79354.07 +/- 41819.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50292605 |
|    mean velocity x | -0.249      |
|    mean velocity y | 1.51        |
|    mean velocity z | 4.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.94e+04   |
| time/              |             |
|    total_timesteps | 641000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 313    |
|    time_elapsed    | 25961  |
|    total_timesteps | 641024 |
-------------------------------
Eval num_timesteps=641500, episode_reward=-110059.89 +/- 25828.93
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.68856275   |
|    mean velocity x      | -0.392        |
|    mean velocity y      | 1.57          |
|    mean velocity z      | 4.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.1e+05      |
| time/                   |               |
|    total_timesteps      | 641500        |
| train/                  |               |
|    approx_kl            | 5.9759564e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.109         |
|    learning_rate        | 0.001         |
|    loss                 | 5.01e+07      |
|    n_updates            | 3130          |
|    policy_gradient_loss | -0.000415     |
|    std                  | 1.55          |
|    value_loss           | 9.73e+07      |
-------------------------------------------
Eval num_timesteps=642000, episode_reward=-72457.53 +/- 59207.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5366381 |
|    mean velocity x | -0.418     |
|    mean velocity y | 0.578      |
|    mean velocity z | 5.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.25e+04  |
| time/              |            |
|    total_timesteps | 642000     |
-----------------------------------
Eval num_timesteps=642500, episode_reward=-67276.46 +/- 40589.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5842388 |
|    mean velocity x | -0.506     |
|    mean velocity y | 0.67       |
|    mean velocity z | 4.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.73e+04  |
| time/              |            |
|    total_timesteps | 642500     |
-----------------------------------
Eval num_timesteps=643000, episode_reward=-66707.62 +/- 56978.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41045263 |
|    mean velocity x | -0.456      |
|    mean velocity y | 0.386       |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.67e+04   |
| time/              |             |
|    total_timesteps | 643000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 314    |
|    time_elapsed    | 26041  |
|    total_timesteps | 643072 |
-------------------------------
Eval num_timesteps=643500, episode_reward=-67408.18 +/- 35721.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47605756   |
|    mean velocity x      | -0.315        |
|    mean velocity y      | 0.615         |
|    mean velocity z      | 5.11          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.74e+04     |
| time/                   |               |
|    total_timesteps      | 643500        |
| train/                  |               |
|    approx_kl            | 5.9134298e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0749        |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+07      |
|    n_updates            | 3140          |
|    policy_gradient_loss | -0.000486     |
|    std                  | 1.55          |
|    value_loss           | 1.09e+08      |
-------------------------------------------
Eval num_timesteps=644000, episode_reward=-111336.27 +/- 26787.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49693307 |
|    mean velocity x | 0.152       |
|    mean velocity y | 1.43        |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 644000      |
------------------------------------
Eval num_timesteps=644500, episode_reward=-63945.84 +/- 52407.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62482893 |
|    mean velocity x | -0.46       |
|    mean velocity y | 1.07        |
|    mean velocity z | 5.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.39e+04   |
| time/              |             |
|    total_timesteps | 644500      |
------------------------------------
Eval num_timesteps=645000, episode_reward=-116830.45 +/- 20299.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56361437 |
|    mean velocity x | -0.443      |
|    mean velocity y | 0.95        |
|    mean velocity z | 5.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.17e+05   |
| time/              |             |
|    total_timesteps | 645000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 315    |
|    time_elapsed    | 26121  |
|    total_timesteps | 645120 |
-------------------------------
Eval num_timesteps=645500, episode_reward=-56504.14 +/- 36689.71
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5580662    |
|    mean velocity x      | -0.176        |
|    mean velocity y      | 1.45          |
|    mean velocity z      | 3.68          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.65e+04     |
| time/                   |               |
|    total_timesteps      | 645500        |
| train/                  |               |
|    approx_kl            | 0.00021592138 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0631        |
|    learning_rate        | 0.001         |
|    loss                 | 2.63e+07      |
|    n_updates            | 3150          |
|    policy_gradient_loss | -0.00104      |
|    std                  | 1.55          |
|    value_loss           | 8.76e+07      |
-------------------------------------------
Eval num_timesteps=646000, episode_reward=-92887.69 +/- 30712.37
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.524797 |
|    mean velocity x | -0.311    |
|    mean velocity y | 1.15      |
|    mean velocity z | 5.02      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.29e+04 |
| time/              |           |
|    total_timesteps | 646000    |
----------------------------------
Eval num_timesteps=646500, episode_reward=-111891.52 +/- 33393.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3388868 |
|    mean velocity x | -0.167     |
|    mean velocity y | 0.392      |
|    mean velocity z | 3.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 646500     |
-----------------------------------
Eval num_timesteps=647000, episode_reward=-51741.72 +/- 26373.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.65443486 |
|    mean velocity x | -0.622      |
|    mean velocity y | 1.06        |
|    mean velocity z | 5.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.17e+04   |
| time/              |             |
|    total_timesteps | 647000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 316    |
|    time_elapsed    | 26202  |
|    total_timesteps | 647168 |
-------------------------------
Eval num_timesteps=647500, episode_reward=-63117.50 +/- 28179.04
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4612174    |
|    mean velocity x      | -0.206        |
|    mean velocity y      | 1.16          |
|    mean velocity z      | 4.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.31e+04     |
| time/                   |               |
|    total_timesteps      | 647500        |
| train/                  |               |
|    approx_kl            | 0.00083960284 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0545        |
|    learning_rate        | 0.001         |
|    loss                 | 8.88e+07      |
|    n_updates            | 3160          |
|    policy_gradient_loss | -0.00209      |
|    std                  | 1.55          |
|    value_loss           | 1.14e+08      |
-------------------------------------------
Eval num_timesteps=648000, episode_reward=-89394.99 +/- 45332.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43441635 |
|    mean velocity x | -1.44       |
|    mean velocity y | -0.000617   |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.94e+04   |
| time/              |             |
|    total_timesteps | 648000      |
------------------------------------
Eval num_timesteps=648500, episode_reward=-84470.53 +/- 19346.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30822775 |
|    mean velocity x | -0.87       |
|    mean velocity y | -0.629      |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.45e+04   |
| time/              |             |
|    total_timesteps | 648500      |
------------------------------------
Eval num_timesteps=649000, episode_reward=-83289.47 +/- 44048.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52421284 |
|    mean velocity x | -0.453      |
|    mean velocity y | 1.14        |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.33e+04   |
| time/              |             |
|    total_timesteps | 649000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 317    |
|    time_elapsed    | 26282  |
|    total_timesteps | 649216 |
-------------------------------
Eval num_timesteps=649500, episode_reward=-112080.89 +/- 56050.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27225384  |
|    mean velocity x      | -0.246       |
|    mean velocity y      | 0.563        |
|    mean velocity z      | 0.476        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.12e+05    |
| time/                   |              |
|    total_timesteps      | 649500       |
| train/                  |              |
|    approx_kl            | 9.195367e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.001        |
|    loss                 | 7.19e+06     |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.000524    |
|    std                  | 1.55         |
|    value_loss           | 3.25e+07     |
------------------------------------------
Eval num_timesteps=650000, episode_reward=-64536.67 +/- 43385.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40289184 |
|    mean velocity x | -0.477      |
|    mean velocity y | 0.13        |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.45e+04   |
| time/              |             |
|    total_timesteps | 650000      |
------------------------------------
Eval num_timesteps=650500, episode_reward=-63755.13 +/- 29499.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47172734 |
|    mean velocity x | -0.626      |
|    mean velocity y | 0.29        |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.38e+04   |
| time/              |             |
|    total_timesteps | 650500      |
------------------------------------
Eval num_timesteps=651000, episode_reward=-75967.02 +/- 41479.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5251085 |
|    mean velocity x | 0.0872     |
|    mean velocity y | 1.6        |
|    mean velocity z | 4.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.6e+04   |
| time/              |            |
|    total_timesteps | 651000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 318    |
|    time_elapsed    | 26362  |
|    total_timesteps | 651264 |
-------------------------------
Eval num_timesteps=651500, episode_reward=-111399.10 +/- 25911.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.58593655   |
|    mean velocity x      | 0.252         |
|    mean velocity y      | 1.67          |
|    mean velocity z      | 3.97          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.11e+05     |
| time/                   |               |
|    total_timesteps      | 651500        |
| train/                  |               |
|    approx_kl            | 0.00019419324 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.001         |
|    loss                 | 3.7e+07       |
|    n_updates            | 3180          |
|    policy_gradient_loss | -0.00102      |
|    std                  | 1.55          |
|    value_loss           | 5.2e+07       |
-------------------------------------------
Eval num_timesteps=652000, episode_reward=-106268.35 +/- 53912.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5166046 |
|    mean velocity x | 0.0146     |
|    mean velocity y | 1.07       |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 652000     |
-----------------------------------
Eval num_timesteps=652500, episode_reward=-75826.51 +/- 59801.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42830604 |
|    mean velocity x | -0.0849     |
|    mean velocity y | 0.958       |
|    mean velocity z | 1.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 652500      |
------------------------------------
Eval num_timesteps=653000, episode_reward=-83401.62 +/- 42957.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52280086 |
|    mean velocity x | -0.432      |
|    mean velocity y | 0.856       |
|    mean velocity z | 5.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.34e+04   |
| time/              |             |
|    total_timesteps | 653000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 319    |
|    time_elapsed    | 26443  |
|    total_timesteps | 653312 |
-------------------------------
Eval num_timesteps=653500, episode_reward=-90228.14 +/- 53621.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.23484394   |
|    mean velocity x      | -0.148        |
|    mean velocity y      | 0.436         |
|    mean velocity z      | 0.484         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.02e+04     |
| time/                   |               |
|    total_timesteps      | 653500        |
| train/                  |               |
|    approx_kl            | 8.8739005e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0882        |
|    learning_rate        | 0.001         |
|    loss                 | 1.42e+07      |
|    n_updates            | 3190          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.55          |
|    value_loss           | 4.35e+07      |
-------------------------------------------
Eval num_timesteps=654000, episode_reward=-67921.31 +/- 50105.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5132848 |
|    mean velocity x | -0.33      |
|    mean velocity y | 0.778      |
|    mean velocity z | 4.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.79e+04  |
| time/              |            |
|    total_timesteps | 654000     |
-----------------------------------
Eval num_timesteps=654500, episode_reward=-92673.32 +/- 42662.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4533707 |
|    mean velocity x | 0.00434    |
|    mean velocity y | 0.751      |
|    mean velocity z | 4.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.27e+04  |
| time/              |            |
|    total_timesteps | 654500     |
-----------------------------------
Eval num_timesteps=655000, episode_reward=-71984.44 +/- 40213.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4538915 |
|    mean velocity x | -1.06      |
|    mean velocity y | 0.516      |
|    mean velocity z | 2.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.2e+04   |
| time/              |            |
|    total_timesteps | 655000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 320    |
|    time_elapsed    | 26523  |
|    total_timesteps | 655360 |
-------------------------------
Eval num_timesteps=655500, episode_reward=-93978.09 +/- 40999.55
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4589081    |
|    mean velocity x      | -0.0942       |
|    mean velocity y      | 1.05          |
|    mean velocity z      | 4.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.4e+04      |
| time/                   |               |
|    total_timesteps      | 655500        |
| train/                  |               |
|    approx_kl            | 0.00046484446 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0837        |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+07      |
|    n_updates            | 3200          |
|    policy_gradient_loss | -0.00136      |
|    std                  | 1.55          |
|    value_loss           | 8.42e+07      |
-------------------------------------------
Eval num_timesteps=656000, episode_reward=-114302.30 +/- 46301.06
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.567993 |
|    mean velocity x | -0.801    |
|    mean velocity y | 0.43      |
|    mean velocity z | 4.61      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.14e+05 |
| time/              |           |
|    total_timesteps | 656000    |
----------------------------------
Eval num_timesteps=656500, episode_reward=-65563.32 +/- 31789.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49377385 |
|    mean velocity x | -0.0328     |
|    mean velocity y | 1.72        |
|    mean velocity z | 4.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.56e+04   |
| time/              |             |
|    total_timesteps | 656500      |
------------------------------------
Eval num_timesteps=657000, episode_reward=-93149.95 +/- 53162.94
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.435437 |
|    mean velocity x | -0.29     |
|    mean velocity y | 0.707     |
|    mean velocity z | 4.69      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.31e+04 |
| time/              |           |
|    total_timesteps | 657000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 321    |
|    time_elapsed    | 26603  |
|    total_timesteps | 657408 |
-------------------------------
Eval num_timesteps=657500, episode_reward=-116469.77 +/- 26715.02
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47455224   |
|    mean velocity x      | -0.423        |
|    mean velocity y      | 0.751         |
|    mean velocity z      | 3.1           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.16e+05     |
| time/                   |               |
|    total_timesteps      | 657500        |
| train/                  |               |
|    approx_kl            | 0.00023181346 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0918        |
|    learning_rate        | 0.001         |
|    loss                 | 6.65e+07      |
|    n_updates            | 3210          |
|    policy_gradient_loss | -0.00138      |
|    std                  | 1.55          |
|    value_loss           | 7.83e+07      |
-------------------------------------------
Eval num_timesteps=658000, episode_reward=-60308.67 +/- 51233.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47422826 |
|    mean velocity x | -0.0454     |
|    mean velocity y | 0.845       |
|    mean velocity z | 4.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.03e+04   |
| time/              |             |
|    total_timesteps | 658000      |
------------------------------------
Eval num_timesteps=658500, episode_reward=-62751.61 +/- 54037.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5549657 |
|    mean velocity x | -0.447     |
|    mean velocity y | 0.995      |
|    mean velocity z | 4.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.28e+04  |
| time/              |            |
|    total_timesteps | 658500     |
-----------------------------------
Eval num_timesteps=659000, episode_reward=-92827.46 +/- 21891.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44092837 |
|    mean velocity x | -0.43       |
|    mean velocity y | 0.753       |
|    mean velocity z | 5.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.28e+04   |
| time/              |             |
|    total_timesteps | 659000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 322    |
|    time_elapsed    | 26684  |
|    total_timesteps | 659456 |
-------------------------------
Eval num_timesteps=659500, episode_reward=-84805.47 +/- 48227.17
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44793937  |
|    mean velocity x      | -0.699       |
|    mean velocity y      | 0.115        |
|    mean velocity z      | 4.77         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.48e+04    |
| time/                   |              |
|    total_timesteps      | 659500       |
| train/                  |              |
|    approx_kl            | 0.0001506478 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.0795       |
|    learning_rate        | 0.001        |
|    loss                 | 4.08e+07     |
|    n_updates            | 3220         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.55         |
|    value_loss           | 9.99e+07     |
------------------------------------------
Eval num_timesteps=660000, episode_reward=-95375.16 +/- 17437.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37292933 |
|    mean velocity x | -1.02       |
|    mean velocity y | 0.315       |
|    mean velocity z | 2.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.54e+04   |
| time/              |             |
|    total_timesteps | 660000      |
------------------------------------
Eval num_timesteps=660500, episode_reward=-105700.27 +/- 22735.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5544048 |
|    mean velocity x | -0.521     |
|    mean velocity y | 1.03       |
|    mean velocity z | 4.95       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 660500     |
-----------------------------------
Eval num_timesteps=661000, episode_reward=-105899.01 +/- 54228.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5544497 |
|    mean velocity x | -0.369     |
|    mean velocity y | 1.49       |
|    mean velocity z | 4.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 661000     |
-----------------------------------
Eval num_timesteps=661500, episode_reward=-80718.19 +/- 31159.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53711176 |
|    mean velocity x | -0.579      |
|    mean velocity y | 1           |
|    mean velocity z | 2.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 661500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 323    |
|    time_elapsed    | 26783  |
|    total_timesteps | 661504 |
-------------------------------
Eval num_timesteps=662000, episode_reward=-83072.32 +/- 34532.54
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4585619    |
|    mean velocity x      | -2.48         |
|    mean velocity y      | -0.952        |
|    mean velocity z      | 6.27          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.31e+04     |
| time/                   |               |
|    total_timesteps      | 662000        |
| train/                  |               |
|    approx_kl            | 6.8929716e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0872        |
|    learning_rate        | 0.001         |
|    loss                 | 2.7e+07       |
|    n_updates            | 3230          |
|    policy_gradient_loss | -0.000543     |
|    std                  | 1.55          |
|    value_loss           | 6.2e+07       |
-------------------------------------------
Eval num_timesteps=662500, episode_reward=-100558.29 +/- 19866.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42175758 |
|    mean velocity x | -0.167      |
|    mean velocity y | 1.22        |
|    mean velocity z | 2.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 662500      |
------------------------------------
Eval num_timesteps=663000, episode_reward=-84581.40 +/- 63096.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46088985 |
|    mean velocity x | -0.705      |
|    mean velocity y | 0.617       |
|    mean velocity z | 4.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.46e+04   |
| time/              |             |
|    total_timesteps | 663000      |
------------------------------------
Eval num_timesteps=663500, episode_reward=-70715.52 +/- 40043.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48030496 |
|    mean velocity x | -0.216      |
|    mean velocity y | 1.17        |
|    mean velocity z | 5.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 663500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 324    |
|    time_elapsed    | 26863  |
|    total_timesteps | 663552 |
-------------------------------
Eval num_timesteps=664000, episode_reward=-72739.50 +/- 22545.78
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45214626   |
|    mean velocity x      | -0.553        |
|    mean velocity y      | 0.0821        |
|    mean velocity z      | 4.28          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.27e+04     |
| time/                   |               |
|    total_timesteps      | 664000        |
| train/                  |               |
|    approx_kl            | 0.00020101012 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0905        |
|    learning_rate        | 0.001         |
|    loss                 | 5.67e+07      |
|    n_updates            | 3240          |
|    policy_gradient_loss | -0.00104      |
|    std                  | 1.55          |
|    value_loss           | 8.72e+07      |
-------------------------------------------
Eval num_timesteps=664500, episode_reward=-106116.33 +/- 24927.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4351714 |
|    mean velocity x | 0.284      |
|    mean velocity y | 1.62       |
|    mean velocity z | 4.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 664500     |
-----------------------------------
Eval num_timesteps=665000, episode_reward=-93831.08 +/- 53294.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4336335 |
|    mean velocity x | -0.888     |
|    mean velocity y | -0.325     |
|    mean velocity z | 4.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.38e+04  |
| time/              |            |
|    total_timesteps | 665000     |
-----------------------------------
Eval num_timesteps=665500, episode_reward=-80990.69 +/- 49365.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34076977 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.413       |
|    mean velocity z | 1.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 665500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 325    |
|    time_elapsed    | 26943  |
|    total_timesteps | 665600 |
-------------------------------
Eval num_timesteps=666000, episode_reward=-100450.81 +/- 45075.86
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44930902  |
|    mean velocity x      | -0.22        |
|    mean velocity y      | 0.465        |
|    mean velocity z      | 4.4          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1e+05       |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0002529406 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.117        |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+07     |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 1.55         |
|    value_loss           | 4.22e+07     |
------------------------------------------
Eval num_timesteps=666500, episode_reward=-88823.82 +/- 25347.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29722363 |
|    mean velocity x | -1.12       |
|    mean velocity y | -0.115      |
|    mean velocity z | 3.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.88e+04   |
| time/              |             |
|    total_timesteps | 666500      |
------------------------------------
Eval num_timesteps=667000, episode_reward=-110085.70 +/- 26835.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5004963 |
|    mean velocity x | -0.341     |
|    mean velocity y | 0.903      |
|    mean velocity z | 5.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 667000     |
-----------------------------------
Eval num_timesteps=667500, episode_reward=-100295.23 +/- 47385.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38171872 |
|    mean velocity x | -0.159      |
|    mean velocity y | 0.636       |
|    mean velocity z | 0.908       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 667500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 326    |
|    time_elapsed    | 27024  |
|    total_timesteps | 667648 |
-------------------------------
Eval num_timesteps=668000, episode_reward=-73701.57 +/- 36874.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.62480515  |
|    mean velocity x      | -1.24        |
|    mean velocity y      | 0.436        |
|    mean velocity z      | 4.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.37e+04    |
| time/                   |              |
|    total_timesteps      | 668000       |
| train/                  |              |
|    approx_kl            | 0.0003979358 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.001        |
|    loss                 | 4.95e+06     |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 1.55         |
|    value_loss           | 4.56e+07     |
------------------------------------------
Eval num_timesteps=668500, episode_reward=-86941.28 +/- 47477.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4824616 |
|    mean velocity x | -0.311     |
|    mean velocity y | 0.798      |
|    mean velocity z | 5.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 668500     |
-----------------------------------
Eval num_timesteps=669000, episode_reward=-88041.98 +/- 33463.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5530751 |
|    mean velocity x | -0.565     |
|    mean velocity y | 1.23       |
|    mean velocity z | 4.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.8e+04   |
| time/              |            |
|    total_timesteps | 669000     |
-----------------------------------
Eval num_timesteps=669500, episode_reward=-68520.21 +/- 48726.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53108263 |
|    mean velocity x | -1.03       |
|    mean velocity y | 0.272       |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 669500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 327    |
|    time_elapsed    | 27104  |
|    total_timesteps | 669696 |
-------------------------------
Eval num_timesteps=670000, episode_reward=-101590.15 +/- 48715.32
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37786695  |
|    mean velocity x      | -1.22        |
|    mean velocity y      | -0.82        |
|    mean velocity z      | 4.66         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.02e+05    |
| time/                   |              |
|    total_timesteps      | 670000       |
| train/                  |              |
|    approx_kl            | 0.0002183237 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.001        |
|    loss                 | 9.33e+06     |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.000738    |
|    std                  | 1.55         |
|    value_loss           | 7.01e+07     |
------------------------------------------
Eval num_timesteps=670500, episode_reward=-106115.21 +/- 51787.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4148253 |
|    mean velocity x | -0.377     |
|    mean velocity y | 0.908      |
|    mean velocity z | 4.96       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 670500     |
-----------------------------------
Eval num_timesteps=671000, episode_reward=-87744.91 +/- 10368.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23280536 |
|    mean velocity x | -0.114      |
|    mean velocity y | 0.359       |
|    mean velocity z | 0.654       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 671000      |
------------------------------------
Eval num_timesteps=671500, episode_reward=-73153.61 +/- 33916.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50221914 |
|    mean velocity x | -0.559      |
|    mean velocity y | 0.87        |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.32e+04   |
| time/              |             |
|    total_timesteps | 671500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 328    |
|    time_elapsed    | 27184  |
|    total_timesteps | 671744 |
-------------------------------
Eval num_timesteps=672000, episode_reward=-102580.00 +/- 48352.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29813927  |
|    mean velocity x      | -0.435       |
|    mean velocity y      | 0.432        |
|    mean velocity z      | 2.58         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.03e+05    |
| time/                   |              |
|    total_timesteps      | 672000       |
| train/                  |              |
|    approx_kl            | 8.233852e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0971       |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+07     |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.000828    |
|    std                  | 1.55         |
|    value_loss           | 5.25e+07     |
------------------------------------------
Eval num_timesteps=672500, episode_reward=-99863.05 +/- 38006.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47202545 |
|    mean velocity x | -0.0175     |
|    mean velocity y | 1.07        |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.99e+04   |
| time/              |             |
|    total_timesteps | 672500      |
------------------------------------
Eval num_timesteps=673000, episode_reward=-93243.46 +/- 48896.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38852984 |
|    mean velocity x | -0.42       |
|    mean velocity y | 0.634       |
|    mean velocity z | 2.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.32e+04   |
| time/              |             |
|    total_timesteps | 673000      |
------------------------------------
Eval num_timesteps=673500, episode_reward=-88767.59 +/- 35282.90
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.531983 |
|    mean velocity x | 0.109     |
|    mean velocity y | 0.98      |
|    mean velocity z | 2.92      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.88e+04 |
| time/              |           |
|    total_timesteps | 673500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 329    |
|    time_elapsed    | 27265  |
|    total_timesteps | 673792 |
-------------------------------
Eval num_timesteps=674000, episode_reward=-73011.42 +/- 42621.12
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3550988   |
|    mean velocity x      | -0.981       |
|    mean velocity y      | -0.492       |
|    mean velocity z      | 3.73         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.3e+04     |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0015022623 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+07     |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00307     |
|    std                  | 1.55         |
|    value_loss           | 2.95e+07     |
------------------------------------------
Eval num_timesteps=674500, episode_reward=-62114.81 +/- 57067.59
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.52062  |
|    mean velocity x | 0.593     |
|    mean velocity y | 1.54      |
|    mean velocity z | 3.55      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.21e+04 |
| time/              |           |
|    total_timesteps | 674500    |
----------------------------------
Eval num_timesteps=675000, episode_reward=-85984.34 +/- 51035.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4940546 |
|    mean velocity x | -0.327     |
|    mean velocity y | 0.802      |
|    mean velocity z | 5.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.6e+04   |
| time/              |            |
|    total_timesteps | 675000     |
-----------------------------------
Eval num_timesteps=675500, episode_reward=-95527.52 +/- 50509.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41286084 |
|    mean velocity x | -0.789      |
|    mean velocity y | -0.234      |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.55e+04   |
| time/              |             |
|    total_timesteps | 675500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 330    |
|    time_elapsed    | 27345  |
|    total_timesteps | 675840 |
-------------------------------
Eval num_timesteps=676000, episode_reward=-61647.63 +/- 55654.51
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.39225394 |
|    mean velocity x      | -0.122      |
|    mean velocity y      | 0.576       |
|    mean velocity z      | 3.86        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.16e+04   |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.000452425 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.001       |
|    loss                 | 5.21e+06    |
|    n_updates            | 3300        |
|    policy_gradient_loss | -0.00223    |
|    std                  | 1.55        |
|    value_loss           | 6.82e+07    |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=-103201.76 +/- 30566.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5405319 |
|    mean velocity x | -0.575     |
|    mean velocity y | 0.808      |
|    mean velocity z | 4.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 676500     |
-----------------------------------
Eval num_timesteps=677000, episode_reward=-96773.27 +/- 22297.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54904115 |
|    mean velocity x | -0.422      |
|    mean velocity y | 1.06        |
|    mean velocity z | 5.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.68e+04   |
| time/              |             |
|    total_timesteps | 677000      |
------------------------------------
Eval num_timesteps=677500, episode_reward=-103492.86 +/- 39174.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45029333 |
|    mean velocity x | -0.298      |
|    mean velocity y | 0.648       |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 677500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 331    |
|    time_elapsed    | 27425  |
|    total_timesteps | 677888 |
-------------------------------
Eval num_timesteps=678000, episode_reward=-93480.20 +/- 29863.14
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43487117  |
|    mean velocity x      | -0.25        |
|    mean velocity y      | 0.733        |
|    mean velocity z      | 4.06         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.35e+04    |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0002688491 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.0863       |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+07     |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.55         |
|    value_loss           | 8.69e+07     |
------------------------------------------
Eval num_timesteps=678500, episode_reward=-75148.80 +/- 30459.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5585592 |
|    mean velocity x | -0.352     |
|    mean velocity y | 1.02       |
|    mean velocity z | 4.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.51e+04  |
| time/              |            |
|    total_timesteps | 678500     |
-----------------------------------
Eval num_timesteps=679000, episode_reward=-107705.13 +/- 17650.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46065074 |
|    mean velocity x | -0.218      |
|    mean velocity y | 0.897       |
|    mean velocity z | 3.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 679000      |
------------------------------------
Eval num_timesteps=679500, episode_reward=-91384.46 +/- 37323.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56583923 |
|    mean velocity x | -0.459      |
|    mean velocity y | 0.71        |
|    mean velocity z | 5.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.14e+04   |
| time/              |             |
|    total_timesteps | 679500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 332    |
|    time_elapsed    | 27506  |
|    total_timesteps | 679936 |
-------------------------------
Eval num_timesteps=680000, episode_reward=-94771.71 +/- 38261.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5242604    |
|    mean velocity x      | -0.303        |
|    mean velocity y      | 0.614         |
|    mean velocity z      | 3.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.48e+04     |
| time/                   |               |
|    total_timesteps      | 680000        |
| train/                  |               |
|    approx_kl            | 0.00037980266 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.088         |
|    learning_rate        | 0.001         |
|    loss                 | 4.91e+07      |
|    n_updates            | 3320          |
|    policy_gradient_loss | -0.00187      |
|    std                  | 1.55          |
|    value_loss           | 7.39e+07      |
-------------------------------------------
Eval num_timesteps=680500, episode_reward=-90853.47 +/- 50472.11
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5277704 |
|    mean velocity x | -0.59      |
|    mean velocity y | 0.721      |
|    mean velocity z | 5.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.09e+04  |
| time/              |            |
|    total_timesteps | 680500     |
-----------------------------------
Eval num_timesteps=681000, episode_reward=-113528.16 +/- 17638.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38344917 |
|    mean velocity x | -0.183      |
|    mean velocity y | 0.489       |
|    mean velocity z | 1.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 681000      |
------------------------------------
Eval num_timesteps=681500, episode_reward=-124839.45 +/- 20551.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5299554 |
|    mean velocity x | -0.0479    |
|    mean velocity y | 1.45       |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.25e+05  |
| time/              |            |
|    total_timesteps | 681500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 333    |
|    time_elapsed    | 27586  |
|    total_timesteps | 681984 |
-------------------------------
Eval num_timesteps=682000, episode_reward=-94909.64 +/- 10610.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3162749    |
|    mean velocity x      | -0.221        |
|    mean velocity y      | 0.425         |
|    mean velocity z      | 0.641         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.49e+04     |
| time/                   |               |
|    total_timesteps      | 682000        |
| train/                  |               |
|    approx_kl            | 0.00018114626 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0948        |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+07      |
|    n_updates            | 3330          |
|    policy_gradient_loss | -0.0008       |
|    std                  | 1.55          |
|    value_loss           | 3.93e+07      |
-------------------------------------------
Eval num_timesteps=682500, episode_reward=-57259.13 +/- 35902.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4551789 |
|    mean velocity x | -0.135     |
|    mean velocity y | 1.33       |
|    mean velocity z | 3.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.73e+04  |
| time/              |            |
|    total_timesteps | 682500     |
-----------------------------------
Eval num_timesteps=683000, episode_reward=-67519.70 +/- 53937.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6058417 |
|    mean velocity x | -0.437     |
|    mean velocity y | 0.954      |
|    mean velocity z | 5.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.75e+04  |
| time/              |            |
|    total_timesteps | 683000     |
-----------------------------------
Eval num_timesteps=683500, episode_reward=-99440.62 +/- 45915.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45241302 |
|    mean velocity x | -0.529      |
|    mean velocity y | 0.278       |
|    mean velocity z | 4.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.94e+04   |
| time/              |             |
|    total_timesteps | 683500      |
------------------------------------
Eval num_timesteps=684000, episode_reward=-67590.58 +/- 33970.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41178897 |
|    mean velocity x | -0.567      |
|    mean velocity y | 0.0734      |
|    mean velocity z | 3.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 684000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 334    |
|    time_elapsed    | 27685  |
|    total_timesteps | 684032 |
-------------------------------
Eval num_timesteps=684500, episode_reward=-80645.71 +/- 46481.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36779565  |
|    mean velocity x      | -0.113       |
|    mean velocity y      | 0.801        |
|    mean velocity z      | 4.95         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.06e+04    |
| time/                   |              |
|    total_timesteps      | 684500       |
| train/                  |              |
|    approx_kl            | 8.113656e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0758       |
|    learning_rate        | 0.001        |
|    loss                 | 6.45e+07     |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 1.55         |
|    value_loss           | 1.17e+08     |
------------------------------------------
Eval num_timesteps=685000, episode_reward=-92235.00 +/- 50710.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5146433 |
|    mean velocity x | -0.268     |
|    mean velocity y | 1.08       |
|    mean velocity z | 4.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.22e+04  |
| time/              |            |
|    total_timesteps | 685000     |
-----------------------------------
Eval num_timesteps=685500, episode_reward=-132102.64 +/- 14351.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4956857 |
|    mean velocity x | -0.986     |
|    mean velocity y | 0.256      |
|    mean velocity z | 3.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.32e+05  |
| time/              |            |
|    total_timesteps | 685500     |
-----------------------------------
Eval num_timesteps=686000, episode_reward=-78059.61 +/- 40280.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62874913 |
|    mean velocity x | -0.644      |
|    mean velocity y | 0.794       |
|    mean velocity z | 5.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.81e+04   |
| time/              |             |
|    total_timesteps | 686000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 335    |
|    time_elapsed    | 27766  |
|    total_timesteps | 686080 |
-------------------------------
Eval num_timesteps=686500, episode_reward=-105651.58 +/- 30856.46
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47172272  |
|    mean velocity x      | -0.394       |
|    mean velocity y      | 0.882        |
|    mean velocity z      | 5.02         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.06e+05    |
| time/                   |              |
|    total_timesteps      | 686500       |
| train/                  |              |
|    approx_kl            | 0.0001095644 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0668       |
|    learning_rate        | 0.001        |
|    loss                 | 5.19e+07     |
|    n_updates            | 3350         |
|    policy_gradient_loss | -0.000953    |
|    std                  | 1.55         |
|    value_loss           | 1.19e+08     |
------------------------------------------
Eval num_timesteps=687000, episode_reward=-87369.93 +/- 38874.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5033686 |
|    mean velocity x | -0.374     |
|    mean velocity y | 0.882      |
|    mean velocity z | 4.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.74e+04  |
| time/              |            |
|    total_timesteps | 687000     |
-----------------------------------
Eval num_timesteps=687500, episode_reward=-137345.71 +/- 12604.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42210123 |
|    mean velocity x | -0.501      |
|    mean velocity y | 0.422       |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.37e+05   |
| time/              |             |
|    total_timesteps | 687500      |
------------------------------------
Eval num_timesteps=688000, episode_reward=-93612.05 +/- 55438.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41880807 |
|    mean velocity x | -0.279      |
|    mean velocity y | 0.62        |
|    mean velocity z | 1.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 688000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 336    |
|    time_elapsed    | 27846  |
|    total_timesteps | 688128 |
-------------------------------
Eval num_timesteps=688500, episode_reward=-126085.12 +/- 31242.06
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34500635   |
|    mean velocity x      | -0.91         |
|    mean velocity y      | -0.738        |
|    mean velocity z      | 5.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.26e+05     |
| time/                   |               |
|    total_timesteps      | 688500        |
| train/                  |               |
|    approx_kl            | 0.00021119652 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0796        |
|    learning_rate        | 0.001         |
|    loss                 | 2.79e+07      |
|    n_updates            | 3360          |
|    policy_gradient_loss | -0.00126      |
|    std                  | 1.54          |
|    value_loss           | 6.85e+07      |
-------------------------------------------
Eval num_timesteps=689000, episode_reward=-92467.48 +/- 24377.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5029131 |
|    mean velocity x | -0.208     |
|    mean velocity y | 1.33       |
|    mean velocity z | 5.06       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.25e+04  |
| time/              |            |
|    total_timesteps | 689000     |
-----------------------------------
Eval num_timesteps=689500, episode_reward=-97640.33 +/- 16626.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4016377 |
|    mean velocity x | -0.442     |
|    mean velocity y | 0.668      |
|    mean velocity z | 0.834      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.76e+04  |
| time/              |            |
|    total_timesteps | 689500     |
-----------------------------------
Eval num_timesteps=690000, episode_reward=-64049.89 +/- 47787.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23523086 |
|    mean velocity x | -0.444      |
|    mean velocity y | 0.281       |
|    mean velocity z | 1.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.4e+04    |
| time/              |             |
|    total_timesteps | 690000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 337    |
|    time_elapsed    | 27926  |
|    total_timesteps | 690176 |
-------------------------------
Eval num_timesteps=690500, episode_reward=-106655.15 +/- 36775.67
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44388872   |
|    mean velocity x      | -0.814        |
|    mean velocity y      | -0.125        |
|    mean velocity z      | 3.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 690500        |
| train/                  |               |
|    approx_kl            | 0.00032616462 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0799        |
|    learning_rate        | 0.001         |
|    loss                 | 1.94e+07      |
|    n_updates            | 3370          |
|    policy_gradient_loss | -0.00107      |
|    std                  | 1.55          |
|    value_loss           | 5.14e+07      |
-------------------------------------------
Eval num_timesteps=691000, episode_reward=-107583.04 +/- 25551.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.64681786 |
|    mean velocity x | -1.55       |
|    mean velocity y | 0.654       |
|    mean velocity z | 5.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 691000      |
------------------------------------
Eval num_timesteps=691500, episode_reward=-95638.51 +/- 19496.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5087578 |
|    mean velocity x | -0.553     |
|    mean velocity y | 0.803      |
|    mean velocity z | 5.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.56e+04  |
| time/              |            |
|    total_timesteps | 691500     |
-----------------------------------
Eval num_timesteps=692000, episode_reward=-106986.90 +/- 23769.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49077636 |
|    mean velocity x | -0.187      |
|    mean velocity y | 1.06        |
|    mean velocity z | 4.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 692000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 338    |
|    time_elapsed    | 28011  |
|    total_timesteps | 692224 |
-------------------------------
Eval num_timesteps=692500, episode_reward=-121974.72 +/- 19592.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4967235    |
|    mean velocity x      | -0.183        |
|    mean velocity y      | 1.3           |
|    mean velocity z      | 4.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.22e+05     |
| time/                   |               |
|    total_timesteps      | 692500        |
| train/                  |               |
|    approx_kl            | 6.0153485e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0908        |
|    learning_rate        | 0.001         |
|    loss                 | 8.83e+07      |
|    n_updates            | 3380          |
|    policy_gradient_loss | -0.000731     |
|    std                  | 1.55          |
|    value_loss           | 1e+08         |
-------------------------------------------
Eval num_timesteps=693000, episode_reward=-58570.09 +/- 47787.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48343047 |
|    mean velocity x | -0.527      |
|    mean velocity y | 0.445       |
|    mean velocity z | 4.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.86e+04   |
| time/              |             |
|    total_timesteps | 693000      |
------------------------------------
Eval num_timesteps=693500, episode_reward=-94424.82 +/- 41208.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5011222 |
|    mean velocity x | -0.899     |
|    mean velocity y | 0.114      |
|    mean velocity z | 4.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.44e+04  |
| time/              |            |
|    total_timesteps | 693500     |
-----------------------------------
Eval num_timesteps=694000, episode_reward=-108927.86 +/- 24775.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44062784 |
|    mean velocity x | -0.245      |
|    mean velocity y | 0.868       |
|    mean velocity z | 4.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 694000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 339    |
|    time_elapsed    | 28092  |
|    total_timesteps | 694272 |
-------------------------------
Eval num_timesteps=694500, episode_reward=-54557.76 +/- 40928.09
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37315845   |
|    mean velocity x      | -0.442        |
|    mean velocity y      | 0.927         |
|    mean velocity z      | 0.772         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.46e+04     |
| time/                   |               |
|    total_timesteps      | 694500        |
| train/                  |               |
|    approx_kl            | 0.00010510048 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0944        |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+07      |
|    n_updates            | 3390          |
|    policy_gradient_loss | -0.000745     |
|    std                  | 1.55          |
|    value_loss           | 6.46e+07      |
-------------------------------------------
Eval num_timesteps=695000, episode_reward=-91593.94 +/- 60507.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.519427 |
|    mean velocity x | -1.35     |
|    mean velocity y | 0.177     |
|    mean velocity z | 5.31      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.16e+04 |
| time/              |           |
|    total_timesteps | 695000    |
----------------------------------
Eval num_timesteps=695500, episode_reward=-84097.74 +/- 50762.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45472765 |
|    mean velocity x | 0.0906      |
|    mean velocity y | 1.43        |
|    mean velocity z | 4.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.41e+04   |
| time/              |             |
|    total_timesteps | 695500      |
------------------------------------
Eval num_timesteps=696000, episode_reward=-98031.01 +/- 14577.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5264326 |
|    mean velocity x | -0.56      |
|    mean velocity y | 0.232      |
|    mean velocity z | 4.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.8e+04   |
| time/              |            |
|    total_timesteps | 696000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 340    |
|    time_elapsed    | 28172  |
|    total_timesteps | 696320 |
-------------------------------
Eval num_timesteps=696500, episode_reward=-76265.54 +/- 52544.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.62127364   |
|    mean velocity x      | -0.328        |
|    mean velocity y      | 1.03          |
|    mean velocity z      | 4.57          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.63e+04     |
| time/                   |               |
|    total_timesteps      | 696500        |
| train/                  |               |
|    approx_kl            | 0.00013091476 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.001         |
|    loss                 | 4.35e+07      |
|    n_updates            | 3400          |
|    policy_gradient_loss | -0.00105      |
|    std                  | 1.55          |
|    value_loss           | 7.39e+07      |
-------------------------------------------
Eval num_timesteps=697000, episode_reward=-97226.06 +/- 40943.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4202692 |
|    mean velocity x | -0.239     |
|    mean velocity y | 0.671      |
|    mean velocity z | 2.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.72e+04  |
| time/              |            |
|    total_timesteps | 697000     |
-----------------------------------
Eval num_timesteps=697500, episode_reward=-103325.40 +/- 41741.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6430159 |
|    mean velocity x | -1.29      |
|    mean velocity y | 0.456      |
|    mean velocity z | 4.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 697500     |
-----------------------------------
Eval num_timesteps=698000, episode_reward=-92746.60 +/- 52335.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50440514 |
|    mean velocity x | -0.512      |
|    mean velocity y | 0.523       |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 698000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 341    |
|    time_elapsed    | 28252  |
|    total_timesteps | 698368 |
-------------------------------
Eval num_timesteps=698500, episode_reward=-98417.64 +/- 35210.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5433068    |
|    mean velocity x      | -0.346        |
|    mean velocity y      | 0.865         |
|    mean velocity z      | 4.36          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.84e+04     |
| time/                   |               |
|    total_timesteps      | 698500        |
| train/                  |               |
|    approx_kl            | 0.00018374005 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.001         |
|    loss                 | 4.86e+07      |
|    n_updates            | 3410          |
|    policy_gradient_loss | -0.000795     |
|    std                  | 1.55          |
|    value_loss           | 6.05e+07      |
-------------------------------------------
Eval num_timesteps=699000, episode_reward=-106424.25 +/- 34606.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50876474 |
|    mean velocity x | -0.419      |
|    mean velocity y | 0.909       |
|    mean velocity z | 5           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 699000      |
------------------------------------
Eval num_timesteps=699500, episode_reward=-106522.40 +/- 36303.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.64554656 |
|    mean velocity x | -0.913      |
|    mean velocity y | 0.749       |
|    mean velocity z | 5.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 699500      |
------------------------------------
Eval num_timesteps=700000, episode_reward=-73636.69 +/- 56953.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59781873 |
|    mean velocity x | -0.446      |
|    mean velocity y | 1.09        |
|    mean velocity z | 4.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 700000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 342    |
|    time_elapsed    | 28333  |
|    total_timesteps | 700416 |
-------------------------------
Eval num_timesteps=700500, episode_reward=-83867.25 +/- 52371.91
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.23733413  |
|    mean velocity x      | -0.304       |
|    mean velocity y      | 0.582        |
|    mean velocity z      | 0.457        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.39e+04    |
| time/                   |              |
|    total_timesteps      | 700500       |
| train/                  |              |
|    approx_kl            | 5.196582e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.001        |
|    loss                 | 5.23e+07     |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.000335    |
|    std                  | 1.55         |
|    value_loss           | 6.93e+07     |
------------------------------------------
Eval num_timesteps=701000, episode_reward=-56043.99 +/- 31861.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52500397 |
|    mean velocity x | -0.321      |
|    mean velocity y | 0.887       |
|    mean velocity z | 4.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.6e+04    |
| time/              |             |
|    total_timesteps | 701000      |
------------------------------------
Eval num_timesteps=701500, episode_reward=-84714.95 +/- 37085.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6300635 |
|    mean velocity x | 0.109      |
|    mean velocity y | 1.85       |
|    mean velocity z | 3.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.47e+04  |
| time/              |            |
|    total_timesteps | 701500     |
-----------------------------------
Eval num_timesteps=702000, episode_reward=-98931.96 +/- 35741.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4923315 |
|    mean velocity x | -0.197     |
|    mean velocity y | 1.25       |
|    mean velocity z | 4.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.89e+04  |
| time/              |            |
|    total_timesteps | 702000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 343    |
|    time_elapsed    | 28413  |
|    total_timesteps | 702464 |
-------------------------------
Eval num_timesteps=702500, episode_reward=-83022.14 +/- 42385.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5004972    |
|    mean velocity x      | -0.229        |
|    mean velocity y      | 0.798         |
|    mean velocity z      | 5.07          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.3e+04      |
| time/                   |               |
|    total_timesteps      | 702500        |
| train/                  |               |
|    approx_kl            | 3.2901356e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.0795        |
|    learning_rate        | 0.001         |
|    loss                 | 5.34e+07      |
|    n_updates            | 3430          |
|    policy_gradient_loss | -0.000552     |
|    std                  | 1.54          |
|    value_loss           | 9.79e+07      |
-------------------------------------------
Eval num_timesteps=703000, episode_reward=-137447.45 +/- 13363.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5122366 |
|    mean velocity x | -0.683     |
|    mean velocity y | 1.03       |
|    mean velocity z | 2.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.37e+05  |
| time/              |            |
|    total_timesteps | 703000     |
-----------------------------------
Eval num_timesteps=703500, episode_reward=-100586.36 +/- 40000.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42613068 |
|    mean velocity x | -0.281      |
|    mean velocity y | 0.789       |
|    mean velocity z | 1.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 703500      |
------------------------------------
Eval num_timesteps=704000, episode_reward=-99347.55 +/- 29492.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5958346 |
|    mean velocity x | -0.62      |
|    mean velocity y | 1.08       |
|    mean velocity z | 4.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.93e+04  |
| time/              |            |
|    total_timesteps | 704000     |
-----------------------------------
Eval num_timesteps=704500, episode_reward=-91658.71 +/- 37376.05
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.58994  |
|    mean velocity x | -0.413    |
|    mean velocity y | 1.34      |
|    mean velocity z | 5.12      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.17e+04 |
| time/              |           |
|    total_timesteps | 704500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 344    |
|    time_elapsed    | 28513  |
|    total_timesteps | 704512 |
-------------------------------
Eval num_timesteps=705000, episode_reward=-79154.85 +/- 59961.32
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3289717   |
|    mean velocity x      | -0.738       |
|    mean velocity y      | -0.18        |
|    mean velocity z      | 3.35         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.92e+04    |
| time/                   |              |
|    total_timesteps      | 705000       |
| train/                  |              |
|    approx_kl            | 6.319949e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.0878       |
|    learning_rate        | 0.001        |
|    loss                 | 2.37e+07     |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.000575    |
|    std                  | 1.55         |
|    value_loss           | 6.27e+07     |
------------------------------------------
Eval num_timesteps=705500, episode_reward=-115041.96 +/- 57126.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58532035 |
|    mean velocity x | -0.0214     |
|    mean velocity y | 1.46        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 705500      |
------------------------------------
Eval num_timesteps=706000, episode_reward=-93951.15 +/- 36096.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5501022 |
|    mean velocity x | -0.959     |
|    mean velocity y | 0.299      |
|    mean velocity z | 4.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.4e+04   |
| time/              |            |
|    total_timesteps | 706000     |
-----------------------------------
Eval num_timesteps=706500, episode_reward=-104523.37 +/- 24826.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33547357 |
|    mean velocity x | -0.353      |
|    mean velocity y | 0.603       |
|    mean velocity z | 1.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 706500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 345    |
|    time_elapsed    | 28593  |
|    total_timesteps | 706560 |
-------------------------------
Eval num_timesteps=707000, episode_reward=-91276.59 +/- 24199.91
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5458736    |
|    mean velocity x      | -0.597        |
|    mean velocity y      | 0.61          |
|    mean velocity z      | 4.69          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.13e+04     |
| time/                   |               |
|    total_timesteps      | 707000        |
| train/                  |               |
|    approx_kl            | 0.00018642095 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.001         |
|    loss                 | 2.37e+06      |
|    n_updates            | 3450          |
|    policy_gradient_loss | -0.000904     |
|    std                  | 1.55          |
|    value_loss           | 5.17e+07      |
-------------------------------------------
Eval num_timesteps=707500, episode_reward=-76536.69 +/- 50551.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5834227 |
|    mean velocity x | -2.27      |
|    mean velocity y | -0.365     |
|    mean velocity z | 6.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.65e+04  |
| time/              |            |
|    total_timesteps | 707500     |
-----------------------------------
Eval num_timesteps=708000, episode_reward=-106055.11 +/- 45017.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35019118 |
|    mean velocity x | -0.00989    |
|    mean velocity y | 0.675       |
|    mean velocity z | 1.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 708000      |
------------------------------------
Eval num_timesteps=708500, episode_reward=-111114.64 +/- 47089.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28876063 |
|    mean velocity x | -0.401      |
|    mean velocity y | 0.596       |
|    mean velocity z | 0.748       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 708500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 346    |
|    time_elapsed    | 28673  |
|    total_timesteps | 708608 |
-------------------------------
Eval num_timesteps=709000, episode_reward=-128363.67 +/- 26610.72
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.521146     |
|    mean velocity x      | -0.302        |
|    mean velocity y      | 0.769         |
|    mean velocity z      | 4.28          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.28e+05     |
| time/                   |               |
|    total_timesteps      | 709000        |
| train/                  |               |
|    approx_kl            | 0.00046819687 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.154         |
|    learning_rate        | 0.001         |
|    loss                 | 6.62e+06      |
|    n_updates            | 3460          |
|    policy_gradient_loss | -0.00224      |
|    std                  | 1.55          |
|    value_loss           | 3.44e+07      |
-------------------------------------------
Eval num_timesteps=709500, episode_reward=-94612.44 +/- 42378.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43699518 |
|    mean velocity x | -0.454      |
|    mean velocity y | 0.854       |
|    mean velocity z | 4.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.46e+04   |
| time/              |             |
|    total_timesteps | 709500      |
------------------------------------
Eval num_timesteps=710000, episode_reward=-90441.89 +/- 39427.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3967686 |
|    mean velocity x | -1.72      |
|    mean velocity y | -0.86      |
|    mean velocity z | 5.11       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.04e+04  |
| time/              |            |
|    total_timesteps | 710000     |
-----------------------------------
Eval num_timesteps=710500, episode_reward=-87308.69 +/- 33382.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5503356 |
|    mean velocity x | -0.157     |
|    mean velocity y | 0.799      |
|    mean velocity z | 3.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.73e+04  |
| time/              |            |
|    total_timesteps | 710500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 347    |
|    time_elapsed    | 28754  |
|    total_timesteps | 710656 |
-------------------------------
Eval num_timesteps=711000, episode_reward=-96739.39 +/- 35334.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.55320704   |
|    mean velocity x      | 0.142         |
|    mean velocity y      | 1.67          |
|    mean velocity z      | 4.27          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.67e+04     |
| time/                   |               |
|    total_timesteps      | 711000        |
| train/                  |               |
|    approx_kl            | 0.00048183824 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 3.05e+07      |
|    n_updates            | 3470          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 1.55          |
|    value_loss           | 4.74e+07      |
-------------------------------------------
Eval num_timesteps=711500, episode_reward=-90863.71 +/- 40213.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44626307 |
|    mean velocity x | 0.047       |
|    mean velocity y | 0.981       |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 711500      |
------------------------------------
Eval num_timesteps=712000, episode_reward=-73329.98 +/- 36586.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2971815 |
|    mean velocity x | -0.0187    |
|    mean velocity y | 0.368      |
|    mean velocity z | 0.839      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.33e+04  |
| time/              |            |
|    total_timesteps | 712000     |
-----------------------------------
Eval num_timesteps=712500, episode_reward=-129054.76 +/- 10978.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46412945 |
|    mean velocity x | -0.161      |
|    mean velocity y | 1.53        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.29e+05   |
| time/              |             |
|    total_timesteps | 712500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 348    |
|    time_elapsed    | 28834  |
|    total_timesteps | 712704 |
-------------------------------
Eval num_timesteps=713000, episode_reward=-128509.80 +/- 16784.16
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.52519846   |
|    mean velocity x      | -0.393        |
|    mean velocity y      | 0.646         |
|    mean velocity z      | 4.99          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.29e+05     |
| time/                   |               |
|    total_timesteps      | 713000        |
| train/                  |               |
|    approx_kl            | 0.00018586445 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 2.29e+07      |
|    n_updates            | 3480          |
|    policy_gradient_loss | -0.00085      |
|    std                  | 1.55          |
|    value_loss           | 7.6e+07       |
-------------------------------------------
Eval num_timesteps=713500, episode_reward=-114587.75 +/- 20297.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46924964 |
|    mean velocity x | -0.0933     |
|    mean velocity y | 0.867       |
|    mean velocity z | 4.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 713500      |
------------------------------------
Eval num_timesteps=714000, episode_reward=-51867.78 +/- 60163.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5209805 |
|    mean velocity x | -0.921     |
|    mean velocity y | 0.538      |
|    mean velocity z | 4.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.19e+04  |
| time/              |            |
|    total_timesteps | 714000     |
-----------------------------------
Eval num_timesteps=714500, episode_reward=-72965.97 +/- 50338.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3048493 |
|    mean velocity x | -0.317     |
|    mean velocity y | 0.517      |
|    mean velocity z | 0.996      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.3e+04   |
| time/              |            |
|    total_timesteps | 714500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 349    |
|    time_elapsed    | 28914  |
|    total_timesteps | 714752 |
-------------------------------
Eval num_timesteps=715000, episode_reward=-103525.72 +/- 50074.61
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5430348   |
|    mean velocity x      | -0.412       |
|    mean velocity y      | 0.984        |
|    mean velocity z      | 5.37         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.04e+05    |
| time/                   |              |
|    total_timesteps      | 715000       |
| train/                  |              |
|    approx_kl            | 0.0001925272 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.117        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+07     |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.55         |
|    value_loss           | 7.1e+07      |
------------------------------------------
Eval num_timesteps=715500, episode_reward=-80681.14 +/- 32854.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5239602 |
|    mean velocity x | -0.34      |
|    mean velocity y | 0.972      |
|    mean velocity z | 5.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 715500     |
-----------------------------------
Eval num_timesteps=716000, episode_reward=-110092.77 +/- 56465.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4117007 |
|    mean velocity x | -0.621     |
|    mean velocity y | 0.086      |
|    mean velocity z | 3.98       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 716000     |
-----------------------------------
Eval num_timesteps=716500, episode_reward=-68125.26 +/- 42174.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3519949 |
|    mean velocity x | -0.325     |
|    mean velocity y | 0.336      |
|    mean velocity z | 3.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.81e+04  |
| time/              |            |
|    total_timesteps | 716500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 350    |
|    time_elapsed    | 28994  |
|    total_timesteps | 716800 |
-------------------------------
Eval num_timesteps=717000, episode_reward=-67304.66 +/- 35346.83
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.49924785    |
|    mean velocity x      | 0.108          |
|    mean velocity y      | 0.775          |
|    mean velocity z      | 2.39           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -6.73e+04      |
| time/                   |                |
|    total_timesteps      | 717000         |
| train/                  |                |
|    approx_kl            | 0.000108269334 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.57          |
|    explained_variance   | 0.121          |
|    learning_rate        | 0.001          |
|    loss                 | 3.56e+07       |
|    n_updates            | 3500           |
|    policy_gradient_loss | -0.000814      |
|    std                  | 1.55           |
|    value_loss           | 6.92e+07       |
--------------------------------------------
Eval num_timesteps=717500, episode_reward=-97091.40 +/- 33299.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5863015 |
|    mean velocity x | 0.451      |
|    mean velocity y | 1.88       |
|    mean velocity z | 3.85       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.71e+04  |
| time/              |            |
|    total_timesteps | 717500     |
-----------------------------------
Eval num_timesteps=718000, episode_reward=-113154.90 +/- 34422.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40660495 |
|    mean velocity x | 0.383       |
|    mean velocity y | 1.54        |
|    mean velocity z | 3.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 718000      |
------------------------------------
Eval num_timesteps=718500, episode_reward=-61851.80 +/- 37997.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24819428 |
|    mean velocity x | -0.275      |
|    mean velocity y | 0.493       |
|    mean velocity z | 0.431       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+04   |
| time/              |             |
|    total_timesteps | 718500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 351    |
|    time_elapsed    | 29075  |
|    total_timesteps | 718848 |
-------------------------------
Eval num_timesteps=719000, episode_reward=-98240.26 +/- 35167.57
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42180675   |
|    mean velocity x      | -0.804        |
|    mean velocity y      | 0.305         |
|    mean velocity z      | 3.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.82e+04     |
| time/                   |               |
|    total_timesteps      | 719000        |
| train/                  |               |
|    approx_kl            | 0.00026285174 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.193         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+07      |
|    n_updates            | 3510          |
|    policy_gradient_loss | -0.00139      |
|    std                  | 1.55          |
|    value_loss           | 2.87e+07      |
-------------------------------------------
Eval num_timesteps=719500, episode_reward=-87560.95 +/- 35203.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5679153 |
|    mean velocity x | -0.945     |
|    mean velocity y | 0.633      |
|    mean velocity z | 4.9        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.76e+04  |
| time/              |            |
|    total_timesteps | 719500     |
-----------------------------------
Eval num_timesteps=720000, episode_reward=-73662.06 +/- 45777.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42392978 |
|    mean velocity x | -0.13       |
|    mean velocity y | 0.937       |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 720000      |
------------------------------------
Eval num_timesteps=720500, episode_reward=-114145.48 +/- 28552.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5133574 |
|    mean velocity x | -1.3       |
|    mean velocity y | -0.19      |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.14e+05  |
| time/              |            |
|    total_timesteps | 720500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 352    |
|    time_elapsed    | 29155  |
|    total_timesteps | 720896 |
-------------------------------
Eval num_timesteps=721000, episode_reward=-89141.91 +/- 50229.59
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44548985   |
|    mean velocity x      | -0.205        |
|    mean velocity y      | 1.45          |
|    mean velocity z      | 4.82          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.91e+04     |
| time/                   |               |
|    total_timesteps      | 721000        |
| train/                  |               |
|    approx_kl            | 0.00012924921 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.001         |
|    loss                 | 5.64e+07      |
|    n_updates            | 3520          |
|    policy_gradient_loss | -0.000712     |
|    std                  | 1.55          |
|    value_loss           | 5.8e+07       |
-------------------------------------------
Eval num_timesteps=721500, episode_reward=-67412.60 +/- 39088.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44062018 |
|    mean velocity x | -0.165      |
|    mean velocity y | 0.786       |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 721500      |
------------------------------------
Eval num_timesteps=722000, episode_reward=-57592.02 +/- 47730.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4600868 |
|    mean velocity x | -0.267     |
|    mean velocity y | 0.638      |
|    mean velocity z | 4.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.76e+04  |
| time/              |            |
|    total_timesteps | 722000     |
-----------------------------------
Eval num_timesteps=722500, episode_reward=-110640.76 +/- 7819.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36720446 |
|    mean velocity x | -1.13       |
|    mean velocity y | 0.338       |
|    mean velocity z | 2.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 722500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 353    |
|    time_elapsed    | 29235  |
|    total_timesteps | 722944 |
-------------------------------
Eval num_timesteps=723000, episode_reward=-104924.75 +/- 55616.40
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5063435   |
|    mean velocity x      | -0.272       |
|    mean velocity y      | 1.21         |
|    mean velocity z      | 5.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 723000       |
| train/                  |              |
|    approx_kl            | 0.0002860393 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.001        |
|    loss                 | 3.26e+07     |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 1.55         |
|    value_loss           | 8.95e+07     |
------------------------------------------
Eval num_timesteps=723500, episode_reward=-100376.13 +/- 43849.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30275872 |
|    mean velocity x | -0.633      |
|    mean velocity y | 0.816       |
|    mean velocity z | 1.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 723500      |
------------------------------------
Eval num_timesteps=724000, episode_reward=-81750.08 +/- 21875.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46371242 |
|    mean velocity x | -0.45       |
|    mean velocity y | 0.411       |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.18e+04   |
| time/              |             |
|    total_timesteps | 724000      |
------------------------------------
Eval num_timesteps=724500, episode_reward=-92207.35 +/- 33681.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5034853 |
|    mean velocity x | -0.506     |
|    mean velocity y | 0.769      |
|    mean velocity z | 4.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.22e+04  |
| time/              |            |
|    total_timesteps | 724500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 354    |
|    time_elapsed    | 29316  |
|    total_timesteps | 724992 |
-------------------------------
Eval num_timesteps=725000, episode_reward=-82860.46 +/- 53161.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47421694  |
|    mean velocity x      | -1.11        |
|    mean velocity y      | 0.191        |
|    mean velocity z      | 4.55         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.29e+04    |
| time/                   |              |
|    total_timesteps      | 725000       |
| train/                  |              |
|    approx_kl            | 4.199936e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.119        |
|    learning_rate        | 0.001        |
|    loss                 | 7.74e+07     |
|    n_updates            | 3540         |
|    policy_gradient_loss | -0.000301    |
|    std                  | 1.55         |
|    value_loss           | 6.97e+07     |
------------------------------------------
Eval num_timesteps=725500, episode_reward=-85767.57 +/- 40818.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48322612 |
|    mean velocity x | -0.305      |
|    mean velocity y | 0.813       |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.58e+04   |
| time/              |             |
|    total_timesteps | 725500      |
------------------------------------
Eval num_timesteps=726000, episode_reward=-107323.87 +/- 16527.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38619938 |
|    mean velocity x | -1.06       |
|    mean velocity y | -0.208      |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 726000      |
------------------------------------
Eval num_timesteps=726500, episode_reward=-103322.87 +/- 23453.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43187517 |
|    mean velocity x | -2.6        |
|    mean velocity y | -1.9        |
|    mean velocity z | 7.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 726500      |
------------------------------------
Eval num_timesteps=727000, episode_reward=-100323.47 +/- 18901.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28056905 |
|    mean velocity x | -1.1        |
|    mean velocity y | -0.306      |
|    mean velocity z | 3.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 727000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 355    |
|    time_elapsed    | 29415  |
|    total_timesteps | 727040 |
-------------------------------
Eval num_timesteps=727500, episode_reward=-67516.39 +/- 56814.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.451065     |
|    mean velocity x      | 0.425         |
|    mean velocity y      | 1.63          |
|    mean velocity z      | 3.76          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.75e+04     |
| time/                   |               |
|    total_timesteps      | 727500        |
| train/                  |               |
|    approx_kl            | 4.0802814e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.232         |
|    learning_rate        | 0.001         |
|    loss                 | 2.74e+07      |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.000426     |
|    std                  | 1.55          |
|    value_loss           | 3.97e+07      |
-------------------------------------------
Eval num_timesteps=728000, episode_reward=-101852.67 +/- 21531.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5157031 |
|    mean velocity x | -0.39      |
|    mean velocity y | 0.721      |
|    mean velocity z | 5.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 728000     |
-----------------------------------
Eval num_timesteps=728500, episode_reward=-85721.89 +/- 55301.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47520763 |
|    mean velocity x | -0.244      |
|    mean velocity y | 1.38        |
|    mean velocity z | 4.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.57e+04   |
| time/              |             |
|    total_timesteps | 728500      |
------------------------------------
Eval num_timesteps=729000, episode_reward=-41686.17 +/- 49685.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5627754 |
|    mean velocity x | -0.375     |
|    mean velocity y | 1.45       |
|    mean velocity z | 4.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.17e+04  |
| time/              |            |
|    total_timesteps | 729000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 356    |
|    time_elapsed    | 29495  |
|    total_timesteps | 729088 |
-------------------------------
Eval num_timesteps=729500, episode_reward=-114437.99 +/- 23530.15
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5180488   |
|    mean velocity x      | -0.529       |
|    mean velocity y      | 1.06         |
|    mean velocity z      | 4.09         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.14e+05    |
| time/                   |              |
|    total_timesteps      | 729500       |
| train/                  |              |
|    approx_kl            | 4.764367e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.102        |
|    learning_rate        | 0.001        |
|    loss                 | 3.57e+07     |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.000363    |
|    std                  | 1.55         |
|    value_loss           | 9.74e+07     |
------------------------------------------
Eval num_timesteps=730000, episode_reward=-93823.34 +/- 46216.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4103014 |
|    mean velocity x | -0.0676    |
|    mean velocity y | 1.01       |
|    mean velocity z | 4.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.38e+04  |
| time/              |            |
|    total_timesteps | 730000     |
-----------------------------------
Eval num_timesteps=730500, episode_reward=-67152.36 +/- 51435.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50768524 |
|    mean velocity x | -0.442      |
|    mean velocity y | 1           |
|    mean velocity z | 4.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 730500      |
------------------------------------
Eval num_timesteps=731000, episode_reward=-85279.63 +/- 31066.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30982712 |
|    mean velocity x | -0.226      |
|    mean velocity y | 0.637       |
|    mean velocity z | 0.553       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 731000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 357    |
|    time_elapsed    | 29576  |
|    total_timesteps | 731136 |
-------------------------------
Eval num_timesteps=731500, episode_reward=-94677.20 +/- 38680.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49744138   |
|    mean velocity x      | -0.38         |
|    mean velocity y      | 1.15          |
|    mean velocity z      | 4.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.47e+04     |
| time/                   |               |
|    total_timesteps      | 731500        |
| train/                  |               |
|    approx_kl            | 2.3774599e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.0766        |
|    learning_rate        | 0.001         |
|    loss                 | 4.6e+07       |
|    n_updates            | 3570          |
|    policy_gradient_loss | -0.00041      |
|    std                  | 1.55          |
|    value_loss           | 8.37e+07      |
-------------------------------------------
Eval num_timesteps=732000, episode_reward=-91204.68 +/- 46446.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53593355 |
|    mean velocity x | -1.23       |
|    mean velocity y | 0.04        |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.12e+04   |
| time/              |             |
|    total_timesteps | 732000      |
------------------------------------
Eval num_timesteps=732500, episode_reward=-60949.02 +/- 18066.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52599084 |
|    mean velocity x | -0.301      |
|    mean velocity y | 1.15        |
|    mean velocity z | 4.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.09e+04   |
| time/              |             |
|    total_timesteps | 732500      |
------------------------------------
Eval num_timesteps=733000, episode_reward=-79004.27 +/- 42580.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.61341083 |
|    mean velocity x | -0.644      |
|    mean velocity y | 1.19        |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 733000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 358    |
|    time_elapsed    | 29656  |
|    total_timesteps | 733184 |
-------------------------------
Eval num_timesteps=733500, episode_reward=-101817.80 +/- 24775.65
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.39730218  |
|    mean velocity x      | -0.788       |
|    mean velocity y      | -0.379       |
|    mean velocity z      | 4.2          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.02e+05    |
| time/                   |              |
|    total_timesteps      | 733500       |
| train/                  |              |
|    approx_kl            | 6.718718e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.112        |
|    learning_rate        | 0.001        |
|    loss                 | 4e+07        |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.000757    |
|    std                  | 1.55         |
|    value_loss           | 7.14e+07     |
------------------------------------------
Eval num_timesteps=734000, episode_reward=-96455.82 +/- 48347.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52854204 |
|    mean velocity x | -0.333      |
|    mean velocity y | 1.33        |
|    mean velocity z | 5           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.65e+04   |
| time/              |             |
|    total_timesteps | 734000      |
------------------------------------
Eval num_timesteps=734500, episode_reward=-64620.84 +/- 34224.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5151374 |
|    mean velocity x | 0.381      |
|    mean velocity y | 1.24       |
|    mean velocity z | 3.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.46e+04  |
| time/              |            |
|    total_timesteps | 734500     |
-----------------------------------
Eval num_timesteps=735000, episode_reward=-105388.73 +/- 18169.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26896527 |
|    mean velocity x | -0.489      |
|    mean velocity y | -0.0377     |
|    mean velocity z | 3.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 735000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 359    |
|    time_elapsed    | 29736  |
|    total_timesteps | 735232 |
-------------------------------
Eval num_timesteps=735500, episode_reward=-113335.59 +/- 16350.19
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39516652   |
|    mean velocity x      | 0.237         |
|    mean velocity y      | 0.821         |
|    mean velocity z      | 3.15          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.13e+05     |
| time/                   |               |
|    total_timesteps      | 735500        |
| train/                  |               |
|    approx_kl            | 0.00022985085 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.001         |
|    loss                 | 2.52e+07      |
|    n_updates            | 3590          |
|    policy_gradient_loss | -0.000906     |
|    std                  | 1.55          |
|    value_loss           | 5.84e+07      |
-------------------------------------------
Eval num_timesteps=736000, episode_reward=-111458.72 +/- 33806.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36397874 |
|    mean velocity x | -0.107      |
|    mean velocity y | 0.617       |
|    mean velocity z | 3.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 736000      |
------------------------------------
Eval num_timesteps=736500, episode_reward=-68022.11 +/- 37089.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6809857 |
|    mean velocity x | 0.542      |
|    mean velocity y | 2.18       |
|    mean velocity z | 4.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.8e+04   |
| time/              |            |
|    total_timesteps | 736500     |
-----------------------------------
Eval num_timesteps=737000, episode_reward=-100470.84 +/- 36490.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3623496 |
|    mean velocity x | -0.245     |
|    mean velocity y | 0.387      |
|    mean velocity z | 1.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+05     |
| time/              |            |
|    total_timesteps | 737000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 360    |
|    time_elapsed    | 29817  |
|    total_timesteps | 737280 |
-------------------------------
Eval num_timesteps=737500, episode_reward=-108442.99 +/- 29651.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44812396   |
|    mean velocity x      | 0.185         |
|    mean velocity y      | 0.884         |
|    mean velocity z      | 2.8           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.08e+05     |
| time/                   |               |
|    total_timesteps      | 737500        |
| train/                  |               |
|    approx_kl            | 0.00018045591 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.001         |
|    loss                 | 9.43e+06      |
|    n_updates            | 3600          |
|    policy_gradient_loss | -0.000738     |
|    std                  | 1.55          |
|    value_loss           | 2.84e+07      |
-------------------------------------------
Eval num_timesteps=738000, episode_reward=-36471.62 +/- 36647.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40785286 |
|    mean velocity x | -0.498      |
|    mean velocity y | 0.71        |
|    mean velocity z | 2.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.65e+04   |
| time/              |             |
|    total_timesteps | 738000      |
------------------------------------
Eval num_timesteps=738500, episode_reward=-106292.94 +/- 22588.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34797508 |
|    mean velocity x | -1.35       |
|    mean velocity y | -0.0281     |
|    mean velocity z | 3.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 738500      |
------------------------------------
Eval num_timesteps=739000, episode_reward=-114168.79 +/- 21013.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41514513 |
|    mean velocity x | -1.06       |
|    mean velocity y | -0.406      |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 739000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 361    |
|    time_elapsed    | 29897  |
|    total_timesteps | 739328 |
-------------------------------
Eval num_timesteps=739500, episode_reward=-100049.60 +/- 53514.67
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31808472   |
|    mean velocity x      | -0.317        |
|    mean velocity y      | 0.172         |
|    mean velocity z      | 3.08          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1e+05        |
| time/                   |               |
|    total_timesteps      | 739500        |
| train/                  |               |
|    approx_kl            | 3.3730263e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+07      |
|    n_updates            | 3610          |
|    policy_gradient_loss | -0.00039      |
|    std                  | 1.55          |
|    value_loss           | 2.35e+07      |
-------------------------------------------
Eval num_timesteps=740000, episode_reward=-88897.93 +/- 35766.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58565503 |
|    mean velocity x | -0.206      |
|    mean velocity y | 1.54        |
|    mean velocity z | 2.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.89e+04   |
| time/              |             |
|    total_timesteps | 740000      |
------------------------------------
Eval num_timesteps=740500, episode_reward=-84431.06 +/- 59015.11
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.46901  |
|    mean velocity x | 0.479     |
|    mean velocity y | 1.73      |
|    mean velocity z | 3.7       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.44e+04 |
| time/              |           |
|    total_timesteps | 740500    |
----------------------------------
Eval num_timesteps=741000, episode_reward=-90311.56 +/- 62420.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53489244 |
|    mean velocity x | -0.456      |
|    mean velocity y | 0.603       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 741000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 362    |
|    time_elapsed    | 29977  |
|    total_timesteps | 741376 |
-------------------------------
Eval num_timesteps=741500, episode_reward=-79481.36 +/- 47020.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40392858   |
|    mean velocity x      | -0.673        |
|    mean velocity y      | 0.381         |
|    mean velocity z      | 2.68          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.95e+04     |
| time/                   |               |
|    total_timesteps      | 741500        |
| train/                  |               |
|    approx_kl            | 3.6390265e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.001         |
|    loss                 | 6.93e+07      |
|    n_updates            | 3620          |
|    policy_gradient_loss | -0.000539     |
|    std                  | 1.55          |
|    value_loss           | 3.62e+07      |
-------------------------------------------
Eval num_timesteps=742000, episode_reward=-71377.45 +/- 45880.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43200836 |
|    mean velocity x | -1.11       |
|    mean velocity y | -0.63       |
|    mean velocity z | 4.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.14e+04   |
| time/              |             |
|    total_timesteps | 742000      |
------------------------------------
Eval num_timesteps=742500, episode_reward=-117267.09 +/- 26572.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5475336 |
|    mean velocity x | -0.517     |
|    mean velocity y | 0.837      |
|    mean velocity z | 4.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.17e+05  |
| time/              |            |
|    total_timesteps | 742500     |
-----------------------------------
Eval num_timesteps=743000, episode_reward=-85646.32 +/- 17402.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21921082 |
|    mean velocity x | -0.344      |
|    mean velocity y | 0.536       |
|    mean velocity z | 0.431       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 743000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 363    |
|    time_elapsed    | 30058  |
|    total_timesteps | 743424 |
-------------------------------
Eval num_timesteps=743500, episode_reward=-107203.42 +/- 54097.01
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4550133    |
|    mean velocity x      | -0.179        |
|    mean velocity y      | 0.973         |
|    mean velocity z      | 4.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 743500        |
| train/                  |               |
|    approx_kl            | 2.8397655e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+07      |
|    n_updates            | 3630          |
|    policy_gradient_loss | -0.000375     |
|    std                  | 1.55          |
|    value_loss           | 6.14e+07      |
-------------------------------------------
Eval num_timesteps=744000, episode_reward=-96083.71 +/- 24461.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5866583 |
|    mean velocity x | -0.021     |
|    mean velocity y | 1.94       |
|    mean velocity z | 4.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.61e+04  |
| time/              |            |
|    total_timesteps | 744000     |
-----------------------------------
Eval num_timesteps=744500, episode_reward=-72562.18 +/- 19330.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46583375 |
|    mean velocity x | -0.574      |
|    mean velocity y | 0.454       |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.26e+04   |
| time/              |             |
|    total_timesteps | 744500      |
------------------------------------
Eval num_timesteps=745000, episode_reward=-78899.07 +/- 30926.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36760077 |
|    mean velocity x | -0.522      |
|    mean velocity y | 0.163       |
|    mean velocity z | 3.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.89e+04   |
| time/              |             |
|    total_timesteps | 745000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 364    |
|    time_elapsed    | 30138  |
|    total_timesteps | 745472 |
-------------------------------
Eval num_timesteps=745500, episode_reward=-84422.62 +/- 58263.44
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.61308986   |
|    mean velocity x      | -0.376        |
|    mean velocity y      | 1.71          |
|    mean velocity z      | 5.25          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.44e+04     |
| time/                   |               |
|    total_timesteps      | 745500        |
| train/                  |               |
|    approx_kl            | 4.7862588e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.001         |
|    loss                 | 5.44e+07      |
|    n_updates            | 3640          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 1.55          |
|    value_loss           | 6.43e+07      |
-------------------------------------------
Eval num_timesteps=746000, episode_reward=-78638.53 +/- 36461.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38003382 |
|    mean velocity x | -1.21       |
|    mean velocity y | -0.261      |
|    mean velocity z | 3.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.86e+04   |
| time/              |             |
|    total_timesteps | 746000      |
------------------------------------
Eval num_timesteps=746500, episode_reward=-109426.96 +/- 33631.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37681973 |
|    mean velocity x | -0.688      |
|    mean velocity y | -0.182      |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 746500      |
------------------------------------
Eval num_timesteps=747000, episode_reward=-119411.17 +/- 18965.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58360577 |
|    mean velocity x | -0.588      |
|    mean velocity y | 0.886       |
|    mean velocity z | 4.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 747000      |
------------------------------------
Eval num_timesteps=747500, episode_reward=-93474.73 +/- 28030.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46155858 |
|    mean velocity x | -0.2        |
|    mean velocity y | 0.882       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.35e+04   |
| time/              |             |
|    total_timesteps | 747500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 365    |
|    time_elapsed    | 30237  |
|    total_timesteps | 747520 |
-------------------------------
Eval num_timesteps=748000, episode_reward=-75604.80 +/- 32409.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.51482      |
|    mean velocity x      | -0.807        |
|    mean velocity y      | 0.162         |
|    mean velocity z      | 5.15          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.56e+04     |
| time/                   |               |
|    total_timesteps      | 748000        |
| train/                  |               |
|    approx_kl            | 0.00020445921 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.001         |
|    loss                 | 7.41e+07      |
|    n_updates            | 3650          |
|    policy_gradient_loss | -0.00113      |
|    std                  | 1.55          |
|    value_loss           | 6.81e+07      |
-------------------------------------------
Eval num_timesteps=748500, episode_reward=-88262.72 +/- 45584.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48680204 |
|    mean velocity x | -0.699      |
|    mean velocity y | 0.703       |
|    mean velocity z | 3.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.83e+04   |
| time/              |             |
|    total_timesteps | 748500      |
------------------------------------
Eval num_timesteps=749000, episode_reward=-90673.49 +/- 38958.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5490186 |
|    mean velocity x | -0.62      |
|    mean velocity y | 0.532      |
|    mean velocity z | 4.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.07e+04  |
| time/              |            |
|    total_timesteps | 749000     |
-----------------------------------
Eval num_timesteps=749500, episode_reward=-112033.09 +/- 26567.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33495528 |
|    mean velocity x | -0.062      |
|    mean velocity y | 0.326       |
|    mean velocity z | 1.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 749500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 366    |
|    time_elapsed    | 30318  |
|    total_timesteps | 749568 |
-------------------------------
Eval num_timesteps=750000, episode_reward=-79109.53 +/- 41509.59
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5253874    |
|    mean velocity x      | -0.445        |
|    mean velocity y      | 0.996         |
|    mean velocity z      | 2.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.91e+04     |
| time/                   |               |
|    total_timesteps      | 750000        |
| train/                  |               |
|    approx_kl            | 2.2911176e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+07       |
|    n_updates            | 3660          |
|    policy_gradient_loss | -0.000269     |
|    std                  | 1.55          |
|    value_loss           | 5e+07         |
-------------------------------------------
Eval num_timesteps=750500, episode_reward=-75770.19 +/- 23925.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6253665 |
|    mean velocity x | -0.714     |
|    mean velocity y | 0.702      |
|    mean velocity z | 5.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.58e+04  |
| time/              |            |
|    total_timesteps | 750500     |
-----------------------------------
Eval num_timesteps=751000, episode_reward=-95657.49 +/- 55097.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5708243 |
|    mean velocity x | 0.0131     |
|    mean velocity y | 1.21       |
|    mean velocity z | 3.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.57e+04  |
| time/              |            |
|    total_timesteps | 751000     |
-----------------------------------
Eval num_timesteps=751500, episode_reward=-86217.28 +/- 43893.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.433099 |
|    mean velocity x | -0.303    |
|    mean velocity y | 0.756     |
|    mean velocity z | 4.42      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.62e+04 |
| time/              |           |
|    total_timesteps | 751500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 367    |
|    time_elapsed    | 30398  |
|    total_timesteps | 751616 |
-------------------------------
Eval num_timesteps=752000, episode_reward=-99292.32 +/- 31803.26
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.565121     |
|    mean velocity x      | -0.402        |
|    mean velocity y      | 1.4           |
|    mean velocity z      | 4.94          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.93e+04     |
| time/                   |               |
|    total_timesteps      | 752000        |
| train/                  |               |
|    approx_kl            | 2.9814692e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.001         |
|    loss                 | 1.84e+07      |
|    n_updates            | 3670          |
|    policy_gradient_loss | -0.000359     |
|    std                  | 1.55          |
|    value_loss           | 8.78e+07      |
-------------------------------------------
Eval num_timesteps=752500, episode_reward=-134335.22 +/- 15354.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31874663 |
|    mean velocity x | -0.206      |
|    mean velocity y | 0.48        |
|    mean velocity z | 0.536       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.34e+05   |
| time/              |             |
|    total_timesteps | 752500      |
------------------------------------
Eval num_timesteps=753000, episode_reward=-103535.16 +/- 41171.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4139872 |
|    mean velocity x | -0.201     |
|    mean velocity y | 1.11       |
|    mean velocity z | 4.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 753000     |
-----------------------------------
Eval num_timesteps=753500, episode_reward=-58979.04 +/- 44148.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46746123 |
|    mean velocity x | -0.281      |
|    mean velocity y | 1.19        |
|    mean velocity z | 4.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.9e+04    |
| time/              |             |
|    total_timesteps | 753500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 368    |
|    time_elapsed    | 30478  |
|    total_timesteps | 753664 |
-------------------------------
Eval num_timesteps=754000, episode_reward=-97440.82 +/- 29132.86
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.47151232    |
|    mean velocity x      | -0.429         |
|    mean velocity y      | 0.321          |
|    mean velocity z      | 4.83           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -9.74e+04      |
| time/                   |                |
|    total_timesteps      | 754000         |
| train/                  |                |
|    approx_kl            | 0.000111257395 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.0866         |
|    learning_rate        | 0.001          |
|    loss                 | 5.16e+07       |
|    n_updates            | 3680           |
|    policy_gradient_loss | -0.00111       |
|    std                  | 1.55           |
|    value_loss           | 8.93e+07       |
--------------------------------------------
Eval num_timesteps=754500, episode_reward=-95863.43 +/- 45460.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49926165 |
|    mean velocity x | -0.588      |
|    mean velocity y | 0.772       |
|    mean velocity z | 4.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.59e+04   |
| time/              |             |
|    total_timesteps | 754500      |
------------------------------------
Eval num_timesteps=755000, episode_reward=-70959.53 +/- 40615.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33738348 |
|    mean velocity x | -1.19       |
|    mean velocity y | -0.356      |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.1e+04    |
| time/              |             |
|    total_timesteps | 755000      |
------------------------------------
Eval num_timesteps=755500, episode_reward=-114054.58 +/- 26286.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22744001 |
|    mean velocity x | -0.752      |
|    mean velocity y | 0.48        |
|    mean velocity z | 1.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 755500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 369    |
|    time_elapsed    | 30558  |
|    total_timesteps | 755712 |
-------------------------------
Eval num_timesteps=756000, episode_reward=-108279.90 +/- 42580.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44426742   |
|    mean velocity x      | -0.917        |
|    mean velocity y      | 0.364         |
|    mean velocity z      | 3.68          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.08e+05     |
| time/                   |               |
|    total_timesteps      | 756000        |
| train/                  |               |
|    approx_kl            | 0.00011115687 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.001         |
|    loss                 | 8.68e+06      |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.000547     |
|    std                  | 1.55          |
|    value_loss           | 4.03e+07      |
-------------------------------------------
Eval num_timesteps=756500, episode_reward=-70067.20 +/- 41406.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5145301 |
|    mean velocity x | -0.305     |
|    mean velocity y | 1.29       |
|    mean velocity z | 4.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.01e+04  |
| time/              |            |
|    total_timesteps | 756500     |
-----------------------------------
Eval num_timesteps=757000, episode_reward=-90479.24 +/- 42971.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42435408 |
|    mean velocity x | -0.248      |
|    mean velocity y | 0.698       |
|    mean velocity z | 2.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.05e+04   |
| time/              |             |
|    total_timesteps | 757000      |
------------------------------------
Eval num_timesteps=757500, episode_reward=-119894.00 +/- 16875.79
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.730681 |
|    mean velocity x | 0.307     |
|    mean velocity y | 2.19      |
|    mean velocity z | 4.28      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.2e+05  |
| time/              |           |
|    total_timesteps | 757500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 370    |
|    time_elapsed    | 30639  |
|    total_timesteps | 757760 |
-------------------------------
Eval num_timesteps=758000, episode_reward=-93291.07 +/- 32160.80
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.58214587   |
|    mean velocity x      | -1.07         |
|    mean velocity y      | 0.682         |
|    mean velocity z      | 4.17          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.33e+04     |
| time/                   |               |
|    total_timesteps      | 758000        |
| train/                  |               |
|    approx_kl            | 0.00015765065 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.143         |
|    learning_rate        | 0.001         |
|    loss                 | 1.36e+07      |
|    n_updates            | 3700          |
|    policy_gradient_loss | -0.000955     |
|    std                  | 1.55          |
|    value_loss           | 5.5e+07       |
-------------------------------------------
Eval num_timesteps=758500, episode_reward=-74120.29 +/- 44738.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47625703 |
|    mean velocity x | -0.225      |
|    mean velocity y | 1.36        |
|    mean velocity z | 4.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.41e+04   |
| time/              |             |
|    total_timesteps | 758500      |
------------------------------------
Eval num_timesteps=759000, episode_reward=-86193.67 +/- 37548.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31251413 |
|    mean velocity x | -0.411      |
|    mean velocity y | 0.692       |
|    mean velocity z | 1.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 759000      |
------------------------------------
Eval num_timesteps=759500, episode_reward=-73775.38 +/- 58525.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53783375 |
|    mean velocity x | -0.416      |
|    mean velocity y | 0.972       |
|    mean velocity z | 5.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.38e+04   |
| time/              |             |
|    total_timesteps | 759500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 371    |
|    time_elapsed    | 30719  |
|    total_timesteps | 759808 |
-------------------------------
Eval num_timesteps=760000, episode_reward=-86727.43 +/- 46336.43
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.2869667     |
|    mean velocity x      | -1.54          |
|    mean velocity y      | -1.46          |
|    mean velocity z      | 6.12           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -8.67e+04      |
| time/                   |                |
|    total_timesteps      | 760000         |
| train/                  |                |
|    approx_kl            | 0.000110128196 |
|    clip_fraction        | 0.000293       |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.117          |
|    learning_rate        | 0.001          |
|    loss                 | 5.33e+07       |
|    n_updates            | 3710           |
|    policy_gradient_loss | -0.000935      |
|    std                  | 1.55           |
|    value_loss           | 8.84e+07       |
--------------------------------------------
Eval num_timesteps=760500, episode_reward=-119610.49 +/- 28392.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53633666 |
|    mean velocity x | -0.457      |
|    mean velocity y | 0.775       |
|    mean velocity z | 5.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 760500      |
------------------------------------
Eval num_timesteps=761000, episode_reward=-115337.37 +/- 23333.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4256054 |
|    mean velocity x | -1.51      |
|    mean velocity y | -0.255     |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.15e+05  |
| time/              |            |
|    total_timesteps | 761000     |
-----------------------------------
Eval num_timesteps=761500, episode_reward=-80960.76 +/- 41639.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51735663 |
|    mean velocity x | -0.452      |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 761500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 372    |
|    time_elapsed    | 30799  |
|    total_timesteps | 761856 |
-------------------------------
Eval num_timesteps=762000, episode_reward=-58415.98 +/- 35868.86
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40888694  |
|    mean velocity x      | 0.176        |
|    mean velocity y      | 0.836        |
|    mean velocity z      | 2.95         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.84e+04    |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 7.828482e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.001        |
|    loss                 | 3.21e+07     |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000585    |
|    std                  | 1.55         |
|    value_loss           | 6.89e+07     |
------------------------------------------
Eval num_timesteps=762500, episode_reward=-87591.80 +/- 37668.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4556452 |
|    mean velocity x | -0.331     |
|    mean velocity y | 0.448      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.76e+04  |
| time/              |            |
|    total_timesteps | 762500     |
-----------------------------------
Eval num_timesteps=763000, episode_reward=-67114.62 +/- 35140.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46090132 |
|    mean velocity x | -0.524      |
|    mean velocity y | 0.303       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 763000      |
------------------------------------
Eval num_timesteps=763500, episode_reward=-100542.86 +/- 29978.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4019391 |
|    mean velocity x | -1.79      |
|    mean velocity y | -0.505     |
|    mean velocity z | 4.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 763500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 373    |
|    time_elapsed    | 30880  |
|    total_timesteps | 763904 |
-------------------------------
Eval num_timesteps=764000, episode_reward=-74429.39 +/- 40268.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4947185   |
|    mean velocity x      | -1.25        |
|    mean velocity y      | 0.413        |
|    mean velocity z      | 3.34         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.44e+04    |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 4.710455e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.124        |
|    learning_rate        | 0.001        |
|    loss                 | 6.78e+07     |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.000719    |
|    std                  | 1.55         |
|    value_loss           | 6.37e+07     |
------------------------------------------
Eval num_timesteps=764500, episode_reward=-62828.89 +/- 44205.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.577328 |
|    mean velocity x | -0.726    |
|    mean velocity y | 0.604     |
|    mean velocity z | 4.14      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.28e+04 |
| time/              |           |
|    total_timesteps | 764500    |
----------------------------------
Eval num_timesteps=765000, episode_reward=-123071.71 +/- 12443.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35419467 |
|    mean velocity x | -0.764      |
|    mean velocity y | 0.619       |
|    mean velocity z | 2.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 765000      |
------------------------------------
Eval num_timesteps=765500, episode_reward=-74771.64 +/- 20182.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4885233 |
|    mean velocity x | 0.438      |
|    mean velocity y | 1.14       |
|    mean velocity z | 3.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.48e+04  |
| time/              |            |
|    total_timesteps | 765500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 374    |
|    time_elapsed    | 30960  |
|    total_timesteps | 765952 |
-------------------------------
Eval num_timesteps=766000, episode_reward=-103379.23 +/- 20393.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47179005   |
|    mean velocity x      | -0.943        |
|    mean velocity y      | 0.648         |
|    mean velocity z      | 2.08          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 766000        |
| train/                  |               |
|    approx_kl            | 0.00016172318 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+07      |
|    n_updates            | 3740          |
|    policy_gradient_loss | -0.000961     |
|    std                  | 1.55          |
|    value_loss           | 2.9e+07       |
-------------------------------------------
Eval num_timesteps=766500, episode_reward=-80387.71 +/- 42054.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42152137 |
|    mean velocity x | -0.204      |
|    mean velocity y | 0.899       |
|    mean velocity z | 3.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.04e+04   |
| time/              |             |
|    total_timesteps | 766500      |
------------------------------------
Eval num_timesteps=767000, episode_reward=-84287.68 +/- 34491.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45692182 |
|    mean velocity x | -0.401      |
|    mean velocity y | 0.899       |
|    mean velocity z | 4.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.43e+04   |
| time/              |             |
|    total_timesteps | 767000      |
------------------------------------
Eval num_timesteps=767500, episode_reward=-60373.13 +/- 39199.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2851884 |
|    mean velocity x | -0.109     |
|    mean velocity y | 0.385      |
|    mean velocity z | 1.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.04e+04  |
| time/              |            |
|    total_timesteps | 767500     |
-----------------------------------
Eval num_timesteps=768000, episode_reward=-95443.44 +/- 17347.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54712105 |
|    mean velocity x | -0.501      |
|    mean velocity y | 0.974       |
|    mean velocity z | 5.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.54e+04   |
| time/              |             |
|    total_timesteps | 768000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 375    |
|    time_elapsed    | 31060  |
|    total_timesteps | 768000 |
-------------------------------
Eval num_timesteps=768500, episode_reward=-90796.71 +/- 31845.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5570337    |
|    mean velocity x      | -0.306        |
|    mean velocity y      | 1.17          |
|    mean velocity z      | 5.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.08e+04     |
| time/                   |               |
|    total_timesteps      | 768500        |
| train/                  |               |
|    approx_kl            | 7.2743045e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.001         |
|    loss                 | 4.53e+07      |
|    n_updates            | 3750          |
|    policy_gradient_loss | -0.000643     |
|    std                  | 1.55          |
|    value_loss           | 4.7e+07       |
-------------------------------------------
Eval num_timesteps=769000, episode_reward=-91406.02 +/- 21276.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56386393 |
|    mean velocity x | -0.431      |
|    mean velocity y | 1.5         |
|    mean velocity z | 4.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.14e+04   |
| time/              |             |
|    total_timesteps | 769000      |
------------------------------------
Eval num_timesteps=769500, episode_reward=-93402.85 +/- 23245.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52557135 |
|    mean velocity x | -1.02       |
|    mean velocity y | 0.63        |
|    mean velocity z | 3.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.34e+04   |
| time/              |             |
|    total_timesteps | 769500      |
------------------------------------
Eval num_timesteps=770000, episode_reward=-114235.05 +/- 31790.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46984938 |
|    mean velocity x | 0.151       |
|    mean velocity y | 1.1         |
|    mean velocity z | 2.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 770000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 376    |
|    time_elapsed    | 31140  |
|    total_timesteps | 770048 |
-------------------------------
Eval num_timesteps=770500, episode_reward=-110005.93 +/- 55880.00
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47151217   |
|    mean velocity x      | -0.433        |
|    mean velocity y      | 0.976         |
|    mean velocity z      | 4.8           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.1e+05      |
| time/                   |               |
|    total_timesteps      | 770500        |
| train/                  |               |
|    approx_kl            | 2.3947185e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.001         |
|    loss                 | 2.68e+07      |
|    n_updates            | 3760          |
|    policy_gradient_loss | -0.000414     |
|    std                  | 1.55          |
|    value_loss           | 8.88e+07      |
-------------------------------------------
Eval num_timesteps=771000, episode_reward=-71156.78 +/- 21846.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5222477 |
|    mean velocity x | 0.492      |
|    mean velocity y | 1.49       |
|    mean velocity z | 3.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.12e+04  |
| time/              |            |
|    total_timesteps | 771000     |
-----------------------------------
Eval num_timesteps=771500, episode_reward=-42313.79 +/- 37342.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4929783 |
|    mean velocity x | -0.0502    |
|    mean velocity y | 1.43       |
|    mean velocity z | 4.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.23e+04  |
| time/              |            |
|    total_timesteps | 771500     |
-----------------------------------
Eval num_timesteps=772000, episode_reward=-85182.03 +/- 44158.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5010076 |
|    mean velocity x | -0.432     |
|    mean velocity y | 0.71       |
|    mean velocity z | 4.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.52e+04  |
| time/              |            |
|    total_timesteps | 772000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 377    |
|    time_elapsed    | 31220  |
|    total_timesteps | 772096 |
-------------------------------
Eval num_timesteps=772500, episode_reward=-106289.99 +/- 25886.14
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4790227    |
|    mean velocity x      | -0.163        |
|    mean velocity y      | 1.14          |
|    mean velocity z      | 4.54          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.06e+05     |
| time/                   |               |
|    total_timesteps      | 772500        |
| train/                  |               |
|    approx_kl            | 2.3727072e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.001         |
|    loss                 | 5.78e+07      |
|    n_updates            | 3770          |
|    policy_gradient_loss | -0.000462     |
|    std                  | 1.55          |
|    value_loss           | 9.37e+07      |
-------------------------------------------
Eval num_timesteps=773000, episode_reward=-70080.66 +/- 21698.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22684869 |
|    mean velocity x | -1.29       |
|    mean velocity y | -0.241      |
|    mean velocity z | 3.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.01e+04   |
| time/              |             |
|    total_timesteps | 773000      |
------------------------------------
Eval num_timesteps=773500, episode_reward=-60468.75 +/- 55851.78
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.520932 |
|    mean velocity x | -1.09     |
|    mean velocity y | 0.155     |
|    mean velocity z | 3.7       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.05e+04 |
| time/              |           |
|    total_timesteps | 773500    |
----------------------------------
Eval num_timesteps=774000, episode_reward=-69498.90 +/- 41099.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5305614 |
|    mean velocity x | -0.644     |
|    mean velocity y | 0.919      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.95e+04  |
| time/              |            |
|    total_timesteps | 774000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 378    |
|    time_elapsed    | 31301  |
|    total_timesteps | 774144 |
-------------------------------
Eval num_timesteps=774500, episode_reward=-92447.69 +/- 48342.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45737994   |
|    mean velocity x      | -0.182        |
|    mean velocity y      | 0.903         |
|    mean velocity z      | 4.39          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.24e+04     |
| time/                   |               |
|    total_timesteps      | 774500        |
| train/                  |               |
|    approx_kl            | 5.3975644e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.001         |
|    loss                 | 6.4e+07       |
|    n_updates            | 3780          |
|    policy_gradient_loss | -0.000437     |
|    std                  | 1.55          |
|    value_loss           | 5.65e+07      |
-------------------------------------------
Eval num_timesteps=775000, episode_reward=-95443.19 +/- 50854.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4927334 |
|    mean velocity x | -0.304     |
|    mean velocity y | 0.962      |
|    mean velocity z | 3.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.54e+04  |
| time/              |            |
|    total_timesteps | 775000     |
-----------------------------------
Eval num_timesteps=775500, episode_reward=-72679.91 +/- 46726.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43830425 |
|    mean velocity x | -1.34       |
|    mean velocity y | -0.00512    |
|    mean velocity z | 3.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.27e+04   |
| time/              |             |
|    total_timesteps | 775500      |
------------------------------------
Eval num_timesteps=776000, episode_reward=-102345.36 +/- 37772.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43871093 |
|    mean velocity x | -0.349      |
|    mean velocity y | 0.795       |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 776000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 379    |
|    time_elapsed    | 31381  |
|    total_timesteps | 776192 |
-------------------------------
Eval num_timesteps=776500, episode_reward=-83324.87 +/- 35867.90
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5513218    |
|    mean velocity x      | 0.177         |
|    mean velocity y      | 1.54          |
|    mean velocity z      | 3.95          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.33e+04     |
| time/                   |               |
|    total_timesteps      | 776500        |
| train/                  |               |
|    approx_kl            | 3.8304628e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.001         |
|    loss                 | 1.88e+07      |
|    n_updates            | 3790          |
|    policy_gradient_loss | -0.000488     |
|    std                  | 1.55          |
|    value_loss           | 6.03e+07      |
-------------------------------------------
Eval num_timesteps=777000, episode_reward=-83103.30 +/- 39972.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16322377 |
|    mean velocity x | 0.0268      |
|    mean velocity y | 0.378       |
|    mean velocity z | 0.281       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 777000      |
------------------------------------
Eval num_timesteps=777500, episode_reward=-103438.49 +/- 24049.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4432099 |
|    mean velocity x | -0.432     |
|    mean velocity y | 0.527      |
|    mean velocity z | 3.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 777500     |
-----------------------------------
Eval num_timesteps=778000, episode_reward=-102227.66 +/- 40779.16
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.522616 |
|    mean velocity x | -0.0397   |
|    mean velocity y | 1.23      |
|    mean velocity z | 4.26      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.02e+05 |
| time/              |           |
|    total_timesteps | 778000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 380    |
|    time_elapsed    | 31461  |
|    total_timesteps | 778240 |
-------------------------------
Eval num_timesteps=778500, episode_reward=-73931.25 +/- 39415.71
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35729358   |
|    mean velocity x      | -0.123        |
|    mean velocity y      | 0.654         |
|    mean velocity z      | 4.35          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.39e+04     |
| time/                   |               |
|    total_timesteps      | 778500        |
| train/                  |               |
|    approx_kl            | 4.5562832e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.001         |
|    loss                 | 1.5e+07       |
|    n_updates            | 3800          |
|    policy_gradient_loss | -0.000702     |
|    std                  | 1.55          |
|    value_loss           | 5.74e+07      |
-------------------------------------------
Eval num_timesteps=779000, episode_reward=-115835.18 +/- 23132.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4763924 |
|    mean velocity x | -0.0375    |
|    mean velocity y | 1.25       |
|    mean velocity z | 3.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.16e+05  |
| time/              |            |
|    total_timesteps | 779000     |
-----------------------------------
Eval num_timesteps=779500, episode_reward=-80451.68 +/- 68178.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22495727 |
|    mean velocity x | -0.14       |
|    mean velocity y | 0.399       |
|    mean velocity z | 0.383       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 779500      |
------------------------------------
Eval num_timesteps=780000, episode_reward=-77852.27 +/- 51783.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3785171 |
|    mean velocity x | -2.23      |
|    mean velocity y | -1.88      |
|    mean velocity z | 7.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.79e+04  |
| time/              |            |
|    total_timesteps | 780000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 381    |
|    time_elapsed    | 31542  |
|    total_timesteps | 780288 |
-------------------------------
Eval num_timesteps=780500, episode_reward=-85572.11 +/- 22563.24
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5480911   |
|    mean velocity x      | -1.17        |
|    mean velocity y      | 0.443        |
|    mean velocity z      | 4.56         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.56e+04    |
| time/                   |              |
|    total_timesteps      | 780500       |
| train/                  |              |
|    approx_kl            | 8.821141e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.001        |
|    loss                 | 2.99e+07     |
|    n_updates            | 3810         |
|    policy_gradient_loss | -0.000703    |
|    std                  | 1.55         |
|    value_loss           | 4.29e+07     |
------------------------------------------
Eval num_timesteps=781000, episode_reward=-81957.40 +/- 45910.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4939142 |
|    mean velocity x | -1.75      |
|    mean velocity y | -0.372     |
|    mean velocity z | 4.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.2e+04   |
| time/              |            |
|    total_timesteps | 781000     |
-----------------------------------
Eval num_timesteps=781500, episode_reward=-104933.47 +/- 19926.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50249183 |
|    mean velocity x | -0.51       |
|    mean velocity y | 0.728       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 781500      |
------------------------------------
Eval num_timesteps=782000, episode_reward=-83053.20 +/- 42271.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5912013 |
|    mean velocity x | 0.3        |
|    mean velocity y | 1.78       |
|    mean velocity z | 3.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.31e+04  |
| time/              |            |
|    total_timesteps | 782000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 382    |
|    time_elapsed    | 31622  |
|    total_timesteps | 782336 |
-------------------------------
Eval num_timesteps=782500, episode_reward=-92476.58 +/- 33414.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.56281394  |
|    mean velocity x      | -0.288       |
|    mean velocity y      | 1.14         |
|    mean velocity z      | 4.05         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.25e+04    |
| time/                   |              |
|    total_timesteps      | 782500       |
| train/                  |              |
|    approx_kl            | 2.711601e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.001        |
|    loss                 | 3.13e+07     |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.000478    |
|    std                  | 1.55         |
|    value_loss           | 5.78e+07     |
------------------------------------------
Eval num_timesteps=783000, episode_reward=-34281.20 +/- 24404.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46367124 |
|    mean velocity x | -1.24       |
|    mean velocity y | 0.28        |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.43e+04   |
| time/              |             |
|    total_timesteps | 783000      |
------------------------------------
Eval num_timesteps=783500, episode_reward=-67632.69 +/- 37521.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47452575 |
|    mean velocity x | 0.695       |
|    mean velocity y | 1.84        |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 783500      |
------------------------------------
Eval num_timesteps=784000, episode_reward=-92974.91 +/- 22708.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38045856 |
|    mean velocity x | -0.167      |
|    mean velocity y | 0.262       |
|    mean velocity z | 2.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.3e+04    |
| time/              |             |
|    total_timesteps | 784000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 383    |
|    time_elapsed    | 31702  |
|    total_timesteps | 784384 |
-------------------------------
Eval num_timesteps=784500, episode_reward=-103965.09 +/- 33023.22
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3384637   |
|    mean velocity x      | -0.751       |
|    mean velocity y      | 0.0927       |
|    mean velocity z      | 3.25         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.04e+05    |
| time/                   |              |
|    total_timesteps      | 784500       |
| train/                  |              |
|    approx_kl            | 0.0001139986 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.231        |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+07     |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.000493    |
|    std                  | 1.56         |
|    value_loss           | 3.21e+07     |
------------------------------------------
Eval num_timesteps=785000, episode_reward=-88593.72 +/- 30964.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46615183 |
|    mean velocity x | -0.234      |
|    mean velocity y | 1.05        |
|    mean velocity z | 4.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.86e+04   |
| time/              |             |
|    total_timesteps | 785000      |
------------------------------------
Eval num_timesteps=785500, episode_reward=-109308.73 +/- 23935.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4864279 |
|    mean velocity x | -0.313     |
|    mean velocity y | 0.645      |
|    mean velocity z | 3.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 785500     |
-----------------------------------
Eval num_timesteps=786000, episode_reward=-88460.87 +/- 49419.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51340586 |
|    mean velocity x | 0.0493      |
|    mean velocity y | 1.08        |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 786000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 384    |
|    time_elapsed    | 31783  |
|    total_timesteps | 786432 |
-------------------------------
Eval num_timesteps=786500, episode_reward=-84303.84 +/- 47760.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.54119337   |
|    mean velocity x      | -0.239        |
|    mean velocity y      | 1.17          |
|    mean velocity z      | 4.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.43e+04     |
| time/                   |               |
|    total_timesteps      | 786500        |
| train/                  |               |
|    approx_kl            | 0.00017306072 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.001         |
|    loss                 | 4.87e+07      |
|    n_updates            | 3840          |
|    policy_gradient_loss | -0.00139      |
|    std                  | 1.56          |
|    value_loss           | 8.09e+07      |
-------------------------------------------
Eval num_timesteps=787000, episode_reward=-86926.71 +/- 28977.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48715222 |
|    mean velocity x | -0.0599     |
|    mean velocity y | 1.4         |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 787000      |
------------------------------------
Eval num_timesteps=787500, episode_reward=-85690.17 +/- 30596.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55584157 |
|    mean velocity x | -0.497      |
|    mean velocity y | 0.593       |
|    mean velocity z | 4.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.57e+04   |
| time/              |             |
|    total_timesteps | 787500      |
------------------------------------
Eval num_timesteps=788000, episode_reward=-81896.74 +/- 48185.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4681172 |
|    mean velocity x | -0.945     |
|    mean velocity y | 0.604      |
|    mean velocity z | 3.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.19e+04  |
| time/              |            |
|    total_timesteps | 788000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 385    |
|    time_elapsed    | 31863  |
|    total_timesteps | 788480 |
-------------------------------
Eval num_timesteps=788500, episode_reward=-86151.18 +/- 34996.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34155542   |
|    mean velocity x      | -0.774        |
|    mean velocity y      | -0.494        |
|    mean velocity z      | 3.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.62e+04     |
| time/                   |               |
|    total_timesteps      | 788500        |
| train/                  |               |
|    approx_kl            | 1.7973885e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.001         |
|    loss                 | 5.01e+07      |
|    n_updates            | 3850          |
|    policy_gradient_loss | -0.000328     |
|    std                  | 1.55          |
|    value_loss           | 5.71e+07      |
-------------------------------------------
Eval num_timesteps=789000, episode_reward=-97989.28 +/- 35944.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54131174 |
|    mean velocity x | 0.386       |
|    mean velocity y | 2.03        |
|    mean velocity z | 3.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.8e+04    |
| time/              |             |
|    total_timesteps | 789000      |
------------------------------------
Eval num_timesteps=789500, episode_reward=-64127.53 +/- 28808.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23675421 |
|    mean velocity x | -0.21       |
|    mean velocity y | 0.479       |
|    mean velocity z | 0.387       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.41e+04   |
| time/              |             |
|    total_timesteps | 789500      |
------------------------------------
Eval num_timesteps=790000, episode_reward=-85341.15 +/- 43211.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52695405 |
|    mean velocity x | -0.995      |
|    mean velocity y | 0.485       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 790000      |
------------------------------------
Eval num_timesteps=790500, episode_reward=-82921.53 +/- 40905.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46389052 |
|    mean velocity x | -0.896      |
|    mean velocity y | 0.28        |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 790500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 386    |
|    time_elapsed    | 31962  |
|    total_timesteps | 790528 |
-------------------------------
Eval num_timesteps=791000, episode_reward=-118888.57 +/- 32791.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.17849298   |
|    mean velocity x      | 0.0593        |
|    mean velocity y      | 0.687         |
|    mean velocity z      | 0.333         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.19e+05     |
| time/                   |               |
|    total_timesteps      | 791000        |
| train/                  |               |
|    approx_kl            | 0.00015905069 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.001         |
|    loss                 | 4.96e+06      |
|    n_updates            | 3860          |
|    policy_gradient_loss | -0.00132      |
|    std                  | 1.56          |
|    value_loss           | 3.04e+07      |
-------------------------------------------
Eval num_timesteps=791500, episode_reward=-89342.68 +/- 30274.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4367146 |
|    mean velocity x | -0.276     |
|    mean velocity y | 0.654      |
|    mean velocity z | 2.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.93e+04  |
| time/              |            |
|    total_timesteps | 791500     |
-----------------------------------
Eval num_timesteps=792000, episode_reward=-82568.70 +/- 26459.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35240707 |
|    mean velocity x | -0.109      |
|    mean velocity y | 0.663       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.26e+04   |
| time/              |             |
|    total_timesteps | 792000      |
------------------------------------
Eval num_timesteps=792500, episode_reward=-97384.53 +/- 48261.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28714746 |
|    mean velocity x | -0.462      |
|    mean velocity y | 0.384       |
|    mean velocity z | 1.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.74e+04   |
| time/              |             |
|    total_timesteps | 792500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 387    |
|    time_elapsed    | 32043  |
|    total_timesteps | 792576 |
-------------------------------
Eval num_timesteps=793000, episode_reward=-115623.72 +/- 29765.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49568725   |
|    mean velocity x      | -0.288        |
|    mean velocity y      | 0.861         |
|    mean velocity z      | 3.24          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.16e+05     |
| time/                   |               |
|    total_timesteps      | 793000        |
| train/                  |               |
|    approx_kl            | 0.00010123223 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+07      |
|    n_updates            | 3870          |
|    policy_gradient_loss | -0.0008       |
|    std                  | 1.56          |
|    value_loss           | 4.2e+07       |
-------------------------------------------
Eval num_timesteps=793500, episode_reward=-83455.56 +/- 34562.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5358463 |
|    mean velocity x | 0.00535    |
|    mean velocity y | 1.03       |
|    mean velocity z | 3.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.35e+04  |
| time/              |            |
|    total_timesteps | 793500     |
-----------------------------------
Eval num_timesteps=794000, episode_reward=-71994.91 +/- 47895.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27183288 |
|    mean velocity x | -0.244      |
|    mean velocity y | 0.15        |
|    mean velocity z | 2.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 794000      |
------------------------------------
Eval num_timesteps=794500, episode_reward=-95119.75 +/- 50890.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43402043 |
|    mean velocity x | -0.951      |
|    mean velocity y | -0.469      |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.51e+04   |
| time/              |             |
|    total_timesteps | 794500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 388    |
|    time_elapsed    | 32123  |
|    total_timesteps | 794624 |
-------------------------------
Eval num_timesteps=795000, episode_reward=-106377.32 +/- 41329.81
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47855937   |
|    mean velocity x      | -0.301        |
|    mean velocity y      | 1.09          |
|    mean velocity z      | 4.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.06e+05     |
| time/                   |               |
|    total_timesteps      | 795000        |
| train/                  |               |
|    approx_kl            | 0.00015533224 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.001         |
|    loss                 | 2.1e+07       |
|    n_updates            | 3880          |
|    policy_gradient_loss | -0.0012       |
|    std                  | 1.56          |
|    value_loss           | 5.67e+07      |
-------------------------------------------
Eval num_timesteps=795500, episode_reward=-72437.78 +/- 39377.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32758433 |
|    mean velocity x | -1.21       |
|    mean velocity y | -0.244      |
|    mean velocity z | 3.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.24e+04   |
| time/              |             |
|    total_timesteps | 795500      |
------------------------------------
Eval num_timesteps=796000, episode_reward=-103661.67 +/- 19404.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5472037 |
|    mean velocity x | -0.428     |
|    mean velocity y | 1.18       |
|    mean velocity z | 4.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 796000     |
-----------------------------------
Eval num_timesteps=796500, episode_reward=-59497.76 +/- 37635.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4125251 |
|    mean velocity x | -0.0516    |
|    mean velocity y | 0.667      |
|    mean velocity z | 3.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.95e+04  |
| time/              |            |
|    total_timesteps | 796500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 389    |
|    time_elapsed    | 32203  |
|    total_timesteps | 796672 |
-------------------------------
Eval num_timesteps=797000, episode_reward=-82452.17 +/- 34288.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36828148  |
|    mean velocity x      | -0.578       |
|    mean velocity y      | 0.809        |
|    mean velocity z      | 1.78         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.25e+04    |
| time/                   |              |
|    total_timesteps      | 797000       |
| train/                  |              |
|    approx_kl            | 9.211281e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.001        |
|    loss                 | 1.83e+07     |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.000596    |
|    std                  | 1.56         |
|    value_loss           | 3.07e+07     |
------------------------------------------
Eval num_timesteps=797500, episode_reward=-86093.81 +/- 45341.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51955163 |
|    mean velocity x | 0.162       |
|    mean velocity y | 1.44        |
|    mean velocity z | 3.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.61e+04   |
| time/              |             |
|    total_timesteps | 797500      |
------------------------------------
Eval num_timesteps=798000, episode_reward=-104089.43 +/- 26676.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47162983 |
|    mean velocity x | -0.536      |
|    mean velocity y | 0.51        |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 798000      |
------------------------------------
Eval num_timesteps=798500, episode_reward=-89629.19 +/- 40313.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4664267 |
|    mean velocity x | -0.627     |
|    mean velocity y | 0.618      |
|    mean velocity z | 3.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.96e+04  |
| time/              |            |
|    total_timesteps | 798500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 390    |
|    time_elapsed    | 32284  |
|    total_timesteps | 798720 |
-------------------------------
Eval num_timesteps=799000, episode_reward=-73799.20 +/- 41154.87
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4370063    |
|    mean velocity x      | -0.157        |
|    mean velocity y      | 1.13          |
|    mean velocity z      | 4.7           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.38e+04     |
| time/                   |               |
|    total_timesteps      | 799000        |
| train/                  |               |
|    approx_kl            | 0.00023643451 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 9.52e+07      |
|    n_updates            | 3900          |
|    policy_gradient_loss | -0.000955     |
|    std                  | 1.56          |
|    value_loss           | 7.53e+07      |
-------------------------------------------
Eval num_timesteps=799500, episode_reward=-61709.51 +/- 52330.17
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.57815  |
|    mean velocity x | 0.0657    |
|    mean velocity y | 1.51      |
|    mean velocity z | 3.55      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.17e+04 |
| time/              |           |
|    total_timesteps | 799500    |
----------------------------------
Eval num_timesteps=800000, episode_reward=-80873.80 +/- 24289.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30417112 |
|    mean velocity x | -1.8        |
|    mean velocity y | -0.829      |
|    mean velocity z | 5.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.09e+04   |
| time/              |             |
|    total_timesteps | 800000      |
------------------------------------
Eval num_timesteps=800500, episode_reward=-42248.69 +/- 44357.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5032173 |
|    mean velocity x | -0.569     |
|    mean velocity y | 1.03       |
|    mean velocity z | 2.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.22e+04  |
| time/              |            |
|    total_timesteps | 800500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 391    |
|    time_elapsed    | 32364  |
|    total_timesteps | 800768 |
-------------------------------
Eval num_timesteps=801000, episode_reward=-51034.76 +/- 58616.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.569455     |
|    mean velocity x      | -0.278        |
|    mean velocity y      | 1.18          |
|    mean velocity z      | 0.906         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.1e+04      |
| time/                   |               |
|    total_timesteps      | 801000        |
| train/                  |               |
|    approx_kl            | 0.00016091933 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+06      |
|    n_updates            | 3910          |
|    policy_gradient_loss | -0.000785     |
|    std                  | 1.56          |
|    value_loss           | 1.35e+07      |
-------------------------------------------
Eval num_timesteps=801500, episode_reward=-95116.76 +/- 35192.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4809995 |
|    mean velocity x | -0.408     |
|    mean velocity y | 0.903      |
|    mean velocity z | 3.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 801500     |
-----------------------------------
Eval num_timesteps=802000, episode_reward=-97930.95 +/- 26407.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44211406 |
|    mean velocity x | -1.18       |
|    mean velocity y | -0.397      |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.79e+04   |
| time/              |             |
|    total_timesteps | 802000      |
------------------------------------
Eval num_timesteps=802500, episode_reward=-97276.06 +/- 53004.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53956336 |
|    mean velocity x | -0.623      |
|    mean velocity y | 0.724       |
|    mean velocity z | 5.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.73e+04   |
| time/              |             |
|    total_timesteps | 802500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 392    |
|    time_elapsed    | 32444  |
|    total_timesteps | 802816 |
-------------------------------
Eval num_timesteps=803000, episode_reward=-84939.85 +/- 25877.17
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4493015   |
|    mean velocity x      | -0.754       |
|    mean velocity y      | -0.00677     |
|    mean velocity z      | 3.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.49e+04    |
| time/                   |              |
|    total_timesteps      | 803000       |
| train/                  |              |
|    approx_kl            | 7.988146e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+07     |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.00063     |
|    std                  | 1.55         |
|    value_loss           | 5.95e+07     |
------------------------------------------
Eval num_timesteps=803500, episode_reward=-98160.67 +/- 44737.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3747765 |
|    mean velocity x | 0.235      |
|    mean velocity y | 1.08       |
|    mean velocity z | 3.44       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.82e+04  |
| time/              |            |
|    total_timesteps | 803500     |
-----------------------------------
Eval num_timesteps=804000, episode_reward=-72963.49 +/- 49316.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4821071 |
|    mean velocity x | -1.1       |
|    mean velocity y | 0.0428     |
|    mean velocity z | 3.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.3e+04   |
| time/              |            |
|    total_timesteps | 804000     |
-----------------------------------
Eval num_timesteps=804500, episode_reward=-80105.09 +/- 40324.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5694672 |
|    mean velocity x | -0.494     |
|    mean velocity y | 0.555      |
|    mean velocity z | 4.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.01e+04  |
| time/              |            |
|    total_timesteps | 804500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 393    |
|    time_elapsed    | 32525  |
|    total_timesteps | 804864 |
-------------------------------
Eval num_timesteps=805000, episode_reward=-65602.80 +/- 31868.68
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.55079585  |
|    mean velocity x      | -0.284       |
|    mean velocity y      | 1.49         |
|    mean velocity z      | 4.37         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.56e+04    |
| time/                   |              |
|    total_timesteps      | 805000       |
| train/                  |              |
|    approx_kl            | 6.715392e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+07      |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.000773    |
|    std                  | 1.55         |
|    value_loss           | 6.02e+07     |
------------------------------------------
Eval num_timesteps=805500, episode_reward=-70651.42 +/- 41871.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45281404 |
|    mean velocity x | 0.0274      |
|    mean velocity y | 1.31        |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 805500      |
------------------------------------
Eval num_timesteps=806000, episode_reward=-70976.89 +/- 51219.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36940625 |
|    mean velocity x | -0.271      |
|    mean velocity y | 0.291       |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.1e+04    |
| time/              |             |
|    total_timesteps | 806000      |
------------------------------------
Eval num_timesteps=806500, episode_reward=-83427.38 +/- 28040.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48834437 |
|    mean velocity x | 0.202       |
|    mean velocity y | 2.03        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.34e+04   |
| time/              |             |
|    total_timesteps | 806500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 394    |
|    time_elapsed    | 32605  |
|    total_timesteps | 806912 |
-------------------------------
Eval num_timesteps=807000, episode_reward=-118409.47 +/- 14538.38
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42666367   |
|    mean velocity x      | -0.291        |
|    mean velocity y      | 0.608         |
|    mean velocity z      | 4.42          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.18e+05     |
| time/                   |               |
|    total_timesteps      | 807000        |
| train/                  |               |
|    approx_kl            | 7.9043035e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.001         |
|    loss                 | 3.63e+07      |
|    n_updates            | 3940          |
|    policy_gradient_loss | -0.000482     |
|    std                  | 1.55          |
|    value_loss           | 6.75e+07      |
-------------------------------------------
Eval num_timesteps=807500, episode_reward=-104826.02 +/- 28502.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5402523 |
|    mean velocity x | -0.552     |
|    mean velocity y | 0.951      |
|    mean velocity z | 4.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.05e+05  |
| time/              |            |
|    total_timesteps | 807500     |
-----------------------------------
Eval num_timesteps=808000, episode_reward=-105513.65 +/- 17736.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4582538 |
|    mean velocity x | -0.34      |
|    mean velocity y | 0.69       |
|    mean velocity z | 4.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 808000     |
-----------------------------------
Eval num_timesteps=808500, episode_reward=-65126.66 +/- 38108.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43143168 |
|    mean velocity x | -0.244      |
|    mean velocity y | 0.468       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.51e+04   |
| time/              |             |
|    total_timesteps | 808500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 395    |
|    time_elapsed    | 32685  |
|    total_timesteps | 808960 |
-------------------------------
Eval num_timesteps=809000, episode_reward=-91457.47 +/- 12641.43
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5469776    |
|    mean velocity x      | -0.249        |
|    mean velocity y      | 1.14          |
|    mean velocity z      | 4.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.15e+04     |
| time/                   |               |
|    total_timesteps      | 809000        |
| train/                  |               |
|    approx_kl            | 0.00020160858 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.001         |
|    loss                 | 3.27e+07      |
|    n_updates            | 3950          |
|    policy_gradient_loss | -0.00136      |
|    std                  | 1.55          |
|    value_loss           | 1.14e+08      |
-------------------------------------------
Eval num_timesteps=809500, episode_reward=-88904.19 +/- 49634.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45725372 |
|    mean velocity x | -0.356      |
|    mean velocity y | 0.574       |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.89e+04   |
| time/              |             |
|    total_timesteps | 809500      |
------------------------------------
Eval num_timesteps=810000, episode_reward=-81694.21 +/- 49653.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37668267 |
|    mean velocity x | -0.897      |
|    mean velocity y | 0.0033      |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.17e+04   |
| time/              |             |
|    total_timesteps | 810000      |
------------------------------------
Eval num_timesteps=810500, episode_reward=-93936.70 +/- 24039.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6872574 |
|    mean velocity x | -0.792     |
|    mean velocity y | 0.817      |
|    mean velocity z | 5.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.39e+04  |
| time/              |            |
|    total_timesteps | 810500     |
-----------------------------------
Eval num_timesteps=811000, episode_reward=-110832.77 +/- 25272.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4137276 |
|    mean velocity x | 0.214      |
|    mean velocity y | 0.841      |
|    mean velocity z | 3.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.11e+05  |
| time/              |            |
|    total_timesteps | 811000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 396    |
|    time_elapsed    | 32785  |
|    total_timesteps | 811008 |
-------------------------------
Eval num_timesteps=811500, episode_reward=-56573.38 +/- 50889.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49818888  |
|    mean velocity x      | -1.16        |
|    mean velocity y      | 0.362        |
|    mean velocity z      | 3.65         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.66e+04    |
| time/                   |              |
|    total_timesteps      | 811500       |
| train/                  |              |
|    approx_kl            | 0.0002289052 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.001        |
|    loss                 | 3.47e+07     |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00111     |
|    std                  | 1.55         |
|    value_loss           | 7.6e+07      |
------------------------------------------
Eval num_timesteps=812000, episode_reward=-76277.92 +/- 39675.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38399056 |
|    mean velocity x | -1.07       |
|    mean velocity y | 0.251       |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.63e+04   |
| time/              |             |
|    total_timesteps | 812000      |
------------------------------------
Eval num_timesteps=812500, episode_reward=-92087.01 +/- 47095.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36749431 |
|    mean velocity x | 0.029       |
|    mean velocity y | 0.43        |
|    mean velocity z | 1.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.21e+04   |
| time/              |             |
|    total_timesteps | 812500      |
------------------------------------
Eval num_timesteps=813000, episode_reward=-82654.51 +/- 24014.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49767736 |
|    mean velocity x | -0.587      |
|    mean velocity y | 0.769       |
|    mean velocity z | 1.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.27e+04   |
| time/              |             |
|    total_timesteps | 813000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 397    |
|    time_elapsed    | 32865  |
|    total_timesteps | 813056 |
-------------------------------
Eval num_timesteps=813500, episode_reward=-63888.11 +/- 24738.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2874759   |
|    mean velocity x      | -0.988       |
|    mean velocity y      | 0.166        |
|    mean velocity z      | 2.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.39e+04    |
| time/                   |              |
|    total_timesteps      | 813500       |
| train/                  |              |
|    approx_kl            | 0.0002554795 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.001        |
|    loss                 | 5.75e+06     |
|    n_updates            | 3970         |
|    policy_gradient_loss | -0.000706    |
|    std                  | 1.55         |
|    value_loss           | 1.93e+07     |
------------------------------------------
Eval num_timesteps=814000, episode_reward=-92272.04 +/- 20710.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40630752 |
|    mean velocity x | -1.06       |
|    mean velocity y | -0.236      |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 814000      |
------------------------------------
Eval num_timesteps=814500, episode_reward=-75803.81 +/- 39841.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3973631 |
|    mean velocity x | -1.51      |
|    mean velocity y | -0.0674    |
|    mean velocity z | 3.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.58e+04  |
| time/              |            |
|    total_timesteps | 814500     |
-----------------------------------
Eval num_timesteps=815000, episode_reward=-88444.30 +/- 42633.29
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.384166 |
|    mean velocity x | -0.0369   |
|    mean velocity y | 0.859     |
|    mean velocity z | 4.11      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.84e+04 |
| time/              |           |
|    total_timesteps | 815000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 398    |
|    time_elapsed    | 32945  |
|    total_timesteps | 815104 |
-------------------------------
Eval num_timesteps=815500, episode_reward=-110586.08 +/- 39181.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46453142   |
|    mean velocity x      | -0.269        |
|    mean velocity y      | 1.45          |
|    mean velocity z      | 4.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.11e+05     |
| time/                   |               |
|    total_timesteps      | 815500        |
| train/                  |               |
|    approx_kl            | 6.1041384e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 5.26e+07      |
|    n_updates            | 3980          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.55          |
|    value_loss           | 6.48e+07      |
-------------------------------------------
Eval num_timesteps=816000, episode_reward=-107257.62 +/- 26604.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2769705 |
|    mean velocity x | 0.136      |
|    mean velocity y | 0.369      |
|    mean velocity z | 3.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 816000     |
-----------------------------------
Eval num_timesteps=816500, episode_reward=-88671.57 +/- 45346.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2657507 |
|    mean velocity x | 0.00896    |
|    mean velocity y | 0.425      |
|    mean velocity z | 0.518      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.87e+04  |
| time/              |            |
|    total_timesteps | 816500     |
-----------------------------------
Eval num_timesteps=817000, episode_reward=-84014.96 +/- 3945.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5168229 |
|    mean velocity x | -0.21      |
|    mean velocity y | 1.34       |
|    mean velocity z | 4.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.4e+04   |
| time/              |            |
|    total_timesteps | 817000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 399    |
|    time_elapsed    | 33026  |
|    total_timesteps | 817152 |
-------------------------------
Eval num_timesteps=817500, episode_reward=-78527.46 +/- 42030.91
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5396693    |
|    mean velocity x      | -0.426        |
|    mean velocity y      | 1.21          |
|    mean velocity z      | 4.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.85e+04     |
| time/                   |               |
|    total_timesteps      | 817500        |
| train/                  |               |
|    approx_kl            | 7.3360716e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.001         |
|    loss                 | 2.19e+07      |
|    n_updates            | 3990          |
|    policy_gradient_loss | -0.000755     |
|    std                  | 1.55          |
|    value_loss           | 5.12e+07      |
-------------------------------------------
Eval num_timesteps=818000, episode_reward=-69402.34 +/- 44388.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52232224 |
|    mean velocity x | -0.643      |
|    mean velocity y | 0.794       |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.94e+04   |
| time/              |             |
|    total_timesteps | 818000      |
------------------------------------
Eval num_timesteps=818500, episode_reward=-89642.10 +/- 14738.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4926512 |
|    mean velocity x | -1.11      |
|    mean velocity y | 0.208      |
|    mean velocity z | 3.56       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.96e+04  |
| time/              |            |
|    total_timesteps | 818500     |
-----------------------------------
Eval num_timesteps=819000, episode_reward=-47712.55 +/- 48850.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32886583 |
|    mean velocity x | -2.09       |
|    mean velocity y | -1.46       |
|    mean velocity z | 6.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.77e+04   |
| time/              |             |
|    total_timesteps | 819000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 400    |
|    time_elapsed    | 33106  |
|    total_timesteps | 819200 |
-------------------------------
Eval num_timesteps=819500, episode_reward=-67267.95 +/- 37443.04
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.540566     |
|    mean velocity x      | -0.225        |
|    mean velocity y      | 1.45          |
|    mean velocity z      | 4.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.73e+04     |
| time/                   |               |
|    total_timesteps      | 819500        |
| train/                  |               |
|    approx_kl            | 4.3310865e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+07      |
|    n_updates            | 4000          |
|    policy_gradient_loss | -0.000458     |
|    std                  | 1.55          |
|    value_loss           | 4.86e+07      |
-------------------------------------------
Eval num_timesteps=820000, episode_reward=-80326.04 +/- 26841.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44383425 |
|    mean velocity x | -0.312      |
|    mean velocity y | 0.843       |
|    mean velocity z | 4.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.03e+04   |
| time/              |             |
|    total_timesteps | 820000      |
------------------------------------
Eval num_timesteps=820500, episode_reward=-94925.76 +/- 38185.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4436752 |
|    mean velocity x | -0.813     |
|    mean velocity y | -0.125     |
|    mean velocity z | 3.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.49e+04  |
| time/              |            |
|    total_timesteps | 820500     |
-----------------------------------
Eval num_timesteps=821000, episode_reward=-38221.35 +/- 37595.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45180225 |
|    mean velocity x | -0.287      |
|    mean velocity y | 0.719       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.82e+04   |
| time/              |             |
|    total_timesteps | 821000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 401    |
|    time_elapsed    | 33186  |
|    total_timesteps | 821248 |
-------------------------------
Eval num_timesteps=821500, episode_reward=-104081.02 +/- 34959.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34736052   |
|    mean velocity x      | 0.493         |
|    mean velocity y      | 0.915         |
|    mean velocity z      | 3.17          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.04e+05     |
| time/                   |               |
|    total_timesteps      | 821500        |
| train/                  |               |
|    approx_kl            | 8.1966165e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.122         |
|    learning_rate        | 0.001         |
|    loss                 | 2.73e+07      |
|    n_updates            | 4010          |
|    policy_gradient_loss | -0.000563     |
|    std                  | 1.55          |
|    value_loss           | 7.71e+07      |
-------------------------------------------
Eval num_timesteps=822000, episode_reward=-78228.23 +/- 22741.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59172606 |
|    mean velocity x | -1.17       |
|    mean velocity y | 0.509       |
|    mean velocity z | 4.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 822000      |
------------------------------------
Eval num_timesteps=822500, episode_reward=-51170.44 +/- 45518.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20053867 |
|    mean velocity x | -0.547      |
|    mean velocity y | 0.626       |
|    mean velocity z | 0.863       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.12e+04   |
| time/              |             |
|    total_timesteps | 822500      |
------------------------------------
Eval num_timesteps=823000, episode_reward=-88226.40 +/- 26748.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29376826 |
|    mean velocity x | -0.759      |
|    mean velocity y | -0.357      |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.82e+04   |
| time/              |             |
|    total_timesteps | 823000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 402    |
|    time_elapsed    | 33267  |
|    total_timesteps | 823296 |
-------------------------------
Eval num_timesteps=823500, episode_reward=-115445.83 +/- 20499.06
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46327496   |
|    mean velocity x      | -0.63         |
|    mean velocity y      | 1.09          |
|    mean velocity z      | 3.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.15e+05     |
| time/                   |               |
|    total_timesteps      | 823500        |
| train/                  |               |
|    approx_kl            | 9.2121045e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.205         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+07       |
|    n_updates            | 4020          |
|    policy_gradient_loss | -0.000878     |
|    std                  | 1.55          |
|    value_loss           | 3.15e+07      |
-------------------------------------------
Eval num_timesteps=824000, episode_reward=-74300.16 +/- 42749.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28669995 |
|    mean velocity x | -0.107      |
|    mean velocity y | 0.194       |
|    mean velocity z | 3.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 824000      |
------------------------------------
Eval num_timesteps=824500, episode_reward=-76078.42 +/- 41491.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37488544 |
|    mean velocity x | 0.0924      |
|    mean velocity y | 0.406       |
|    mean velocity z | 1.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 824500      |
------------------------------------
Eval num_timesteps=825000, episode_reward=-77271.95 +/- 40635.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55920315 |
|    mean velocity x | -0.585      |
|    mean velocity y | 0.76        |
|    mean velocity z | 3.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.73e+04   |
| time/              |             |
|    total_timesteps | 825000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 403    |
|    time_elapsed    | 33347  |
|    total_timesteps | 825344 |
-------------------------------
Eval num_timesteps=825500, episode_reward=-114401.15 +/- 11856.45
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.52782774   |
|    mean velocity x      | -0.45         |
|    mean velocity y      | 1.08          |
|    mean velocity z      | 4.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.14e+05     |
| time/                   |               |
|    total_timesteps      | 825500        |
| train/                  |               |
|    approx_kl            | 4.1269843e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.127         |
|    learning_rate        | 0.001         |
|    loss                 | 4.6e+07       |
|    n_updates            | 4030          |
|    policy_gradient_loss | -0.00038      |
|    std                  | 1.55          |
|    value_loss           | 5.79e+07      |
-------------------------------------------
Eval num_timesteps=826000, episode_reward=-69559.39 +/- 50050.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4768858 |
|    mean velocity x | -0.391     |
|    mean velocity y | 1.19       |
|    mean velocity z | 4.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.96e+04  |
| time/              |            |
|    total_timesteps | 826000     |
-----------------------------------
Eval num_timesteps=826500, episode_reward=-90972.93 +/- 8823.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47289458 |
|    mean velocity x | -0.123      |
|    mean velocity y | 1.47        |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 826500      |
------------------------------------
Eval num_timesteps=827000, episode_reward=-72466.26 +/- 45266.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31823218 |
|    mean velocity x | -0.308      |
|    mean velocity y | 0.0593      |
|    mean velocity z | 2.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.25e+04   |
| time/              |             |
|    total_timesteps | 827000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 404    |
|    time_elapsed    | 33427  |
|    total_timesteps | 827392 |
-------------------------------
Eval num_timesteps=827500, episode_reward=-67252.58 +/- 31931.00
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5607957   |
|    mean velocity x      | -0.466       |
|    mean velocity y      | 0.791        |
|    mean velocity z      | 4.79         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.73e+04    |
| time/                   |              |
|    total_timesteps      | 827500       |
| train/                  |              |
|    approx_kl            | 3.441982e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+07     |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.000726    |
|    std                  | 1.55         |
|    value_loss           | 7.73e+07     |
------------------------------------------
Eval num_timesteps=828000, episode_reward=-89716.87 +/- 28949.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42370608 |
|    mean velocity x | -0.299      |
|    mean velocity y | 0.96        |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 828000      |
------------------------------------
Eval num_timesteps=828500, episode_reward=-39426.55 +/- 43901.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5153989 |
|    mean velocity x | -0.639     |
|    mean velocity y | 1.06       |
|    mean velocity z | 3.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.94e+04  |
| time/              |            |
|    total_timesteps | 828500     |
-----------------------------------
Eval num_timesteps=829000, episode_reward=-80562.80 +/- 34536.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46158192 |
|    mean velocity x | -0.366      |
|    mean velocity y | 0.647       |
|    mean velocity z | 4.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 829000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 405    |
|    time_elapsed    | 33508  |
|    total_timesteps | 829440 |
-------------------------------
Eval num_timesteps=829500, episode_reward=-104508.80 +/- 32049.59
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39926922   |
|    mean velocity x      | -0.0123       |
|    mean velocity y      | 0.682         |
|    mean velocity z      | 3.91          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 829500        |
| train/                  |               |
|    approx_kl            | 0.00035237716 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.001         |
|    loss                 | 4.44e+07      |
|    n_updates            | 4050          |
|    policy_gradient_loss | -0.0013       |
|    std                  | 1.55          |
|    value_loss           | 9.38e+07      |
-------------------------------------------
Eval num_timesteps=830000, episode_reward=-61299.19 +/- 42721.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40724957 |
|    mean velocity x | -0.114      |
|    mean velocity y | 0.845       |
|    mean velocity z | 2.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.13e+04   |
| time/              |             |
|    total_timesteps | 830000      |
------------------------------------
Eval num_timesteps=830500, episode_reward=-83694.21 +/- 54401.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46444827 |
|    mean velocity x | -3.02       |
|    mean velocity y | -1.53       |
|    mean velocity z | 8.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 830500      |
------------------------------------
Eval num_timesteps=831000, episode_reward=-76464.56 +/- 35327.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3899167 |
|    mean velocity x | -0.346     |
|    mean velocity y | 0.505      |
|    mean velocity z | 1.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.65e+04  |
| time/              |            |
|    total_timesteps | 831000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 406    |
|    time_elapsed    | 33588  |
|    total_timesteps | 831488 |
-------------------------------
Eval num_timesteps=831500, episode_reward=-74654.17 +/- 28621.37
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4501352    |
|    mean velocity x      | -0.0464       |
|    mean velocity y      | 0.702         |
|    mean velocity z      | 3.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.47e+04     |
| time/                   |               |
|    total_timesteps      | 831500        |
| train/                  |               |
|    approx_kl            | 0.00010668085 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+07      |
|    n_updates            | 4060          |
|    policy_gradient_loss | -0.000865     |
|    std                  | 1.55          |
|    value_loss           | 3.14e+07      |
-------------------------------------------
Eval num_timesteps=832000, episode_reward=-56513.47 +/- 20086.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35123786 |
|    mean velocity x | -0.439      |
|    mean velocity y | 0.302       |
|    mean velocity z | 2.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.65e+04   |
| time/              |             |
|    total_timesteps | 832000      |
------------------------------------
Eval num_timesteps=832500, episode_reward=-97574.25 +/- 14851.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51509887 |
|    mean velocity x | -0.433      |
|    mean velocity y | 0.889       |
|    mean velocity z | 5.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.76e+04   |
| time/              |             |
|    total_timesteps | 832500      |
------------------------------------
Eval num_timesteps=833000, episode_reward=-92004.67 +/- 49616.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4047885 |
|    mean velocity x | 0.208      |
|    mean velocity y | 0.704      |
|    mean velocity z | 2.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.2e+04   |
| time/              |            |
|    total_timesteps | 833000     |
-----------------------------------
Eval num_timesteps=833500, episode_reward=-59613.11 +/- 39713.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37767658 |
|    mean velocity x | -0.00266    |
|    mean velocity y | 0.785       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.96e+04   |
| time/              |             |
|    total_timesteps | 833500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 407    |
|    time_elapsed    | 33687  |
|    total_timesteps | 833536 |
-------------------------------
Eval num_timesteps=834000, episode_reward=-72527.32 +/- 47522.48
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5204313   |
|    mean velocity x      | 0.439        |
|    mean velocity y      | 1.56         |
|    mean velocity z      | 3.42         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.25e+04    |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 4.944866e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.127        |
|    learning_rate        | 0.001        |
|    loss                 | 5e+07        |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.000685    |
|    std                  | 1.55         |
|    value_loss           | 7.25e+07     |
------------------------------------------
Eval num_timesteps=834500, episode_reward=-86881.85 +/- 24288.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5095833 |
|    mean velocity x | -0.312     |
|    mean velocity y | 1.31       |
|    mean velocity z | 4.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 834500     |
-----------------------------------
Eval num_timesteps=835000, episode_reward=-79030.39 +/- 38537.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48380834 |
|    mean velocity x | -0.48       |
|    mean velocity y | 0.478       |
|    mean velocity z | 4.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 835000      |
------------------------------------
Eval num_timesteps=835500, episode_reward=-69298.89 +/- 52344.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47280493 |
|    mean velocity x | -0.351      |
|    mean velocity y | 1.11        |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.93e+04   |
| time/              |             |
|    total_timesteps | 835500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 408    |
|    time_elapsed    | 33768  |
|    total_timesteps | 835584 |
-------------------------------
Eval num_timesteps=836000, episode_reward=-110213.97 +/- 32545.45
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5516075    |
|    mean velocity x      | -0.507        |
|    mean velocity y      | 0.75          |
|    mean velocity z      | 5.06          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.1e+05      |
| time/                   |               |
|    total_timesteps      | 836000        |
| train/                  |               |
|    approx_kl            | 1.4659774e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.116         |
|    learning_rate        | 0.001         |
|    loss                 | 8.97e+07      |
|    n_updates            | 4080          |
|    policy_gradient_loss | -0.000361     |
|    std                  | 1.55          |
|    value_loss           | 1.22e+08      |
-------------------------------------------
Eval num_timesteps=836500, episode_reward=-60409.73 +/- 39606.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4315356 |
|    mean velocity x | -0.313     |
|    mean velocity y | 0.591      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.04e+04  |
| time/              |            |
|    total_timesteps | 836500     |
-----------------------------------
Eval num_timesteps=837000, episode_reward=-64946.29 +/- 45738.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36982465 |
|    mean velocity x | -0.755      |
|    mean velocity y | 0.295       |
|    mean velocity z | 2.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.49e+04   |
| time/              |             |
|    total_timesteps | 837000      |
------------------------------------
Eval num_timesteps=837500, episode_reward=-68446.60 +/- 45232.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5004767 |
|    mean velocity x | -1.19      |
|    mean velocity y | 0.421      |
|    mean velocity z | 3.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.84e+04  |
| time/              |            |
|    total_timesteps | 837500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 409    |
|    time_elapsed    | 33848  |
|    total_timesteps | 837632 |
-------------------------------
Eval num_timesteps=838000, episode_reward=-79571.30 +/- 50038.39
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.49725103    |
|    mean velocity x      | -0.414         |
|    mean velocity y      | 1.12           |
|    mean velocity z      | 4.64           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -7.96e+04      |
| time/                   |                |
|    total_timesteps      | 838000         |
| train/                  |                |
|    approx_kl            | 1.31171255e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.104          |
|    learning_rate        | 0.001          |
|    loss                 | 2.46e+07       |
|    n_updates            | 4090           |
|    policy_gradient_loss | -0.000276      |
|    std                  | 1.55           |
|    value_loss           | 6.46e+07       |
--------------------------------------------
Eval num_timesteps=838500, episode_reward=-97673.56 +/- 30645.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42459622 |
|    mean velocity x | -1.05       |
|    mean velocity y | 0.242       |
|    mean velocity z | 2.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.77e+04   |
| time/              |             |
|    total_timesteps | 838500      |
------------------------------------
Eval num_timesteps=839000, episode_reward=-98637.79 +/- 44511.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56091654 |
|    mean velocity x | -0.959      |
|    mean velocity y | 0.497       |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.86e+04   |
| time/              |             |
|    total_timesteps | 839000      |
------------------------------------
Eval num_timesteps=839500, episode_reward=-93497.05 +/- 22422.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56348175 |
|    mean velocity x | -0.412      |
|    mean velocity y | 0.706       |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.35e+04   |
| time/              |             |
|    total_timesteps | 839500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 410    |
|    time_elapsed    | 33928  |
|    total_timesteps | 839680 |
-------------------------------
Eval num_timesteps=840000, episode_reward=-66700.30 +/- 59607.23
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6411769   |
|    mean velocity x      | -0.483       |
|    mean velocity y      | 1.1          |
|    mean velocity z      | 5.47         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.67e+04    |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 7.076273e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+07     |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.000566    |
|    std                  | 1.55         |
|    value_loss           | 7.06e+07     |
------------------------------------------
Eval num_timesteps=840500, episode_reward=-74953.70 +/- 30519.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33509523 |
|    mean velocity x | -0.263      |
|    mean velocity y | 0.609       |
|    mean velocity z | 0.622       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.5e+04    |
| time/              |             |
|    total_timesteps | 840500      |
------------------------------------
Eval num_timesteps=841000, episode_reward=-95914.53 +/- 53741.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39375702 |
|    mean velocity x | -0.297      |
|    mean velocity y | 0.741       |
|    mean velocity z | 3.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.59e+04   |
| time/              |             |
|    total_timesteps | 841000      |
------------------------------------
Eval num_timesteps=841500, episode_reward=-93772.71 +/- 20219.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53900564 |
|    mean velocity x | -0.458      |
|    mean velocity y | 0.768       |
|    mean velocity z | 4.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.38e+04   |
| time/              |             |
|    total_timesteps | 841500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 411    |
|    time_elapsed    | 34008  |
|    total_timesteps | 841728 |
-------------------------------
Eval num_timesteps=842000, episode_reward=-82031.84 +/- 42399.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40919346  |
|    mean velocity x      | -0.393       |
|    mean velocity y      | 0.698        |
|    mean velocity z      | 0.758        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.2e+04     |
| time/                   |              |
|    total_timesteps      | 842000       |
| train/                  |              |
|    approx_kl            | 2.606734e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.137        |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+07     |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.000543    |
|    std                  | 1.55         |
|    value_loss           | 4.47e+07     |
------------------------------------------
Eval num_timesteps=842500, episode_reward=-88410.90 +/- 31713.94
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.511076 |
|    mean velocity x | -0.42     |
|    mean velocity y | 0.768     |
|    mean velocity z | 2.69      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.84e+04 |
| time/              |           |
|    total_timesteps | 842500    |
----------------------------------
Eval num_timesteps=843000, episode_reward=-100423.20 +/- 21181.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5195742 |
|    mean velocity x | -0.154     |
|    mean velocity y | 1.67       |
|    mean velocity z | 4.21       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+05     |
| time/              |            |
|    total_timesteps | 843000     |
-----------------------------------
Eval num_timesteps=843500, episode_reward=-93027.10 +/- 31025.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5541048 |
|    mean velocity x | -0.432     |
|    mean velocity y | 0.774      |
|    mean velocity z | 5.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.3e+04   |
| time/              |            |
|    total_timesteps | 843500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 412    |
|    time_elapsed    | 34089  |
|    total_timesteps | 843776 |
-------------------------------
Eval num_timesteps=844000, episode_reward=-87760.48 +/- 34350.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.56943756   |
|    mean velocity x      | -0.169        |
|    mean velocity y      | 1.67          |
|    mean velocity z      | 4.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.78e+04     |
| time/                   |               |
|    total_timesteps      | 844000        |
| train/                  |               |
|    approx_kl            | 5.8021455e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.143         |
|    learning_rate        | 0.001         |
|    loss                 | 5.8e+07       |
|    n_updates            | 4120          |
|    policy_gradient_loss | -0.000576     |
|    std                  | 1.55          |
|    value_loss           | 7.09e+07      |
-------------------------------------------
Eval num_timesteps=844500, episode_reward=-89947.80 +/- 30577.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44951817 |
|    mean velocity x | 0.258       |
|    mean velocity y | 1.75        |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.99e+04   |
| time/              |             |
|    total_timesteps | 844500      |
------------------------------------
Eval num_timesteps=845000, episode_reward=-88685.90 +/- 39891.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31628925 |
|    mean velocity x | -0.117      |
|    mean velocity y | 0.497       |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.87e+04   |
| time/              |             |
|    total_timesteps | 845000      |
------------------------------------
Eval num_timesteps=845500, episode_reward=-97016.78 +/- 27766.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40924427 |
|    mean velocity x | -0.423      |
|    mean velocity y | 0.301       |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.7e+04    |
| time/              |             |
|    total_timesteps | 845500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 413    |
|    time_elapsed    | 34169  |
|    total_timesteps | 845824 |
-------------------------------
Eval num_timesteps=846000, episode_reward=-86083.02 +/- 44325.20
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46220502  |
|    mean velocity x      | -0.795       |
|    mean velocity y      | -0.0477      |
|    mean velocity z      | 3.92         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.61e+04    |
| time/                   |              |
|    total_timesteps      | 846000       |
| train/                  |              |
|    approx_kl            | 8.408472e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+07     |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.000535    |
|    std                  | 1.55         |
|    value_loss           | 5.55e+07     |
------------------------------------------
Eval num_timesteps=846500, episode_reward=-58176.73 +/- 36447.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43195957 |
|    mean velocity x | -0.659      |
|    mean velocity y | -0.0441     |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.82e+04   |
| time/              |             |
|    total_timesteps | 846500      |
------------------------------------
Eval num_timesteps=847000, episode_reward=-47852.13 +/- 31404.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3912023 |
|    mean velocity x | -0.11      |
|    mean velocity y | 1.21       |
|    mean velocity z | 4.23       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.79e+04  |
| time/              |            |
|    total_timesteps | 847000     |
-----------------------------------
Eval num_timesteps=847500, episode_reward=-63319.95 +/- 50452.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5040496 |
|    mean velocity x | -0.192     |
|    mean velocity y | 1.45       |
|    mean velocity z | 4.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.33e+04  |
| time/              |            |
|    total_timesteps | 847500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 414    |
|    time_elapsed    | 34249  |
|    total_timesteps | 847872 |
-------------------------------
Eval num_timesteps=848000, episode_reward=-70922.02 +/- 54991.24
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4845846    |
|    mean velocity x      | 0.0187        |
|    mean velocity y      | 1.54          |
|    mean velocity z      | 3.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.09e+04     |
| time/                   |               |
|    total_timesteps      | 848000        |
| train/                  |               |
|    approx_kl            | 9.8777906e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 1.99e+07      |
|    n_updates            | 4140          |
|    policy_gradient_loss | -0.000711     |
|    std                  | 1.55          |
|    value_loss           | 7.81e+07      |
-------------------------------------------
Eval num_timesteps=848500, episode_reward=-73214.61 +/- 23126.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3959544 |
|    mean velocity x | 0.0531     |
|    mean velocity y | 1.56       |
|    mean velocity z | 4.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.32e+04  |
| time/              |            |
|    total_timesteps | 848500     |
-----------------------------------
Eval num_timesteps=849000, episode_reward=-93944.71 +/- 59963.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41161343 |
|    mean velocity x | -1.24       |
|    mean velocity y | -0.404      |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.39e+04   |
| time/              |             |
|    total_timesteps | 849000      |
------------------------------------
Eval num_timesteps=849500, episode_reward=-116300.38 +/- 18166.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5805191 |
|    mean velocity x | -0.364     |
|    mean velocity y | 1.59       |
|    mean velocity z | 4.44       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.16e+05  |
| time/              |            |
|    total_timesteps | 849500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 415    |
|    time_elapsed    | 34330  |
|    total_timesteps | 849920 |
-------------------------------
Eval num_timesteps=850000, episode_reward=-78294.12 +/- 47154.95
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34390834  |
|    mean velocity x      | -0.442       |
|    mean velocity y      | 0.701        |
|    mean velocity z      | 0.974        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.83e+04    |
| time/                   |              |
|    total_timesteps      | 850000       |
| train/                  |              |
|    approx_kl            | 3.757543e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.001        |
|    loss                 | 3.3e+07      |
|    n_updates            | 4150         |
|    policy_gradient_loss | -0.000876    |
|    std                  | 1.55         |
|    value_loss           | 5.54e+07     |
------------------------------------------
Eval num_timesteps=850500, episode_reward=-78057.51 +/- 39696.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5113993 |
|    mean velocity x | -1.01      |
|    mean velocity y | 0.0459     |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 850500     |
-----------------------------------
Eval num_timesteps=851000, episode_reward=-101279.56 +/- 19326.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51797765 |
|    mean velocity x | -0.438      |
|    mean velocity y | 0.966       |
|    mean velocity z | 4.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 851000      |
------------------------------------
Eval num_timesteps=851500, episode_reward=-79975.60 +/- 57036.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2567464 |
|    mean velocity x | -1.55      |
|    mean velocity y | -0.72      |
|    mean velocity z | 4.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8e+04     |
| time/              |            |
|    total_timesteps | 851500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 416    |
|    time_elapsed    | 34410  |
|    total_timesteps | 851968 |
-------------------------------
Eval num_timesteps=852000, episode_reward=-61535.28 +/- 51417.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30362108   |
|    mean velocity x      | -0.683        |
|    mean velocity y      | 0.613         |
|    mean velocity z      | 2.54          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.15e+04     |
| time/                   |               |
|    total_timesteps      | 852000        |
| train/                  |               |
|    approx_kl            | 4.0700746e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.001         |
|    loss                 | 3e+07         |
|    n_updates            | 4160          |
|    policy_gradient_loss | -0.000418     |
|    std                  | 1.55          |
|    value_loss           | 4.45e+07      |
-------------------------------------------
Eval num_timesteps=852500, episode_reward=-78908.17 +/- 47239.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6403153 |
|    mean velocity x | -0.35      |
|    mean velocity y | 1.46       |
|    mean velocity z | 4.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 852500     |
-----------------------------------
Eval num_timesteps=853000, episode_reward=-15052.62 +/- 11359.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19971293 |
|    mean velocity x | -0.132      |
|    mean velocity y | 0.497       |
|    mean velocity z | 0.406       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.51e+04   |
| time/              |             |
|    total_timesteps | 853000      |
------------------------------------
New best mean reward!
Eval num_timesteps=853500, episode_reward=-70257.24 +/- 51881.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29448017 |
|    mean velocity x | -0.846      |
|    mean velocity y | 0.318       |
|    mean velocity z | 2.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.03e+04   |
| time/              |             |
|    total_timesteps | 853500      |
------------------------------------
Eval num_timesteps=854000, episode_reward=-59554.09 +/- 20497.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35332286 |
|    mean velocity x | -1.7        |
|    mean velocity y | -0.28       |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.96e+04   |
| time/              |             |
|    total_timesteps | 854000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 417    |
|    time_elapsed    | 34510  |
|    total_timesteps | 854016 |
-------------------------------
Eval num_timesteps=854500, episode_reward=-99850.49 +/- 31080.72
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6783766    |
|    mean velocity x      | 0.0961        |
|    mean velocity y      | 1.86          |
|    mean velocity z      | 7.14          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.99e+04     |
| time/                   |               |
|    total_timesteps      | 854500        |
| train/                  |               |
|    approx_kl            | 2.6245572e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.001         |
|    loss                 | 7.21e+06      |
|    n_updates            | 4170          |
|    policy_gradient_loss | -0.000426     |
|    std                  | 1.56          |
|    value_loss           | 3.89e+07      |
-------------------------------------------
Eval num_timesteps=855000, episode_reward=-73074.38 +/- 21253.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39949384 |
|    mean velocity x | -0.0517     |
|    mean velocity y | 1.12        |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.31e+04   |
| time/              |             |
|    total_timesteps | 855000      |
------------------------------------
Eval num_timesteps=855500, episode_reward=-93317.15 +/- 21341.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5468057 |
|    mean velocity x | -0.444     |
|    mean velocity y | 0.747      |
|    mean velocity z | 4.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.33e+04  |
| time/              |            |
|    total_timesteps | 855500     |
-----------------------------------
Eval num_timesteps=856000, episode_reward=-118620.99 +/- 9772.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4374289 |
|    mean velocity x | -0.339     |
|    mean velocity y | 0.88       |
|    mean velocity z | 4.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.19e+05  |
| time/              |            |
|    total_timesteps | 856000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 418    |
|    time_elapsed    | 34590  |
|    total_timesteps | 856064 |
-------------------------------
Eval num_timesteps=856500, episode_reward=-66909.75 +/- 50651.05
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.3186726     |
|    mean velocity x      | -0.639         |
|    mean velocity y      | 0.352          |
|    mean velocity z      | 2.52           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -6.69e+04      |
| time/                   |                |
|    total_timesteps      | 856500         |
| train/                  |                |
|    approx_kl            | 0.000118951575 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.152          |
|    learning_rate        | 0.001          |
|    loss                 | 1.08e+07       |
|    n_updates            | 4180           |
|    policy_gradient_loss | -0.000731      |
|    std                  | 1.55           |
|    value_loss           | 9.27e+07       |
--------------------------------------------
Eval num_timesteps=857000, episode_reward=-33494.34 +/- 24686.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58568615 |
|    mean velocity x | -0.496      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.35e+04   |
| time/              |             |
|    total_timesteps | 857000      |
------------------------------------
Eval num_timesteps=857500, episode_reward=-86001.62 +/- 32355.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35736373 |
|    mean velocity x | -0.117      |
|    mean velocity y | 0.398       |
|    mean velocity z | 1.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.6e+04    |
| time/              |             |
|    total_timesteps | 857500      |
------------------------------------
Eval num_timesteps=858000, episode_reward=-102295.99 +/- 35363.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3383903 |
|    mean velocity x | 0.0316     |
|    mean velocity y | 0.538      |
|    mean velocity z | 3.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 858000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 419    |
|    time_elapsed    | 34670  |
|    total_timesteps | 858112 |
-------------------------------
Eval num_timesteps=858500, episode_reward=-73864.69 +/- 52298.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25873956   |
|    mean velocity x      | -0.122        |
|    mean velocity y      | 0.455         |
|    mean velocity z      | 0.446         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.39e+04     |
| time/                   |               |
|    total_timesteps      | 858500        |
| train/                  |               |
|    approx_kl            | 0.00017499391 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.001         |
|    loss                 | 2.14e+07      |
|    n_updates            | 4190          |
|    policy_gradient_loss | -0.00133      |
|    std                  | 1.55          |
|    value_loss           | 4.95e+07      |
-------------------------------------------
Eval num_timesteps=859000, episode_reward=-112350.02 +/- 11507.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4689522 |
|    mean velocity x | 0.255      |
|    mean velocity y | 0.904      |
|    mean velocity z | 2.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 859000     |
-----------------------------------
Eval num_timesteps=859500, episode_reward=-66732.99 +/- 45618.85
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5003684 |
|    mean velocity x | -0.391     |
|    mean velocity y | 0.89       |
|    mean velocity z | 4.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.67e+04  |
| time/              |            |
|    total_timesteps | 859500     |
-----------------------------------
Eval num_timesteps=860000, episode_reward=-71249.10 +/- 47520.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4495994 |
|    mean velocity x | -0.756     |
|    mean velocity y | 0.407      |
|    mean velocity z | 3.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.12e+04  |
| time/              |            |
|    total_timesteps | 860000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 420    |
|    time_elapsed    | 34751  |
|    total_timesteps | 860160 |
-------------------------------
Eval num_timesteps=860500, episode_reward=-59800.58 +/- 51150.55
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36685017   |
|    mean velocity x      | -0.54         |
|    mean velocity y      | 0.244         |
|    mean velocity z      | 3.59          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.98e+04     |
| time/                   |               |
|    total_timesteps      | 860500        |
| train/                  |               |
|    approx_kl            | 7.7342585e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 4.2e+07       |
|    n_updates            | 4200          |
|    policy_gradient_loss | -0.000687     |
|    std                  | 1.55          |
|    value_loss           | 6.28e+07      |
-------------------------------------------
Eval num_timesteps=861000, episode_reward=-85100.76 +/- 27109.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36366835 |
|    mean velocity x | 0.0859      |
|    mean velocity y | 0.961       |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.51e+04   |
| time/              |             |
|    total_timesteps | 861000      |
------------------------------------
Eval num_timesteps=861500, episode_reward=-74518.73 +/- 49961.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46038342 |
|    mean velocity x | -0.33       |
|    mean velocity y | 0.582       |
|    mean velocity z | 3.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.45e+04   |
| time/              |             |
|    total_timesteps | 861500      |
------------------------------------
Eval num_timesteps=862000, episode_reward=-53387.62 +/- 28043.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5671847 |
|    mean velocity x | -0.522     |
|    mean velocity y | 0.894      |
|    mean velocity z | 3.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.34e+04  |
| time/              |            |
|    total_timesteps | 862000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 421    |
|    time_elapsed    | 34831  |
|    total_timesteps | 862208 |
-------------------------------
Eval num_timesteps=862500, episode_reward=-93599.39 +/- 49046.39
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3154725   |
|    mean velocity x      | -0.257       |
|    mean velocity y      | 0.269        |
|    mean velocity z      | 2.15         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.36e+04    |
| time/                   |              |
|    total_timesteps      | 862500       |
| train/                  |              |
|    approx_kl            | 8.208386e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+07     |
|    n_updates            | 4210         |
|    policy_gradient_loss | -0.000472    |
|    std                  | 1.55         |
|    value_loss           | 4.6e+07      |
------------------------------------------
Eval num_timesteps=863000, episode_reward=-73549.44 +/- 48679.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4376674 |
|    mean velocity x | -0.283     |
|    mean velocity y | 0.88       |
|    mean velocity z | 4.15       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.35e+04  |
| time/              |            |
|    total_timesteps | 863000     |
-----------------------------------
Eval num_timesteps=863500, episode_reward=-76786.36 +/- 39722.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5997137 |
|    mean velocity x | 0.182      |
|    mean velocity y | 1.71       |
|    mean velocity z | 3.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.68e+04  |
| time/              |            |
|    total_timesteps | 863500     |
-----------------------------------
Eval num_timesteps=864000, episode_reward=-90285.94 +/- 51847.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37772477 |
|    mean velocity x | -0.45       |
|    mean velocity y | 0.563       |
|    mean velocity z | 0.904       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 864000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 422    |
|    time_elapsed    | 34911  |
|    total_timesteps | 864256 |
-------------------------------
Eval num_timesteps=864500, episode_reward=-63807.66 +/- 54097.04
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44601116  |
|    mean velocity x      | -0.687       |
|    mean velocity y      | -0.0117      |
|    mean velocity z      | 4.02         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.38e+04    |
| time/                   |              |
|    total_timesteps      | 864500       |
| train/                  |              |
|    approx_kl            | 5.999161e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.001        |
|    loss                 | 3.46e+06     |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.000991    |
|    std                  | 1.55         |
|    value_loss           | 5.17e+07     |
------------------------------------------
Eval num_timesteps=865000, episode_reward=-125039.09 +/- 18038.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62616795 |
|    mean velocity x | -0.45       |
|    mean velocity y | 1.36        |
|    mean velocity z | 4.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.25e+05   |
| time/              |             |
|    total_timesteps | 865000      |
------------------------------------
Eval num_timesteps=865500, episode_reward=-107676.61 +/- 18593.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5625502 |
|    mean velocity x | -0.505     |
|    mean velocity y | 0.883      |
|    mean velocity z | 4.37       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 865500     |
-----------------------------------
Eval num_timesteps=866000, episode_reward=-106956.18 +/- 20723.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32019144 |
|    mean velocity x | -1.2        |
|    mean velocity y | -0.134      |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 866000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 423    |
|    time_elapsed    | 34998  |
|    total_timesteps | 866304 |
-------------------------------
Eval num_timesteps=866500, episode_reward=-97868.19 +/- 19799.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5552741    |
|    mean velocity x      | 0.226         |
|    mean velocity y      | 1.35          |
|    mean velocity z      | 3.9           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.79e+04     |
| time/                   |               |
|    total_timesteps      | 866500        |
| train/                  |               |
|    approx_kl            | 0.00040836557 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+07      |
|    n_updates            | 4230          |
|    policy_gradient_loss | -0.00152      |
|    std                  | 1.55          |
|    value_loss           | 6.8e+07       |
-------------------------------------------
Eval num_timesteps=867000, episode_reward=-94896.16 +/- 47010.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3777418 |
|    mean velocity x | -0.421     |
|    mean velocity y | 0.642      |
|    mean velocity z | 3.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.49e+04  |
| time/              |            |
|    total_timesteps | 867000     |
-----------------------------------
Eval num_timesteps=867500, episode_reward=-85689.10 +/- 33332.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33872154 |
|    mean velocity x | -0.118      |
|    mean velocity y | 0.432       |
|    mean velocity z | 1.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.57e+04   |
| time/              |             |
|    total_timesteps | 867500      |
------------------------------------
Eval num_timesteps=868000, episode_reward=-89593.38 +/- 31193.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47705537 |
|    mean velocity x | -0.556      |
|    mean velocity y | 0.722       |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 868000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 424    |
|    time_elapsed    | 35078  |
|    total_timesteps | 868352 |
-------------------------------
Eval num_timesteps=868500, episode_reward=-46360.17 +/- 51840.14
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4287285    |
|    mean velocity x      | -0.27         |
|    mean velocity y      | 1.04          |
|    mean velocity z      | 4.29          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.64e+04     |
| time/                   |               |
|    total_timesteps      | 868500        |
| train/                  |               |
|    approx_kl            | 0.00026511046 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.001         |
|    loss                 | 2.28e+07      |
|    n_updates            | 4240          |
|    policy_gradient_loss | -0.00139      |
|    std                  | 1.55          |
|    value_loss           | 4.33e+07      |
-------------------------------------------
Eval num_timesteps=869000, episode_reward=-112206.38 +/- 51883.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5287195 |
|    mean velocity x | -0.41      |
|    mean velocity y | 1          |
|    mean velocity z | 4.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 869000     |
-----------------------------------
Eval num_timesteps=869500, episode_reward=-95863.72 +/- 52791.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43941513 |
|    mean velocity x | 0.198       |
|    mean velocity y | 1.17        |
|    mean velocity z | 3.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.59e+04   |
| time/              |             |
|    total_timesteps | 869500      |
------------------------------------
Eval num_timesteps=870000, episode_reward=-78956.64 +/- 55656.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39776453 |
|    mean velocity x | -0.902      |
|    mean velocity y | 0.19        |
|    mean velocity z | 3.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 870000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 425    |
|    time_elapsed    | 35158  |
|    total_timesteps | 870400 |
-------------------------------
Eval num_timesteps=870500, episode_reward=-109600.55 +/- 27422.63
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41296774  |
|    mean velocity x      | -0.678       |
|    mean velocity y      | 0.418        |
|    mean velocity z      | 3.75         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.1e+05     |
| time/                   |              |
|    total_timesteps      | 870500       |
| train/                  |              |
|    approx_kl            | 3.610665e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.001        |
|    loss                 | 2.57e+07     |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.000404    |
|    std                  | 1.55         |
|    value_loss           | 6.04e+07     |
------------------------------------------
Eval num_timesteps=871000, episode_reward=-100570.48 +/- 27865.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5650945 |
|    mean velocity x | -1.03      |
|    mean velocity y | 0.529      |
|    mean velocity z | 4.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 871000     |
-----------------------------------
Eval num_timesteps=871500, episode_reward=-82076.73 +/- 32258.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38265193 |
|    mean velocity x | -1.32       |
|    mean velocity y | 0.108       |
|    mean velocity z | 3.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.21e+04   |
| time/              |             |
|    total_timesteps | 871500      |
------------------------------------
Eval num_timesteps=872000, episode_reward=-65962.05 +/- 52525.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4741755 |
|    mean velocity x | -0.429     |
|    mean velocity y | 0.491      |
|    mean velocity z | 4.56       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.6e+04   |
| time/              |            |
|    total_timesteps | 872000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 426    |
|    time_elapsed    | 35238  |
|    total_timesteps | 872448 |
-------------------------------
Eval num_timesteps=872500, episode_reward=-100128.07 +/- 24468.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24568152   |
|    mean velocity x      | -0.442        |
|    mean velocity y      | 0.813         |
|    mean velocity z      | 0.754         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1e+05        |
| time/                   |               |
|    total_timesteps      | 872500        |
| train/                  |               |
|    approx_kl            | 5.3403608e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.001         |
|    loss                 | 2.92e+07      |
|    n_updates            | 4260          |
|    policy_gradient_loss | -0.000908     |
|    std                  | 1.55          |
|    value_loss           | 4.1e+07       |
-------------------------------------------
Eval num_timesteps=873000, episode_reward=-113689.41 +/- 28454.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5061816 |
|    mean velocity x | -0.47      |
|    mean velocity y | 0.821      |
|    mean velocity z | 4.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.14e+05  |
| time/              |            |
|    total_timesteps | 873000     |
-----------------------------------
Eval num_timesteps=873500, episode_reward=-75098.66 +/- 35894.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3695521 |
|    mean velocity x | -0.395     |
|    mean velocity y | 0.56       |
|    mean velocity z | 4.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.51e+04  |
| time/              |            |
|    total_timesteps | 873500     |
-----------------------------------
Eval num_timesteps=874000, episode_reward=-22892.04 +/- 22492.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5465778 |
|    mean velocity x | -0.542     |
|    mean velocity y | 0.88       |
|    mean velocity z | 4.85       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.29e+04  |
| time/              |            |
|    total_timesteps | 874000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 427    |
|    time_elapsed    | 35319  |
|    total_timesteps | 874496 |
-------------------------------
Eval num_timesteps=874500, episode_reward=-82879.81 +/- 29280.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.58994234   |
|    mean velocity x      | 0.314         |
|    mean velocity y      | 1.6           |
|    mean velocity z      | 3.68          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.29e+04     |
| time/                   |               |
|    total_timesteps      | 874500        |
| train/                  |               |
|    approx_kl            | 7.4981945e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.001         |
|    loss                 | 7.98e+07      |
|    n_updates            | 4270          |
|    policy_gradient_loss | -0.000188     |
|    std                  | 1.55          |
|    value_loss           | 1.02e+08      |
-------------------------------------------
Eval num_timesteps=875000, episode_reward=-69317.59 +/- 41778.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39571446 |
|    mean velocity x | -1.1        |
|    mean velocity y | 0.284       |
|    mean velocity z | 3.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.93e+04   |
| time/              |             |
|    total_timesteps | 875000      |
------------------------------------
Eval num_timesteps=875500, episode_reward=-89679.12 +/- 27745.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16876648 |
|    mean velocity x | -0.177      |
|    mean velocity y | 0.259       |
|    mean velocity z | 0.378       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 875500      |
------------------------------------
Eval num_timesteps=876000, episode_reward=-64845.69 +/- 37649.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57739794 |
|    mean velocity x | -0.776      |
|    mean velocity y | 0.967       |
|    mean velocity z | 4.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 876000      |
------------------------------------
Eval num_timesteps=876500, episode_reward=-112374.61 +/- 22114.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43977684 |
|    mean velocity x | -0.829      |
|    mean velocity y | 0.0152      |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 876500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 428    |
|    time_elapsed    | 35418  |
|    total_timesteps | 876544 |
-------------------------------
Eval num_timesteps=877000, episode_reward=-78016.73 +/- 34639.11
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4818489    |
|    mean velocity x      | -0.57         |
|    mean velocity y      | 0.276         |
|    mean velocity z      | 4.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.8e+04      |
| time/                   |               |
|    total_timesteps      | 877000        |
| train/                  |               |
|    approx_kl            | 5.4584147e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.223         |
|    learning_rate        | 0.001         |
|    loss                 | 3.03e+07      |
|    n_updates            | 4280          |
|    policy_gradient_loss | -0.000503     |
|    std                  | 1.55          |
|    value_loss           | 4.45e+07      |
-------------------------------------------
Eval num_timesteps=877500, episode_reward=-105825.61 +/- 24873.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47594568 |
|    mean velocity x | 0.354       |
|    mean velocity y | 1.3         |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 877500      |
------------------------------------
Eval num_timesteps=878000, episode_reward=-96567.66 +/- 50578.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2836807 |
|    mean velocity x | -0.0793    |
|    mean velocity y | 0.844      |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.66e+04  |
| time/              |            |
|    total_timesteps | 878000     |
-----------------------------------
Eval num_timesteps=878500, episode_reward=-85113.50 +/- 35952.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3012622 |
|    mean velocity x | -0.668     |
|    mean velocity y | -0.208     |
|    mean velocity z | 3.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.51e+04  |
| time/              |            |
|    total_timesteps | 878500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 429    |
|    time_elapsed    | 35498  |
|    total_timesteps | 878592 |
-------------------------------
Eval num_timesteps=879000, episode_reward=-117318.92 +/- 28953.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38629165   |
|    mean velocity x      | -0.365        |
|    mean velocity y      | 0.507         |
|    mean velocity z      | 2.28          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.17e+05     |
| time/                   |               |
|    total_timesteps      | 879000        |
| train/                  |               |
|    approx_kl            | 0.00011921022 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.001         |
|    loss                 | 4.28e+07      |
|    n_updates            | 4290          |
|    policy_gradient_loss | -0.000635     |
|    std                  | 1.55          |
|    value_loss           | 5.62e+07      |
-------------------------------------------
Eval num_timesteps=879500, episode_reward=-112067.11 +/- 25735.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47616538 |
|    mean velocity x | -0.325      |
|    mean velocity y | 0.837       |
|    mean velocity z | 2.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 879500      |
------------------------------------
Eval num_timesteps=880000, episode_reward=-73132.89 +/- 52163.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5322714 |
|    mean velocity x | -0.519     |
|    mean velocity y | 0.931      |
|    mean velocity z | 5.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 880000     |
-----------------------------------
Eval num_timesteps=880500, episode_reward=-105909.92 +/- 33171.93
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.563571 |
|    mean velocity x | -0.59     |
|    mean velocity y | 0.818     |
|    mean velocity z | 5.05      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.06e+05 |
| time/              |           |
|    total_timesteps | 880500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 430    |
|    time_elapsed    | 35579  |
|    total_timesteps | 880640 |
-------------------------------
Eval num_timesteps=881000, episode_reward=-112060.20 +/- 41654.88
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.51734596   |
|    mean velocity x      | -0.314        |
|    mean velocity y      | 0.734         |
|    mean velocity z      | 4.03          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.12e+05     |
| time/                   |               |
|    total_timesteps      | 881000        |
| train/                  |               |
|    approx_kl            | 0.00021224725 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.001         |
|    loss                 | 3.48e+07      |
|    n_updates            | 4300          |
|    policy_gradient_loss | -0.00106      |
|    std                  | 1.55          |
|    value_loss           | 9.02e+07      |
-------------------------------------------
Eval num_timesteps=881500, episode_reward=-34593.57 +/- 34937.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4073998 |
|    mean velocity x | -0.857     |
|    mean velocity y | -0.284     |
|    mean velocity z | 3.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.46e+04  |
| time/              |            |
|    total_timesteps | 881500     |
-----------------------------------
Eval num_timesteps=882000, episode_reward=-80851.12 +/- 34481.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5165066 |
|    mean velocity x | 0.221      |
|    mean velocity y | 1.67       |
|    mean velocity z | 3.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.09e+04  |
| time/              |            |
|    total_timesteps | 882000     |
-----------------------------------
Eval num_timesteps=882500, episode_reward=-110054.83 +/- 21980.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3516584 |
|    mean velocity x | -1.75      |
|    mean velocity y | -0.372     |
|    mean velocity z | 3.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 882500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 431    |
|    time_elapsed    | 35659  |
|    total_timesteps | 882688 |
-------------------------------
Eval num_timesteps=883000, episode_reward=-75082.13 +/- 25125.80
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4029803    |
|    mean velocity x      | -1.89         |
|    mean velocity y      | -0.486        |
|    mean velocity z      | 4.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.51e+04     |
| time/                   |               |
|    total_timesteps      | 883000        |
| train/                  |               |
|    approx_kl            | 0.00016626416 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.001         |
|    loss                 | 8.45e+06      |
|    n_updates            | 4310          |
|    policy_gradient_loss | -0.000928     |
|    std                  | 1.55          |
|    value_loss           | 3.24e+07      |
-------------------------------------------
Eval num_timesteps=883500, episode_reward=-56843.36 +/- 40797.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23028159 |
|    mean velocity x | -0.277      |
|    mean velocity y | 0.452       |
|    mean velocity z | 0.425       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.68e+04   |
| time/              |             |
|    total_timesteps | 883500      |
------------------------------------
Eval num_timesteps=884000, episode_reward=-86192.91 +/- 45131.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38292465 |
|    mean velocity x | -0.207      |
|    mean velocity y | 0.559       |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 884000      |
------------------------------------
Eval num_timesteps=884500, episode_reward=-68513.40 +/- 41031.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54125524 |
|    mean velocity x | 0.237       |
|    mean velocity y | 1.24        |
|    mean velocity z | 3.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 884500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 432    |
|    time_elapsed    | 35739  |
|    total_timesteps | 884736 |
-------------------------------
Eval num_timesteps=885000, episode_reward=-99629.00 +/- 50618.97
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44493315   |
|    mean velocity x      | 0.689         |
|    mean velocity y      | 1.46          |
|    mean velocity z      | 3.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.96e+04     |
| time/                   |               |
|    total_timesteps      | 885000        |
| train/                  |               |
|    approx_kl            | 0.00046342297 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.219         |
|    learning_rate        | 0.001         |
|    loss                 | 2.21e+07      |
|    n_updates            | 4320          |
|    policy_gradient_loss | -0.00139      |
|    std                  | 1.55          |
|    value_loss           | 3.62e+07      |
-------------------------------------------
Eval num_timesteps=885500, episode_reward=-82077.10 +/- 49682.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5168632 |
|    mean velocity x | -0.425     |
|    mean velocity y | 0.974      |
|    mean velocity z | 4.49       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.21e+04  |
| time/              |            |
|    total_timesteps | 885500     |
-----------------------------------
Eval num_timesteps=886000, episode_reward=-117834.17 +/- 14012.10
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4513844 |
|    mean velocity x | -0.279     |
|    mean velocity y | 0.763      |
|    mean velocity z | 4.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 886000     |
-----------------------------------
Eval num_timesteps=886500, episode_reward=-60284.40 +/- 55320.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45520076 |
|    mean velocity x | -0.712      |
|    mean velocity y | 0.687       |
|    mean velocity z | 2.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.03e+04   |
| time/              |             |
|    total_timesteps | 886500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 433    |
|    time_elapsed    | 35820  |
|    total_timesteps | 886784 |
-------------------------------
Eval num_timesteps=887000, episode_reward=-74643.66 +/- 41580.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46187043   |
|    mean velocity x      | -0.142        |
|    mean velocity y      | 1.03          |
|    mean velocity z      | 4.5           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.46e+04     |
| time/                   |               |
|    total_timesteps      | 887000        |
| train/                  |               |
|    approx_kl            | 0.00013639039 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.119         |
|    learning_rate        | 0.001         |
|    loss                 | 4.95e+07      |
|    n_updates            | 4330          |
|    policy_gradient_loss | -0.000619     |
|    std                  | 1.55          |
|    value_loss           | 1.01e+08      |
-------------------------------------------
Eval num_timesteps=887500, episode_reward=-19469.35 +/- 24715.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47138587 |
|    mean velocity x | -1.11       |
|    mean velocity y | 0.0813      |
|    mean velocity z | 3.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.95e+04   |
| time/              |             |
|    total_timesteps | 887500      |
------------------------------------
Eval num_timesteps=888000, episode_reward=-101655.92 +/- 28912.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5338159 |
|    mean velocity x | -0.377     |
|    mean velocity y | 1          |
|    mean velocity z | 3.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 888000     |
-----------------------------------
Eval num_timesteps=888500, episode_reward=-122343.84 +/- 14466.40
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.397853 |
|    mean velocity x | -0.973    |
|    mean velocity y | 0.58      |
|    mean velocity z | 1.84      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.22e+05 |
| time/              |           |
|    total_timesteps | 888500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 434    |
|    time_elapsed    | 35900  |
|    total_timesteps | 888832 |
-------------------------------
Eval num_timesteps=889000, episode_reward=-101128.22 +/- 51577.79
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46017477   |
|    mean velocity x      | -0.0406       |
|    mean velocity y      | 1.46          |
|    mean velocity z      | 4.16          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+05     |
| time/                   |               |
|    total_timesteps      | 889000        |
| train/                  |               |
|    approx_kl            | 1.3922545e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.001         |
|    loss                 | 5.01e+07      |
|    n_updates            | 4340          |
|    policy_gradient_loss | -0.000219     |
|    std                  | 1.55          |
|    value_loss           | 4.52e+07      |
-------------------------------------------
Eval num_timesteps=889500, episode_reward=-46239.25 +/- 29609.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5788878 |
|    mean velocity x | 0.618      |
|    mean velocity y | 1.69       |
|    mean velocity z | 3.88       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.62e+04  |
| time/              |            |
|    total_timesteps | 889500     |
-----------------------------------
Eval num_timesteps=890000, episode_reward=-98460.58 +/- 42375.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35928348 |
|    mean velocity x | -0.0268     |
|    mean velocity y | 0.659       |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.85e+04   |
| time/              |             |
|    total_timesteps | 890000      |
------------------------------------
Eval num_timesteps=890500, episode_reward=-68699.34 +/- 55219.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4648702 |
|    mean velocity x | -0.797     |
|    mean velocity y | -0.139     |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.87e+04  |
| time/              |            |
|    total_timesteps | 890500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 435    |
|    time_elapsed    | 35980  |
|    total_timesteps | 890880 |
-------------------------------
Eval num_timesteps=891000, episode_reward=-108816.05 +/- 10586.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4653075   |
|    mean velocity x      | -0.372       |
|    mean velocity y      | 1.02         |
|    mean velocity z      | 4.86         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.09e+05    |
| time/                   |              |
|    total_timesteps      | 891000       |
| train/                  |              |
|    approx_kl            | 3.871252e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.001        |
|    loss                 | 5.9e+07      |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.000295    |
|    std                  | 1.55         |
|    value_loss           | 8.01e+07     |
------------------------------------------
Eval num_timesteps=891500, episode_reward=-39380.64 +/- 38390.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62961674 |
|    mean velocity x | -0.222      |
|    mean velocity y | 1.96        |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.94e+04   |
| time/              |             |
|    total_timesteps | 891500      |
------------------------------------
Eval num_timesteps=892000, episode_reward=-95869.93 +/- 33874.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5175776 |
|    mean velocity x | -1.35      |
|    mean velocity y | 0.275      |
|    mean velocity z | 4.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.59e+04  |
| time/              |            |
|    total_timesteps | 892000     |
-----------------------------------
Eval num_timesteps=892500, episode_reward=-73274.73 +/- 32096.67
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.498061 |
|    mean velocity x | 0.219     |
|    mean velocity y | 1.35      |
|    mean velocity z | 3.8       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.33e+04 |
| time/              |           |
|    total_timesteps | 892500    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 436    |
|    time_elapsed    | 36061  |
|    total_timesteps | 892928 |
-------------------------------
Eval num_timesteps=893000, episode_reward=-70020.19 +/- 49962.88
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36524376   |
|    mean velocity x      | -0.581        |
|    mean velocity y      | 0.441         |
|    mean velocity z      | 3.02          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7e+04        |
| time/                   |               |
|    total_timesteps      | 893000        |
| train/                  |               |
|    approx_kl            | 4.7850684e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 4.9e+07       |
|    n_updates            | 4360          |
|    policy_gradient_loss | -0.000538     |
|    std                  | 1.55          |
|    value_loss           | 4.86e+07      |
-------------------------------------------
Eval num_timesteps=893500, episode_reward=-97659.27 +/- 27021.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.508068 |
|    mean velocity x | -0.424    |
|    mean velocity y | 0.852     |
|    mean velocity z | 4.77      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.77e+04 |
| time/              |           |
|    total_timesteps | 893500    |
----------------------------------
Eval num_timesteps=894000, episode_reward=-82454.26 +/- 58452.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38415143 |
|    mean velocity x | -1.03       |
|    mean velocity y | 0.201       |
|    mean velocity z | 2.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.25e+04   |
| time/              |             |
|    total_timesteps | 894000      |
------------------------------------
Eval num_timesteps=894500, episode_reward=-97315.88 +/- 39603.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41867366 |
|    mean velocity x | -0.245      |
|    mean velocity y | 0.693       |
|    mean velocity z | 4.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.73e+04   |
| time/              |             |
|    total_timesteps | 894500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 437    |
|    time_elapsed    | 36141  |
|    total_timesteps | 894976 |
-------------------------------
Eval num_timesteps=895000, episode_reward=-114611.49 +/- 54379.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5749491   |
|    mean velocity x      | -0.504       |
|    mean velocity y      | 1.15         |
|    mean velocity z      | 5.13         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.15e+05    |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 7.744177e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.001        |
|    loss                 | 2.24e+07     |
|    n_updates            | 4370         |
|    policy_gradient_loss | -0.000703    |
|    std                  | 1.55         |
|    value_loss           | 9.72e+07     |
------------------------------------------
Eval num_timesteps=895500, episode_reward=-76482.89 +/- 40099.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5640558 |
|    mean velocity x | -0.223     |
|    mean velocity y | 1.38       |
|    mean velocity z | 3.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.65e+04  |
| time/              |            |
|    total_timesteps | 895500     |
-----------------------------------
Eval num_timesteps=896000, episode_reward=-97170.61 +/- 20033.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4755895 |
|    mean velocity x | -0.947     |
|    mean velocity y | 0.234      |
|    mean velocity z | 4.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.72e+04  |
| time/              |            |
|    total_timesteps | 896000     |
-----------------------------------
Eval num_timesteps=896500, episode_reward=-84951.30 +/- 36351.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40226838 |
|    mean velocity x | -0.622      |
|    mean velocity y | 0.923       |
|    mean velocity z | 1.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 896500      |
------------------------------------
Eval num_timesteps=897000, episode_reward=-95480.42 +/- 27666.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31964874 |
|    mean velocity x | -1.52       |
|    mean velocity y | -0.126      |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.55e+04   |
| time/              |             |
|    total_timesteps | 897000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 438    |
|    time_elapsed    | 36240  |
|    total_timesteps | 897024 |
-------------------------------
Eval num_timesteps=897500, episode_reward=-125801.39 +/- 12660.72
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.4985962     |
|    mean velocity x      | 0.151          |
|    mean velocity y      | 1.03           |
|    mean velocity z      | 3.44           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1.26e+05      |
| time/                   |                |
|    total_timesteps      | 897500         |
| train/                  |                |
|    approx_kl            | 0.000119024655 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.234          |
|    learning_rate        | 0.001          |
|    loss                 | 2.43e+07       |
|    n_updates            | 4380           |
|    policy_gradient_loss | -0.00116       |
|    std                  | 1.55           |
|    value_loss           | 3.87e+07       |
--------------------------------------------
Eval num_timesteps=898000, episode_reward=-132394.92 +/- 12092.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44480667 |
|    mean velocity x | -0.188      |
|    mean velocity y | 0.939       |
|    mean velocity z | 4.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.32e+05   |
| time/              |             |
|    total_timesteps | 898000      |
------------------------------------
Eval num_timesteps=898500, episode_reward=-96214.06 +/- 20066.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57568395 |
|    mean velocity x | -0.477      |
|    mean velocity y | 1.37        |
|    mean velocity z | 4.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.62e+04   |
| time/              |             |
|    total_timesteps | 898500      |
------------------------------------
Eval num_timesteps=899000, episode_reward=-48172.64 +/- 44177.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3653058 |
|    mean velocity x | -0.254     |
|    mean velocity y | 0.368      |
|    mean velocity z | 4.55       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.82e+04  |
| time/              |            |
|    total_timesteps | 899000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 439    |
|    time_elapsed    | 36321  |
|    total_timesteps | 899072 |
-------------------------------
Eval num_timesteps=899500, episode_reward=-114446.99 +/- 26222.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49605992   |
|    mean velocity x      | -0.8          |
|    mean velocity y      | 0.436         |
|    mean velocity z      | 4.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.14e+05     |
| time/                   |               |
|    total_timesteps      | 899500        |
| train/                  |               |
|    approx_kl            | 2.8808136e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.001         |
|    loss                 | 3.15e+07      |
|    n_updates            | 4390          |
|    policy_gradient_loss | -0.000419     |
|    std                  | 1.55          |
|    value_loss           | 9.14e+07      |
-------------------------------------------
Eval num_timesteps=900000, episode_reward=-70891.33 +/- 42632.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33868575 |
|    mean velocity x | -0.174      |
|    mean velocity y | 0.234       |
|    mean velocity z | 3.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 900000      |
------------------------------------
Eval num_timesteps=900500, episode_reward=-106024.86 +/- 14811.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4657362 |
|    mean velocity x | -0.298     |
|    mean velocity y | 0.829      |
|    mean velocity z | 4.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 900500     |
-----------------------------------
Eval num_timesteps=901000, episode_reward=-92184.12 +/- 28980.09
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.327023 |
|    mean velocity x | -0.543    |
|    mean velocity y | 0.27      |
|    mean velocity z | 2.72      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.22e+04 |
| time/              |           |
|    total_timesteps | 901000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 440    |
|    time_elapsed    | 36401  |
|    total_timesteps | 901120 |
-------------------------------
Eval num_timesteps=901500, episode_reward=-58497.75 +/- 43468.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32905504   |
|    mean velocity x      | -0.894        |
|    mean velocity y      | 0.264         |
|    mean velocity z      | 2.52          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.85e+04     |
| time/                   |               |
|    total_timesteps      | 901500        |
| train/                  |               |
|    approx_kl            | 2.2519642e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.001         |
|    loss                 | 4.68e+07      |
|    n_updates            | 4400          |
|    policy_gradient_loss | -0.000247     |
|    std                  | 1.55          |
|    value_loss           | 5.94e+07      |
-------------------------------------------
Eval num_timesteps=902000, episode_reward=-53869.53 +/- 28191.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.74036866 |
|    mean velocity x | 0.646       |
|    mean velocity y | 2.6         |
|    mean velocity z | 5.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.39e+04   |
| time/              |             |
|    total_timesteps | 902000      |
------------------------------------
Eval num_timesteps=902500, episode_reward=-62143.96 +/- 24170.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3275185 |
|    mean velocity x | -0.281     |
|    mean velocity y | 0.532      |
|    mean velocity z | 0.494      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.21e+04  |
| time/              |            |
|    total_timesteps | 902500     |
-----------------------------------
Eval num_timesteps=903000, episode_reward=-61000.13 +/- 45508.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45718104 |
|    mean velocity x | -0.0618     |
|    mean velocity y | 0.969       |
|    mean velocity z | 4.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.1e+04    |
| time/              |             |
|    total_timesteps | 903000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 441    |
|    time_elapsed    | 36481  |
|    total_timesteps | 903168 |
-------------------------------
Eval num_timesteps=903500, episode_reward=-93644.32 +/- 29761.39
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42005512  |
|    mean velocity x      | -0.394       |
|    mean velocity y      | 0.7          |
|    mean velocity z      | 1.94         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.36e+04    |
| time/                   |              |
|    total_timesteps      | 903500       |
| train/                  |              |
|    approx_kl            | 3.374924e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.001        |
|    loss                 | 6.88e+06     |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.000411    |
|    std                  | 1.55         |
|    value_loss           | 2.63e+07     |
------------------------------------------
Eval num_timesteps=904000, episode_reward=-64195.47 +/- 32100.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42896947 |
|    mean velocity x | -0.182      |
|    mean velocity y | 1.07        |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 904000      |
------------------------------------
Eval num_timesteps=904500, episode_reward=-119456.79 +/- 22814.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35928473 |
|    mean velocity x | -1.69       |
|    mean velocity y | -1.23       |
|    mean velocity z | 5.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 904500      |
------------------------------------
Eval num_timesteps=905000, episode_reward=-60150.14 +/- 30928.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38646424 |
|    mean velocity x | 0.0253      |
|    mean velocity y | 0.703       |
|    mean velocity z | 2.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.02e+04   |
| time/              |             |
|    total_timesteps | 905000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 442    |
|    time_elapsed    | 36562  |
|    total_timesteps | 905216 |
-------------------------------
Eval num_timesteps=905500, episode_reward=-77862.88 +/- 32088.80
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46263683  |
|    mean velocity x      | -1.05        |
|    mean velocity y      | -0.362       |
|    mean velocity z      | 4.09         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.79e+04    |
| time/                   |              |
|    total_timesteps      | 905500       |
| train/                  |              |
|    approx_kl            | 7.772172e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+07     |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.000694    |
|    std                  | 1.55         |
|    value_loss           | 5.94e+07     |
------------------------------------------
Eval num_timesteps=906000, episode_reward=-112938.18 +/- 19839.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5229031 |
|    mean velocity x | -0.431     |
|    mean velocity y | 0.813      |
|    mean velocity z | 4.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.13e+05  |
| time/              |            |
|    total_timesteps | 906000     |
-----------------------------------
Eval num_timesteps=906500, episode_reward=-75644.44 +/- 39157.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46014774 |
|    mean velocity x | -0.961      |
|    mean velocity y | -0.347      |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.56e+04   |
| time/              |             |
|    total_timesteps | 906500      |
------------------------------------
Eval num_timesteps=907000, episode_reward=-62194.37 +/- 29098.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51903796 |
|    mean velocity x | -0.466      |
|    mean velocity y | 0.905       |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.22e+04   |
| time/              |             |
|    total_timesteps | 907000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 443    |
|    time_elapsed    | 36642  |
|    total_timesteps | 907264 |
-------------------------------
Eval num_timesteps=907500, episode_reward=-71827.63 +/- 12711.60
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.59980613  |
|    mean velocity x      | 0.59         |
|    mean velocity y      | 2.14         |
|    mean velocity z      | 3.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.18e+04    |
| time/                   |              |
|    total_timesteps      | 907500       |
| train/                  |              |
|    approx_kl            | 8.605674e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.001        |
|    loss                 | 3.76e+07     |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.000768    |
|    std                  | 1.55         |
|    value_loss           | 6.79e+07     |
------------------------------------------
Eval num_timesteps=908000, episode_reward=-71518.99 +/- 52620.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42322847 |
|    mean velocity x | -0.103      |
|    mean velocity y | 1.13        |
|    mean velocity z | 3.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.15e+04   |
| time/              |             |
|    total_timesteps | 908000      |
------------------------------------
Eval num_timesteps=908500, episode_reward=-114648.19 +/- 21712.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46967056 |
|    mean velocity x | 0.257       |
|    mean velocity y | 1.15        |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 908500      |
------------------------------------
Eval num_timesteps=909000, episode_reward=-49056.87 +/- 38749.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41029108 |
|    mean velocity x | -0.975      |
|    mean velocity y | -0.1        |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.91e+04   |
| time/              |             |
|    total_timesteps | 909000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 444    |
|    time_elapsed    | 36722  |
|    total_timesteps | 909312 |
-------------------------------
Eval num_timesteps=909500, episode_reward=-97461.92 +/- 39811.60
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3776212    |
|    mean velocity x      | -0.695        |
|    mean velocity y      | 0.197         |
|    mean velocity z      | 3.11          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.75e+04     |
| time/                   |               |
|    total_timesteps      | 909500        |
| train/                  |               |
|    approx_kl            | 5.5134908e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.001         |
|    loss                 | 2.01e+07      |
|    n_updates            | 4440          |
|    policy_gradient_loss | -0.000641     |
|    std                  | 1.55          |
|    value_loss           | 3.3e+07       |
-------------------------------------------
Eval num_timesteps=910000, episode_reward=-63670.10 +/- 20446.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4355824 |
|    mean velocity x | -0.262     |
|    mean velocity y | 1.13       |
|    mean velocity z | 4.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.37e+04  |
| time/              |            |
|    total_timesteps | 910000     |
-----------------------------------
Eval num_timesteps=910500, episode_reward=-85715.01 +/- 17247.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5139654 |
|    mean velocity x | -0.0253    |
|    mean velocity y | 1.39       |
|    mean velocity z | 4.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.57e+04  |
| time/              |            |
|    total_timesteps | 910500     |
-----------------------------------
Eval num_timesteps=911000, episode_reward=-78262.86 +/- 41005.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4629171 |
|    mean velocity x | -0.212     |
|    mean velocity y | 0.779      |
|    mean velocity z | 4.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.83e+04  |
| time/              |            |
|    total_timesteps | 911000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 445    |
|    time_elapsed    | 36803  |
|    total_timesteps | 911360 |
-------------------------------
Eval num_timesteps=911500, episode_reward=-84527.57 +/- 42448.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49513152   |
|    mean velocity x      | 0.235         |
|    mean velocity y      | 1.48          |
|    mean velocity z      | 3.78          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.45e+04     |
| time/                   |               |
|    total_timesteps      | 911500        |
| train/                  |               |
|    approx_kl            | 7.8450365e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.001         |
|    loss                 | 5.87e+07      |
|    n_updates            | 4450          |
|    policy_gradient_loss | -0.000659     |
|    std                  | 1.55          |
|    value_loss           | 9.83e+07      |
-------------------------------------------
Eval num_timesteps=912000, episode_reward=-102996.26 +/- 33594.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49457026 |
|    mean velocity x | 0.516       |
|    mean velocity y | 1.6         |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 912000      |
------------------------------------
Eval num_timesteps=912500, episode_reward=-78272.24 +/- 24729.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5296329 |
|    mean velocity x | -0.256     |
|    mean velocity y | 1.48       |
|    mean velocity z | 4.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.83e+04  |
| time/              |            |
|    total_timesteps | 912500     |
-----------------------------------
Eval num_timesteps=913000, episode_reward=-54134.20 +/- 42011.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34316126 |
|    mean velocity x | 0.148       |
|    mean velocity y | 1.04        |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.41e+04   |
| time/              |             |
|    total_timesteps | 913000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 446    |
|    time_elapsed    | 36883  |
|    total_timesteps | 913408 |
-------------------------------
Eval num_timesteps=913500, episode_reward=-74200.23 +/- 20344.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.62910694   |
|    mean velocity x      | 1.03          |
|    mean velocity y      | 2.15          |
|    mean velocity z      | 4.17          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.42e+04     |
| time/                   |               |
|    total_timesteps      | 913500        |
| train/                  |               |
|    approx_kl            | 0.00017702102 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.178         |
|    learning_rate        | 0.001         |
|    loss                 | 1.99e+07      |
|    n_updates            | 4460          |
|    policy_gradient_loss | -0.00123      |
|    std                  | 1.55          |
|    value_loss           | 7.72e+07      |
-------------------------------------------
Eval num_timesteps=914000, episode_reward=-97583.15 +/- 29434.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.84165156 |
|    mean velocity x | 0.0283      |
|    mean velocity y | 2.42        |
|    mean velocity z | 7.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.76e+04   |
| time/              |             |
|    total_timesteps | 914000      |
------------------------------------
Eval num_timesteps=914500, episode_reward=-72453.01 +/- 31675.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5921601 |
|    mean velocity x | -0.462     |
|    mean velocity y | 1.08       |
|    mean velocity z | 5.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.25e+04  |
| time/              |            |
|    total_timesteps | 914500     |
-----------------------------------
Eval num_timesteps=915000, episode_reward=-104690.64 +/- 19143.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47853574 |
|    mean velocity x | -0.194      |
|    mean velocity y | 1.11        |
|    mean velocity z | 4.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 915000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 447    |
|    time_elapsed    | 36963  |
|    total_timesteps | 915456 |
-------------------------------
Eval num_timesteps=915500, episode_reward=-121845.17 +/- 64727.31
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4166279    |
|    mean velocity x      | -0.23         |
|    mean velocity y      | 0.65          |
|    mean velocity z      | 4.48          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.22e+05     |
| time/                   |               |
|    total_timesteps      | 915500        |
| train/                  |               |
|    approx_kl            | 5.0585775e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.001         |
|    loss                 | 7.92e+07      |
|    n_updates            | 4470          |
|    policy_gradient_loss | -0.000437     |
|    std                  | 1.56          |
|    value_loss           | 1.22e+08      |
-------------------------------------------
Eval num_timesteps=916000, episode_reward=-61582.53 +/- 43519.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44375885 |
|    mean velocity x | -1.66       |
|    mean velocity y | 0.0864      |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.16e+04   |
| time/              |             |
|    total_timesteps | 916000      |
------------------------------------
Eval num_timesteps=916500, episode_reward=-55253.63 +/- 54318.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5047581 |
|    mean velocity x | -0.453     |
|    mean velocity y | 1.29       |
|    mean velocity z | 4.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.53e+04  |
| time/              |            |
|    total_timesteps | 916500     |
-----------------------------------
Eval num_timesteps=917000, episode_reward=-94773.10 +/- 24918.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47233227 |
|    mean velocity x | -0.132      |
|    mean velocity y | 0.898       |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.48e+04   |
| time/              |             |
|    total_timesteps | 917000      |
------------------------------------
Eval num_timesteps=917500, episode_reward=-75756.62 +/- 10432.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6963578 |
|    mean velocity x | -0.297     |
|    mean velocity y | 1.9        |
|    mean velocity z | 5.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.58e+04  |
| time/              |            |
|    total_timesteps | 917500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 448    |
|    time_elapsed    | 37063  |
|    total_timesteps | 917504 |
-------------------------------
Eval num_timesteps=918000, episode_reward=-53984.36 +/- 35346.64
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36628878  |
|    mean velocity x      | -0.0786      |
|    mean velocity y      | 0.643        |
|    mean velocity z      | 1.61         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.4e+04     |
| time/                   |              |
|    total_timesteps      | 918000       |
| train/                  |              |
|    approx_kl            | 7.925296e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.001        |
|    loss                 | 7.18e+07     |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.000651    |
|    std                  | 1.55         |
|    value_loss           | 7.43e+07     |
------------------------------------------
Eval num_timesteps=918500, episode_reward=-64609.84 +/- 51116.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4859389 |
|    mean velocity x | -0.678     |
|    mean velocity y | 0.742      |
|    mean velocity z | 4.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.46e+04  |
| time/              |            |
|    total_timesteps | 918500     |
-----------------------------------
Eval num_timesteps=919000, episode_reward=-102322.75 +/- 23375.74
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.656918 |
|    mean velocity x | 0.259     |
|    mean velocity y | 2.47      |
|    mean velocity z | 4.82      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.02e+05 |
| time/              |           |
|    total_timesteps | 919000    |
----------------------------------
Eval num_timesteps=919500, episode_reward=-92531.44 +/- 23526.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45817262 |
|    mean velocity x | -0.378      |
|    mean velocity y | 0.431       |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.25e+04   |
| time/              |             |
|    total_timesteps | 919500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 449    |
|    time_elapsed    | 37143  |
|    total_timesteps | 919552 |
-------------------------------
Eval num_timesteps=920000, episode_reward=-89370.09 +/- 40026.70
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3356572    |
|    mean velocity x      | -0.0178       |
|    mean velocity y      | 0.37          |
|    mean velocity z      | 1.29          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.94e+04     |
| time/                   |               |
|    total_timesteps      | 920000        |
| train/                  |               |
|    approx_kl            | 3.4900237e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.001         |
|    loss                 | 1.49e+07      |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000444     |
|    std                  | 1.55          |
|    value_loss           | 4.5e+07       |
-------------------------------------------
Eval num_timesteps=920500, episode_reward=-86380.28 +/- 14607.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57635313 |
|    mean velocity x | -1.15       |
|    mean velocity y | 0.538       |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 920500      |
------------------------------------
Eval num_timesteps=921000, episode_reward=-99409.98 +/- 18837.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2060304 |
|    mean velocity x | -0.258     |
|    mean velocity y | 0.319      |
|    mean velocity z | 0.294      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.94e+04  |
| time/              |            |
|    total_timesteps | 921000     |
-----------------------------------
Eval num_timesteps=921500, episode_reward=-80774.21 +/- 32722.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3908098 |
|    mean velocity x | -0.495     |
|    mean velocity y | 0.594      |
|    mean velocity z | 1.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.08e+04  |
| time/              |            |
|    total_timesteps | 921500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 450    |
|    time_elapsed    | 37224  |
|    total_timesteps | 921600 |
-------------------------------
Eval num_timesteps=922000, episode_reward=-83228.94 +/- 42215.48
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47128394  |
|    mean velocity x      | -0.437       |
|    mean velocity y      | 0.785        |
|    mean velocity z      | 4.82         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.32e+04    |
| time/                   |              |
|    total_timesteps      | 922000       |
| train/                  |              |
|    approx_kl            | 7.219534e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+07     |
|    n_updates            | 4500         |
|    policy_gradient_loss | -0.000523    |
|    std                  | 1.55         |
|    value_loss           | 4.2e+07      |
------------------------------------------
Eval num_timesteps=922500, episode_reward=-104406.71 +/- 24088.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5552658 |
|    mean velocity x | 0.152      |
|    mean velocity y | 1.54       |
|    mean velocity z | 3.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 922500     |
-----------------------------------
Eval num_timesteps=923000, episode_reward=-83720.38 +/- 25144.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54810965 |
|    mean velocity x | -0.734      |
|    mean velocity y | 0.689       |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 923000      |
------------------------------------
Eval num_timesteps=923500, episode_reward=-85872.68 +/- 40635.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4685119 |
|    mean velocity x | -0.458     |
|    mean velocity y | 0.598      |
|    mean velocity z | 4.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.59e+04  |
| time/              |            |
|    total_timesteps | 923500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 451    |
|    time_elapsed    | 37304  |
|    total_timesteps | 923648 |
-------------------------------
Eval num_timesteps=924000, episode_reward=-82615.96 +/- 38526.38
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36770853  |
|    mean velocity x      | -0.673       |
|    mean velocity y      | -0.233       |
|    mean velocity z      | 3.93         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.26e+04    |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 8.036336e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+07     |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.000924    |
|    std                  | 1.55         |
|    value_loss           | 6.22e+07     |
------------------------------------------
Eval num_timesteps=924500, episode_reward=-90294.12 +/- 40423.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5033224 |
|    mean velocity x | -0.266     |
|    mean velocity y | 1.12       |
|    mean velocity z | 3.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.03e+04  |
| time/              |            |
|    total_timesteps | 924500     |
-----------------------------------
Eval num_timesteps=925000, episode_reward=-86184.39 +/- 35372.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4710534 |
|    mean velocity x | -0.539     |
|    mean velocity y | 0.721      |
|    mean velocity z | 4.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.62e+04  |
| time/              |            |
|    total_timesteps | 925000     |
-----------------------------------
Eval num_timesteps=925500, episode_reward=-88456.11 +/- 36145.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40462402 |
|    mean velocity x | -0.062      |
|    mean velocity y | 1.07        |
|    mean velocity z | 4.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 925500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 452    |
|    time_elapsed    | 37384  |
|    total_timesteps | 925696 |
-------------------------------
Eval num_timesteps=926000, episode_reward=-71866.52 +/- 49578.41
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40988854   |
|    mean velocity x      | 0.0177        |
|    mean velocity y      | 1.13          |
|    mean velocity z      | 3.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.19e+04     |
| time/                   |               |
|    total_timesteps      | 926000        |
| train/                  |               |
|    approx_kl            | 1.9050785e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.001         |
|    loss                 | 3.26e+07      |
|    n_updates            | 4520          |
|    policy_gradient_loss | -0.000391     |
|    std                  | 1.55          |
|    value_loss           | 7.51e+07      |
-------------------------------------------
Eval num_timesteps=926500, episode_reward=-101265.76 +/- 40347.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5039021 |
|    mean velocity x | -0.0918    |
|    mean velocity y | 1.5        |
|    mean velocity z | 4.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 926500     |
-----------------------------------
Eval num_timesteps=927000, episode_reward=-81795.81 +/- 46306.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32767352 |
|    mean velocity x | -1.25       |
|    mean velocity y | -0.811      |
|    mean velocity z | 3.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.18e+04   |
| time/              |             |
|    total_timesteps | 927000      |
------------------------------------
Eval num_timesteps=927500, episode_reward=-89847.62 +/- 45244.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52320063 |
|    mean velocity x | -0.382      |
|    mean velocity y | 0.827       |
|    mean velocity z | 4.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.98e+04   |
| time/              |             |
|    total_timesteps | 927500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 453    |
|    time_elapsed    | 37465  |
|    total_timesteps | 927744 |
-------------------------------
Eval num_timesteps=928000, episode_reward=-64443.47 +/- 42877.03
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43319687   |
|    mean velocity x      | 0.124         |
|    mean velocity y      | 0.861         |
|    mean velocity z      | 3.78          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.44e+04     |
| time/                   |               |
|    total_timesteps      | 928000        |
| train/                  |               |
|    approx_kl            | 3.8435246e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.001         |
|    loss                 | 4.82e+07      |
|    n_updates            | 4530          |
|    policy_gradient_loss | -0.000692     |
|    std                  | 1.55          |
|    value_loss           | 6.18e+07      |
-------------------------------------------
Eval num_timesteps=928500, episode_reward=-113060.95 +/- 21416.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8683773 |
|    mean velocity x | 0.61       |
|    mean velocity y | 3.02       |
|    mean velocity z | 5.98       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.13e+05  |
| time/              |            |
|    total_timesteps | 928500     |
-----------------------------------
Eval num_timesteps=929000, episode_reward=-92676.81 +/- 42284.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4648083 |
|    mean velocity x | 0.37       |
|    mean velocity y | 1.51       |
|    mean velocity z | 3.56       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.27e+04  |
| time/              |            |
|    total_timesteps | 929000     |
-----------------------------------
Eval num_timesteps=929500, episode_reward=-91232.24 +/- 16096.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57123107 |
|    mean velocity x | -1.24       |
|    mean velocity y | -0.0494     |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.12e+04   |
| time/              |             |
|    total_timesteps | 929500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 454    |
|    time_elapsed    | 37545  |
|    total_timesteps | 929792 |
-------------------------------
Eval num_timesteps=930000, episode_reward=-89598.77 +/- 30416.71
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5512395   |
|    mean velocity x      | -0.527       |
|    mean velocity y      | 0.548        |
|    mean velocity z      | 5.17         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.96e+04    |
| time/                   |              |
|    total_timesteps      | 930000       |
| train/                  |              |
|    approx_kl            | 6.494502e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+07     |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.000632    |
|    std                  | 1.55         |
|    value_loss           | 4.3e+07      |
------------------------------------------
Eval num_timesteps=930500, episode_reward=-102275.36 +/- 31582.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4823163 |
|    mean velocity x | -0.306     |
|    mean velocity y | 0.751      |
|    mean velocity z | 4.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 930500     |
-----------------------------------
Eval num_timesteps=931000, episode_reward=-92566.53 +/- 45815.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5260039 |
|    mean velocity x | 0.555      |
|    mean velocity y | 1.73       |
|    mean velocity z | 3.4        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.26e+04  |
| time/              |            |
|    total_timesteps | 931000     |
-----------------------------------
Eval num_timesteps=931500, episode_reward=-90626.04 +/- 24683.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55641115 |
|    mean velocity x | -0.277      |
|    mean velocity y | 1.57        |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.06e+04   |
| time/              |             |
|    total_timesteps | 931500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 455    |
|    time_elapsed    | 37625  |
|    total_timesteps | 931840 |
-------------------------------
Eval num_timesteps=932000, episode_reward=-86962.08 +/- 32143.22
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45051232   |
|    mean velocity x      | -1.07         |
|    mean velocity y      | 0.12          |
|    mean velocity z      | 3.69          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.7e+04      |
| time/                   |               |
|    total_timesteps      | 932000        |
| train/                  |               |
|    approx_kl            | 0.00012192226 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+07      |
|    n_updates            | 4550          |
|    policy_gradient_loss | -0.000635     |
|    std                  | 1.55          |
|    value_loss           | 5.2e+07       |
-------------------------------------------
Eval num_timesteps=932500, episode_reward=-83031.61 +/- 11475.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4453616 |
|    mean velocity x | -0.213     |
|    mean velocity y | 1.28       |
|    mean velocity z | 4.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.3e+04   |
| time/              |            |
|    total_timesteps | 932500     |
-----------------------------------
Eval num_timesteps=933000, episode_reward=-90403.78 +/- 26857.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49601898 |
|    mean velocity x | 0.197       |
|    mean velocity y | 0.956       |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.04e+04   |
| time/              |             |
|    total_timesteps | 933000      |
------------------------------------
Eval num_timesteps=933500, episode_reward=-89802.14 +/- 46956.88
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3864153 |
|    mean velocity x | 0.119      |
|    mean velocity y | 1.18       |
|    mean velocity z | 4.44       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.98e+04  |
| time/              |            |
|    total_timesteps | 933500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 456    |
|    time_elapsed    | 37706  |
|    total_timesteps | 933888 |
-------------------------------
Eval num_timesteps=934000, episode_reward=-93128.95 +/- 42572.38
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49449745  |
|    mean velocity x      | -0.207       |
|    mean velocity y      | 1.06         |
|    mean velocity z      | 4.59         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.31e+04    |
| time/                   |              |
|    total_timesteps      | 934000       |
| train/                  |              |
|    approx_kl            | 7.945171e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.001        |
|    loss                 | 2.7e+07      |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.000234    |
|    std                  | 1.55         |
|    value_loss           | 8.48e+07     |
------------------------------------------
Eval num_timesteps=934500, episode_reward=-43152.36 +/- 31811.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50695115 |
|    mean velocity x | -0.285      |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.32e+04   |
| time/              |             |
|    total_timesteps | 934500      |
------------------------------------
Eval num_timesteps=935000, episode_reward=-101663.82 +/- 25546.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2888733 |
|    mean velocity x | -0.674     |
|    mean velocity y | 0.538      |
|    mean velocity z | 1.44       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 935000     |
-----------------------------------
Eval num_timesteps=935500, episode_reward=-78873.66 +/- 40845.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4365445 |
|    mean velocity x | 0.0489     |
|    mean velocity y | 1.2        |
|    mean velocity z | 4.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 935500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 457    |
|    time_elapsed    | 37786  |
|    total_timesteps | 935936 |
-------------------------------
Eval num_timesteps=936000, episode_reward=-122701.04 +/- 53656.20
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38785565   |
|    mean velocity x      | -1.12         |
|    mean velocity y      | 0.0863        |
|    mean velocity z      | 2.96          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.23e+05     |
| time/                   |               |
|    total_timesteps      | 936000        |
| train/                  |               |
|    approx_kl            | 0.00010674354 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+07      |
|    n_updates            | 4570          |
|    policy_gradient_loss | -0.000526     |
|    std                  | 1.55          |
|    value_loss           | 3.68e+07      |
-------------------------------------------
Eval num_timesteps=936500, episode_reward=-87455.86 +/- 27735.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49604377 |
|    mean velocity x | -0.193      |
|    mean velocity y | 1.12        |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.75e+04   |
| time/              |             |
|    total_timesteps | 936500      |
------------------------------------
Eval num_timesteps=937000, episode_reward=-41823.73 +/- 48757.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41694522 |
|    mean velocity x | 0.631       |
|    mean velocity y | 1.56        |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.18e+04   |
| time/              |             |
|    total_timesteps | 937000      |
------------------------------------
Eval num_timesteps=937500, episode_reward=-109972.72 +/- 26555.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39305443 |
|    mean velocity x | -0.383      |
|    mean velocity y | 0.713       |
|    mean velocity z | 2.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 937500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 458    |
|    time_elapsed    | 37866  |
|    total_timesteps | 937984 |
-------------------------------
Eval num_timesteps=938000, episode_reward=-93803.25 +/- 22120.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33955073   |
|    mean velocity x      | -0.704        |
|    mean velocity y      | 0.288         |
|    mean velocity z      | 2.57          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.38e+04     |
| time/                   |               |
|    total_timesteps      | 938000        |
| train/                  |               |
|    approx_kl            | 2.6918307e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.001         |
|    loss                 | 2.8e+07       |
|    n_updates            | 4580          |
|    policy_gradient_loss | -0.000348     |
|    std                  | 1.55          |
|    value_loss           | 4.2e+07       |
-------------------------------------------
Eval num_timesteps=938500, episode_reward=-38425.35 +/- 48447.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.86042124 |
|    mean velocity x | 0.49        |
|    mean velocity y | 2.17        |
|    mean velocity z | 9.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.84e+04   |
| time/              |             |
|    total_timesteps | 938500      |
------------------------------------
Eval num_timesteps=939000, episode_reward=-106643.72 +/- 29415.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3482261 |
|    mean velocity x | -0.0158    |
|    mean velocity y | 0.792      |
|    mean velocity z | 2.66       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 939000     |
-----------------------------------
Eval num_timesteps=939500, episode_reward=-79125.90 +/- 41560.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45678174 |
|    mean velocity x | -0.846      |
|    mean velocity y | 0.533       |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.91e+04   |
| time/              |             |
|    total_timesteps | 939500      |
------------------------------------
Eval num_timesteps=940000, episode_reward=-72485.11 +/- 42548.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8163526 |
|    mean velocity x | 0.701      |
|    mean velocity y | 3.33       |
|    mean velocity z | 4.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.25e+04  |
| time/              |            |
|    total_timesteps | 940000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 459    |
|    time_elapsed    | 37966  |
|    total_timesteps | 940032 |
-------------------------------
Eval num_timesteps=940500, episode_reward=-102110.70 +/- 42804.14
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.472199      |
|    mean velocity x      | -0.562         |
|    mean velocity y      | 0.454          |
|    mean velocity z      | 4.43           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1.02e+05      |
| time/                   |                |
|    total_timesteps      | 940500         |
| train/                  |                |
|    approx_kl            | 0.000104416686 |
|    clip_fraction        | 4.88e-05       |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.253          |
|    learning_rate        | 0.001          |
|    loss                 | 3.48e+07       |
|    n_updates            | 4590           |
|    policy_gradient_loss | -0.000969      |
|    std                  | 1.55           |
|    value_loss           | 6.15e+07       |
--------------------------------------------
Eval num_timesteps=941000, episode_reward=-69205.63 +/- 54992.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4724265 |
|    mean velocity x | -0.15      |
|    mean velocity y | 1.03       |
|    mean velocity z | 4.18       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.92e+04  |
| time/              |            |
|    total_timesteps | 941000     |
-----------------------------------
Eval num_timesteps=941500, episode_reward=-91153.63 +/- 34701.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3221236 |
|    mean velocity x | -0.031     |
|    mean velocity y | 0.381      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.12e+04  |
| time/              |            |
|    total_timesteps | 941500     |
-----------------------------------
Eval num_timesteps=942000, episode_reward=-101022.11 +/- 24148.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45150453 |
|    mean velocity x | -0.189      |
|    mean velocity y | 0.635       |
|    mean velocity z | 3.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 942000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 460    |
|    time_elapsed    | 38046  |
|    total_timesteps | 942080 |
-------------------------------
Eval num_timesteps=942500, episode_reward=-82720.43 +/- 50418.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40149462   |
|    mean velocity x      | -0.0322       |
|    mean velocity y      | 1.24          |
|    mean velocity z      | 4.55          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.27e+04     |
| time/                   |               |
|    total_timesteps      | 942500        |
| train/                  |               |
|    approx_kl            | 4.7562004e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.001         |
|    loss                 | 7.34e+07      |
|    n_updates            | 4600          |
|    policy_gradient_loss | -0.000431     |
|    std                  | 1.55          |
|    value_loss           | 9.02e+07      |
-------------------------------------------
Eval num_timesteps=943000, episode_reward=-61581.31 +/- 36841.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24876586 |
|    mean velocity x | -0.71       |
|    mean velocity y | 0.142       |
|    mean velocity z | 2.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.16e+04   |
| time/              |             |
|    total_timesteps | 943000      |
------------------------------------
Eval num_timesteps=943500, episode_reward=-97310.14 +/- 46376.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59064215 |
|    mean velocity x | -0.459      |
|    mean velocity y | 1.38        |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.73e+04   |
| time/              |             |
|    total_timesteps | 943500      |
------------------------------------
Eval num_timesteps=944000, episode_reward=-82658.38 +/- 45818.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.66022104 |
|    mean velocity x | -0.753      |
|    mean velocity y | 0.972       |
|    mean velocity z | 5.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.27e+04   |
| time/              |             |
|    total_timesteps | 944000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 461    |
|    time_elapsed    | 38126  |
|    total_timesteps | 944128 |
-------------------------------
Eval num_timesteps=944500, episode_reward=-100722.39 +/- 19988.24
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46219134   |
|    mean velocity x      | -0.0582       |
|    mean velocity y      | 1.51          |
|    mean velocity z      | 4.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+05     |
| time/                   |               |
|    total_timesteps      | 944500        |
| train/                  |               |
|    approx_kl            | 1.1672935e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.175         |
|    learning_rate        | 0.001         |
|    loss                 | 2.19e+07      |
|    n_updates            | 4610          |
|    policy_gradient_loss | -0.000286     |
|    std                  | 1.55          |
|    value_loss           | 5.46e+07      |
-------------------------------------------
Eval num_timesteps=945000, episode_reward=-105097.16 +/- 13088.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40778533 |
|    mean velocity x | -0.192      |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 945000      |
------------------------------------
Eval num_timesteps=945500, episode_reward=-73063.23 +/- 20476.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48034143 |
|    mean velocity x | -0.0707     |
|    mean velocity y | 1.06        |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.31e+04   |
| time/              |             |
|    total_timesteps | 945500      |
------------------------------------
Eval num_timesteps=946000, episode_reward=-76059.11 +/- 33847.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4745213 |
|    mean velocity x | -0.54      |
|    mean velocity y | 0.484      |
|    mean velocity z | 4.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.61e+04  |
| time/              |            |
|    total_timesteps | 946000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 462    |
|    time_elapsed    | 38206  |
|    total_timesteps | 946176 |
-------------------------------
Eval num_timesteps=946500, episode_reward=-75968.77 +/- 44083.60
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4079795   |
|    mean velocity x      | 0.086        |
|    mean velocity y      | 0.862        |
|    mean velocity z      | 3.03         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.6e+04     |
| time/                   |              |
|    total_timesteps      | 946500       |
| train/                  |              |
|    approx_kl            | 6.517017e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.144        |
|    learning_rate        | 0.001        |
|    loss                 | 3.69e+07     |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.000214    |
|    std                  | 1.55         |
|    value_loss           | 7.28e+07     |
------------------------------------------
Eval num_timesteps=947000, episode_reward=-42621.81 +/- 38881.13
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.547469 |
|    mean velocity x | -0.248    |
|    mean velocity y | 1.55      |
|    mean velocity z | 5.12      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.26e+04 |
| time/              |           |
|    total_timesteps | 947000    |
----------------------------------
Eval num_timesteps=947500, episode_reward=-86381.62 +/- 46237.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24164398 |
|    mean velocity x | -0.936      |
|    mean velocity y | -0.445      |
|    mean velocity z | 3.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 947500      |
------------------------------------
Eval num_timesteps=948000, episode_reward=-89333.81 +/- 21288.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49095026 |
|    mean velocity x | -0.66       |
|    mean velocity y | 0.824       |
|    mean velocity z | 4.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 948000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 463    |
|    time_elapsed    | 38287  |
|    total_timesteps | 948224 |
-------------------------------
Eval num_timesteps=948500, episode_reward=-62849.18 +/- 56678.04
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42610252   |
|    mean velocity x      | -0.294        |
|    mean velocity y      | 0.873         |
|    mean velocity z      | 2.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.28e+04     |
| time/                   |               |
|    total_timesteps      | 948500        |
| train/                  |               |
|    approx_kl            | 3.1501084e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 4.33e+07      |
|    n_updates            | 4630          |
|    policy_gradient_loss | -0.000629     |
|    std                  | 1.55          |
|    value_loss           | 5.79e+07      |
-------------------------------------------
Eval num_timesteps=949000, episode_reward=-95439.07 +/- 27492.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34612688 |
|    mean velocity x | -0.703      |
|    mean velocity y | 0.0952      |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.54e+04   |
| time/              |             |
|    total_timesteps | 949000      |
------------------------------------
Eval num_timesteps=949500, episode_reward=-84889.07 +/- 20967.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45978054 |
|    mean velocity x | -0.113      |
|    mean velocity y | 0.709       |
|    mean velocity z | 2.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.49e+04   |
| time/              |             |
|    total_timesteps | 949500      |
------------------------------------
Eval num_timesteps=950000, episode_reward=-99043.59 +/- 26829.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38873816 |
|    mean velocity x | -1.3        |
|    mean velocity y | -0.214      |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.9e+04    |
| time/              |             |
|    total_timesteps | 950000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 464    |
|    time_elapsed    | 38367  |
|    total_timesteps | 950272 |
-------------------------------
Eval num_timesteps=950500, episode_reward=-89801.25 +/- 49545.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.55663073   |
|    mean velocity x      | -0.504        |
|    mean velocity y      | 0.758         |
|    mean velocity z      | 5.05          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.98e+04     |
| time/                   |               |
|    total_timesteps      | 950500        |
| train/                  |               |
|    approx_kl            | 5.2757794e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.174         |
|    learning_rate        | 0.001         |
|    loss                 | 4.07e+07      |
|    n_updates            | 4640          |
|    policy_gradient_loss | -0.00044      |
|    std                  | 1.55          |
|    value_loss           | 5.34e+07      |
-------------------------------------------
Eval num_timesteps=951000, episode_reward=-115548.65 +/- 37715.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5818503 |
|    mean velocity x | 0.157      |
|    mean velocity y | 1.77       |
|    mean velocity z | 2.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.16e+05  |
| time/              |            |
|    total_timesteps | 951000     |
-----------------------------------
Eval num_timesteps=951500, episode_reward=-85272.45 +/- 29231.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45162296 |
|    mean velocity x | -0.238      |
|    mean velocity y | 0.889       |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 951500      |
------------------------------------
Eval num_timesteps=952000, episode_reward=-100556.41 +/- 36022.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46662405 |
|    mean velocity x | -0.397      |
|    mean velocity y | 0.867       |
|    mean velocity z | 4.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 952000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 465    |
|    time_elapsed    | 38448  |
|    total_timesteps | 952320 |
-------------------------------
Eval num_timesteps=952500, episode_reward=-75744.10 +/- 21022.88
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37429315   |
|    mean velocity x      | 0.0202        |
|    mean velocity y      | 0.73          |
|    mean velocity z      | 3.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.57e+04     |
| time/                   |               |
|    total_timesteps      | 952500        |
| train/                  |               |
|    approx_kl            | 5.5938668e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.001         |
|    loss                 | 5.97e+07      |
|    n_updates            | 4650          |
|    policy_gradient_loss | -0.00071      |
|    std                  | 1.55          |
|    value_loss           | 6.78e+07      |
-------------------------------------------
Eval num_timesteps=953000, episode_reward=-88367.99 +/- 29801.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41793752 |
|    mean velocity x | -0.213      |
|    mean velocity y | 1           |
|    mean velocity z | 4.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 953000      |
------------------------------------
Eval num_timesteps=953500, episode_reward=-89275.19 +/- 27809.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40512607 |
|    mean velocity x | 0.232       |
|    mean velocity y | 1           |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 953500      |
------------------------------------
Eval num_timesteps=954000, episode_reward=-72345.49 +/- 41174.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3301458 |
|    mean velocity x | -0.976     |
|    mean velocity y | -0.685     |
|    mean velocity z | 3.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 954000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 466    |
|    time_elapsed    | 38528  |
|    total_timesteps | 954368 |
-------------------------------
Eval num_timesteps=954500, episode_reward=-109340.12 +/- 10612.56
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6512466    |
|    mean velocity x      | -0.554        |
|    mean velocity y      | 1.16          |
|    mean velocity z      | 4.96          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.09e+05     |
| time/                   |               |
|    total_timesteps      | 954500        |
| train/                  |               |
|    approx_kl            | 1.6348291e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.001         |
|    loss                 | 7.21e+07      |
|    n_updates            | 4660          |
|    policy_gradient_loss | -0.000157     |
|    std                  | 1.55          |
|    value_loss           | 7.44e+07      |
-------------------------------------------
Eval num_timesteps=955000, episode_reward=-73633.98 +/- 55198.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4723508 |
|    mean velocity x | -0.198     |
|    mean velocity y | 1.16       |
|    mean velocity z | 4.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.36e+04  |
| time/              |            |
|    total_timesteps | 955000     |
-----------------------------------
Eval num_timesteps=955500, episode_reward=-90256.39 +/- 23060.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54633856 |
|    mean velocity x | -0.444      |
|    mean velocity y | 0.858       |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 955500      |
------------------------------------
Eval num_timesteps=956000, episode_reward=-41159.17 +/- 20530.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24022232 |
|    mean velocity x | -0.449      |
|    mean velocity y | 0.685       |
|    mean velocity z | 0.496       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.12e+04   |
| time/              |             |
|    total_timesteps | 956000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 467    |
|    time_elapsed    | 38608  |
|    total_timesteps | 956416 |
-------------------------------
Eval num_timesteps=956500, episode_reward=-87077.92 +/- 42615.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.50371796   |
|    mean velocity x      | 0.118         |
|    mean velocity y      | 1.58          |
|    mean velocity z      | 3.99          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.71e+04     |
| time/                   |               |
|    total_timesteps      | 956500        |
| train/                  |               |
|    approx_kl            | 0.00029342438 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.001         |
|    loss                 | 8.79e+06      |
|    n_updates            | 4670          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 1.55          |
|    value_loss           | 4.64e+07      |
-------------------------------------------
Eval num_timesteps=957000, episode_reward=-77087.47 +/- 41522.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48950148 |
|    mean velocity x | -0.267      |
|    mean velocity y | 1.12        |
|    mean velocity z | 2.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 957000      |
------------------------------------
Eval num_timesteps=957500, episode_reward=-89208.21 +/- 29947.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14532605 |
|    mean velocity x | -0.0785     |
|    mean velocity y | 0.497       |
|    mean velocity z | 0.213       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.92e+04   |
| time/              |             |
|    total_timesteps | 957500      |
------------------------------------
Eval num_timesteps=958000, episode_reward=-101379.57 +/- 21477.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37609583 |
|    mean velocity x | -0.264      |
|    mean velocity y | 0.535       |
|    mean velocity z | 1.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 958000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 468    |
|    time_elapsed    | 38688  |
|    total_timesteps | 958464 |
-------------------------------
Eval num_timesteps=958500, episode_reward=-75227.57 +/- 56146.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.504423     |
|    mean velocity x      | 0.282         |
|    mean velocity y      | 1.97          |
|    mean velocity z      | 4.07          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.52e+04     |
| time/                   |               |
|    total_timesteps      | 958500        |
| train/                  |               |
|    approx_kl            | 0.00024029624 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.001         |
|    loss                 | 8.13e+06      |
|    n_updates            | 4680          |
|    policy_gradient_loss | -0.0016       |
|    std                  | 1.55          |
|    value_loss           | 1.46e+07      |
-------------------------------------------
Eval num_timesteps=959000, episode_reward=-102146.82 +/- 25116.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40142274 |
|    mean velocity x | 0.0471      |
|    mean velocity y | 1.19        |
|    mean velocity z | 3.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 959000      |
------------------------------------
Eval num_timesteps=959500, episode_reward=-93670.96 +/- 41272.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -1.0073596 |
|    mean velocity x | 0.726      |
|    mean velocity y | 3          |
|    mean velocity z | 9.39       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.37e+04  |
| time/              |            |
|    total_timesteps | 959500     |
-----------------------------------
Eval num_timesteps=960000, episode_reward=-75122.69 +/- 34432.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45629966 |
|    mean velocity x | 0.199       |
|    mean velocity y | 1.15        |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 960000      |
------------------------------------
Eval num_timesteps=960500, episode_reward=-47284.22 +/- 42051.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5035341 |
|    mean velocity x | -0.542     |
|    mean velocity y | 0.788      |
|    mean velocity z | 4.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.73e+04  |
| time/              |            |
|    total_timesteps | 960500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 469    |
|    time_elapsed    | 38788  |
|    total_timesteps | 960512 |
-------------------------------
Eval num_timesteps=961000, episode_reward=-103171.51 +/- 25978.22
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3714828    |
|    mean velocity x      | -0.339        |
|    mean velocity y      | 0.352         |
|    mean velocity z      | 1.99          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 961000        |
| train/                  |               |
|    approx_kl            | 2.1232263e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.001         |
|    loss                 | 4.96e+07      |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.000298     |
|    std                  | 1.55          |
|    value_loss           | 7.65e+07      |
-------------------------------------------
Eval num_timesteps=961500, episode_reward=-76352.38 +/- 49260.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4399498 |
|    mean velocity x | -0.423     |
|    mean velocity y | 0.424      |
|    mean velocity z | 3.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.64e+04  |
| time/              |            |
|    total_timesteps | 961500     |
-----------------------------------
Eval num_timesteps=962000, episode_reward=-90261.70 +/- 21373.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3241397 |
|    mean velocity x | -0.111     |
|    mean velocity y | 0.374      |
|    mean velocity z | 2.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.03e+04  |
| time/              |            |
|    total_timesteps | 962000     |
-----------------------------------
Eval num_timesteps=962500, episode_reward=-65345.08 +/- 47894.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52322406 |
|    mean velocity x | -0.353      |
|    mean velocity y | 1.11        |
|    mean velocity z | 4.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.53e+04   |
| time/              |             |
|    total_timesteps | 962500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 470    |
|    time_elapsed    | 38868  |
|    total_timesteps | 962560 |
-------------------------------
Eval num_timesteps=963000, episode_reward=-83731.23 +/- 49507.73
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4967856    |
|    mean velocity x      | -0.423        |
|    mean velocity y      | 0.541         |
|    mean velocity z      | 4.59          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.37e+04     |
| time/                   |               |
|    total_timesteps      | 963000        |
| train/                  |               |
|    approx_kl            | 4.8596266e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+07      |
|    n_updates            | 4700          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 1.55          |
|    value_loss           | 8.25e+07      |
-------------------------------------------
Eval num_timesteps=963500, episode_reward=-75851.69 +/- 59599.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18983121 |
|    mean velocity x | -2.34       |
|    mean velocity y | -1.5        |
|    mean velocity z | 6.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 963500      |
------------------------------------
Eval num_timesteps=964000, episode_reward=-65238.19 +/- 53956.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3245555 |
|    mean velocity x | -0.105     |
|    mean velocity y | 0.269      |
|    mean velocity z | 0.822      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.52e+04  |
| time/              |            |
|    total_timesteps | 964000     |
-----------------------------------
Eval num_timesteps=964500, episode_reward=-80164.97 +/- 22470.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47471684 |
|    mean velocity x | -0.0673     |
|    mean velocity y | 1.53        |
|    mean velocity z | 4.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.02e+04   |
| time/              |             |
|    total_timesteps | 964500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 471    |
|    time_elapsed    | 38948  |
|    total_timesteps | 964608 |
-------------------------------
Eval num_timesteps=965000, episode_reward=-69711.72 +/- 39553.02
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.64212656  |
|    mean velocity x      | -0.435       |
|    mean velocity y      | 1.57         |
|    mean velocity z      | 4.56         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.97e+04    |
| time/                   |              |
|    total_timesteps      | 965000       |
| train/                  |              |
|    approx_kl            | 6.087951e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.001        |
|    loss                 | 9.6e+06      |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.000333    |
|    std                  | 1.55         |
|    value_loss           | 3.69e+07     |
------------------------------------------
Eval num_timesteps=965500, episode_reward=-92700.44 +/- 12485.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41952556 |
|    mean velocity x | -1.22       |
|    mean velocity y | 0.144       |
|    mean velocity z | 3.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 965500      |
------------------------------------
Eval num_timesteps=966000, episode_reward=-58758.38 +/- 40635.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3684069 |
|    mean velocity x | -1.22      |
|    mean velocity y | -0.22      |
|    mean velocity z | 3.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.88e+04  |
| time/              |            |
|    total_timesteps | 966000     |
-----------------------------------
Eval num_timesteps=966500, episode_reward=-45486.84 +/- 27976.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46351984 |
|    mean velocity x | -0.215      |
|    mean velocity y | 0.978       |
|    mean velocity z | 2.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.55e+04   |
| time/              |             |
|    total_timesteps | 966500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 472    |
|    time_elapsed    | 39029  |
|    total_timesteps | 966656 |
-------------------------------
Eval num_timesteps=967000, episode_reward=-110011.32 +/- 28885.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34473702   |
|    mean velocity x      | -0.47         |
|    mean velocity y      | 0.586         |
|    mean velocity z      | 3.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.1e+05      |
| time/                   |               |
|    total_timesteps      | 967000        |
| train/                  |               |
|    approx_kl            | 2.3559318e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.26          |
|    learning_rate        | 0.001         |
|    loss                 | 1.64e+07      |
|    n_updates            | 4720          |
|    policy_gradient_loss | -0.000262     |
|    std                  | 1.55          |
|    value_loss           | 2.64e+07      |
-------------------------------------------
Eval num_timesteps=967500, episode_reward=-54660.55 +/- 38328.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5953311 |
|    mean velocity x | -0.253     |
|    mean velocity y | 1.67       |
|    mean velocity z | 4.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.47e+04  |
| time/              |            |
|    total_timesteps | 967500     |
-----------------------------------
Eval num_timesteps=968000, episode_reward=-67851.59 +/- 53326.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24561831 |
|    mean velocity x | -0.396      |
|    mean velocity y | -0.16       |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.79e+04   |
| time/              |             |
|    total_timesteps | 968000      |
------------------------------------
Eval num_timesteps=968500, episode_reward=-73424.00 +/- 56240.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56566006 |
|    mean velocity x | -0.333      |
|    mean velocity y | 1.39        |
|    mean velocity z | 5.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.34e+04   |
| time/              |             |
|    total_timesteps | 968500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 473    |
|    time_elapsed    | 39109  |
|    total_timesteps | 968704 |
-------------------------------
Eval num_timesteps=969000, episode_reward=-94447.57 +/- 53613.10
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.55276686  |
|    mean velocity x      | -0.563       |
|    mean velocity y      | 0.722        |
|    mean velocity z      | 4.06         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.44e+04    |
| time/                   |              |
|    total_timesteps      | 969000       |
| train/                  |              |
|    approx_kl            | 2.569941e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.001        |
|    loss                 | 3.99e+07     |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.00072     |
|    std                  | 1.55         |
|    value_loss           | 7.73e+07     |
------------------------------------------
Eval num_timesteps=969500, episode_reward=-121012.14 +/- 12202.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28814447 |
|    mean velocity x | -0.125      |
|    mean velocity y | 0.422       |
|    mean velocity z | 1.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 969500      |
------------------------------------
Eval num_timesteps=970000, episode_reward=-76416.60 +/- 45690.71
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.439781 |
|    mean velocity x | 0.201     |
|    mean velocity y | 1.1       |
|    mean velocity z | 2.82      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.64e+04 |
| time/              |           |
|    total_timesteps | 970000    |
----------------------------------
Eval num_timesteps=970500, episode_reward=-75687.22 +/- 24516.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6089176 |
|    mean velocity x | 0.204      |
|    mean velocity y | 1.49       |
|    mean velocity z | 3.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.57e+04  |
| time/              |            |
|    total_timesteps | 970500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 474    |
|    time_elapsed    | 39190  |
|    total_timesteps | 970752 |
-------------------------------
Eval num_timesteps=971000, episode_reward=-95217.40 +/- 32316.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5637531    |
|    mean velocity x      | -0.45         |
|    mean velocity y      | 1.11          |
|    mean velocity z      | 4.59          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.52e+04     |
| time/                   |               |
|    total_timesteps      | 971000        |
| train/                  |               |
|    approx_kl            | 9.4921765e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.223         |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+07      |
|    n_updates            | 4740          |
|    policy_gradient_loss | -0.000555     |
|    std                  | 1.55          |
|    value_loss           | 3.01e+07      |
-------------------------------------------
Eval num_timesteps=971500, episode_reward=-88475.74 +/- 21553.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41658142 |
|    mean velocity x | -0.198      |
|    mean velocity y | 0.762       |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 971500      |
------------------------------------
Eval num_timesteps=972000, episode_reward=-60367.35 +/- 47615.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56479615 |
|    mean velocity x | -0.499      |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.04e+04   |
| time/              |             |
|    total_timesteps | 972000      |
------------------------------------
Eval num_timesteps=972500, episode_reward=-103875.42 +/- 29518.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33153233 |
|    mean velocity x | -0.294      |
|    mean velocity y | 0.0477      |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 972500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 475    |
|    time_elapsed    | 39270  |
|    total_timesteps | 972800 |
-------------------------------
Eval num_timesteps=973000, episode_reward=-72512.12 +/- 30149.33
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42646062  |
|    mean velocity x      | -0.376       |
|    mean velocity y      | 0.943        |
|    mean velocity z      | 3.2          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.25e+04    |
| time/                   |              |
|    total_timesteps      | 973000       |
| train/                  |              |
|    approx_kl            | 0.0001404304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+07      |
|    n_updates            | 4750         |
|    policy_gradient_loss | -0.000769    |
|    std                  | 1.55         |
|    value_loss           | 7.62e+07     |
------------------------------------------
Eval num_timesteps=973500, episode_reward=-85461.16 +/- 47878.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5389487 |
|    mean velocity x | -0.196     |
|    mean velocity y | 1.04       |
|    mean velocity z | 2.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.55e+04  |
| time/              |            |
|    total_timesteps | 973500     |
-----------------------------------
Eval num_timesteps=974000, episode_reward=-77178.56 +/- 50442.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41361842 |
|    mean velocity x | -0.503      |
|    mean velocity y | 0.0246      |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 974000      |
------------------------------------
Eval num_timesteps=974500, episode_reward=-33840.75 +/- 46434.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5078363 |
|    mean velocity x | -0.362     |
|    mean velocity y | 0.933      |
|    mean velocity z | 4.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.38e+04  |
| time/              |            |
|    total_timesteps | 974500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 476    |
|    time_elapsed    | 39350  |
|    total_timesteps | 974848 |
-------------------------------
Eval num_timesteps=975000, episode_reward=-91087.56 +/- 40509.57
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3557755   |
|    mean velocity x      | -0.873       |
|    mean velocity y      | 0.177        |
|    mean velocity z      | 3.08         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.11e+04    |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 2.513238e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+07     |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.000377    |
|    std                  | 1.55         |
|    value_loss           | 5.45e+07     |
------------------------------------------
Eval num_timesteps=975500, episode_reward=-100036.21 +/- 11489.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42700577 |
|    mean velocity x | 0.301       |
|    mean velocity y | 1.29        |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 975500      |
------------------------------------
Eval num_timesteps=976000, episode_reward=-84895.49 +/- 47379.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4092819 |
|    mean velocity x | 0.37       |
|    mean velocity y | 1.19       |
|    mean velocity z | 3.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.49e+04  |
| time/              |            |
|    total_timesteps | 976000     |
-----------------------------------
Eval num_timesteps=976500, episode_reward=-56735.94 +/- 32545.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3288319 |
|    mean velocity x | -0.0326    |
|    mean velocity y | 0.73       |
|    mean velocity z | 3.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.67e+04  |
| time/              |            |
|    total_timesteps | 976500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 477    |
|    time_elapsed    | 39430  |
|    total_timesteps | 976896 |
-------------------------------
Eval num_timesteps=977000, episode_reward=-115196.78 +/- 17006.62
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45097438   |
|    mean velocity x      | -0.45         |
|    mean velocity y      | 0.491         |
|    mean velocity z      | 4.38          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.15e+05     |
| time/                   |               |
|    total_timesteps      | 977000        |
| train/                  |               |
|    approx_kl            | 2.3643195e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 3.19e+07      |
|    n_updates            | 4770          |
|    policy_gradient_loss | -0.000606     |
|    std                  | 1.55          |
|    value_loss           | 6.3e+07       |
-------------------------------------------
Eval num_timesteps=977500, episode_reward=-86900.40 +/- 27260.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5028236 |
|    mean velocity x | -0.432     |
|    mean velocity y | 0.85       |
|    mean velocity z | 4.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 977500     |
-----------------------------------
Eval num_timesteps=978000, episode_reward=-82544.50 +/- 45384.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42256457 |
|    mean velocity x | -1.44       |
|    mean velocity y | -0.978      |
|    mean velocity z | 6.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.25e+04   |
| time/              |             |
|    total_timesteps | 978000      |
------------------------------------
Eval num_timesteps=978500, episode_reward=-89036.05 +/- 19110.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38547358 |
|    mean velocity x | 0.378       |
|    mean velocity y | 1.01        |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.9e+04    |
| time/              |             |
|    total_timesteps | 978500      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 478    |
|    time_elapsed    | 39511  |
|    total_timesteps | 978944 |
-------------------------------
Eval num_timesteps=979000, episode_reward=-69108.29 +/- 29688.00
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.42844445    |
|    mean velocity x      | -0.127         |
|    mean velocity y      | 0.874          |
|    mean velocity z      | 4.37           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -6.91e+04      |
| time/                   |                |
|    total_timesteps      | 979000         |
| train/                  |                |
|    approx_kl            | 0.000110458466 |
|    clip_fraction        | 0.000195       |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.55          |
|    explained_variance   | 0.178          |
|    learning_rate        | 0.001          |
|    loss                 | 3.14e+07       |
|    n_updates            | 4780           |
|    policy_gradient_loss | -0.000877      |
|    std                  | 1.55           |
|    value_loss           | 7.48e+07       |
--------------------------------------------
Eval num_timesteps=979500, episode_reward=-70457.87 +/- 34979.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32939565 |
|    mean velocity x | -0.239      |
|    mean velocity y | 0.606       |
|    mean velocity z | 0.898       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 979500      |
------------------------------------
Eval num_timesteps=980000, episode_reward=-81339.27 +/- 47294.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62753135 |
|    mean velocity x | 0.059       |
|    mean velocity y | 2.22        |
|    mean velocity z | 5.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 980000      |
------------------------------------
Eval num_timesteps=980500, episode_reward=-85212.44 +/- 23615.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5081368 |
|    mean velocity x | -0.717     |
|    mean velocity y | 0.641      |
|    mean velocity z | 4.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.52e+04  |
| time/              |            |
|    total_timesteps | 980500     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 479    |
|    time_elapsed    | 39591  |
|    total_timesteps | 980992 |
-------------------------------
Eval num_timesteps=981000, episode_reward=-75052.02 +/- 37011.18
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35361102  |
|    mean velocity x      | -0.332       |
|    mean velocity y      | 0.413        |
|    mean velocity z      | 2.33         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.51e+04    |
| time/                   |              |
|    total_timesteps      | 981000       |
| train/                  |              |
|    approx_kl            | 6.433265e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+07     |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000517    |
|    std                  | 1.55         |
|    value_loss           | 3.27e+07     |
------------------------------------------
Eval num_timesteps=981500, episode_reward=-77441.90 +/- 36567.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34479102 |
|    mean velocity x | -0.0742     |
|    mean velocity y | 0.936       |
|    mean velocity z | 1.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.74e+04   |
| time/              |             |
|    total_timesteps | 981500      |
------------------------------------
Eval num_timesteps=982000, episode_reward=-98859.91 +/- 15915.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31243688 |
|    mean velocity x | -0.381      |
|    mean velocity y | 0.736       |
|    mean velocity z | 0.713       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.89e+04   |
| time/              |             |
|    total_timesteps | 982000      |
------------------------------------
Eval num_timesteps=982500, episode_reward=-62508.64 +/- 43454.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44319177 |
|    mean velocity x | 0.376       |
|    mean velocity y | 0.859       |
|    mean velocity z | 1.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 982500      |
------------------------------------
Eval num_timesteps=983000, episode_reward=-93892.34 +/- 35687.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44123918 |
|    mean velocity x | -0.669      |
|    mean velocity y | 0.904       |
|    mean velocity z | 2.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.39e+04   |
| time/              |             |
|    total_timesteps | 983000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 480    |
|    time_elapsed    | 39691  |
|    total_timesteps | 983040 |
-------------------------------
Eval num_timesteps=983500, episode_reward=-61106.66 +/- 45969.17
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4263606    |
|    mean velocity x      | -0.0357       |
|    mean velocity y      | 1.42          |
|    mean velocity z      | 4.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.11e+04     |
| time/                   |               |
|    total_timesteps      | 983500        |
| train/                  |               |
|    approx_kl            | 0.00024131907 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.164         |
|    learning_rate        | 0.001         |
|    loss                 | 4e+06         |
|    n_updates            | 4800          |
|    policy_gradient_loss | -0.00161      |
|    std                  | 1.55          |
|    value_loss           | 2.76e+07      |
-------------------------------------------
Eval num_timesteps=984000, episode_reward=-81310.81 +/- 15163.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3639252 |
|    mean velocity x | -1.19      |
|    mean velocity y | -0.578     |
|    mean velocity z | 3.9        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.13e+04  |
| time/              |            |
|    total_timesteps | 984000     |
-----------------------------------
Eval num_timesteps=984500, episode_reward=-94416.75 +/- 26355.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43282667 |
|    mean velocity x | -0.893      |
|    mean velocity y | 0.301       |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.44e+04   |
| time/              |             |
|    total_timesteps | 984500      |
------------------------------------
Eval num_timesteps=985000, episode_reward=-95767.89 +/- 14072.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56365836 |
|    mean velocity x | -0.515      |
|    mean velocity y | 0.833       |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.58e+04   |
| time/              |             |
|    total_timesteps | 985000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 481    |
|    time_elapsed    | 39771  |
|    total_timesteps | 985088 |
-------------------------------
Eval num_timesteps=985500, episode_reward=-98515.22 +/- 55967.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5312599    |
|    mean velocity x      | -0.254        |
|    mean velocity y      | 1.09          |
|    mean velocity z      | 3.19          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.85e+04     |
| time/                   |               |
|    total_timesteps      | 985500        |
| train/                  |               |
|    approx_kl            | 0.00015192301 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.232         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+07      |
|    n_updates            | 4810          |
|    policy_gradient_loss | -0.000885     |
|    std                  | 1.55          |
|    value_loss           | 4.99e+07      |
-------------------------------------------
Eval num_timesteps=986000, episode_reward=-62253.58 +/- 43126.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28344843 |
|    mean velocity x | -0.201      |
|    mean velocity y | 0.471       |
|    mean velocity z | 0.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+04   |
| time/              |             |
|    total_timesteps | 986000      |
------------------------------------
Eval num_timesteps=986500, episode_reward=-94980.29 +/- 24275.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26655224 |
|    mean velocity x | -0.181      |
|    mean velocity y | 0.521       |
|    mean velocity z | 0.483       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.5e+04    |
| time/              |             |
|    total_timesteps | 986500      |
------------------------------------
Eval num_timesteps=987000, episode_reward=-84878.62 +/- 33260.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31652537 |
|    mean velocity x | -1.07       |
|    mean velocity y | -0.475      |
|    mean velocity z | 3.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.49e+04   |
| time/              |             |
|    total_timesteps | 987000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 482    |
|    time_elapsed    | 39851  |
|    total_timesteps | 987136 |
-------------------------------
Eval num_timesteps=987500, episode_reward=-86936.00 +/- 27171.32
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4183275    |
|    mean velocity x      | -0.213        |
|    mean velocity y      | 1.09          |
|    mean velocity z      | 3.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.69e+04     |
| time/                   |               |
|    total_timesteps      | 987500        |
| train/                  |               |
|    approx_kl            | 0.00014254288 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.001         |
|    loss                 | 2.39e+06      |
|    n_updates            | 4820          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.55          |
|    value_loss           | 2.1e+07       |
-------------------------------------------
Eval num_timesteps=988000, episode_reward=-94558.82 +/- 34260.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39087954 |
|    mean velocity x | -0.109      |
|    mean velocity y | 0.689       |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.46e+04   |
| time/              |             |
|    total_timesteps | 988000      |
------------------------------------
Eval num_timesteps=988500, episode_reward=-82125.58 +/- 37559.19
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.102121025 |
|    mean velocity x | -2.27        |
|    mean velocity y | -1.79        |
|    mean velocity z | 6.25         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -8.21e+04    |
| time/              |              |
|    total_timesteps | 988500       |
-------------------------------------
Eval num_timesteps=989000, episode_reward=-38784.49 +/- 48399.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27187136 |
|    mean velocity x | 0.154       |
|    mean velocity y | 0.37        |
|    mean velocity z | 0.751       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.88e+04   |
| time/              |             |
|    total_timesteps | 989000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 483    |
|    time_elapsed    | 39932  |
|    total_timesteps | 989184 |
-------------------------------
Eval num_timesteps=989500, episode_reward=-63957.93 +/- 51741.57
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44317573  |
|    mean velocity x      | -1.05        |
|    mean velocity y      | 0.11         |
|    mean velocity z      | 3.71         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.4e+04     |
| time/                   |              |
|    total_timesteps      | 989500       |
| train/                  |              |
|    approx_kl            | 0.0004442138 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.001        |
|    loss                 | 5.16e+07     |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1.55         |
|    value_loss           | 5.05e+07     |
------------------------------------------
Eval num_timesteps=990000, episode_reward=-98800.24 +/- 26861.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20739119 |
|    mean velocity x | -1.7        |
|    mean velocity y | -1.36       |
|    mean velocity z | 5.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.88e+04   |
| time/              |             |
|    total_timesteps | 990000      |
------------------------------------
Eval num_timesteps=990500, episode_reward=-66511.70 +/- 29383.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38904175 |
|    mean velocity x | -0.295      |
|    mean velocity y | 0.449       |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.65e+04   |
| time/              |             |
|    total_timesteps | 990500      |
------------------------------------
Eval num_timesteps=991000, episode_reward=-91572.15 +/- 32667.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45309615 |
|    mean velocity x | -0.783      |
|    mean velocity y | 0.239       |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.16e+04   |
| time/              |             |
|    total_timesteps | 991000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 484    |
|    time_elapsed    | 40012  |
|    total_timesteps | 991232 |
-------------------------------
Eval num_timesteps=991500, episode_reward=-42079.40 +/- 29774.17
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49811313   |
|    mean velocity x      | 0.0479        |
|    mean velocity y      | 0.993         |
|    mean velocity z      | 3.03          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.21e+04     |
| time/                   |               |
|    total_timesteps      | 991500        |
| train/                  |               |
|    approx_kl            | 0.00026868738 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.302         |
|    learning_rate        | 0.001         |
|    loss                 | 2.51e+07      |
|    n_updates            | 4840          |
|    policy_gradient_loss | -0.00138      |
|    std                  | 1.55          |
|    value_loss           | 4.52e+07      |
-------------------------------------------
Eval num_timesteps=992000, episode_reward=-93580.85 +/- 50911.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39176866 |
|    mean velocity x | -0.276      |
|    mean velocity y | 0.707       |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 992000      |
------------------------------------
Eval num_timesteps=992500, episode_reward=-87054.57 +/- 17767.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41889733 |
|    mean velocity x | 0.0964      |
|    mean velocity y | 0.838       |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.71e+04   |
| time/              |             |
|    total_timesteps | 992500      |
------------------------------------
Eval num_timesteps=993000, episode_reward=-71660.36 +/- 38673.81
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.363214 |
|    mean velocity x | 0.00787   |
|    mean velocity y | 1.27      |
|    mean velocity z | 3.69      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.17e+04 |
| time/              |           |
|    total_timesteps | 993000    |
----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 485    |
|    time_elapsed    | 40093  |
|    total_timesteps | 993280 |
-------------------------------
Eval num_timesteps=993500, episode_reward=-81971.44 +/- 14600.91
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49117118  |
|    mean velocity x      | -0.065       |
|    mean velocity y      | 0.955        |
|    mean velocity z      | 3.26         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.2e+04     |
| time/                   |              |
|    total_timesteps      | 993500       |
| train/                  |              |
|    approx_kl            | 3.328733e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.001        |
|    loss                 | 2.63e+07     |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.000526    |
|    std                  | 1.55         |
|    value_loss           | 5.32e+07     |
------------------------------------------
Eval num_timesteps=994000, episode_reward=-73023.27 +/- 52287.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5110027 |
|    mean velocity x | -0.303     |
|    mean velocity y | 1.48       |
|    mean velocity z | 4.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.3e+04   |
| time/              |            |
|    total_timesteps | 994000     |
-----------------------------------
Eval num_timesteps=994500, episode_reward=-80253.82 +/- 46116.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45298198 |
|    mean velocity x | -0.343      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.03e+04   |
| time/              |             |
|    total_timesteps | 994500      |
------------------------------------
Eval num_timesteps=995000, episode_reward=-75291.15 +/- 40797.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52732366 |
|    mean velocity x | -0.43       |
|    mean velocity y | 1.32        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.53e+04   |
| time/              |             |
|    total_timesteps | 995000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 486    |
|    time_elapsed    | 40173  |
|    total_timesteps | 995328 |
-------------------------------
Eval num_timesteps=995500, episode_reward=-97344.42 +/- 35963.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3574611   |
|    mean velocity x      | -0.42        |
|    mean velocity y      | 0.876        |
|    mean velocity z      | 0.531        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.73e+04    |
| time/                   |              |
|    total_timesteps      | 995500       |
| train/                  |              |
|    approx_kl            | 7.685184e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.001        |
|    loss                 | 2.62e+07     |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.00073     |
|    std                  | 1.55         |
|    value_loss           | 5.89e+07     |
------------------------------------------
Eval num_timesteps=996000, episode_reward=-109383.57 +/- 7620.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4325286 |
|    mean velocity x | -0.167     |
|    mean velocity y | 0.63       |
|    mean velocity z | 4.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 996000     |
-----------------------------------
Eval num_timesteps=996500, episode_reward=-84621.65 +/- 48972.45
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.553985 |
|    mean velocity x | -0.677    |
|    mean velocity y | 0.544     |
|    mean velocity z | 4.71      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.46e+04 |
| time/              |           |
|    total_timesteps | 996500    |
----------------------------------
Eval num_timesteps=997000, episode_reward=-68861.12 +/- 36553.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22861111 |
|    mean velocity x | -0.402      |
|    mean velocity y | -0.0398     |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.89e+04   |
| time/              |             |
|    total_timesteps | 997000      |
------------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 487    |
|    time_elapsed    | 40253  |
|    total_timesteps | 997376 |
-------------------------------
Eval num_timesteps=997500, episode_reward=-93675.24 +/- 15372.05
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4311963    |
|    mean velocity x      | -0.322        |
|    mean velocity y      | 0.991         |
|    mean velocity z      | 4.11          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.37e+04     |
| time/                   |               |
|    total_timesteps      | 997500        |
| train/                  |               |
|    approx_kl            | 2.6513619e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.001         |
|    loss                 | 4.7e+07       |
|    n_updates            | 4870          |
|    policy_gradient_loss | -0.000541     |
|    std                  | 1.55          |
|    value_loss           | 7.88e+07      |
-------------------------------------------
Eval num_timesteps=998000, episode_reward=-69338.44 +/- 56585.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41852772 |
|    mean velocity x | 0.0852      |
|    mean velocity y | 0.715       |
|    mean velocity z | 2.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.93e+04   |
| time/              |             |
|    total_timesteps | 998000      |
------------------------------------
Eval num_timesteps=998500, episode_reward=-94369.94 +/- 10948.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4326477 |
|    mean velocity x | -0.199     |
|    mean velocity y | 1.25       |
|    mean velocity z | 4.15       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.44e+04  |
| time/              |            |
|    total_timesteps | 998500     |
-----------------------------------
Eval num_timesteps=999000, episode_reward=-99419.53 +/- 41210.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3426202 |
|    mean velocity x | -0.268     |
|    mean velocity y | 0.436      |
|    mean velocity z | 1.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.94e+04  |
| time/              |            |
|    total_timesteps | 999000     |
-----------------------------------
-------------------------------
| time/              |        |
|    fps             | 24     |
|    iterations      | 488    |
|    time_elapsed    | 40334  |
|    total_timesteps | 999424 |
-------------------------------
Eval num_timesteps=999500, episode_reward=-73376.84 +/- 36038.57
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34959742   |
|    mean velocity x      | -1.02         |
|    mean velocity y      | -0.757        |
|    mean velocity z      | 4.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.34e+04     |
| time/                   |               |
|    total_timesteps      | 999500        |
| train/                  |               |
|    approx_kl            | 2.5649846e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.228         |
|    learning_rate        | 0.001         |
|    loss                 | 1.97e+06      |
|    n_updates            | 4880          |
|    policy_gradient_loss | -0.000526     |
|    std                  | 1.55          |
|    value_loss           | 3.46e+07      |
-------------------------------------------
Eval num_timesteps=1000000, episode_reward=-91831.29 +/- 26569.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5643043 |
|    mean velocity x | -0.428     |
|    mean velocity y | 1.12       |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.18e+04  |
| time/              |            |
|    total_timesteps | 1000000    |
-----------------------------------
Eval num_timesteps=1000500, episode_reward=-30318.67 +/- 28028.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38903016 |
|    mean velocity x | -0.54       |
|    mean velocity y | 0.0443      |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.03e+04   |
| time/              |             |
|    total_timesteps | 1000500     |
------------------------------------
Eval num_timesteps=1001000, episode_reward=-73568.57 +/- 47368.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44604775 |
|    mean velocity x | -0.363      |
|    mean velocity y | 0.791       |
|    mean velocity z | 4.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1001000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 489     |
|    time_elapsed    | 40414   |
|    total_timesteps | 1001472 |
--------------------------------
Eval num_timesteps=1001500, episode_reward=-53201.82 +/- 40975.31
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42646846   |
|    mean velocity x      | -0.837        |
|    mean velocity y      | 0.0869        |
|    mean velocity z      | 3.61          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.32e+04     |
| time/                   |               |
|    total_timesteps      | 1001500       |
| train/                  |               |
|    approx_kl            | 0.00012941423 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.14          |
|    learning_rate        | 0.001         |
|    loss                 | 4.86e+07      |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.000816     |
|    std                  | 1.55          |
|    value_loss           | 9.26e+07      |
-------------------------------------------
Eval num_timesteps=1002000, episode_reward=-77499.63 +/- 28005.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23986907 |
|    mean velocity x | -0.198      |
|    mean velocity y | 0.526       |
|    mean velocity z | 2.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.75e+04   |
| time/              |             |
|    total_timesteps | 1002000     |
------------------------------------
Eval num_timesteps=1002500, episode_reward=-108553.09 +/- 26683.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45589945 |
|    mean velocity x | -0.471      |
|    mean velocity y | 0.931       |
|    mean velocity z | 4.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1002500     |
------------------------------------
Eval num_timesteps=1003000, episode_reward=-103852.76 +/- 22774.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35632288 |
|    mean velocity x | -0.645      |
|    mean velocity y | 0.612       |
|    mean velocity z | 2.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1003000     |
------------------------------------
Eval num_timesteps=1003500, episode_reward=-78897.32 +/- 44454.11
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.285704 |
|    mean velocity x | -1.18     |
|    mean velocity y | -1.05     |
|    mean velocity z | 5.4       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.89e+04 |
| time/              |           |
|    total_timesteps | 1003500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 490     |
|    time_elapsed    | 40513   |
|    total_timesteps | 1003520 |
--------------------------------
Eval num_timesteps=1004000, episode_reward=-83862.04 +/- 15189.95
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45448628   |
|    mean velocity x      | 0.311         |
|    mean velocity y      | 1.68          |
|    mean velocity z      | 3.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.39e+04     |
| time/                   |               |
|    total_timesteps      | 1004000       |
| train/                  |               |
|    approx_kl            | 3.0890544e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 2.94e+07      |
|    n_updates            | 4900          |
|    policy_gradient_loss | -0.000505     |
|    std                  | 1.55          |
|    value_loss           | 3.86e+07      |
-------------------------------------------
Eval num_timesteps=1004500, episode_reward=-76483.79 +/- 42152.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5016939 |
|    mean velocity x | -0.185     |
|    mean velocity y | 1.06       |
|    mean velocity z | 4.71       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.65e+04  |
| time/              |            |
|    total_timesteps | 1004500    |
-----------------------------------
Eval num_timesteps=1005000, episode_reward=-77117.54 +/- 41086.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3788133 |
|    mean velocity x | -0.104     |
|    mean velocity y | 0.687      |
|    mean velocity z | 4.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.71e+04  |
| time/              |            |
|    total_timesteps | 1005000    |
-----------------------------------
Eval num_timesteps=1005500, episode_reward=-85247.40 +/- 27243.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38882557 |
|    mean velocity x | -0.379      |
|    mean velocity y | 0.361       |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.52e+04   |
| time/              |             |
|    total_timesteps | 1005500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 491     |
|    time_elapsed    | 40594   |
|    total_timesteps | 1005568 |
--------------------------------
Eval num_timesteps=1006000, episode_reward=-96337.76 +/- 38840.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48993805   |
|    mean velocity x      | -0.0452       |
|    mean velocity y      | 1.27          |
|    mean velocity z      | 2.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.63e+04     |
| time/                   |               |
|    total_timesteps      | 1006000       |
| train/                  |               |
|    approx_kl            | 0.00018938829 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.001         |
|    loss                 | 6.03e+07      |
|    n_updates            | 4910          |
|    policy_gradient_loss | -0.00108      |
|    std                  | 1.55          |
|    value_loss           | 8.55e+07      |
-------------------------------------------
Eval num_timesteps=1006500, episode_reward=-76253.74 +/- 44706.30
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.297004 |
|    mean velocity x | 0.256     |
|    mean velocity y | 0.477     |
|    mean velocity z | 2.53      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.63e+04 |
| time/              |           |
|    total_timesteps | 1006500   |
----------------------------------
Eval num_timesteps=1007000, episode_reward=-58368.51 +/- 32387.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27580655 |
|    mean velocity x | 0.14        |
|    mean velocity y | 0.208       |
|    mean velocity z | 1.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.84e+04   |
| time/              |             |
|    total_timesteps | 1007000     |
------------------------------------
Eval num_timesteps=1007500, episode_reward=-113469.50 +/- 30817.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30100226 |
|    mean velocity x | -0.0113     |
|    mean velocity y | 0.433       |
|    mean velocity z | 3.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 1007500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 492     |
|    time_elapsed    | 40674   |
|    total_timesteps | 1007616 |
--------------------------------
Eval num_timesteps=1008000, episode_reward=-93485.33 +/- 47248.86
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4575188   |
|    mean velocity x      | -0.0974      |
|    mean velocity y      | 1.14         |
|    mean velocity z      | 4.13         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.35e+04    |
| time/                   |              |
|    total_timesteps      | 1008000      |
| train/                  |              |
|    approx_kl            | 7.350108e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.001        |
|    loss                 | 1.69e+07     |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.000484    |
|    std                  | 1.55         |
|    value_loss           | 4.88e+07     |
------------------------------------------
Eval num_timesteps=1008500, episode_reward=-85881.04 +/- 38016.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3699046 |
|    mean velocity x | 0.0528     |
|    mean velocity y | 0.518      |
|    mean velocity z | 2.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.59e+04  |
| time/              |            |
|    total_timesteps | 1008500    |
-----------------------------------
Eval num_timesteps=1009000, episode_reward=-69981.84 +/- 37481.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3172753 |
|    mean velocity x | -1.18      |
|    mean velocity y | -0.928     |
|    mean velocity z | 5.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7e+04     |
| time/              |            |
|    total_timesteps | 1009000    |
-----------------------------------
Eval num_timesteps=1009500, episode_reward=-92802.68 +/- 18183.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42720538 |
|    mean velocity x | -0.103      |
|    mean velocity y | 1.26        |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.28e+04   |
| time/              |             |
|    total_timesteps | 1009500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 493     |
|    time_elapsed    | 40754   |
|    total_timesteps | 1009664 |
--------------------------------
Eval num_timesteps=1010000, episode_reward=-58067.86 +/- 32742.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4156177    |
|    mean velocity x      | 0.00154       |
|    mean velocity y      | 1.29          |
|    mean velocity z      | 3.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.81e+04     |
| time/                   |               |
|    total_timesteps      | 1010000       |
| train/                  |               |
|    approx_kl            | 1.8529332e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+07      |
|    n_updates            | 4930          |
|    policy_gradient_loss | -0.000325     |
|    std                  | 1.55          |
|    value_loss           | 6.62e+07      |
-------------------------------------------
Eval num_timesteps=1010500, episode_reward=-61820.97 +/- 33932.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34838647 |
|    mean velocity x | -0.211      |
|    mean velocity y | 0.482       |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 1010500     |
------------------------------------
Eval num_timesteps=1011000, episode_reward=-72627.24 +/- 38955.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44626656 |
|    mean velocity x | -0.268      |
|    mean velocity y | 1.17        |
|    mean velocity z | 4.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.26e+04   |
| time/              |             |
|    total_timesteps | 1011000     |
------------------------------------
Eval num_timesteps=1011500, episode_reward=-44112.89 +/- 25058.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34945768 |
|    mean velocity x | 0.148       |
|    mean velocity y | 0.912       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.41e+04   |
| time/              |             |
|    total_timesteps | 1011500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 494     |
|    time_elapsed    | 40835   |
|    total_timesteps | 1011712 |
--------------------------------
Eval num_timesteps=1012000, episode_reward=-100307.96 +/- 25946.15
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30876207   |
|    mean velocity x      | -0.385        |
|    mean velocity y      | 0.729         |
|    mean velocity z      | 0.565         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1e+05        |
| time/                   |               |
|    total_timesteps      | 1012000       |
| train/                  |               |
|    approx_kl            | 4.2860134e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.14          |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+07       |
|    n_updates            | 4940          |
|    policy_gradient_loss | -0.000389     |
|    std                  | 1.55          |
|    value_loss           | 6.7e+07       |
-------------------------------------------
Eval num_timesteps=1012500, episode_reward=-72999.05 +/- 35391.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38877276 |
|    mean velocity x | -0.792      |
|    mean velocity y | 0.171       |
|    mean velocity z | 2.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.3e+04    |
| time/              |             |
|    total_timesteps | 1012500     |
------------------------------------
Eval num_timesteps=1013000, episode_reward=-113139.64 +/- 31329.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46540773 |
|    mean velocity x | -0.204      |
|    mean velocity y | 0.691       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 1013000     |
------------------------------------
Eval num_timesteps=1013500, episode_reward=-68195.83 +/- 55956.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39339522 |
|    mean velocity x | 0.0324      |
|    mean velocity y | 1.1         |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.82e+04   |
| time/              |             |
|    total_timesteps | 1013500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 495     |
|    time_elapsed    | 40915   |
|    total_timesteps | 1013760 |
--------------------------------
Eval num_timesteps=1014000, episode_reward=-81785.39 +/- 19746.72
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.1512949    |
|    mean velocity x      | -0.128        |
|    mean velocity y      | 0.519         |
|    mean velocity z      | 0.143         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.18e+04     |
| time/                   |               |
|    total_timesteps      | 1014000       |
| train/                  |               |
|    approx_kl            | 2.3244153e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+07      |
|    n_updates            | 4950          |
|    policy_gradient_loss | -0.000479     |
|    std                  | 1.55          |
|    value_loss           | 4.05e+07      |
-------------------------------------------
Eval num_timesteps=1014500, episode_reward=-77123.80 +/- 42242.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46604717 |
|    mean velocity x | -0.0848     |
|    mean velocity y | 0.578       |
|    mean velocity z | 2.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 1014500     |
------------------------------------
Eval num_timesteps=1015000, episode_reward=-89702.93 +/- 18676.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42150936 |
|    mean velocity x | -1.02       |
|    mean velocity y | 0.0782      |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 1015000     |
------------------------------------
Eval num_timesteps=1015500, episode_reward=-64457.12 +/- 32495.85
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2506434 |
|    mean velocity x | -0.0156    |
|    mean velocity y | 0.249      |
|    mean velocity z | 3.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.45e+04  |
| time/              |            |
|    total_timesteps | 1015500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 496     |
|    time_elapsed    | 40995   |
|    total_timesteps | 1015808 |
--------------------------------
Eval num_timesteps=1016000, episode_reward=-91953.79 +/- 24227.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3367815    |
|    mean velocity x      | -1.26         |
|    mean velocity y      | -0.0659       |
|    mean velocity z      | 2.85          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.2e+04      |
| time/                   |               |
|    total_timesteps      | 1016000       |
| train/                  |               |
|    approx_kl            | 0.00012406637 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.001         |
|    loss                 | 2.13e+07      |
|    n_updates            | 4960          |
|    policy_gradient_loss | -0.000667     |
|    std                  | 1.55          |
|    value_loss           | 3.66e+07      |
-------------------------------------------
Eval num_timesteps=1016500, episode_reward=-83875.62 +/- 44837.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5823103 |
|    mean velocity x | 0.576      |
|    mean velocity y | 1.8        |
|    mean velocity z | 3.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.39e+04  |
| time/              |            |
|    total_timesteps | 1016500    |
-----------------------------------
Eval num_timesteps=1017000, episode_reward=-72691.54 +/- 36140.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43555364 |
|    mean velocity x | -0.141      |
|    mean velocity y | 0.853       |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.27e+04   |
| time/              |             |
|    total_timesteps | 1017000     |
------------------------------------
Eval num_timesteps=1017500, episode_reward=-54681.10 +/- 50098.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41393277 |
|    mean velocity x | -0.258      |
|    mean velocity y | 0.87        |
|    mean velocity z | 2.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.47e+04   |
| time/              |             |
|    total_timesteps | 1017500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 497     |
|    time_elapsed    | 41076   |
|    total_timesteps | 1017856 |
--------------------------------
Eval num_timesteps=1018000, episode_reward=-65833.07 +/- 48791.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5174456    |
|    mean velocity x      | -0.512        |
|    mean velocity y      | 0.871         |
|    mean velocity z      | 4.51          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.58e+04     |
| time/                   |               |
|    total_timesteps      | 1018000       |
| train/                  |               |
|    approx_kl            | 2.9962335e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.001         |
|    loss                 | 2.24e+07      |
|    n_updates            | 4970          |
|    policy_gradient_loss | -0.00038      |
|    std                  | 1.55          |
|    value_loss           | 6.61e+07      |
-------------------------------------------
Eval num_timesteps=1018500, episode_reward=-99998.28 +/- 26636.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47997496 |
|    mean velocity x | -0.375      |
|    mean velocity y | 0.766       |
|    mean velocity z | 4.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1018500     |
------------------------------------
Eval num_timesteps=1019000, episode_reward=-67442.14 +/- 55284.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4498305 |
|    mean velocity x | -0.0783    |
|    mean velocity y | 1.5        |
|    mean velocity z | 4.15       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.74e+04  |
| time/              |            |
|    total_timesteps | 1019000    |
-----------------------------------
Eval num_timesteps=1019500, episode_reward=-121065.96 +/- 27204.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37422955 |
|    mean velocity x | -0.3        |
|    mean velocity y | 0.325       |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 1019500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 498     |
|    time_elapsed    | 41156   |
|    total_timesteps | 1019904 |
--------------------------------
Eval num_timesteps=1020000, episode_reward=-73467.29 +/- 42746.01
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3135793    |
|    mean velocity x      | -0.00296      |
|    mean velocity y      | 0.507         |
|    mean velocity z      | 1.94          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.35e+04     |
| time/                   |               |
|    total_timesteps      | 1020000       |
| train/                  |               |
|    approx_kl            | 0.00026299534 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.152         |
|    learning_rate        | 0.001         |
|    loss                 | 4.69e+07      |
|    n_updates            | 4980          |
|    policy_gradient_loss | -0.00143      |
|    std                  | 1.55          |
|    value_loss           | 6.72e+07      |
-------------------------------------------
Eval num_timesteps=1020500, episode_reward=-48774.71 +/- 32473.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32128602 |
|    mean velocity x | -0.314      |
|    mean velocity y | 0.344       |
|    mean velocity z | 1.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.88e+04   |
| time/              |             |
|    total_timesteps | 1020500     |
------------------------------------
Eval num_timesteps=1021000, episode_reward=-52301.85 +/- 36204.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43263677 |
|    mean velocity x | -0.372      |
|    mean velocity y | 0.433       |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.23e+04   |
| time/              |             |
|    total_timesteps | 1021000     |
------------------------------------
Eval num_timesteps=1021500, episode_reward=-83591.96 +/- 47612.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.8239806 |
|    mean velocity x | -0.164     |
|    mean velocity y | 1.98       |
|    mean velocity z | 6.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.36e+04  |
| time/              |            |
|    total_timesteps | 1021500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 499     |
|    time_elapsed    | 41236   |
|    total_timesteps | 1021952 |
--------------------------------
Eval num_timesteps=1022000, episode_reward=-94930.61 +/- 21014.91
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45661727   |
|    mean velocity x      | -0.131        |
|    mean velocity y      | 1.09          |
|    mean velocity z      | 4.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.49e+04     |
| time/                   |               |
|    total_timesteps      | 1022000       |
| train/                  |               |
|    approx_kl            | 5.1796524e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.001         |
|    loss                 | 5.86e+07      |
|    n_updates            | 4990          |
|    policy_gradient_loss | -0.000807     |
|    std                  | 1.55          |
|    value_loss           | 7.58e+07      |
-------------------------------------------
Eval num_timesteps=1022500, episode_reward=-86645.06 +/- 46073.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37752947 |
|    mean velocity x | -0.132      |
|    mean velocity y | 0.453       |
|    mean velocity z | 2.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 1022500     |
------------------------------------
Eval num_timesteps=1023000, episode_reward=-67747.30 +/- 28586.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47318342 |
|    mean velocity x | 0.394       |
|    mean velocity y | 1.66        |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.77e+04   |
| time/              |             |
|    total_timesteps | 1023000     |
------------------------------------
Eval num_timesteps=1023500, episode_reward=-87733.36 +/- 41618.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39577398 |
|    mean velocity x | -1.22       |
|    mean velocity y | 0.115       |
|    mean velocity z | 3.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 1023500     |
------------------------------------
Eval num_timesteps=1024000, episode_reward=-108332.57 +/- 21991.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47922277 |
|    mean velocity x | -0.191      |
|    mean velocity y | 1.32        |
|    mean velocity z | 4.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1024000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 500     |
|    time_elapsed    | 41336   |
|    total_timesteps | 1024000 |
--------------------------------
Eval num_timesteps=1024500, episode_reward=-102657.83 +/- 29782.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.21701892   |
|    mean velocity x      | -0.35         |
|    mean velocity y      | 0.498         |
|    mean velocity z      | 0.875         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 1024500       |
| train/                  |               |
|    approx_kl            | 4.8305228e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+07      |
|    n_updates            | 5000          |
|    policy_gradient_loss | -0.000636     |
|    std                  | 1.55          |
|    value_loss           | 4.05e+07      |
-------------------------------------------
Eval num_timesteps=1025000, episode_reward=-92315.30 +/- 48527.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41710114 |
|    mean velocity x | -0.64       |
|    mean velocity y | 0.336       |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 1025000     |
------------------------------------
Eval num_timesteps=1025500, episode_reward=-62674.41 +/- 26479.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46910003 |
|    mean velocity x | -0.48       |
|    mean velocity y | 0.846       |
|    mean velocity z | 5.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.27e+04   |
| time/              |             |
|    total_timesteps | 1025500     |
------------------------------------
Eval num_timesteps=1026000, episode_reward=-80930.31 +/- 52920.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3515368 |
|    mean velocity x | -1.1       |
|    mean velocity y | -0.208     |
|    mean velocity z | 3.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.09e+04  |
| time/              |            |
|    total_timesteps | 1026000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 501     |
|    time_elapsed    | 41416   |
|    total_timesteps | 1026048 |
--------------------------------
Eval num_timesteps=1026500, episode_reward=-101668.87 +/- 24434.34
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5635707   |
|    mean velocity x      | -0.442       |
|    mean velocity y      | 0.654        |
|    mean velocity z      | 5.47         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.02e+05    |
| time/                   |              |
|    total_timesteps      | 1026500      |
| train/                  |              |
|    approx_kl            | 4.472499e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.001        |
|    loss                 | 4.18e+07     |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.000556    |
|    std                  | 1.55         |
|    value_loss           | 5.48e+07     |
------------------------------------------
Eval num_timesteps=1027000, episode_reward=-90919.23 +/- 33941.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32766178 |
|    mean velocity x | -0.496      |
|    mean velocity y | 0.58        |
|    mean velocity z | 1.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 1027000     |
------------------------------------
Eval num_timesteps=1027500, episode_reward=-110292.38 +/- 23609.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55511636 |
|    mean velocity x | -0.983      |
|    mean velocity y | 0.631       |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 1027500     |
------------------------------------
Eval num_timesteps=1028000, episode_reward=-86300.38 +/- 50969.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43193555 |
|    mean velocity x | -0.311      |
|    mean velocity y | 0.806       |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.63e+04   |
| time/              |             |
|    total_timesteps | 1028000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 502     |
|    time_elapsed    | 41496   |
|    total_timesteps | 1028096 |
--------------------------------
Eval num_timesteps=1028500, episode_reward=-112765.96 +/- 25226.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25158796   |
|    mean velocity x      | -1.56         |
|    mean velocity y      | -0.341        |
|    mean velocity z      | 3.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.13e+05     |
| time/                   |               |
|    total_timesteps      | 1028500       |
| train/                  |               |
|    approx_kl            | 3.0160241e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.001         |
|    loss                 | 3.56e+07      |
|    n_updates            | 5020          |
|    policy_gradient_loss | -0.00027      |
|    std                  | 1.55          |
|    value_loss           | 5.1e+07       |
-------------------------------------------
Eval num_timesteps=1029000, episode_reward=-78947.82 +/- 39649.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26412854 |
|    mean velocity x | -0.19       |
|    mean velocity y | 0.465       |
|    mean velocity z | 0.294       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.89e+04   |
| time/              |             |
|    total_timesteps | 1029000     |
------------------------------------
Eval num_timesteps=1029500, episode_reward=-50201.18 +/- 40868.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6247683 |
|    mean velocity x | -0.45      |
|    mean velocity y | 1.19       |
|    mean velocity z | 5.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.02e+04  |
| time/              |            |
|    total_timesteps | 1029500    |
-----------------------------------
Eval num_timesteps=1030000, episode_reward=-87662.34 +/- 25062.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37485072 |
|    mean velocity x | 0.0748      |
|    mean velocity y | 1.18        |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 1030000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 503     |
|    time_elapsed    | 41577   |
|    total_timesteps | 1030144 |
--------------------------------
Eval num_timesteps=1030500, episode_reward=-86212.48 +/- 33305.31
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.51177156   |
|    mean velocity x      | 0.264         |
|    mean velocity y      | 1.53          |
|    mean velocity z      | 3.19          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.62e+04     |
| time/                   |               |
|    total_timesteps      | 1030500       |
| train/                  |               |
|    approx_kl            | 0.00016577655 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.263         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+07      |
|    n_updates            | 5030          |
|    policy_gradient_loss | -0.00127      |
|    std                  | 1.55          |
|    value_loss           | 4.09e+07      |
-------------------------------------------
Eval num_timesteps=1031000, episode_reward=-97460.81 +/- 21034.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3429688 |
|    mean velocity x | 0.034      |
|    mean velocity y | 0.354      |
|    mean velocity z | 1.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.75e+04  |
| time/              |            |
|    total_timesteps | 1031000    |
-----------------------------------
Eval num_timesteps=1031500, episode_reward=-64995.33 +/- 41381.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34574747 |
|    mean velocity x | -0.824      |
|    mean velocity y | 0.229       |
|    mean velocity z | 3.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 1031500     |
------------------------------------
Eval num_timesteps=1032000, episode_reward=-100201.08 +/- 61692.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19437955 |
|    mean velocity x | -0.0671     |
|    mean velocity y | 0.404       |
|    mean velocity z | 0.167       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1032000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 504     |
|    time_elapsed    | 41657   |
|    total_timesteps | 1032192 |
--------------------------------
Eval num_timesteps=1032500, episode_reward=-60273.95 +/- 33566.67
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27811053  |
|    mean velocity x      | -0.137       |
|    mean velocity y      | 0.467        |
|    mean velocity z      | 0.539        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.03e+04    |
| time/                   |              |
|    total_timesteps      | 1032500      |
| train/                  |              |
|    approx_kl            | 0.0012814173 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+07     |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 1.54         |
|    value_loss           | 9.82e+06     |
------------------------------------------
Eval num_timesteps=1033000, episode_reward=-89252.95 +/- 21484.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24323806 |
|    mean velocity x | -0.221      |
|    mean velocity y | 0.455       |
|    mean velocity z | 0.452       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1033000     |
------------------------------------
Eval num_timesteps=1033500, episode_reward=-80749.19 +/- 31008.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4189562 |
|    mean velocity x | -0.198     |
|    mean velocity y | 0.923      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 1033500    |
-----------------------------------
Eval num_timesteps=1034000, episode_reward=-83819.29 +/- 40131.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44273624 |
|    mean velocity x | -0.878      |
|    mean velocity y | 0.259       |
|    mean velocity z | 4.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.38e+04   |
| time/              |             |
|    total_timesteps | 1034000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 505     |
|    time_elapsed    | 41738   |
|    total_timesteps | 1034240 |
--------------------------------
Eval num_timesteps=1034500, episode_reward=-113894.93 +/- 15546.26
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48044837  |
|    mean velocity x      | -0.212       |
|    mean velocity y      | 1.06         |
|    mean velocity z      | 4.47         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.14e+05    |
| time/                   |              |
|    total_timesteps      | 1034500      |
| train/                  |              |
|    approx_kl            | 0.0010079204 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.19         |
|    learning_rate        | 0.001        |
|    loss                 | 7.09e+07     |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 1.55         |
|    value_loss           | 8.27e+07     |
------------------------------------------
Eval num_timesteps=1035000, episode_reward=-74226.61 +/- 54601.28
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.484175 |
|    mean velocity x | -0.32     |
|    mean velocity y | 1.22      |
|    mean velocity z | 4.71      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.42e+04 |
| time/              |           |
|    total_timesteps | 1035000   |
----------------------------------
Eval num_timesteps=1035500, episode_reward=-67195.38 +/- 55226.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23528019 |
|    mean velocity x | -0.181      |
|    mean velocity y | 0.543       |
|    mean velocity z | 0.257       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 1035500     |
------------------------------------
Eval num_timesteps=1036000, episode_reward=-41385.79 +/- 41691.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44414008 |
|    mean velocity x | -0.33       |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.14e+04   |
| time/              |             |
|    total_timesteps | 1036000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 506     |
|    time_elapsed    | 41818   |
|    total_timesteps | 1036288 |
--------------------------------
Eval num_timesteps=1036500, episode_reward=-75298.37 +/- 40532.66
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.52352244  |
|    mean velocity x      | -0.369       |
|    mean velocity y      | 0.968        |
|    mean velocity z      | 4.54         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.53e+04    |
| time/                   |              |
|    total_timesteps      | 1036500      |
| train/                  |              |
|    approx_kl            | 3.674868e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.001        |
|    loss                 | 7.71e+07     |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.000399    |
|    std                  | 1.55         |
|    value_loss           | 8.87e+07     |
------------------------------------------
Eval num_timesteps=1037000, episode_reward=-72810.78 +/- 29464.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37779245 |
|    mean velocity x | -0.278      |
|    mean velocity y | 0.675       |
|    mean velocity z | 3.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 1037000     |
------------------------------------
Eval num_timesteps=1037500, episode_reward=-98977.99 +/- 11259.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3723167 |
|    mean velocity x | 0.116      |
|    mean velocity y | 0.844      |
|    mean velocity z | 2.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.9e+04   |
| time/              |            |
|    total_timesteps | 1037500    |
-----------------------------------
Eval num_timesteps=1038000, episode_reward=-90487.65 +/- 21161.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44889086 |
|    mean velocity x | -0.505      |
|    mean velocity y | 0.802       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.05e+04   |
| time/              |             |
|    total_timesteps | 1038000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 507     |
|    time_elapsed    | 41898   |
|    total_timesteps | 1038336 |
--------------------------------
Eval num_timesteps=1038500, episode_reward=-107625.58 +/- 18617.12
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49140796  |
|    mean velocity x      | 0.499        |
|    mean velocity y      | 1.63         |
|    mean velocity z      | 3.41         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.08e+05    |
| time/                   |              |
|    total_timesteps      | 1038500      |
| train/                  |              |
|    approx_kl            | 1.550332e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+07     |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.00023     |
|    std                  | 1.55         |
|    value_loss           | 3.72e+07     |
------------------------------------------
Eval num_timesteps=1039000, episode_reward=-79248.26 +/- 46065.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42052382 |
|    mean velocity x | -0.669      |
|    mean velocity y | 0.549       |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.92e+04   |
| time/              |             |
|    total_timesteps | 1039000     |
------------------------------------
Eval num_timesteps=1039500, episode_reward=-95085.05 +/- 28318.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44020703 |
|    mean velocity x | -0.406      |
|    mean velocity y | 0.464       |
|    mean velocity z | 4.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.51e+04   |
| time/              |             |
|    total_timesteps | 1039500     |
------------------------------------
Eval num_timesteps=1040000, episode_reward=-74287.28 +/- 31062.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.60958606 |
|    mean velocity x | -0.69       |
|    mean velocity y | 0.762       |
|    mean velocity z | 4.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 1040000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 508     |
|    time_elapsed    | 41979   |
|    total_timesteps | 1040384 |
--------------------------------
Eval num_timesteps=1040500, episode_reward=-84053.05 +/- 19616.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43682703  |
|    mean velocity x      | 0.288        |
|    mean velocity y      | 1.12         |
|    mean velocity z      | 3.34         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.41e+04    |
| time/                   |              |
|    total_timesteps      | 1040500      |
| train/                  |              |
|    approx_kl            | 9.859708e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.189        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+07     |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.000308    |
|    std                  | 1.55         |
|    value_loss           | 8.01e+07     |
------------------------------------------
Eval num_timesteps=1041000, episode_reward=-82246.37 +/- 47065.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32021102 |
|    mean velocity x | -0.924      |
|    mean velocity y | 0.0968      |
|    mean velocity z | 2.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.22e+04   |
| time/              |             |
|    total_timesteps | 1041000     |
------------------------------------
Eval num_timesteps=1041500, episode_reward=-55175.76 +/- 23957.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5351151 |
|    mean velocity x | -0.362     |
|    mean velocity y | 1.26       |
|    mean velocity z | 5.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.52e+04  |
| time/              |            |
|    total_timesteps | 1041500    |
-----------------------------------
Eval num_timesteps=1042000, episode_reward=-108781.28 +/- 28526.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45628256 |
|    mean velocity x | -0.525      |
|    mean velocity y | 1.17        |
|    mean velocity z | 2.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1042000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 509     |
|    time_elapsed    | 42059   |
|    total_timesteps | 1042432 |
--------------------------------
Eval num_timesteps=1042500, episode_reward=-87739.68 +/- 34141.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.52863145   |
|    mean velocity x      | 0.349         |
|    mean velocity y      | 1.5           |
|    mean velocity z      | 3.64          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.77e+04     |
| time/                   |               |
|    total_timesteps      | 1042500       |
| train/                  |               |
|    approx_kl            | 1.4687335e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.001         |
|    loss                 | 3.11e+07      |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.000182     |
|    std                  | 1.55          |
|    value_loss           | 4.05e+07      |
-------------------------------------------
Eval num_timesteps=1043000, episode_reward=-118076.93 +/- 16035.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5078616 |
|    mean velocity x | 0.45       |
|    mean velocity y | 1.48       |
|    mean velocity z | 3.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 1043000    |
-----------------------------------
Eval num_timesteps=1043500, episode_reward=-67682.65 +/- 30224.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3915454 |
|    mean velocity x | -1.28      |
|    mean velocity y | -0.117     |
|    mean velocity z | 3.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.77e+04  |
| time/              |            |
|    total_timesteps | 1043500    |
-----------------------------------
Eval num_timesteps=1044000, episode_reward=-70903.09 +/- 34886.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42261308 |
|    mean velocity x | -0.0627     |
|    mean velocity y | 1.11        |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 1044000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 510     |
|    time_elapsed    | 42140   |
|    total_timesteps | 1044480 |
--------------------------------
Eval num_timesteps=1044500, episode_reward=-82200.85 +/- 32259.67
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4037909    |
|    mean velocity x      | -0.21         |
|    mean velocity y      | 0.778         |
|    mean velocity z      | 4.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.22e+04     |
| time/                   |               |
|    total_timesteps      | 1044500       |
| train/                  |               |
|    approx_kl            | 2.7312024e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.213         |
|    learning_rate        | 0.001         |
|    loss                 | 3.63e+07      |
|    n_updates            | 5100          |
|    policy_gradient_loss | -0.000326     |
|    std                  | 1.55          |
|    value_loss           | 7.21e+07      |
-------------------------------------------
Eval num_timesteps=1045000, episode_reward=-82736.64 +/- 26410.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4299544 |
|    mean velocity x | -0.361     |
|    mean velocity y | 0.806      |
|    mean velocity z | 4.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.27e+04  |
| time/              |            |
|    total_timesteps | 1045000    |
-----------------------------------
Eval num_timesteps=1045500, episode_reward=-82051.97 +/- 23486.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4996926 |
|    mean velocity x | -0.504     |
|    mean velocity y | 0.77       |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.21e+04  |
| time/              |            |
|    total_timesteps | 1045500    |
-----------------------------------
Eval num_timesteps=1046000, episode_reward=-82878.33 +/- 51082.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3875484 |
|    mean velocity x | -0.276     |
|    mean velocity y | 0.937      |
|    mean velocity z | 4.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.29e+04  |
| time/              |            |
|    total_timesteps | 1046000    |
-----------------------------------
Eval num_timesteps=1046500, episode_reward=-88425.86 +/- 44109.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52589357 |
|    mean velocity x | -0.23       |
|    mean velocity y | 1.48        |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 1046500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 511     |
|    time_elapsed    | 42239   |
|    total_timesteps | 1046528 |
--------------------------------
Eval num_timesteps=1047000, episode_reward=-94982.23 +/- 48264.80
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5141454   |
|    mean velocity x      | 0.38         |
|    mean velocity y      | 1.58         |
|    mean velocity z      | 3.69         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.5e+04     |
| time/                   |              |
|    total_timesteps      | 1047000      |
| train/                  |              |
|    approx_kl            | 1.604596e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+07     |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.000388    |
|    std                  | 1.55         |
|    value_loss           | 9.31e+07     |
------------------------------------------
Eval num_timesteps=1047500, episode_reward=-85190.78 +/- 46079.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44000262 |
|    mean velocity x | -0.369      |
|    mean velocity y | 0.802       |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.52e+04   |
| time/              |             |
|    total_timesteps | 1047500     |
------------------------------------
Eval num_timesteps=1048000, episode_reward=-59205.12 +/- 46686.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5080383 |
|    mean velocity x | -0.182     |
|    mean velocity y | 1.12       |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.92e+04  |
| time/              |            |
|    total_timesteps | 1048000    |
-----------------------------------
Eval num_timesteps=1048500, episode_reward=-74281.98 +/- 56717.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45385906 |
|    mean velocity x | -0.105      |
|    mean velocity y | 0.975       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 1048500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 512     |
|    time_elapsed    | 42319   |
|    total_timesteps | 1048576 |
--------------------------------
Eval num_timesteps=1049000, episode_reward=-99455.87 +/- 23112.73
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.5065622  |
|    mean velocity x      | -0.457      |
|    mean velocity y      | 0.928       |
|    mean velocity z      | 4.28        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.95e+04   |
| time/                   |             |
|    total_timesteps      | 1049000     |
| train/                  |             |
|    approx_kl            | 4.44366e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.001       |
|    loss                 | 2.83e+07    |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.000189   |
|    std                  | 1.55        |
|    value_loss           | 9.08e+07    |
-----------------------------------------
Eval num_timesteps=1049500, episode_reward=-87902.54 +/- 31740.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5109074 |
|    mean velocity x | -0.0214    |
|    mean velocity y | 1.26       |
|    mean velocity z | 4.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.79e+04  |
| time/              |            |
|    total_timesteps | 1049500    |
-----------------------------------
Eval num_timesteps=1050000, episode_reward=-84456.86 +/- 33956.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4683753 |
|    mean velocity x | -0.0665    |
|    mean velocity y | 1.15       |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.45e+04  |
| time/              |            |
|    total_timesteps | 1050000    |
-----------------------------------
Eval num_timesteps=1050500, episode_reward=-43369.39 +/- 30200.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40789968 |
|    mean velocity x | -0.129      |
|    mean velocity y | 1.13        |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.34e+04   |
| time/              |             |
|    total_timesteps | 1050500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 513     |
|    time_elapsed    | 42400   |
|    total_timesteps | 1050624 |
--------------------------------
Eval num_timesteps=1051000, episode_reward=-119828.50 +/- 31300.15
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47141612   |
|    mean velocity x      | -0.916        |
|    mean velocity y      | 0.347         |
|    mean velocity z      | 4.35          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.2e+05      |
| time/                   |               |
|    total_timesteps      | 1051000       |
| train/                  |               |
|    approx_kl            | 3.3488555e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.001         |
|    loss                 | 4.18e+07      |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000417     |
|    std                  | 1.55          |
|    value_loss           | 6.79e+07      |
-------------------------------------------
Eval num_timesteps=1051500, episode_reward=-83858.24 +/- 21873.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42695373 |
|    mean velocity x | -0.292      |
|    mean velocity y | 0.429       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.39e+04   |
| time/              |             |
|    total_timesteps | 1051500     |
------------------------------------
Eval num_timesteps=1052000, episode_reward=-77393.64 +/- 22569.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43513274 |
|    mean velocity x | 0.0712      |
|    mean velocity y | 0.711       |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.74e+04   |
| time/              |             |
|    total_timesteps | 1052000     |
------------------------------------
Eval num_timesteps=1052500, episode_reward=-109195.68 +/- 29612.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53623754 |
|    mean velocity x | -0.668      |
|    mean velocity y | 0.636       |
|    mean velocity z | 4.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1052500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 514     |
|    time_elapsed    | 42480   |
|    total_timesteps | 1052672 |
--------------------------------
Eval num_timesteps=1053000, episode_reward=-70807.94 +/- 44162.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4939688    |
|    mean velocity x      | -0.425        |
|    mean velocity y      | 0.998         |
|    mean velocity z      | 4.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.08e+04     |
| time/                   |               |
|    total_timesteps      | 1053000       |
| train/                  |               |
|    approx_kl            | 1.2745353e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.001         |
|    loss                 | 2.24e+07      |
|    n_updates            | 5140          |
|    policy_gradient_loss | -0.000414     |
|    std                  | 1.55          |
|    value_loss           | 7.21e+07      |
-------------------------------------------
Eval num_timesteps=1053500, episode_reward=-65239.39 +/- 52183.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3762038 |
|    mean velocity x | -0.178     |
|    mean velocity y | 0.748      |
|    mean velocity z | 0.713      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.52e+04  |
| time/              |            |
|    total_timesteps | 1053500    |
-----------------------------------
Eval num_timesteps=1054000, episode_reward=-81973.46 +/- 48787.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45448047 |
|    mean velocity x | 0.301       |
|    mean velocity y | 1.29        |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 1054000     |
------------------------------------
Eval num_timesteps=1054500, episode_reward=-80402.43 +/- 43053.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31269968 |
|    mean velocity x | -0.317      |
|    mean velocity y | -0.011      |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.04e+04   |
| time/              |             |
|    total_timesteps | 1054500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 515     |
|    time_elapsed    | 42560   |
|    total_timesteps | 1054720 |
--------------------------------
Eval num_timesteps=1055000, episode_reward=-91780.39 +/- 33476.99
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.37338525    |
|    mean velocity x      | -1.37          |
|    mean velocity y      | 0.0664         |
|    mean velocity z      | 3.46           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -9.18e+04      |
| time/                   |                |
|    total_timesteps      | 1055000        |
| train/                  |                |
|    approx_kl            | 0.000108799984 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.57          |
|    explained_variance   | 0.231          |
|    learning_rate        | 0.001          |
|    loss                 | 3.56e+07       |
|    n_updates            | 5150           |
|    policy_gradient_loss | -0.000705      |
|    std                  | 1.55           |
|    value_loss           | 3.62e+07       |
--------------------------------------------
Eval num_timesteps=1055500, episode_reward=-68039.05 +/- 35350.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3627906 |
|    mean velocity x | -0.437     |
|    mean velocity y | 0.039      |
|    mean velocity z | 4.21       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.8e+04   |
| time/              |            |
|    total_timesteps | 1055500    |
-----------------------------------
Eval num_timesteps=1056000, episode_reward=-68637.43 +/- 45151.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42433447 |
|    mean velocity x | -0.676      |
|    mean velocity y | -0.2        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.86e+04   |
| time/              |             |
|    total_timesteps | 1056000     |
------------------------------------
Eval num_timesteps=1056500, episode_reward=-88792.94 +/- 32191.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50310916 |
|    mean velocity x | -0.352      |
|    mean velocity y | 0.986       |
|    mean velocity z | 4.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.88e+04   |
| time/              |             |
|    total_timesteps | 1056500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 516     |
|    time_elapsed    | 42641   |
|    total_timesteps | 1056768 |
--------------------------------
Eval num_timesteps=1057000, episode_reward=-93745.70 +/- 48873.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42549816   |
|    mean velocity x      | 0.238         |
|    mean velocity y      | 1.06          |
|    mean velocity z      | 3.03          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.37e+04     |
| time/                   |               |
|    total_timesteps      | 1057000       |
| train/                  |               |
|    approx_kl            | 5.1234558e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 5160          |
|    policy_gradient_loss | -0.000626     |
|    std                  | 1.55          |
|    value_loss           | 8.58e+07      |
-------------------------------------------
Eval num_timesteps=1057500, episode_reward=-95093.15 +/- 22127.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4725669 |
|    mean velocity x | -0.299     |
|    mean velocity y | 0.78       |
|    mean velocity z | 4.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 1057500    |
-----------------------------------
Eval num_timesteps=1058000, episode_reward=-131261.19 +/- 39544.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4276928 |
|    mean velocity x | -0.164     |
|    mean velocity y | 0.942      |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.31e+05  |
| time/              |            |
|    total_timesteps | 1058000    |
-----------------------------------
Eval num_timesteps=1058500, episode_reward=-54998.87 +/- 32595.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3683043 |
|    mean velocity x | -1.19      |
|    mean velocity y | -0.454     |
|    mean velocity z | 3.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.5e+04   |
| time/              |            |
|    total_timesteps | 1058500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 517     |
|    time_elapsed    | 42721   |
|    total_timesteps | 1058816 |
--------------------------------
Eval num_timesteps=1059000, episode_reward=-80446.69 +/- 39034.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.17027064   |
|    mean velocity x      | -0.0515       |
|    mean velocity y      | 0.36          |
|    mean velocity z      | 0.364         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.04e+04     |
| time/                   |               |
|    total_timesteps      | 1059000       |
| train/                  |               |
|    approx_kl            | 4.6135043e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.001         |
|    loss                 | 4.04e+07      |
|    n_updates            | 5170          |
|    policy_gradient_loss | -0.000615     |
|    std                  | 1.55          |
|    value_loss           | 5.52e+07      |
-------------------------------------------
Eval num_timesteps=1059500, episode_reward=-67030.18 +/- 29733.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2994276 |
|    mean velocity x | -0.309     |
|    mean velocity y | 0.205      |
|    mean velocity z | 3.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.7e+04   |
| time/              |            |
|    total_timesteps | 1059500    |
-----------------------------------
Eval num_timesteps=1060000, episode_reward=-90851.02 +/- 30850.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40674004 |
|    mean velocity x | -0.0382     |
|    mean velocity y | 1.19        |
|    mean velocity z | 3.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 1060000     |
------------------------------------
Eval num_timesteps=1060500, episode_reward=-68092.74 +/- 42231.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57456326 |
|    mean velocity x | -0.307      |
|    mean velocity y | 1.31        |
|    mean velocity z | 5.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1060500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 518     |
|    time_elapsed    | 42801   |
|    total_timesteps | 1060864 |
--------------------------------
Eval num_timesteps=1061000, episode_reward=-106982.90 +/- 25952.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.74682385   |
|    mean velocity x      | 0.821         |
|    mean velocity y      | 2.66          |
|    mean velocity z      | 5.43          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 1061000       |
| train/                  |               |
|    approx_kl            | 2.6523601e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+07      |
|    n_updates            | 5180          |
|    policy_gradient_loss | -0.000309     |
|    std                  | 1.55          |
|    value_loss           | 3.79e+07      |
-------------------------------------------
Eval num_timesteps=1061500, episode_reward=-70670.64 +/- 27017.83
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.601614 |
|    mean velocity x | -0.296    |
|    mean velocity y | 1.55      |
|    mean velocity z | 5.06      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.07e+04 |
| time/              |           |
|    total_timesteps | 1061500   |
----------------------------------
Eval num_timesteps=1062000, episode_reward=-84009.92 +/- 44067.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4322443 |
|    mean velocity x | -0.216     |
|    mean velocity y | 0.885      |
|    mean velocity z | 4.56       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.4e+04   |
| time/              |            |
|    total_timesteps | 1062000    |
-----------------------------------
Eval num_timesteps=1062500, episode_reward=-102728.22 +/- 19973.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2647712 |
|    mean velocity x | -0.184     |
|    mean velocity y | 0.425      |
|    mean velocity z | 2.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1062500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 519     |
|    time_elapsed    | 42881   |
|    total_timesteps | 1062912 |
--------------------------------
Eval num_timesteps=1063000, episode_reward=-106496.63 +/- 15816.71
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31130922   |
|    mean velocity x      | -0.491        |
|    mean velocity y      | 0.577         |
|    mean velocity z      | 2.81          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.06e+05     |
| time/                   |               |
|    total_timesteps      | 1063000       |
| train/                  |               |
|    approx_kl            | 2.6701688e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.001         |
|    loss                 | 3.6e+07       |
|    n_updates            | 5190          |
|    policy_gradient_loss | -0.000554     |
|    std                  | 1.55          |
|    value_loss           | 5.73e+07      |
-------------------------------------------
Eval num_timesteps=1063500, episode_reward=-81067.43 +/- 31655.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36687416 |
|    mean velocity x | 0.109       |
|    mean velocity y | 0.78        |
|    mean velocity z | 3           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.11e+04   |
| time/              |             |
|    total_timesteps | 1063500     |
------------------------------------
Eval num_timesteps=1064000, episode_reward=-99380.56 +/- 17353.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44207746 |
|    mean velocity x | 0.332       |
|    mean velocity y | 1.23        |
|    mean velocity z | 3.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.94e+04   |
| time/              |             |
|    total_timesteps | 1064000     |
------------------------------------
Eval num_timesteps=1064500, episode_reward=-82880.33 +/- 31326.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3668244 |
|    mean velocity x | -0.384     |
|    mean velocity y | 0.413      |
|    mean velocity z | 3.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.29e+04  |
| time/              |            |
|    total_timesteps | 1064500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 520     |
|    time_elapsed    | 42961   |
|    total_timesteps | 1064960 |
--------------------------------
Eval num_timesteps=1065000, episode_reward=-84424.81 +/- 11197.36
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46519825  |
|    mean velocity x      | -1.64        |
|    mean velocity y      | -0.278       |
|    mean velocity z      | 5.91         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.44e+04    |
| time/                   |              |
|    total_timesteps      | 1065000      |
| train/                  |              |
|    approx_kl            | 9.773168e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.001        |
|    loss                 | 3.37e+07     |
|    n_updates            | 5200         |
|    policy_gradient_loss | -0.000606    |
|    std                  | 1.55         |
|    value_loss           | 4.14e+07     |
------------------------------------------
Eval num_timesteps=1065500, episode_reward=-92583.49 +/- 32150.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33816594 |
|    mean velocity x | -0.507      |
|    mean velocity y | 0.997       |
|    mean velocity z | 1.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.26e+04   |
| time/              |             |
|    total_timesteps | 1065500     |
------------------------------------
Eval num_timesteps=1066000, episode_reward=-103978.17 +/- 34679.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4381747 |
|    mean velocity x | -0.272     |
|    mean velocity y | 1.27       |
|    mean velocity z | 4.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1066000    |
-----------------------------------
Eval num_timesteps=1066500, episode_reward=-74389.09 +/- 44659.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5672687 |
|    mean velocity x | 0.418      |
|    mean velocity y | 1.97       |
|    mean velocity z | 4.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.44e+04  |
| time/              |            |
|    total_timesteps | 1066500    |
-----------------------------------
Eval num_timesteps=1067000, episode_reward=-96937.99 +/- 38086.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5038126 |
|    mean velocity x | -0.356     |
|    mean velocity y | 0.947      |
|    mean velocity z | 4.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.69e+04  |
| time/              |            |
|    total_timesteps | 1067000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 521     |
|    time_elapsed    | 43061   |
|    total_timesteps | 1067008 |
--------------------------------
Eval num_timesteps=1067500, episode_reward=-101644.24 +/- 27378.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.54010856   |
|    mean velocity x      | -0.018        |
|    mean velocity y      | 1.59          |
|    mean velocity z      | 3.5           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 1067500       |
| train/                  |               |
|    approx_kl            | 2.1588377e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.001         |
|    loss                 | 3.93e+07      |
|    n_updates            | 5210          |
|    policy_gradient_loss | -0.000358     |
|    std                  | 1.55          |
|    value_loss           | 7.52e+07      |
-------------------------------------------
Eval num_timesteps=1068000, episode_reward=-79540.99 +/- 42278.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3993072 |
|    mean velocity x | -0.732     |
|    mean velocity y | 0.33       |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.95e+04  |
| time/              |            |
|    total_timesteps | 1068000    |
-----------------------------------
Eval num_timesteps=1068500, episode_reward=-77838.81 +/- 29524.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53375304 |
|    mean velocity x | -0.602      |
|    mean velocity y | 0.744       |
|    mean velocity z | 4.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 1068500     |
------------------------------------
Eval num_timesteps=1069000, episode_reward=-99736.52 +/- 23991.51
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4615954 |
|    mean velocity x | -0.344     |
|    mean velocity y | 0.708      |
|    mean velocity z | 4.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.97e+04  |
| time/              |            |
|    total_timesteps | 1069000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 522     |
|    time_elapsed    | 43141   |
|    total_timesteps | 1069056 |
--------------------------------
Eval num_timesteps=1069500, episode_reward=-88321.18 +/- 46756.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5198966    |
|    mean velocity x      | 0.365         |
|    mean velocity y      | 1.53          |
|    mean velocity z      | 3.97          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.83e+04     |
| time/                   |               |
|    total_timesteps      | 1069500       |
| train/                  |               |
|    approx_kl            | 3.3198477e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+07      |
|    n_updates            | 5220          |
|    policy_gradient_loss | -0.000507     |
|    std                  | 1.55          |
|    value_loss           | 8.07e+07      |
-------------------------------------------
Eval num_timesteps=1070000, episode_reward=-104123.50 +/- 31288.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50984764 |
|    mean velocity x | 0.558       |
|    mean velocity y | 1.69        |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1070000     |
------------------------------------
Eval num_timesteps=1070500, episode_reward=-101396.02 +/- 39814.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42745173 |
|    mean velocity x | 0.115       |
|    mean velocity y | 1.19        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1070500     |
------------------------------------
Eval num_timesteps=1071000, episode_reward=-100470.70 +/- 30796.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47807539 |
|    mean velocity x | -0.479      |
|    mean velocity y | 0.324       |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1071000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 523     |
|    time_elapsed    | 43221   |
|    total_timesteps | 1071104 |
--------------------------------
Eval num_timesteps=1071500, episode_reward=-90597.20 +/- 52555.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3213201   |
|    mean velocity x      | -0.539       |
|    mean velocity y      | 0.236        |
|    mean velocity z      | 2.92         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.06e+04    |
| time/                   |              |
|    total_timesteps      | 1071500      |
| train/                  |              |
|    approx_kl            | 3.197597e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.001        |
|    loss                 | 5.39e+07     |
|    n_updates            | 5230         |
|    policy_gradient_loss | -0.000546    |
|    std                  | 1.55         |
|    value_loss           | 6.12e+07     |
------------------------------------------
Eval num_timesteps=1072000, episode_reward=-93817.37 +/- 34359.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2742181 |
|    mean velocity x | -1.74      |
|    mean velocity y | -0.684     |
|    mean velocity z | 4.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.38e+04  |
| time/              |            |
|    total_timesteps | 1072000    |
-----------------------------------
Eval num_timesteps=1072500, episode_reward=-110225.74 +/- 29293.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40064704 |
|    mean velocity x | -0.46       |
|    mean velocity y | 0.492       |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 1072500     |
------------------------------------
Eval num_timesteps=1073000, episode_reward=-61759.30 +/- 52382.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5082055 |
|    mean velocity x | 0.318      |
|    mean velocity y | 1.51       |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.18e+04  |
| time/              |            |
|    total_timesteps | 1073000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 524     |
|    time_elapsed    | 43302   |
|    total_timesteps | 1073152 |
--------------------------------
Eval num_timesteps=1073500, episode_reward=-79120.24 +/- 39394.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3490095    |
|    mean velocity x      | 0.0689        |
|    mean velocity y      | 0.861         |
|    mean velocity z      | 3.78          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.91e+04     |
| time/                   |               |
|    total_timesteps      | 1073500       |
| train/                  |               |
|    approx_kl            | 0.00020252369 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.263         |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+07      |
|    n_updates            | 5240          |
|    policy_gradient_loss | -0.00168      |
|    std                  | 1.55          |
|    value_loss           | 4.72e+07      |
-------------------------------------------
Eval num_timesteps=1074000, episode_reward=-57201.13 +/- 45044.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37576416 |
|    mean velocity x | -0.412      |
|    mean velocity y | 0.491       |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.72e+04   |
| time/              |             |
|    total_timesteps | 1074000     |
------------------------------------
Eval num_timesteps=1074500, episode_reward=-128159.77 +/- 10209.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55480874 |
|    mean velocity x | -0.4        |
|    mean velocity y | 1.04        |
|    mean velocity z | 5.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.28e+05   |
| time/              |             |
|    total_timesteps | 1074500     |
------------------------------------
Eval num_timesteps=1075000, episode_reward=-87030.96 +/- 14696.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42475352 |
|    mean velocity x | -0.119      |
|    mean velocity y | 0.823       |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.7e+04    |
| time/              |             |
|    total_timesteps | 1075000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 525     |
|    time_elapsed    | 43382   |
|    total_timesteps | 1075200 |
--------------------------------
Eval num_timesteps=1075500, episode_reward=-43641.26 +/- 32874.16
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39207163   |
|    mean velocity x      | 0.0681        |
|    mean velocity y      | 0.997         |
|    mean velocity z      | 3.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.36e+04     |
| time/                   |               |
|    total_timesteps      | 1075500       |
| train/                  |               |
|    approx_kl            | 3.3052347e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.001         |
|    loss                 | 5.26e+07      |
|    n_updates            | 5250          |
|    policy_gradient_loss | -0.000433     |
|    std                  | 1.55          |
|    value_loss           | 6.02e+07      |
-------------------------------------------
Eval num_timesteps=1076000, episode_reward=-88574.85 +/- 47295.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3507452 |
|    mean velocity x | 0.22       |
|    mean velocity y | 1.12       |
|    mean velocity z | 3.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.86e+04  |
| time/              |            |
|    total_timesteps | 1076000    |
-----------------------------------
Eval num_timesteps=1076500, episode_reward=-75794.83 +/- 42763.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56009185 |
|    mean velocity x | -0.041      |
|    mean velocity y | 1.17        |
|    mean velocity z | 3.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 1076500     |
------------------------------------
Eval num_timesteps=1077000, episode_reward=-91429.86 +/- 37660.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5273095 |
|    mean velocity x | -0.311     |
|    mean velocity y | 1.05       |
|    mean velocity z | 3.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.14e+04  |
| time/              |            |
|    total_timesteps | 1077000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 526     |
|    time_elapsed    | 43462   |
|    total_timesteps | 1077248 |
--------------------------------
Eval num_timesteps=1077500, episode_reward=-91442.70 +/- 27608.03
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4541718   |
|    mean velocity x      | -0.335       |
|    mean velocity y      | 0.993        |
|    mean velocity z      | 4.61         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.14e+04    |
| time/                   |              |
|    total_timesteps      | 1077500      |
| train/                  |              |
|    approx_kl            | 6.525559e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.001        |
|    loss                 | 6.59e+07     |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.000589    |
|    std                  | 1.55         |
|    value_loss           | 6.78e+07     |
------------------------------------------
Eval num_timesteps=1078000, episode_reward=-142544.91 +/- 42618.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38203576 |
|    mean velocity x | -0.45       |
|    mean velocity y | 0.34        |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.43e+05   |
| time/              |             |
|    total_timesteps | 1078000     |
------------------------------------
Eval num_timesteps=1078500, episode_reward=-85848.39 +/- 56052.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43930274 |
|    mean velocity x | -0.168      |
|    mean velocity y | 1.32        |
|    mean velocity z | 4.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.58e+04   |
| time/              |             |
|    total_timesteps | 1078500     |
------------------------------------
Eval num_timesteps=1079000, episode_reward=-71174.62 +/- 25759.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25762346 |
|    mean velocity x | -0.62       |
|    mean velocity y | 0.51        |
|    mean velocity z | 1.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.12e+04   |
| time/              |             |
|    total_timesteps | 1079000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 527     |
|    time_elapsed    | 43543   |
|    total_timesteps | 1079296 |
--------------------------------
Eval num_timesteps=1079500, episode_reward=-104646.32 +/- 36738.39
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39674133   |
|    mean velocity x      | 0.308         |
|    mean velocity y      | 1.11          |
|    mean velocity z      | 3.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 1079500       |
| train/                  |               |
|    approx_kl            | 7.8815996e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+07      |
|    n_updates            | 5270          |
|    policy_gradient_loss | -0.00067      |
|    std                  | 1.55          |
|    value_loss           | 6.32e+07      |
-------------------------------------------
Eval num_timesteps=1080000, episode_reward=-52836.48 +/- 30120.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4628781 |
|    mean velocity x | -0.179     |
|    mean velocity y | 0.957      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.28e+04  |
| time/              |            |
|    total_timesteps | 1080000    |
-----------------------------------
Eval num_timesteps=1080500, episode_reward=-121226.00 +/- 17612.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34418803 |
|    mean velocity x | -0.0583     |
|    mean velocity y | 0.492       |
|    mean velocity z | 1.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 1080500     |
------------------------------------
Eval num_timesteps=1081000, episode_reward=-72295.95 +/- 47308.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42679957 |
|    mean velocity x | 0.386       |
|    mean velocity y | 1.38        |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 1081000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 528     |
|    time_elapsed    | 43623   |
|    total_timesteps | 1081344 |
--------------------------------
Eval num_timesteps=1081500, episode_reward=-84659.49 +/- 52666.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32146198   |
|    mean velocity x      | 0.0829        |
|    mean velocity y      | 0.804         |
|    mean velocity z      | 3.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.47e+04     |
| time/                   |               |
|    total_timesteps      | 1081500       |
| train/                  |               |
|    approx_kl            | 2.8850453e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.001         |
|    loss                 | 7.67e+06      |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.000277     |
|    std                  | 1.55          |
|    value_loss           | 5.77e+07      |
-------------------------------------------
Eval num_timesteps=1082000, episode_reward=-60325.42 +/- 35557.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43247166 |
|    mean velocity x | -0.268      |
|    mean velocity y | 0.695       |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.03e+04   |
| time/              |             |
|    total_timesteps | 1082000     |
------------------------------------
Eval num_timesteps=1082500, episode_reward=-109629.84 +/- 17935.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44256118 |
|    mean velocity x | -0.434      |
|    mean velocity y | 1.06        |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 1082500     |
------------------------------------
Eval num_timesteps=1083000, episode_reward=-95602.66 +/- 22573.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22305943 |
|    mean velocity x | -1.07       |
|    mean velocity y | -0.251      |
|    mean velocity z | 2.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.56e+04   |
| time/              |             |
|    total_timesteps | 1083000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 529     |
|    time_elapsed    | 43711   |
|    total_timesteps | 1083392 |
--------------------------------
Eval num_timesteps=1083500, episode_reward=-36404.43 +/- 44642.25
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.29323137   |
|    mean velocity x      | -0.363        |
|    mean velocity y      | -0.0944       |
|    mean velocity z      | 3.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.64e+04     |
| time/                   |               |
|    total_timesteps      | 1083500       |
| train/                  |               |
|    approx_kl            | 1.2106233e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.187         |
|    learning_rate        | 0.001         |
|    loss                 | 1.51e+07      |
|    n_updates            | 5290          |
|    policy_gradient_loss | -0.000219     |
|    std                  | 1.55          |
|    value_loss           | 6.36e+07      |
-------------------------------------------
Eval num_timesteps=1084000, episode_reward=-95421.66 +/- 34109.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17874336 |
|    mean velocity x | -0.579      |
|    mean velocity y | 0.199       |
|    mean velocity z | 1.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.54e+04   |
| time/              |             |
|    total_timesteps | 1084000     |
------------------------------------
Eval num_timesteps=1084500, episode_reward=-69916.39 +/- 40636.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5779194 |
|    mean velocity x | 0.0787     |
|    mean velocity y | 1.83       |
|    mean velocity z | 4.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.99e+04  |
| time/              |            |
|    total_timesteps | 1084500    |
-----------------------------------
Eval num_timesteps=1085000, episode_reward=-105107.32 +/- 26096.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4086165 |
|    mean velocity x | 0.519      |
|    mean velocity y | 1.52       |
|    mean velocity z | 3.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.05e+05  |
| time/              |            |
|    total_timesteps | 1085000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 530     |
|    time_elapsed    | 43791   |
|    total_timesteps | 1085440 |
--------------------------------
Eval num_timesteps=1085500, episode_reward=-61430.87 +/- 27175.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30512983   |
|    mean velocity x      | -0.705        |
|    mean velocity y      | 0.23          |
|    mean velocity z      | 2.79          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.14e+04     |
| time/                   |               |
|    total_timesteps      | 1085500       |
| train/                  |               |
|    approx_kl            | 4.0694926e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+07      |
|    n_updates            | 5300          |
|    policy_gradient_loss | -0.000487     |
|    std                  | 1.55          |
|    value_loss           | 2.59e+07      |
-------------------------------------------
Eval num_timesteps=1086000, episode_reward=-106730.23 +/- 31028.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37006667 |
|    mean velocity x | -0.758      |
|    mean velocity y | 0.0219      |
|    mean velocity z | 3.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1086000     |
------------------------------------
Eval num_timesteps=1086500, episode_reward=-67520.94 +/- 35200.10
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4455273 |
|    mean velocity x | -0.238     |
|    mean velocity y | 0.99       |
|    mean velocity z | 4.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.75e+04  |
| time/              |            |
|    total_timesteps | 1086500    |
-----------------------------------
Eval num_timesteps=1087000, episode_reward=-79390.84 +/- 50128.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35728675 |
|    mean velocity x | -0.487      |
|    mean velocity y | 0.185       |
|    mean velocity z | 3.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.94e+04   |
| time/              |             |
|    total_timesteps | 1087000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 531     |
|    time_elapsed    | 43871   |
|    total_timesteps | 1087488 |
--------------------------------
Eval num_timesteps=1087500, episode_reward=-95640.17 +/- 64170.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5281857    |
|    mean velocity x      | -0.974        |
|    mean velocity y      | 0.198         |
|    mean velocity z      | 3.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.56e+04     |
| time/                   |               |
|    total_timesteps      | 1087500       |
| train/                  |               |
|    approx_kl            | 2.0502834e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.001         |
|    loss                 | 1.71e+07      |
|    n_updates            | 5310          |
|    policy_gradient_loss | -0.000423     |
|    std                  | 1.55          |
|    value_loss           | 6.56e+07      |
-------------------------------------------
Eval num_timesteps=1088000, episode_reward=-96433.73 +/- 22184.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48060206 |
|    mean velocity x | -1.02       |
|    mean velocity y | 0.559       |
|    mean velocity z | 3.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.64e+04   |
| time/              |             |
|    total_timesteps | 1088000     |
------------------------------------
Eval num_timesteps=1088500, episode_reward=-77327.40 +/- 40285.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3508026 |
|    mean velocity x | -0.0263    |
|    mean velocity y | 1.12       |
|    mean velocity z | 4.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.73e+04  |
| time/              |            |
|    total_timesteps | 1088500    |
-----------------------------------
Eval num_timesteps=1089000, episode_reward=-84579.33 +/- 57832.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33836016 |
|    mean velocity x | -0.474      |
|    mean velocity y | 0.507       |
|    mean velocity z | 3.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.46e+04   |
| time/              |             |
|    total_timesteps | 1089000     |
------------------------------------
Eval num_timesteps=1089500, episode_reward=-68530.05 +/- 49742.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23462363 |
|    mean velocity x | -1.07       |
|    mean velocity y | -0.577      |
|    mean velocity z | 3.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 1089500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 532     |
|    time_elapsed    | 43970   |
|    total_timesteps | 1089536 |
--------------------------------
Eval num_timesteps=1090000, episode_reward=-92702.38 +/- 34910.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.53803086   |
|    mean velocity x      | -0.963        |
|    mean velocity y      | 0.524         |
|    mean velocity z      | 4.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.27e+04     |
| time/                   |               |
|    total_timesteps      | 1090000       |
| train/                  |               |
|    approx_kl            | 5.6777906e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.001         |
|    loss                 | 2.33e+07      |
|    n_updates            | 5320          |
|    policy_gradient_loss | -0.00082      |
|    std                  | 1.55          |
|    value_loss           | 6.01e+07      |
-------------------------------------------
Eval num_timesteps=1090500, episode_reward=-103035.53 +/- 30428.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5095599 |
|    mean velocity x | -0.333     |
|    mean velocity y | 1.22       |
|    mean velocity z | 4.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1090500    |
-----------------------------------
Eval num_timesteps=1091000, episode_reward=-105699.11 +/- 45601.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56010634 |
|    mean velocity x | -0.797      |
|    mean velocity y | 0.736       |
|    mean velocity z | 4.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1091000     |
------------------------------------
Eval num_timesteps=1091500, episode_reward=-91026.63 +/- 15176.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3251285 |
|    mean velocity x | -1.56      |
|    mean velocity y | -0.327     |
|    mean velocity z | 3.78       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.1e+04   |
| time/              |            |
|    total_timesteps | 1091500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 533     |
|    time_elapsed    | 44051   |
|    total_timesteps | 1091584 |
--------------------------------
Eval num_timesteps=1092000, episode_reward=-92946.34 +/- 46469.88
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25266325   |
|    mean velocity x      | -0.135        |
|    mean velocity y      | 0.461         |
|    mean velocity z      | 0.498         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.29e+04     |
| time/                   |               |
|    total_timesteps      | 1092000       |
| train/                  |               |
|    approx_kl            | 0.00020201976 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.318         |
|    learning_rate        | 0.001         |
|    loss                 | 1.8e+07       |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.00103      |
|    std                  | 1.55          |
|    value_loss           | 4.31e+07      |
-------------------------------------------
Eval num_timesteps=1092500, episode_reward=-95303.06 +/- 41505.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4858779 |
|    mean velocity x | -0.00836   |
|    mean velocity y | 1.11       |
|    mean velocity z | 3.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.53e+04  |
| time/              |            |
|    total_timesteps | 1092500    |
-----------------------------------
Eval num_timesteps=1093000, episode_reward=-71196.72 +/- 31563.84
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3969318 |
|    mean velocity x | -0.676     |
|    mean velocity y | 0.066      |
|    mean velocity z | 3.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.12e+04  |
| time/              |            |
|    total_timesteps | 1093000    |
-----------------------------------
Eval num_timesteps=1093500, episode_reward=-102565.30 +/- 23547.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2165244 |
|    mean velocity x | -0.334     |
|    mean velocity y | 0.295      |
|    mean velocity z | 0.979      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1093500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 534     |
|    time_elapsed    | 44131   |
|    total_timesteps | 1093632 |
--------------------------------
Eval num_timesteps=1094000, episode_reward=-74248.49 +/- 27127.66
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34970814   |
|    mean velocity x      | 0.349         |
|    mean velocity y      | 0.782         |
|    mean velocity z      | 2.72          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.42e+04     |
| time/                   |               |
|    total_timesteps      | 1094000       |
| train/                  |               |
|    approx_kl            | 0.00011223368 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.001         |
|    loss                 | 2.33e+07      |
|    n_updates            | 5340          |
|    policy_gradient_loss | -0.000998     |
|    std                  | 1.55          |
|    value_loss           | 3.38e+07      |
-------------------------------------------
Eval num_timesteps=1094500, episode_reward=-65804.43 +/- 31296.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5428926 |
|    mean velocity x | -0.477     |
|    mean velocity y | 0.875      |
|    mean velocity z | 4.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.58e+04  |
| time/              |            |
|    total_timesteps | 1094500    |
-----------------------------------
Eval num_timesteps=1095000, episode_reward=-112598.36 +/- 68287.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33151978 |
|    mean velocity x | -0.291      |
|    mean velocity y | 0.367       |
|    mean velocity z | 2.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 1095000     |
------------------------------------
Eval num_timesteps=1095500, episode_reward=-97261.77 +/- 16905.08
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.330874 |
|    mean velocity x | 0.387     |
|    mean velocity y | 0.834     |
|    mean velocity z | 3.09      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.73e+04 |
| time/              |           |
|    total_timesteps | 1095500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 535     |
|    time_elapsed    | 44211   |
|    total_timesteps | 1095680 |
--------------------------------
Eval num_timesteps=1096000, episode_reward=-59836.73 +/- 58063.02
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42809027  |
|    mean velocity x      | -0.668       |
|    mean velocity y      | 0.394        |
|    mean velocity z      | 3.1          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.98e+04    |
| time/                   |              |
|    total_timesteps      | 1096000      |
| train/                  |              |
|    approx_kl            | 0.0018054182 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.19         |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+07     |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.00382     |
|    std                  | 1.55         |
|    value_loss           | 5.32e+07     |
------------------------------------------
Eval num_timesteps=1096500, episode_reward=-82704.10 +/- 47874.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38149184 |
|    mean velocity x | -0.409      |
|    mean velocity y | 0.319       |
|    mean velocity z | 3.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.27e+04   |
| time/              |             |
|    total_timesteps | 1096500     |
------------------------------------
Eval num_timesteps=1097000, episode_reward=-60700.13 +/- 33928.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3993472 |
|    mean velocity x | -0.116     |
|    mean velocity y | 0.932      |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.07e+04  |
| time/              |            |
|    total_timesteps | 1097000    |
-----------------------------------
Eval num_timesteps=1097500, episode_reward=-80275.37 +/- 8649.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44582355 |
|    mean velocity x | -0.316      |
|    mean velocity y | 0.927       |
|    mean velocity z | 4.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.03e+04   |
| time/              |             |
|    total_timesteps | 1097500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 536     |
|    time_elapsed    | 44292   |
|    total_timesteps | 1097728 |
--------------------------------
Eval num_timesteps=1098000, episode_reward=-75092.80 +/- 32286.39
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40001756  |
|    mean velocity x      | -0.236       |
|    mean velocity y      | 1.13         |
|    mean velocity z      | 4.8          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.51e+04    |
| time/                   |              |
|    total_timesteps      | 1098000      |
| train/                  |              |
|    approx_kl            | 4.432653e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.001        |
|    loss                 | 9.58e+07     |
|    n_updates            | 5360         |
|    policy_gradient_loss | -0.000943    |
|    std                  | 1.55         |
|    value_loss           | 1.06e+08     |
------------------------------------------
Eval num_timesteps=1098500, episode_reward=-100449.70 +/- 50152.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4543474 |
|    mean velocity x | -0.144     |
|    mean velocity y | 1.24       |
|    mean velocity z | 4.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+05     |
| time/              |            |
|    total_timesteps | 1098500    |
-----------------------------------
Eval num_timesteps=1099000, episode_reward=-76865.23 +/- 56365.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27087486 |
|    mean velocity x | -0.831      |
|    mean velocity y | -0.323      |
|    mean velocity z | 3.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 1099000     |
------------------------------------
Eval num_timesteps=1099500, episode_reward=-69467.79 +/- 44325.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43366382 |
|    mean velocity x | 0.292       |
|    mean velocity y | 1.61        |
|    mean velocity z | 3.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.95e+04   |
| time/              |             |
|    total_timesteps | 1099500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 537     |
|    time_elapsed    | 44372   |
|    total_timesteps | 1099776 |
--------------------------------
Eval num_timesteps=1100000, episode_reward=-78881.69 +/- 41252.16
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4484781   |
|    mean velocity x      | -0.235       |
|    mean velocity y      | 1.03         |
|    mean velocity z      | 4.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.89e+04    |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 2.955785e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.001        |
|    loss                 | 4.78e+07     |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.000855    |
|    std                  | 1.55         |
|    value_loss           | 6.77e+07     |
------------------------------------------
Eval num_timesteps=1100500, episode_reward=-64123.61 +/- 53530.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5873098 |
|    mean velocity x | -0.544     |
|    mean velocity y | 0.819      |
|    mean velocity z | 5.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.41e+04  |
| time/              |            |
|    total_timesteps | 1100500    |
-----------------------------------
Eval num_timesteps=1101000, episode_reward=-126035.36 +/- 24595.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44383106 |
|    mean velocity x | 0.173       |
|    mean velocity y | 0.745       |
|    mean velocity z | 3.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.26e+05   |
| time/              |             |
|    total_timesteps | 1101000     |
------------------------------------
Eval num_timesteps=1101500, episode_reward=-84622.20 +/- 27788.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4794119 |
|    mean velocity x | -0.34      |
|    mean velocity y | 0.715      |
|    mean velocity z | 4.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.46e+04  |
| time/              |            |
|    total_timesteps | 1101500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 538     |
|    time_elapsed    | 44453   |
|    total_timesteps | 1101824 |
--------------------------------
Eval num_timesteps=1102000, episode_reward=-64655.32 +/- 31012.99
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49673173   |
|    mean velocity x      | 0.626         |
|    mean velocity y      | 1.57          |
|    mean velocity z      | 3.72          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.47e+04     |
| time/                   |               |
|    total_timesteps      | 1102000       |
| train/                  |               |
|    approx_kl            | 5.8792793e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.001         |
|    loss                 | 6.56e+07      |
|    n_updates            | 5380          |
|    policy_gradient_loss | -0.000872     |
|    std                  | 1.55          |
|    value_loss           | 6.93e+07      |
-------------------------------------------
Eval num_timesteps=1102500, episode_reward=-61758.90 +/- 47785.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42488843 |
|    mean velocity x | -0.798      |
|    mean velocity y | 0.847       |
|    mean velocity z | 2.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 1102500     |
------------------------------------
Eval num_timesteps=1103000, episode_reward=-36794.85 +/- 36216.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2610973 |
|    mean velocity x | -0.546     |
|    mean velocity y | 0.027      |
|    mean velocity z | 2.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.68e+04  |
| time/              |            |
|    total_timesteps | 1103000    |
-----------------------------------
Eval num_timesteps=1103500, episode_reward=-100646.35 +/- 21518.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25194702 |
|    mean velocity x | -0.214      |
|    mean velocity y | 0.483       |
|    mean velocity z | 0.452       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1103500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 539     |
|    time_elapsed    | 44533   |
|    total_timesteps | 1103872 |
--------------------------------
Eval num_timesteps=1104000, episode_reward=-60889.09 +/- 47271.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45828387   |
|    mean velocity x      | -0.284        |
|    mean velocity y      | 0.655         |
|    mean velocity z      | 4.97          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.09e+04     |
| time/                   |               |
|    total_timesteps      | 1104000       |
| train/                  |               |
|    approx_kl            | 5.1399606e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.181         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+07      |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.000756     |
|    std                  | 1.55          |
|    value_loss           | 3.88e+07      |
-------------------------------------------
Eval num_timesteps=1104500, episode_reward=-92727.14 +/- 26935.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4182443 |
|    mean velocity x | -1.63      |
|    mean velocity y | -0.129     |
|    mean velocity z | 5.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.27e+04  |
| time/              |            |
|    total_timesteps | 1104500    |
-----------------------------------
Eval num_timesteps=1105000, episode_reward=-48348.06 +/- 40807.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42971164 |
|    mean velocity x | -0.253      |
|    mean velocity y | 0.934       |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.83e+04   |
| time/              |             |
|    total_timesteps | 1105000     |
------------------------------------
Eval num_timesteps=1105500, episode_reward=-55053.47 +/- 57427.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37202924 |
|    mean velocity x | -0.3        |
|    mean velocity y | 0.29        |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.51e+04   |
| time/              |             |
|    total_timesteps | 1105500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 540     |
|    time_elapsed    | 44613   |
|    total_timesteps | 1105920 |
--------------------------------
Eval num_timesteps=1106000, episode_reward=-59680.61 +/- 49146.46
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2727898    |
|    mean velocity x      | -0.525        |
|    mean velocity y      | 0.0231        |
|    mean velocity z      | 3.12          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.97e+04     |
| time/                   |               |
|    total_timesteps      | 1106000       |
| train/                  |               |
|    approx_kl            | 4.2179105e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.001         |
|    loss                 | 4.48e+07      |
|    n_updates            | 5400          |
|    policy_gradient_loss | -0.000836     |
|    std                  | 1.55          |
|    value_loss           | 7.78e+07      |
-------------------------------------------
Eval num_timesteps=1106500, episode_reward=-89738.26 +/- 28370.01
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3931522 |
|    mean velocity x | -0.302     |
|    mean velocity y | 0.595      |
|    mean velocity z | 4.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.97e+04  |
| time/              |            |
|    total_timesteps | 1106500    |
-----------------------------------
Eval num_timesteps=1107000, episode_reward=-108629.56 +/- 18036.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45686895 |
|    mean velocity x | -2.34       |
|    mean velocity y | -0.581      |
|    mean velocity z | 6.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1107000     |
------------------------------------
Eval num_timesteps=1107500, episode_reward=-87067.56 +/- 36267.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46078786 |
|    mean velocity x | 0.289       |
|    mean velocity y | 1.3         |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.71e+04   |
| time/              |             |
|    total_timesteps | 1107500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 541     |
|    time_elapsed    | 44694   |
|    total_timesteps | 1107968 |
--------------------------------
Eval num_timesteps=1108000, episode_reward=-73937.22 +/- 21934.78
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2658306    |
|    mean velocity x      | 0.338         |
|    mean velocity y      | 0.819         |
|    mean velocity z      | 3.59          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.39e+04     |
| time/                   |               |
|    total_timesteps      | 1108000       |
| train/                  |               |
|    approx_kl            | 1.9376661e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.001         |
|    loss                 | 4.88e+07      |
|    n_updates            | 5410          |
|    policy_gradient_loss | -0.000396     |
|    std                  | 1.55          |
|    value_loss           | 6.34e+07      |
-------------------------------------------
Eval num_timesteps=1108500, episode_reward=-76792.36 +/- 46845.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31404248 |
|    mean velocity x | -0.393      |
|    mean velocity y | 0.357       |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.68e+04   |
| time/              |             |
|    total_timesteps | 1108500     |
------------------------------------
Eval num_timesteps=1109000, episode_reward=-92986.43 +/- 46410.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35198477 |
|    mean velocity x | -0.431      |
|    mean velocity y | 0.479       |
|    mean velocity z | 3.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.3e+04    |
| time/              |             |
|    total_timesteps | 1109000     |
------------------------------------
Eval num_timesteps=1109500, episode_reward=-107092.61 +/- 29416.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17329758 |
|    mean velocity x | -0.0176     |
|    mean velocity y | 0.414       |
|    mean velocity z | 0.305       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1109500     |
------------------------------------
Eval num_timesteps=1110000, episode_reward=-104512.09 +/- 47978.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4122102 |
|    mean velocity x | -1.59      |
|    mean velocity y | 0.0612     |
|    mean velocity z | 3.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.05e+05  |
| time/              |            |
|    total_timesteps | 1110000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 542     |
|    time_elapsed    | 44793   |
|    total_timesteps | 1110016 |
--------------------------------
Eval num_timesteps=1110500, episode_reward=-63500.09 +/- 30269.68
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45582315   |
|    mean velocity x      | -0.742        |
|    mean velocity y      | 0.543         |
|    mean velocity z      | 4.07          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.35e+04     |
| time/                   |               |
|    total_timesteps      | 1110500       |
| train/                  |               |
|    approx_kl            | 2.4420384e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+07      |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000241     |
|    std                  | 1.55          |
|    value_loss           | 3.28e+07      |
-------------------------------------------
Eval num_timesteps=1111000, episode_reward=-49063.90 +/- 41692.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42016333 |
|    mean velocity x | -0.647      |
|    mean velocity y | 0.252       |
|    mean velocity z | 3.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.91e+04   |
| time/              |             |
|    total_timesteps | 1111000     |
------------------------------------
Eval num_timesteps=1111500, episode_reward=-84309.81 +/- 34739.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47854254 |
|    mean velocity x | -0.134      |
|    mean velocity y | 1.26        |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.43e+04   |
| time/              |             |
|    total_timesteps | 1111500     |
------------------------------------
Eval num_timesteps=1112000, episode_reward=-68378.01 +/- 43519.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37745073 |
|    mean velocity x | -0.516      |
|    mean velocity y | 0.153       |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 1112000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 543     |
|    time_elapsed    | 44873   |
|    total_timesteps | 1112064 |
--------------------------------
Eval num_timesteps=1112500, episode_reward=-84365.02 +/- 37737.45
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46861988   |
|    mean velocity x      | -0.0748       |
|    mean velocity y      | 1.11          |
|    mean velocity z      | 4.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.44e+04     |
| time/                   |               |
|    total_timesteps      | 1112500       |
| train/                  |               |
|    approx_kl            | 1.0279007e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.178         |
|    learning_rate        | 0.001         |
|    loss                 | 9.16e+07      |
|    n_updates            | 5430          |
|    policy_gradient_loss | -0.000229     |
|    std                  | 1.55          |
|    value_loss           | 8.58e+07      |
-------------------------------------------
Eval num_timesteps=1113000, episode_reward=-68917.28 +/- 48300.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5055116 |
|    mean velocity x | -0.259     |
|    mean velocity y | 1.33       |
|    mean velocity z | 4.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.89e+04  |
| time/              |            |
|    total_timesteps | 1113000    |
-----------------------------------
Eval num_timesteps=1113500, episode_reward=-74738.94 +/- 24022.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41307637 |
|    mean velocity x | -0.362      |
|    mean velocity y | 0.317       |
|    mean velocity z | 4.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.47e+04   |
| time/              |             |
|    total_timesteps | 1113500     |
------------------------------------
Eval num_timesteps=1114000, episode_reward=-89280.23 +/- 13386.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46599764 |
|    mean velocity x | -0.193      |
|    mean velocity y | 1.23        |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1114000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 544     |
|    time_elapsed    | 44954   |
|    total_timesteps | 1114112 |
--------------------------------
Eval num_timesteps=1114500, episode_reward=-39207.39 +/- 21701.02
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2562462    |
|    mean velocity x      | -0.373        |
|    mean velocity y      | 0.482         |
|    mean velocity z      | 0.695         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.92e+04     |
| time/                   |               |
|    total_timesteps      | 1114500       |
| train/                  |               |
|    approx_kl            | 0.00019705438 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.143         |
|    learning_rate        | 0.001         |
|    loss                 | 3.22e+07      |
|    n_updates            | 5440          |
|    policy_gradient_loss | -0.00135      |
|    std                  | 1.55          |
|    value_loss           | 8.46e+07      |
-------------------------------------------
Eval num_timesteps=1115000, episode_reward=-114979.90 +/- 22482.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47562176 |
|    mean velocity x | -0.376      |
|    mean velocity y | 0.949       |
|    mean velocity z | 4.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 1115000     |
------------------------------------
Eval num_timesteps=1115500, episode_reward=-51823.12 +/- 52651.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26999956 |
|    mean velocity x | -0.00759    |
|    mean velocity y | 0.489       |
|    mean velocity z | 0.426       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.18e+04   |
| time/              |             |
|    total_timesteps | 1115500     |
------------------------------------
Eval num_timesteps=1116000, episode_reward=-81631.81 +/- 11978.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34862745 |
|    mean velocity x | -0.274      |
|    mean velocity y | 0.733       |
|    mean velocity z | 2.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 1116000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 545     |
|    time_elapsed    | 45034   |
|    total_timesteps | 1116160 |
--------------------------------
Eval num_timesteps=1116500, episode_reward=-75813.57 +/- 26189.47
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38259575   |
|    mean velocity x      | -1.01         |
|    mean velocity y      | 0.381         |
|    mean velocity z      | 2.66          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.58e+04     |
| time/                   |               |
|    total_timesteps      | 1116500       |
| train/                  |               |
|    approx_kl            | 3.0187832e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+07      |
|    n_updates            | 5450          |
|    policy_gradient_loss | -0.000669     |
|    std                  | 1.55          |
|    value_loss           | 4.43e+07      |
-------------------------------------------
Eval num_timesteps=1117000, episode_reward=-85461.14 +/- 41468.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44772893 |
|    mean velocity x | -0.195      |
|    mean velocity y | 0.937       |
|    mean velocity z | 4.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.55e+04   |
| time/              |             |
|    total_timesteps | 1117000     |
------------------------------------
Eval num_timesteps=1117500, episode_reward=-66575.25 +/- 42965.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38201135 |
|    mean velocity x | -0.615      |
|    mean velocity y | 0.532       |
|    mean velocity z | 2.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.66e+04   |
| time/              |             |
|    total_timesteps | 1117500     |
------------------------------------
Eval num_timesteps=1118000, episode_reward=-85069.05 +/- 40481.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4685586 |
|    mean velocity x | -0.743     |
|    mean velocity y | 0.591      |
|    mean velocity z | 4.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.51e+04  |
| time/              |            |
|    total_timesteps | 1118000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 546     |
|    time_elapsed    | 45115   |
|    total_timesteps | 1118208 |
--------------------------------
Eval num_timesteps=1118500, episode_reward=-71237.07 +/- 30965.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42564747  |
|    mean velocity x      | -0.32        |
|    mean velocity y      | 0.699        |
|    mean velocity z      | 2.19         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.12e+04    |
| time/                   |              |
|    total_timesteps      | 1118500      |
| train/                  |              |
|    approx_kl            | 9.216892e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.001        |
|    loss                 | 2.63e+07     |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.000619    |
|    std                  | 1.55         |
|    value_loss           | 5.1e+07      |
------------------------------------------
Eval num_timesteps=1119000, episode_reward=-90327.48 +/- 49328.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46569586 |
|    mean velocity x | -0.497      |
|    mean velocity y | 0.397       |
|    mean velocity z | 4.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 1119000     |
------------------------------------
Eval num_timesteps=1119500, episode_reward=-94858.92 +/- 29084.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43229347 |
|    mean velocity x | -0.294      |
|    mean velocity y | 0.422       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 1119500     |
------------------------------------
Eval num_timesteps=1120000, episode_reward=-108337.98 +/- 14879.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5254583 |
|    mean velocity x | 1.13       |
|    mean velocity y | 1.93       |
|    mean velocity z | 4.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 1120000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 547     |
|    time_elapsed    | 45195   |
|    total_timesteps | 1120256 |
--------------------------------
Eval num_timesteps=1120500, episode_reward=-61618.49 +/- 36935.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32870662   |
|    mean velocity x      | 0.131         |
|    mean velocity y      | 0.402         |
|    mean velocity z      | 2.32          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.16e+04     |
| time/                   |               |
|    total_timesteps      | 1120500       |
| train/                  |               |
|    approx_kl            | 7.5564603e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.205         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+07      |
|    n_updates            | 5470          |
|    policy_gradient_loss | -0.00026      |
|    std                  | 1.55          |
|    value_loss           | 6.04e+07      |
-------------------------------------------
Eval num_timesteps=1121000, episode_reward=-85488.10 +/- 18732.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46850818 |
|    mean velocity x | 0.359       |
|    mean velocity y | 1.4         |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.55e+04   |
| time/              |             |
|    total_timesteps | 1121000     |
------------------------------------
Eval num_timesteps=1121500, episode_reward=-67648.88 +/- 27382.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41544697 |
|    mean velocity x | -0.284      |
|    mean velocity y | 0.615       |
|    mean velocity z | 4.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 1121500     |
------------------------------------
Eval num_timesteps=1122000, episode_reward=-109154.79 +/- 32091.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5129485 |
|    mean velocity x | -0.291     |
|    mean velocity y | 0.978      |
|    mean velocity z | 4.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 1122000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 548     |
|    time_elapsed    | 45275   |
|    total_timesteps | 1122304 |
--------------------------------
Eval num_timesteps=1122500, episode_reward=-104264.33 +/- 10082.02
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41591576   |
|    mean velocity x      | -0.351        |
|    mean velocity y      | 0.703         |
|    mean velocity z      | 3             |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.04e+05     |
| time/                   |               |
|    total_timesteps      | 1122500       |
| train/                  |               |
|    approx_kl            | 0.00010218265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.001         |
|    loss                 | 7.31e+06      |
|    n_updates            | 5480          |
|    policy_gradient_loss | -0.000996     |
|    std                  | 1.55          |
|    value_loss           | 8.21e+07      |
-------------------------------------------
Eval num_timesteps=1123000, episode_reward=-44894.49 +/- 50885.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36182168 |
|    mean velocity x | -0.343      |
|    mean velocity y | 0.162       |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.49e+04   |
| time/              |             |
|    total_timesteps | 1123000     |
------------------------------------
Eval num_timesteps=1123500, episode_reward=-50092.08 +/- 40458.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7859055 |
|    mean velocity x | -1.14      |
|    mean velocity y | 1.08       |
|    mean velocity z | 11.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.01e+04  |
| time/              |            |
|    total_timesteps | 1123500    |
-----------------------------------
Eval num_timesteps=1124000, episode_reward=-112311.18 +/- 20810.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2565263 |
|    mean velocity x | -0.0476    |
|    mean velocity y | 0.669      |
|    mean velocity z | 0.738      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 1124000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 549     |
|    time_elapsed    | 45355   |
|    total_timesteps | 1124352 |
--------------------------------
Eval num_timesteps=1124500, episode_reward=-105347.27 +/- 19007.32
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45181867   |
|    mean velocity x      | -1.53         |
|    mean velocity y      | 0.146         |
|    mean velocity z      | 3.66          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 1124500       |
| train/                  |               |
|    approx_kl            | 3.2237847e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.001         |
|    loss                 | 7.92e+06      |
|    n_updates            | 5490          |
|    policy_gradient_loss | -0.000367     |
|    std                  | 1.55          |
|    value_loss           | 5.05e+07      |
-------------------------------------------
Eval num_timesteps=1125000, episode_reward=-53391.17 +/- 43413.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25973907 |
|    mean velocity x | -0.0335     |
|    mean velocity y | 0.464       |
|    mean velocity z | 0.979       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.34e+04   |
| time/              |             |
|    total_timesteps | 1125000     |
------------------------------------
Eval num_timesteps=1125500, episode_reward=-70013.92 +/- 44413.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5087647 |
|    mean velocity x | -0.467     |
|    mean velocity y | 0.861      |
|    mean velocity z | 4.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7e+04     |
| time/              |            |
|    total_timesteps | 1125500    |
-----------------------------------
Eval num_timesteps=1126000, episode_reward=-76336.22 +/- 43041.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4077419 |
|    mean velocity x | -0.245     |
|    mean velocity y | 0.596      |
|    mean velocity z | 4.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 1126000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 550     |
|    time_elapsed    | 45436   |
|    total_timesteps | 1126400 |
--------------------------------
Eval num_timesteps=1126500, episode_reward=-104087.00 +/- 35950.73
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48770797  |
|    mean velocity x      | 0.287        |
|    mean velocity y      | 1.38         |
|    mean velocity z      | 3.42         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.04e+05    |
| time/                   |              |
|    total_timesteps      | 1126500      |
| train/                  |              |
|    approx_kl            | 2.487062e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.207        |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+07      |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.000352    |
|    std                  | 1.55         |
|    value_loss           | 5.58e+07     |
------------------------------------------
Eval num_timesteps=1127000, episode_reward=-98130.25 +/- 24041.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3706585 |
|    mean velocity x | -0.0423    |
|    mean velocity y | 1.11       |
|    mean velocity z | 4.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.81e+04  |
| time/              |            |
|    total_timesteps | 1127000    |
-----------------------------------
Eval num_timesteps=1127500, episode_reward=-74309.58 +/- 30463.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49453357 |
|    mean velocity x | -0.259      |
|    mean velocity y | 1.32        |
|    mean velocity z | 4.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 1127500     |
------------------------------------
Eval num_timesteps=1128000, episode_reward=-87843.23 +/- 36182.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4568339 |
|    mean velocity x | -2.34      |
|    mean velocity y | -0.835     |
|    mean velocity z | 5.95       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.78e+04  |
| time/              |            |
|    total_timesteps | 1128000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 551     |
|    time_elapsed    | 45516   |
|    total_timesteps | 1128448 |
--------------------------------
Eval num_timesteps=1128500, episode_reward=-83805.97 +/- 37942.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.28535202   |
|    mean velocity x      | 0.204         |
|    mean velocity y      | 0.433         |
|    mean velocity z      | 3.22          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.38e+04     |
| time/                   |               |
|    total_timesteps      | 1128500       |
| train/                  |               |
|    approx_kl            | 1.3598066e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.001         |
|    loss                 | 6.66e+07      |
|    n_updates            | 5510          |
|    policy_gradient_loss | -0.000357     |
|    std                  | 1.55          |
|    value_loss           | 8.9e+07       |
-------------------------------------------
Eval num_timesteps=1129000, episode_reward=-73344.83 +/- 42226.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2301326 |
|    mean velocity x | -0.302     |
|    mean velocity y | 0.454      |
|    mean velocity z | 0.873      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.33e+04  |
| time/              |            |
|    total_timesteps | 1129000    |
-----------------------------------
Eval num_timesteps=1129500, episode_reward=-65054.35 +/- 41824.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3361123 |
|    mean velocity x | -0.3       |
|    mean velocity y | 0.634      |
|    mean velocity z | 1.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.51e+04  |
| time/              |            |
|    total_timesteps | 1129500    |
-----------------------------------
Eval num_timesteps=1130000, episode_reward=-97243.13 +/- 16509.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37347764 |
|    mean velocity x | -0.532      |
|    mean velocity y | 0.838       |
|    mean velocity z | 2.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.72e+04   |
| time/              |             |
|    total_timesteps | 1130000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 552     |
|    time_elapsed    | 45597   |
|    total_timesteps | 1130496 |
--------------------------------
Eval num_timesteps=1130500, episode_reward=-64856.19 +/- 38526.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.23457257   |
|    mean velocity x      | -0.522        |
|    mean velocity y      | 0.33          |
|    mean velocity z      | 1.72          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.49e+04     |
| time/                   |               |
|    total_timesteps      | 1130500       |
| train/                  |               |
|    approx_kl            | 0.00014704253 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.232         |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+07       |
|    n_updates            | 5520          |
|    policy_gradient_loss | -0.000478     |
|    std                  | 1.56          |
|    value_loss           | 7.83e+06      |
-------------------------------------------
Eval num_timesteps=1131000, episode_reward=-85463.72 +/- 43769.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42082593 |
|    mean velocity x | -0.103      |
|    mean velocity y | 0.828       |
|    mean velocity z | 2.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.55e+04   |
| time/              |             |
|    total_timesteps | 1131000     |
------------------------------------
Eval num_timesteps=1131500, episode_reward=-62840.54 +/- 46793.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43461597 |
|    mean velocity x | -0.0874     |
|    mean velocity y | 1.18        |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 1131500     |
------------------------------------
Eval num_timesteps=1132000, episode_reward=-94023.89 +/- 39427.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42301723 |
|    mean velocity x | 0.0144      |
|    mean velocity y | 1.39        |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.4e+04    |
| time/              |             |
|    total_timesteps | 1132000     |
------------------------------------
Eval num_timesteps=1132500, episode_reward=-91640.97 +/- 47189.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34397593 |
|    mean velocity x | -0.339      |
|    mean velocity y | 0.148       |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.16e+04   |
| time/              |             |
|    total_timesteps | 1132500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 553     |
|    time_elapsed    | 45696   |
|    total_timesteps | 1132544 |
--------------------------------
Eval num_timesteps=1133000, episode_reward=-55672.74 +/- 52918.24
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27921787  |
|    mean velocity x      | 0.41         |
|    mean velocity y      | 1.27         |
|    mean velocity z      | 3.51         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.57e+04    |
| time/                   |              |
|    total_timesteps      | 1133000      |
| train/                  |              |
|    approx_kl            | 4.260993e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.001        |
|    loss                 | 2.45e+07     |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.000776    |
|    std                  | 1.56         |
|    value_loss           | 7.09e+07     |
------------------------------------------
Eval num_timesteps=1133500, episode_reward=-87081.62 +/- 45715.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46989045 |
|    mean velocity x | -0.336      |
|    mean velocity y | 1.13        |
|    mean velocity z | 4.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.71e+04   |
| time/              |             |
|    total_timesteps | 1133500     |
------------------------------------
Eval num_timesteps=1134000, episode_reward=-70996.73 +/- 40162.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41013047 |
|    mean velocity x | -0.192      |
|    mean velocity y | 1.05        |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.1e+04    |
| time/              |             |
|    total_timesteps | 1134000     |
------------------------------------
Eval num_timesteps=1134500, episode_reward=-85305.06 +/- 19158.85
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4277921 |
|    mean velocity x | -0.928     |
|    mean velocity y | 0.169      |
|    mean velocity z | 3.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.53e+04  |
| time/              |            |
|    total_timesteps | 1134500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 554     |
|    time_elapsed    | 45776   |
|    total_timesteps | 1134592 |
--------------------------------
Eval num_timesteps=1135000, episode_reward=-76362.77 +/- 47594.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42491907   |
|    mean velocity x      | -1.29         |
|    mean velocity y      | 0.0862        |
|    mean velocity z      | 3.14          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.64e+04     |
| time/                   |               |
|    total_timesteps      | 1135000       |
| train/                  |               |
|    approx_kl            | 1.0101212e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.001         |
|    loss                 | 2.66e+07      |
|    n_updates            | 5540          |
|    policy_gradient_loss | -0.000379     |
|    std                  | 1.56          |
|    value_loss           | 6.78e+07      |
-------------------------------------------
Eval num_timesteps=1135500, episode_reward=-60518.42 +/- 48203.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5029566 |
|    mean velocity x | -0.26      |
|    mean velocity y | 1.37       |
|    mean velocity z | 4.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.05e+04  |
| time/              |            |
|    total_timesteps | 1135500    |
-----------------------------------
Eval num_timesteps=1136000, episode_reward=-70412.35 +/- 35414.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46193835 |
|    mean velocity x | -0.797      |
|    mean velocity y | 0.276       |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.04e+04   |
| time/              |             |
|    total_timesteps | 1136000     |
------------------------------------
Eval num_timesteps=1136500, episode_reward=-49439.98 +/- 30880.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.67261475 |
|    mean velocity x | -0.466      |
|    mean velocity y | 1.33        |
|    mean velocity z | 5.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.94e+04   |
| time/              |             |
|    total_timesteps | 1136500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 555     |
|    time_elapsed    | 45857   |
|    total_timesteps | 1136640 |
--------------------------------
Eval num_timesteps=1137000, episode_reward=-55825.22 +/- 44626.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47387394   |
|    mean velocity x      | -1.07         |
|    mean velocity y      | 0.0387        |
|    mean velocity z      | 4.42          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.58e+04     |
| time/                   |               |
|    total_timesteps      | 1137000       |
| train/                  |               |
|    approx_kl            | 2.5971123e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.001         |
|    loss                 | 2.94e+07      |
|    n_updates            | 5550          |
|    policy_gradient_loss | -0.000439     |
|    std                  | 1.56          |
|    value_loss           | 6.2e+07       |
-------------------------------------------
Eval num_timesteps=1137500, episode_reward=-78794.58 +/- 39630.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2709862 |
|    mean velocity x | -0.0431    |
|    mean velocity y | 0.221      |
|    mean velocity z | 3.23       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.88e+04  |
| time/              |            |
|    total_timesteps | 1137500    |
-----------------------------------
Eval num_timesteps=1138000, episode_reward=-93423.08 +/- 24442.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46690008 |
|    mean velocity x | -0.115      |
|    mean velocity y | 0.499       |
|    mean velocity z | 2.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.34e+04   |
| time/              |             |
|    total_timesteps | 1138000     |
------------------------------------
Eval num_timesteps=1138500, episode_reward=-71101.68 +/- 54661.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41127002 |
|    mean velocity x | -0.327      |
|    mean velocity y | 1.07        |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.11e+04   |
| time/              |             |
|    total_timesteps | 1138500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 556     |
|    time_elapsed    | 45937   |
|    total_timesteps | 1138688 |
--------------------------------
Eval num_timesteps=1139000, episode_reward=-79627.35 +/- 50928.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35674018   |
|    mean velocity x      | 0.0785        |
|    mean velocity y      | 0.542         |
|    mean velocity z      | 1.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.96e+04     |
| time/                   |               |
|    total_timesteps      | 1139000       |
| train/                  |               |
|    approx_kl            | 3.9004197e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.001         |
|    loss                 | 1.92e+07      |
|    n_updates            | 5560          |
|    policy_gradient_loss | -0.000498     |
|    std                  | 1.56          |
|    value_loss           | 3.34e+07      |
-------------------------------------------
Eval num_timesteps=1139500, episode_reward=-63678.18 +/- 17360.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4976841 |
|    mean velocity x | 0.361      |
|    mean velocity y | 1.61       |
|    mean velocity z | 3.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.37e+04  |
| time/              |            |
|    total_timesteps | 1139500    |
-----------------------------------
Eval num_timesteps=1140000, episode_reward=-37443.59 +/- 26501.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42073765 |
|    mean velocity x | -0.137      |
|    mean velocity y | 0.845       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.74e+04   |
| time/              |             |
|    total_timesteps | 1140000     |
------------------------------------
Eval num_timesteps=1140500, episode_reward=-68059.79 +/- 40697.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46642765 |
|    mean velocity x | -0.139      |
|    mean velocity y | 1.27        |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1140500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 557     |
|    time_elapsed    | 46017   |
|    total_timesteps | 1140736 |
--------------------------------
Eval num_timesteps=1141000, episode_reward=-78408.02 +/- 44757.61
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4779153    |
|    mean velocity x      | -0.584        |
|    mean velocity y      | 0.88          |
|    mean velocity z      | 3.48          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.84e+04     |
| time/                   |               |
|    total_timesteps      | 1141000       |
| train/                  |               |
|    approx_kl            | 0.00010295672 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.001         |
|    loss                 | 4.33e+07      |
|    n_updates            | 5570          |
|    policy_gradient_loss | -0.00085      |
|    std                  | 1.56          |
|    value_loss           | 5.43e+07      |
-------------------------------------------
Eval num_timesteps=1141500, episode_reward=-89676.83 +/- 45901.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37840858 |
|    mean velocity x | -0.514      |
|    mean velocity y | 0.423       |
|    mean velocity z | 3.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 1141500     |
------------------------------------
Eval num_timesteps=1142000, episode_reward=-108018.53 +/- 20888.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4564449 |
|    mean velocity x | -0.469     |
|    mean velocity y | 0.421      |
|    mean velocity z | 4.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 1142000    |
-----------------------------------
Eval num_timesteps=1142500, episode_reward=-116880.68 +/- 17790.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35529357 |
|    mean velocity x | -0.656      |
|    mean velocity y | -0.0807     |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.17e+05   |
| time/              |             |
|    total_timesteps | 1142500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 558     |
|    time_elapsed    | 46098   |
|    total_timesteps | 1142784 |
--------------------------------
Eval num_timesteps=1143000, episode_reward=-73530.19 +/- 46829.23
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4208758   |
|    mean velocity x      | -0.351       |
|    mean velocity y      | 0.891        |
|    mean velocity z      | 4.22         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.35e+04    |
| time/                   |              |
|    total_timesteps      | 1143000      |
| train/                  |              |
|    approx_kl            | 9.165204e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.001        |
|    loss                 | 8.23e+06     |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.000177    |
|    std                  | 1.56         |
|    value_loss           | 6.03e+07     |
------------------------------------------
Eval num_timesteps=1143500, episode_reward=-67151.07 +/- 37927.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34788033 |
|    mean velocity x | -0.619      |
|    mean velocity y | 0.73        |
|    mean velocity z | 1.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 1143500     |
------------------------------------
Eval num_timesteps=1144000, episode_reward=-111477.45 +/- 20096.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19294298 |
|    mean velocity x | -0.0888     |
|    mean velocity y | 0.476       |
|    mean velocity z | 0.408       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1144000     |
------------------------------------
Eval num_timesteps=1144500, episode_reward=-49833.99 +/- 42121.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41663113 |
|    mean velocity x | -0.041      |
|    mean velocity y | 0.455       |
|    mean velocity z | 2.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.98e+04   |
| time/              |             |
|    total_timesteps | 1144500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 559     |
|    time_elapsed    | 46178   |
|    total_timesteps | 1144832 |
--------------------------------
Eval num_timesteps=1145000, episode_reward=-102567.80 +/- 20888.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4833122    |
|    mean velocity x      | -0.413        |
|    mean velocity y      | 0.947         |
|    mean velocity z      | 4.56          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 1145000       |
| train/                  |               |
|    approx_kl            | 5.4392964e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.001         |
|    loss                 | 4.28e+06      |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.000532     |
|    std                  | 1.56          |
|    value_loss           | 2.88e+07      |
-------------------------------------------
Eval num_timesteps=1145500, episode_reward=-57777.29 +/- 31978.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36937237 |
|    mean velocity x | -0.634      |
|    mean velocity y | -0.121      |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.78e+04   |
| time/              |             |
|    total_timesteps | 1145500     |
------------------------------------
Eval num_timesteps=1146000, episode_reward=-49250.66 +/- 38917.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50128436 |
|    mean velocity x | -0.442      |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.93e+04   |
| time/              |             |
|    total_timesteps | 1146000     |
------------------------------------
Eval num_timesteps=1146500, episode_reward=-61361.52 +/- 39322.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28719923 |
|    mean velocity x | -0.746      |
|    mean velocity y | -0.089      |
|    mean velocity z | 2.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.14e+04   |
| time/              |             |
|    total_timesteps | 1146500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 560     |
|    time_elapsed    | 46259   |
|    total_timesteps | 1146880 |
--------------------------------
Eval num_timesteps=1147000, episode_reward=-67024.06 +/- 39634.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5442524    |
|    mean velocity x      | -0.189        |
|    mean velocity y      | 1.32          |
|    mean velocity z      | 5.2           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.7e+04      |
| time/                   |               |
|    total_timesteps      | 1147000       |
| train/                  |               |
|    approx_kl            | 3.3182238e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.001         |
|    loss                 | 7.64e+07      |
|    n_updates            | 5600          |
|    policy_gradient_loss | -0.000483     |
|    std                  | 1.56          |
|    value_loss           | 6.86e+07      |
-------------------------------------------
Eval num_timesteps=1147500, episode_reward=-51526.74 +/- 30414.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4544394 |
|    mean velocity x | -0.292     |
|    mean velocity y | 1.04       |
|    mean velocity z | 2.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.15e+04  |
| time/              |            |
|    total_timesteps | 1147500    |
-----------------------------------
Eval num_timesteps=1148000, episode_reward=-63647.09 +/- 41714.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49046832 |
|    mean velocity x | -1.06       |
|    mean velocity y | 0.567       |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.36e+04   |
| time/              |             |
|    total_timesteps | 1148000     |
------------------------------------
Eval num_timesteps=1148500, episode_reward=-106392.16 +/- 24609.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4587973 |
|    mean velocity x | 0.213      |
|    mean velocity y | 1.28       |
|    mean velocity z | 3.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 1148500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 561     |
|    time_elapsed    | 46339   |
|    total_timesteps | 1148928 |
--------------------------------
Eval num_timesteps=1149000, episode_reward=-70619.72 +/- 57630.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44755676   |
|    mean velocity x      | -0.572        |
|    mean velocity y      | 0.54          |
|    mean velocity z      | 3.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.06e+04     |
| time/                   |               |
|    total_timesteps      | 1149000       |
| train/                  |               |
|    approx_kl            | 0.00018120158 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.246         |
|    learning_rate        | 0.001         |
|    loss                 | 2.09e+07      |
|    n_updates            | 5610          |
|    policy_gradient_loss | -0.00138      |
|    std                  | 1.56          |
|    value_loss           | 4.06e+07      |
-------------------------------------------
Eval num_timesteps=1149500, episode_reward=-61876.36 +/- 32311.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36751586 |
|    mean velocity x | -0.0277     |
|    mean velocity y | 0.685       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+04   |
| time/              |             |
|    total_timesteps | 1149500     |
------------------------------------
Eval num_timesteps=1150000, episode_reward=-95693.88 +/- 26100.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43249395 |
|    mean velocity x | -0.69       |
|    mean velocity y | 0.108       |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.57e+04   |
| time/              |             |
|    total_timesteps | 1150000     |
------------------------------------
Eval num_timesteps=1150500, episode_reward=-62444.53 +/- 52015.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5402415 |
|    mean velocity x | 0.229      |
|    mean velocity y | 2.08       |
|    mean velocity z | 3.85       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.24e+04  |
| time/              |            |
|    total_timesteps | 1150500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 562     |
|    time_elapsed    | 46419   |
|    total_timesteps | 1150976 |
--------------------------------
Eval num_timesteps=1151000, episode_reward=-45214.03 +/- 49803.39
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3422296    |
|    mean velocity x      | 0.117         |
|    mean velocity y      | 0.805         |
|    mean velocity z      | 3.65          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.52e+04     |
| time/                   |               |
|    total_timesteps      | 1151000       |
| train/                  |               |
|    approx_kl            | 2.8357783e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.001         |
|    loss                 | 5.74e+06      |
|    n_updates            | 5620          |
|    policy_gradient_loss | -0.000334     |
|    std                  | 1.56          |
|    value_loss           | 6.11e+07      |
-------------------------------------------
Eval num_timesteps=1151500, episode_reward=-73134.24 +/- 41799.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39921325 |
|    mean velocity x | -0.152      |
|    mean velocity y | 1.03        |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.31e+04   |
| time/              |             |
|    total_timesteps | 1151500     |
------------------------------------
Eval num_timesteps=1152000, episode_reward=-86812.16 +/- 45817.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40027967 |
|    mean velocity x | -0.0285     |
|    mean velocity y | 1.28        |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.68e+04   |
| time/              |             |
|    total_timesteps | 1152000     |
------------------------------------
Eval num_timesteps=1152500, episode_reward=-63059.77 +/- 39338.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42483267 |
|    mean velocity x | -0.164      |
|    mean velocity y | 0.885       |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.31e+04   |
| time/              |             |
|    total_timesteps | 1152500     |
------------------------------------
Eval num_timesteps=1153000, episode_reward=-78367.20 +/- 43775.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5390573 |
|    mean velocity x | -0.474     |
|    mean velocity y | 0.697      |
|    mean velocity z | 4.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.84e+04  |
| time/              |            |
|    total_timesteps | 1153000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 563     |
|    time_elapsed    | 46518   |
|    total_timesteps | 1153024 |
--------------------------------
Eval num_timesteps=1153500, episode_reward=-87124.87 +/- 36166.35
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24108835   |
|    mean velocity x      | -0.147        |
|    mean velocity y      | 0.464         |
|    mean velocity z      | 0.24          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.71e+04     |
| time/                   |               |
|    total_timesteps      | 1153500       |
| train/                  |               |
|    approx_kl            | 0.00010968346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.18          |
|    learning_rate        | 0.001         |
|    loss                 | 2.11e+07      |
|    n_updates            | 5630          |
|    policy_gradient_loss | -0.000638     |
|    std                  | 1.56          |
|    value_loss           | 9.31e+07      |
-------------------------------------------
Eval num_timesteps=1154000, episode_reward=-72581.98 +/- 43698.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5387133 |
|    mean velocity x | -0.0248    |
|    mean velocity y | 1.73       |
|    mean velocity z | 3.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.26e+04  |
| time/              |            |
|    total_timesteps | 1154000    |
-----------------------------------
Eval num_timesteps=1154500, episode_reward=-102148.98 +/- 19579.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38574713 |
|    mean velocity x | -0.69       |
|    mean velocity y | -0.213      |
|    mean velocity z | 3.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1154500     |
------------------------------------
Eval num_timesteps=1155000, episode_reward=-74481.57 +/- 37991.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5682908 |
|    mean velocity x | 0.596      |
|    mean velocity y | 2.08       |
|    mean velocity z | 3.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.45e+04  |
| time/              |            |
|    total_timesteps | 1155000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 564     |
|    time_elapsed    | 46599   |
|    total_timesteps | 1155072 |
--------------------------------
Eval num_timesteps=1155500, episode_reward=-72727.82 +/- 47793.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34986976   |
|    mean velocity x      | -0.235        |
|    mean velocity y      | 0.602         |
|    mean velocity z      | 2.08          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.27e+04     |
| time/                   |               |
|    total_timesteps      | 1155500       |
| train/                  |               |
|    approx_kl            | 1.4606194e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.291         |
|    learning_rate        | 0.001         |
|    loss                 | 2.56e+07      |
|    n_updates            | 5640          |
|    policy_gradient_loss | -0.000246     |
|    std                  | 1.56          |
|    value_loss           | 3.4e+07       |
-------------------------------------------
Eval num_timesteps=1156000, episode_reward=-50765.93 +/- 49548.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38607275 |
|    mean velocity x | -0.292      |
|    mean velocity y | 0.481       |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.08e+04   |
| time/              |             |
|    total_timesteps | 1156000     |
------------------------------------
Eval num_timesteps=1156500, episode_reward=-78671.70 +/- 38219.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4179486 |
|    mean velocity x | -0.336     |
|    mean velocity y | 0.62       |
|    mean velocity z | 4.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.87e+04  |
| time/              |            |
|    total_timesteps | 1156500    |
-----------------------------------
Eval num_timesteps=1157000, episode_reward=-84143.86 +/- 45843.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44364387 |
|    mean velocity x | -0.0389     |
|    mean velocity y | 0.921       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.41e+04   |
| time/              |             |
|    total_timesteps | 1157000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 565     |
|    time_elapsed    | 46679   |
|    total_timesteps | 1157120 |
--------------------------------
Eval num_timesteps=1157500, episode_reward=-101940.45 +/- 32798.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27673057   |
|    mean velocity x      | -0.443        |
|    mean velocity y      | 0.00786       |
|    mean velocity z      | 3.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 1157500       |
| train/                  |               |
|    approx_kl            | 1.0646996e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.001         |
|    loss                 | 8.4e+06       |
|    n_updates            | 5650          |
|    policy_gradient_loss | -0.000264     |
|    std                  | 1.56          |
|    value_loss           | 7.75e+07      |
-------------------------------------------
Eval num_timesteps=1158000, episode_reward=-80486.37 +/- 43755.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26151708 |
|    mean velocity x | -2.55       |
|    mean velocity y | -2.03       |
|    mean velocity z | 7.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 1158000     |
------------------------------------
Eval num_timesteps=1158500, episode_reward=-46919.68 +/- 36473.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44996884 |
|    mean velocity x | -0.26       |
|    mean velocity y | 1.23        |
|    mean velocity z | 4.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.69e+04   |
| time/              |             |
|    total_timesteps | 1158500     |
------------------------------------
Eval num_timesteps=1159000, episode_reward=-84862.00 +/- 26321.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47550207 |
|    mean velocity x | -0.502      |
|    mean velocity y | 0.647       |
|    mean velocity z | 4.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.49e+04   |
| time/              |             |
|    total_timesteps | 1159000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 566     |
|    time_elapsed    | 46759   |
|    total_timesteps | 1159168 |
--------------------------------
Eval num_timesteps=1159500, episode_reward=-83290.47 +/- 27479.58
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4749659   |
|    mean velocity x      | -0.0631      |
|    mean velocity y      | 1.45         |
|    mean velocity z      | 4.16         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.33e+04    |
| time/                   |              |
|    total_timesteps      | 1159500      |
| train/                  |              |
|    approx_kl            | 9.892334e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+07     |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.000216    |
|    std                  | 1.56         |
|    value_loss           | 7.25e+07     |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=-80962.35 +/- 43943.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4887572 |
|    mean velocity x | -0.446     |
|    mean velocity y | 0.726      |
|    mean velocity z | 4.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 1160000    |
-----------------------------------
Eval num_timesteps=1160500, episode_reward=-65172.76 +/- 37737.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46563646 |
|    mean velocity x | -0.399      |
|    mean velocity y | 0.797       |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.52e+04   |
| time/              |             |
|    total_timesteps | 1160500     |
------------------------------------
Eval num_timesteps=1161000, episode_reward=-87131.55 +/- 44800.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48520747 |
|    mean velocity x | -0.848      |
|    mean velocity y | 0.37        |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.71e+04   |
| time/              |             |
|    total_timesteps | 1161000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 567     |
|    time_elapsed    | 46840   |
|    total_timesteps | 1161216 |
--------------------------------
Eval num_timesteps=1161500, episode_reward=-71790.55 +/- 44400.14
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2783556   |
|    mean velocity x      | -0.164       |
|    mean velocity y      | 0.645        |
|    mean velocity z      | 0.835        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.18e+04    |
| time/                   |              |
|    total_timesteps      | 1161500      |
| train/                  |              |
|    approx_kl            | 3.651713e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+07     |
|    n_updates            | 5670         |
|    policy_gradient_loss | -0.000357    |
|    std                  | 1.56         |
|    value_loss           | 6.55e+07     |
------------------------------------------
Eval num_timesteps=1162000, episode_reward=-68133.84 +/- 44528.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4618201 |
|    mean velocity x | -0.51      |
|    mean velocity y | 0.38       |
|    mean velocity z | 4.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.81e+04  |
| time/              |            |
|    total_timesteps | 1162000    |
-----------------------------------
Eval num_timesteps=1162500, episode_reward=-71582.25 +/- 39262.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3321884 |
|    mean velocity x | -0.877     |
|    mean velocity y | 0.0493     |
|    mean velocity z | 3.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.16e+04  |
| time/              |            |
|    total_timesteps | 1162500    |
-----------------------------------
Eval num_timesteps=1163000, episode_reward=-64148.00 +/- 38336.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5172593 |
|    mean velocity x | -0.352     |
|    mean velocity y | 1.23       |
|    mean velocity z | 4.71       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.41e+04  |
| time/              |            |
|    total_timesteps | 1163000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 568     |
|    time_elapsed    | 46920   |
|    total_timesteps | 1163264 |
--------------------------------
Eval num_timesteps=1163500, episode_reward=-90410.74 +/- 22615.35
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3016048    |
|    mean velocity x      | -0.133        |
|    mean velocity y      | 0.521         |
|    mean velocity z      | 0.423         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.04e+04     |
| time/                   |               |
|    total_timesteps      | 1163500       |
| train/                  |               |
|    approx_kl            | 7.7970646e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.001         |
|    loss                 | 4.74e+06      |
|    n_updates            | 5680          |
|    policy_gradient_loss | -0.000826     |
|    std                  | 1.56          |
|    value_loss           | 6.21e+07      |
-------------------------------------------
Eval num_timesteps=1164000, episode_reward=-98166.19 +/- 20881.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50723875 |
|    mean velocity x | -0.377      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.82e+04   |
| time/              |             |
|    total_timesteps | 1164000     |
------------------------------------
Eval num_timesteps=1164500, episode_reward=-82659.17 +/- 47517.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47295308 |
|    mean velocity x | -0.171      |
|    mean velocity y | 0.979       |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.27e+04   |
| time/              |             |
|    total_timesteps | 1164500     |
------------------------------------
Eval num_timesteps=1165000, episode_reward=-74546.36 +/- 39500.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46383148 |
|    mean velocity x | 0.0711      |
|    mean velocity y | 1.12        |
|    mean velocity z | 3.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.45e+04   |
| time/              |             |
|    total_timesteps | 1165000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 569     |
|    time_elapsed    | 47000   |
|    total_timesteps | 1165312 |
--------------------------------
Eval num_timesteps=1165500, episode_reward=-72795.99 +/- 21750.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.56147105   |
|    mean velocity x      | -0.53         |
|    mean velocity y      | 0.986         |
|    mean velocity z      | 5.09          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.28e+04     |
| time/                   |               |
|    total_timesteps      | 1165500       |
| train/                  |               |
|    approx_kl            | 2.7238391e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.001         |
|    loss                 | 6.33e+07      |
|    n_updates            | 5690          |
|    policy_gradient_loss | -0.00028      |
|    std                  | 1.56          |
|    value_loss           | 7.55e+07      |
-------------------------------------------
Eval num_timesteps=1166000, episode_reward=-64752.22 +/- 39029.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45791903 |
|    mean velocity x | -0.237      |
|    mean velocity y | 1.53        |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 1166000     |
------------------------------------
Eval num_timesteps=1166500, episode_reward=-89842.13 +/- 37573.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26280254 |
|    mean velocity x | -0.0101     |
|    mean velocity y | 0.564       |
|    mean velocity z | 0.593       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.98e+04   |
| time/              |             |
|    total_timesteps | 1166500     |
------------------------------------
Eval num_timesteps=1167000, episode_reward=-80853.30 +/- 20788.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30054587 |
|    mean velocity x | -0.465      |
|    mean velocity y | 0.635       |
|    mean velocity z | 1.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.09e+04   |
| time/              |             |
|    total_timesteps | 1167000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 570     |
|    time_elapsed    | 47081   |
|    total_timesteps | 1167360 |
--------------------------------
Eval num_timesteps=1167500, episode_reward=-92446.21 +/- 63067.23
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4353563    |
|    mean velocity x      | -0.233        |
|    mean velocity y      | 0.933         |
|    mean velocity z      | 4.57          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.24e+04     |
| time/                   |               |
|    total_timesteps      | 1167500       |
| train/                  |               |
|    approx_kl            | 7.4823445e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.167         |
|    learning_rate        | 0.001         |
|    loss                 | 3.96e+07      |
|    n_updates            | 5700          |
|    policy_gradient_loss | -0.000804     |
|    std                  | 1.56          |
|    value_loss           | 5.52e+07      |
-------------------------------------------
Eval num_timesteps=1168000, episode_reward=-95238.14 +/- 40178.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47924665 |
|    mean velocity x | -0.159      |
|    mean velocity y | 1.25        |
|    mean velocity z | 4.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.52e+04   |
| time/              |             |
|    total_timesteps | 1168000     |
------------------------------------
Eval num_timesteps=1168500, episode_reward=-72475.67 +/- 42477.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5111742 |
|    mean velocity x | -1.54      |
|    mean velocity y | -0.743     |
|    mean velocity z | 7.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.25e+04  |
| time/              |            |
|    total_timesteps | 1168500    |
-----------------------------------
Eval num_timesteps=1169000, episode_reward=-82759.22 +/- 48412.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2286753 |
|    mean velocity x | -0.0578    |
|    mean velocity y | 0.498      |
|    mean velocity z | 0.542      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.28e+04  |
| time/              |            |
|    total_timesteps | 1169000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 571     |
|    time_elapsed    | 47161   |
|    total_timesteps | 1169408 |
--------------------------------
Eval num_timesteps=1169500, episode_reward=-56627.91 +/- 17304.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3797224   |
|    mean velocity x      | 0.386        |
|    mean velocity y      | 1.28         |
|    mean velocity z      | 3.78         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.66e+04    |
| time/                   |              |
|    total_timesteps      | 1169500      |
| train/                  |              |
|    approx_kl            | 3.976145e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+07     |
|    n_updates            | 5710         |
|    policy_gradient_loss | -0.000485    |
|    std                  | 1.56         |
|    value_loss           | 4.84e+07     |
------------------------------------------
Eval num_timesteps=1170000, episode_reward=-88466.69 +/- 42924.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49531624 |
|    mean velocity x | -0.117      |
|    mean velocity y | 1.41        |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 1170000     |
------------------------------------
Eval num_timesteps=1170500, episode_reward=-78902.05 +/- 41843.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5236103 |
|    mean velocity x | 0.673      |
|    mean velocity y | 1.98       |
|    mean velocity z | 3.85       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 1170500    |
-----------------------------------
Eval num_timesteps=1171000, episode_reward=-122827.88 +/- 14640.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48365277 |
|    mean velocity x | -1.13       |
|    mean velocity y | 0.202       |
|    mean velocity z | 3.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 1171000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 572     |
|    time_elapsed    | 47241   |
|    total_timesteps | 1171456 |
--------------------------------
Eval num_timesteps=1171500, episode_reward=-59223.56 +/- 28692.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.39270908  |
|    mean velocity x      | -0.306       |
|    mean velocity y      | 0.545        |
|    mean velocity z      | 3.43         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.92e+04    |
| time/                   |              |
|    total_timesteps      | 1171500      |
| train/                  |              |
|    approx_kl            | 0.0002103211 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.254        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+07     |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.56         |
|    value_loss           | 5.06e+07     |
------------------------------------------
Eval num_timesteps=1172000, episode_reward=-96045.58 +/- 48154.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53802496 |
|    mean velocity x | 0.204       |
|    mean velocity y | 1.52        |
|    mean velocity z | 4.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.6e+04    |
| time/              |             |
|    total_timesteps | 1172000     |
------------------------------------
Eval num_timesteps=1172500, episode_reward=-76391.10 +/- 45561.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3137437 |
|    mean velocity x | -0.159     |
|    mean velocity y | 0.425      |
|    mean velocity z | 2.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.64e+04  |
| time/              |            |
|    total_timesteps | 1172500    |
-----------------------------------
Eval num_timesteps=1173000, episode_reward=-89891.57 +/- 30988.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38306707 |
|    mean velocity x | 0.164       |
|    mean velocity y | 0.765       |
|    mean velocity z | 2.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.99e+04   |
| time/              |             |
|    total_timesteps | 1173000     |
------------------------------------
Eval num_timesteps=1173500, episode_reward=-86566.38 +/- 18420.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3302846 |
|    mean velocity x | 0.395      |
|    mean velocity y | 0.777      |
|    mean velocity z | 3.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.66e+04  |
| time/              |            |
|    total_timesteps | 1173500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 573     |
|    time_elapsed    | 47341   |
|    total_timesteps | 1173504 |
--------------------------------
Eval num_timesteps=1174000, episode_reward=-80187.66 +/- 62713.61
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49252626   |
|    mean velocity x      | -0.378        |
|    mean velocity y      | 0.864         |
|    mean velocity z      | 4.56          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.02e+04     |
| time/                   |               |
|    total_timesteps      | 1174000       |
| train/                  |               |
|    approx_kl            | 0.00011925545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+07      |
|    n_updates            | 5730          |
|    policy_gradient_loss | -0.000509     |
|    std                  | 1.56          |
|    value_loss           | 3.94e+07      |
-------------------------------------------
Eval num_timesteps=1174500, episode_reward=-77361.24 +/- 45845.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4820026 |
|    mean velocity x | -0.349     |
|    mean velocity y | 0.601      |
|    mean velocity z | 4.95       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.74e+04  |
| time/              |            |
|    total_timesteps | 1174500    |
-----------------------------------
Eval num_timesteps=1175000, episode_reward=-58750.73 +/- 36153.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46135643 |
|    mean velocity x | -0.383      |
|    mean velocity y | 0.524       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.88e+04   |
| time/              |             |
|    total_timesteps | 1175000     |
------------------------------------
Eval num_timesteps=1175500, episode_reward=-68019.70 +/- 22763.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48875684 |
|    mean velocity x | -0.0928     |
|    mean velocity y | 1.32        |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.8e+04    |
| time/              |             |
|    total_timesteps | 1175500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 574     |
|    time_elapsed    | 47421   |
|    total_timesteps | 1175552 |
--------------------------------
Eval num_timesteps=1176000, episode_reward=-55234.46 +/- 24881.30
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4296002    |
|    mean velocity x      | -0.23         |
|    mean velocity y      | 1.12          |
|    mean velocity z      | 4.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.52e+04     |
| time/                   |               |
|    total_timesteps      | 1176000       |
| train/                  |               |
|    approx_kl            | 0.00020405205 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.001         |
|    loss                 | 5.04e+07      |
|    n_updates            | 5740          |
|    policy_gradient_loss | -0.00113      |
|    std                  | 1.56          |
|    value_loss           | 1.05e+08      |
-------------------------------------------
Eval num_timesteps=1176500, episode_reward=-67803.40 +/- 41478.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47862726 |
|    mean velocity x | -0.385      |
|    mean velocity y | 1.36        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.78e+04   |
| time/              |             |
|    total_timesteps | 1176500     |
------------------------------------
Eval num_timesteps=1177000, episode_reward=-64983.65 +/- 29775.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44132727 |
|    mean velocity x | 0.0803      |
|    mean velocity y | 1.19        |
|    mean velocity z | 3.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 1177000     |
------------------------------------
Eval num_timesteps=1177500, episode_reward=-98624.23 +/- 27806.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46151277 |
|    mean velocity x | 0.269       |
|    mean velocity y | 1.35        |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.86e+04   |
| time/              |             |
|    total_timesteps | 1177500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 575     |
|    time_elapsed    | 47501   |
|    total_timesteps | 1177600 |
--------------------------------
Eval num_timesteps=1178000, episode_reward=-83410.21 +/- 51687.16
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31959313   |
|    mean velocity x      | 0.339         |
|    mean velocity y      | 0.773         |
|    mean velocity z      | 1.82          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.34e+04     |
| time/                   |               |
|    total_timesteps      | 1178000       |
| train/                  |               |
|    approx_kl            | 3.2431184e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.249         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+07      |
|    n_updates            | 5750          |
|    policy_gradient_loss | -0.000634     |
|    std                  | 1.56          |
|    value_loss           | 4.77e+07      |
-------------------------------------------
Eval num_timesteps=1178500, episode_reward=-86481.71 +/- 50773.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33161926 |
|    mean velocity x | -0.022      |
|    mean velocity y | 0.698       |
|    mean velocity z | 1.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.65e+04   |
| time/              |             |
|    total_timesteps | 1178500     |
------------------------------------
Eval num_timesteps=1179000, episode_reward=-90646.71 +/- 43407.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3224963 |
|    mean velocity x | 0.135      |
|    mean velocity y | 0.727      |
|    mean velocity z | 4.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.06e+04  |
| time/              |            |
|    total_timesteps | 1179000    |
-----------------------------------
Eval num_timesteps=1179500, episode_reward=-72082.82 +/- 40323.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45863688 |
|    mean velocity x | -0.198      |
|    mean velocity y | 0.923       |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.21e+04   |
| time/              |             |
|    total_timesteps | 1179500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 576     |
|    time_elapsed    | 47582   |
|    total_timesteps | 1179648 |
--------------------------------
Eval num_timesteps=1180000, episode_reward=-83196.58 +/- 21710.24
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43410423  |
|    mean velocity x      | -0.872       |
|    mean velocity y      | 0.00943      |
|    mean velocity z      | 3.95         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.32e+04    |
| time/                   |              |
|    total_timesteps      | 1180000      |
| train/                  |              |
|    approx_kl            | 4.357405e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.001        |
|    loss                 | 5.88e+07     |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.000474    |
|    std                  | 1.56         |
|    value_loss           | 6.83e+07     |
------------------------------------------
Eval num_timesteps=1180500, episode_reward=-99470.81 +/- 22078.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43323368 |
|    mean velocity x | -0.28       |
|    mean velocity y | 0.859       |
|    mean velocity z | 4.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 1180500     |
------------------------------------
Eval num_timesteps=1181000, episode_reward=-105899.78 +/- 36579.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48186538 |
|    mean velocity x | -0.211      |
|    mean velocity y | 0.928       |
|    mean velocity z | 4.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1181000     |
------------------------------------
Eval num_timesteps=1181500, episode_reward=-68741.51 +/- 27037.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46705502 |
|    mean velocity x | -0.325      |
|    mean velocity y | 0.861       |
|    mean velocity z | 4.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.87e+04   |
| time/              |             |
|    total_timesteps | 1181500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 577     |
|    time_elapsed    | 47662   |
|    total_timesteps | 1181696 |
--------------------------------
Eval num_timesteps=1182000, episode_reward=-73319.51 +/- 31892.11
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41014108   |
|    mean velocity x      | 0.172         |
|    mean velocity y      | 0.878         |
|    mean velocity z      | 2.1           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.33e+04     |
| time/                   |               |
|    total_timesteps      | 1182000       |
| train/                  |               |
|    approx_kl            | 5.5858254e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.001         |
|    loss                 | 5.9e+07       |
|    n_updates            | 5770          |
|    policy_gradient_loss | -0.000533     |
|    std                  | 1.56          |
|    value_loss           | 9.24e+07      |
-------------------------------------------
Eval num_timesteps=1182500, episode_reward=-100312.61 +/- 32379.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26663575 |
|    mean velocity x | 0.161       |
|    mean velocity y | 0.392       |
|    mean velocity z | 2.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1182500     |
------------------------------------
Eval num_timesteps=1183000, episode_reward=-85748.50 +/- 45271.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3760391 |
|    mean velocity x | -1.14      |
|    mean velocity y | -0.0202    |
|    mean velocity z | 3.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.57e+04  |
| time/              |            |
|    total_timesteps | 1183000    |
-----------------------------------
Eval num_timesteps=1183500, episode_reward=-87886.23 +/- 42332.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26688746 |
|    mean velocity x | -0.86       |
|    mean velocity y | -0.136      |
|    mean velocity z | 2.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.79e+04   |
| time/              |             |
|    total_timesteps | 1183500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 578     |
|    time_elapsed    | 47742   |
|    total_timesteps | 1183744 |
--------------------------------
Eval num_timesteps=1184000, episode_reward=-32509.49 +/- 37832.58
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.28823754  |
|    mean velocity x      | -0.0672      |
|    mean velocity y      | 0.413        |
|    mean velocity z      | 1.15         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.25e+04    |
| time/                   |              |
|    total_timesteps      | 1184000      |
| train/                  |              |
|    approx_kl            | 6.695639e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.32         |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+07     |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.000595    |
|    std                  | 1.56         |
|    value_loss           | 1.85e+07     |
------------------------------------------
Eval num_timesteps=1184500, episode_reward=-62886.28 +/- 40745.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42070806 |
|    mean velocity x | -0.489      |
|    mean velocity y | 0.563       |
|    mean velocity z | 4.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.29e+04   |
| time/              |             |
|    total_timesteps | 1184500     |
------------------------------------
Eval num_timesteps=1185000, episode_reward=-83473.16 +/- 44235.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22095022 |
|    mean velocity x | -0.265      |
|    mean velocity y | -0.222      |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.35e+04   |
| time/              |             |
|    total_timesteps | 1185000     |
------------------------------------
Eval num_timesteps=1185500, episode_reward=-80760.35 +/- 24642.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41679332 |
|    mean velocity x | 0.131       |
|    mean velocity y | 0.843       |
|    mean velocity z | 2.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.08e+04   |
| time/              |             |
|    total_timesteps | 1185500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 579     |
|    time_elapsed    | 47823   |
|    total_timesteps | 1185792 |
--------------------------------
Eval num_timesteps=1186000, episode_reward=-83894.24 +/- 38727.76
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2866709   |
|    mean velocity x      | -0.424       |
|    mean velocity y      | 0.719        |
|    mean velocity z      | 1.54         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.39e+04    |
| time/                   |              |
|    total_timesteps      | 1186000      |
| train/                  |              |
|    approx_kl            | 5.387247e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+07      |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.000503    |
|    std                  | 1.56         |
|    value_loss           | 4.17e+07     |
------------------------------------------
Eval num_timesteps=1186500, episode_reward=-91305.63 +/- 39281.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13465714 |
|    mean velocity x | -0.0913     |
|    mean velocity y | 0.417       |
|    mean velocity z | 0.201       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.13e+04   |
| time/              |             |
|    total_timesteps | 1186500     |
------------------------------------
Eval num_timesteps=1187000, episode_reward=-107285.37 +/- 22134.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3673326 |
|    mean velocity x | 0.093      |
|    mean velocity y | 0.607      |
|    mean velocity z | 3.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 1187000    |
-----------------------------------
Eval num_timesteps=1187500, episode_reward=-78007.42 +/- 49050.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28220826 |
|    mean velocity x | -0.16       |
|    mean velocity y | 0.396       |
|    mean velocity z | 0.869       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.8e+04    |
| time/              |             |
|    total_timesteps | 1187500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 580     |
|    time_elapsed    | 47903   |
|    total_timesteps | 1187840 |
--------------------------------
Eval num_timesteps=1188000, episode_reward=-91375.32 +/- 17632.92
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46698943  |
|    mean velocity x      | 0.446        |
|    mean velocity y      | 1.47         |
|    mean velocity z      | 3.39         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.14e+04    |
| time/                   |              |
|    total_timesteps      | 1188000      |
| train/                  |              |
|    approx_kl            | 0.0003991545 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.001        |
|    loss                 | 9.81e+05     |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.56         |
|    value_loss           | 1.61e+07     |
------------------------------------------
Eval num_timesteps=1188500, episode_reward=-79558.26 +/- 49428.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3193445 |
|    mean velocity x | -1.69      |
|    mean velocity y | -1.09      |
|    mean velocity z | 5.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.96e+04  |
| time/              |            |
|    total_timesteps | 1188500    |
-----------------------------------
Eval num_timesteps=1189000, episode_reward=-109185.89 +/- 54325.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40982214 |
|    mean velocity x | -0.475      |
|    mean velocity y | 0.256       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1189000     |
------------------------------------
Eval num_timesteps=1189500, episode_reward=-63183.24 +/- 23548.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3681262 |
|    mean velocity x | 0.36       |
|    mean velocity y | 0.977      |
|    mean velocity z | 3.49       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.32e+04  |
| time/              |            |
|    total_timesteps | 1189500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 581     |
|    time_elapsed    | 47983   |
|    total_timesteps | 1189888 |
--------------------------------
Eval num_timesteps=1190000, episode_reward=-105363.10 +/- 30206.32
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45028627   |
|    mean velocity x      | 0.029         |
|    mean velocity y      | 1.79          |
|    mean velocity z      | 4.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 1190000       |
| train/                  |               |
|    approx_kl            | 0.00013447332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.285         |
|    learning_rate        | 0.001         |
|    loss                 | 3.68e+07      |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.56          |
|    value_loss           | 6.22e+07      |
-------------------------------------------
Eval num_timesteps=1190500, episode_reward=-81203.49 +/- 42228.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27900445 |
|    mean velocity x | -0.834      |
|    mean velocity y | 0.428       |
|    mean velocity z | 1.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 1190500     |
------------------------------------
Eval num_timesteps=1191000, episode_reward=-66937.26 +/- 45945.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38512948 |
|    mean velocity x | -0.64       |
|    mean velocity y | 0.256       |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.69e+04   |
| time/              |             |
|    total_timesteps | 1191000     |
------------------------------------
Eval num_timesteps=1191500, episode_reward=-29137.38 +/- 38788.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27935058 |
|    mean velocity x | -0.169      |
|    mean velocity y | 0.238       |
|    mean velocity z | 3.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.91e+04   |
| time/              |             |
|    total_timesteps | 1191500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 582     |
|    time_elapsed    | 48064   |
|    total_timesteps | 1191936 |
--------------------------------
Eval num_timesteps=1192000, episode_reward=-93308.46 +/- 28550.98
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5104685    |
|    mean velocity x      | -0.426        |
|    mean velocity y      | 1.26          |
|    mean velocity z      | 4.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.33e+04     |
| time/                   |               |
|    total_timesteps      | 1192000       |
| train/                  |               |
|    approx_kl            | 4.8878166e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.001         |
|    loss                 | 9.68e+06      |
|    n_updates            | 5820          |
|    policy_gradient_loss | -0.000692     |
|    std                  | 1.56          |
|    value_loss           | 6.21e+07      |
-------------------------------------------
Eval num_timesteps=1192500, episode_reward=-49889.95 +/- 25996.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42727876 |
|    mean velocity x | -0.323      |
|    mean velocity y | 0.5         |
|    mean velocity z | 4.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.99e+04   |
| time/              |             |
|    total_timesteps | 1192500     |
------------------------------------
Eval num_timesteps=1193000, episode_reward=-115283.34 +/- 18397.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37951738 |
|    mean velocity x | 0.445       |
|    mean velocity y | 1.34        |
|    mean velocity z | 3.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 1193000     |
------------------------------------
Eval num_timesteps=1193500, episode_reward=-87684.04 +/- 28046.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36678424 |
|    mean velocity x | 0.283       |
|    mean velocity y | 0.992       |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 1193500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 583     |
|    time_elapsed    | 48144   |
|    total_timesteps | 1193984 |
--------------------------------
Eval num_timesteps=1194000, episode_reward=-75400.93 +/- 30099.72
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43162447   |
|    mean velocity x      | -0.359        |
|    mean velocity y      | 0.755         |
|    mean velocity z      | 4.64          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.54e+04     |
| time/                   |               |
|    total_timesteps      | 1194000       |
| train/                  |               |
|    approx_kl            | 3.2684795e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.196         |
|    learning_rate        | 0.001         |
|    loss                 | 7.18e+07      |
|    n_updates            | 5830          |
|    policy_gradient_loss | -0.000545     |
|    std                  | 1.56          |
|    value_loss           | 8.84e+07      |
-------------------------------------------
Eval num_timesteps=1194500, episode_reward=-82079.62 +/- 32366.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4101527 |
|    mean velocity x | -2.54      |
|    mean velocity y | -1.29      |
|    mean velocity z | 6.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.21e+04  |
| time/              |            |
|    total_timesteps | 1194500    |
-----------------------------------
Eval num_timesteps=1195000, episode_reward=-91994.18 +/- 57712.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26463675 |
|    mean velocity x | -0.252      |
|    mean velocity y | 0.417       |
|    mean velocity z | 0.357       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.2e+04    |
| time/              |             |
|    total_timesteps | 1195000     |
------------------------------------
Eval num_timesteps=1195500, episode_reward=-59716.58 +/- 47499.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37426612 |
|    mean velocity x | -0.0817     |
|    mean velocity y | 0.379       |
|    mean velocity z | 1.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.97e+04   |
| time/              |             |
|    total_timesteps | 1195500     |
------------------------------------
Eval num_timesteps=1196000, episode_reward=-90722.39 +/- 31834.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47641796 |
|    mean velocity x | -0.14       |
|    mean velocity y | 1.57        |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.07e+04   |
| time/              |             |
|    total_timesteps | 1196000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 584     |
|    time_elapsed    | 48243   |
|    total_timesteps | 1196032 |
--------------------------------
Eval num_timesteps=1196500, episode_reward=-117160.56 +/- 20518.92
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5111076    |
|    mean velocity x      | -0.821        |
|    mean velocity y      | 0.658         |
|    mean velocity z      | 4.1           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.17e+05     |
| time/                   |               |
|    total_timesteps      | 1196500       |
| train/                  |               |
|    approx_kl            | 6.8924826e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.329         |
|    learning_rate        | 0.001         |
|    loss                 | 2.82e+07      |
|    n_updates            | 5840          |
|    policy_gradient_loss | -0.000585     |
|    std                  | 1.56          |
|    value_loss           | 4.17e+07      |
-------------------------------------------
Eval num_timesteps=1197000, episode_reward=-70726.56 +/- 47015.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34508333 |
|    mean velocity x | -0.185      |
|    mean velocity y | 0.676       |
|    mean velocity z | 0.469       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 1197000     |
------------------------------------
Eval num_timesteps=1197500, episode_reward=-85279.50 +/- 30084.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45101583 |
|    mean velocity x | -0.616      |
|    mean velocity y | 0.715       |
|    mean velocity z | 4.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 1197500     |
------------------------------------
Eval num_timesteps=1198000, episode_reward=-82469.23 +/- 32319.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40781367 |
|    mean velocity x | -0.493      |
|    mean velocity y | 0.165       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.25e+04   |
| time/              |             |
|    total_timesteps | 1198000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 585     |
|    time_elapsed    | 48323   |
|    total_timesteps | 1198080 |
--------------------------------
Eval num_timesteps=1198500, episode_reward=-47997.09 +/- 40839.59
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.22209296   |
|    mean velocity x      | -0.31         |
|    mean velocity y      | 0.257         |
|    mean velocity z      | 0.262         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.8e+04      |
| time/                   |               |
|    total_timesteps      | 1198500       |
| train/                  |               |
|    approx_kl            | 5.6563294e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 3.3e+07       |
|    n_updates            | 5850          |
|    policy_gradient_loss | -0.000846     |
|    std                  | 1.56          |
|    value_loss           | 3.53e+07      |
-------------------------------------------
Eval num_timesteps=1199000, episode_reward=-103352.43 +/- 13022.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15495923 |
|    mean velocity x | -0.816      |
|    mean velocity y | -0.605      |
|    mean velocity z | 2.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1199000     |
------------------------------------
Eval num_timesteps=1199500, episode_reward=-82427.72 +/- 24597.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3522289 |
|    mean velocity x | -0.199     |
|    mean velocity y | 0.683      |
|    mean velocity z | 4.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 1199500    |
-----------------------------------
Eval num_timesteps=1200000, episode_reward=-99016.28 +/- 29423.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18123439 |
|    mean velocity x | 0.0026      |
|    mean velocity y | 0.448       |
|    mean velocity z | 0.458       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.9e+04    |
| time/              |             |
|    total_timesteps | 1200000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 586     |
|    time_elapsed    | 48404   |
|    total_timesteps | 1200128 |
--------------------------------
Eval num_timesteps=1200500, episode_reward=-79982.44 +/- 41859.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42898905   |
|    mean velocity x      | -0.0944       |
|    mean velocity y      | 1.48          |
|    mean velocity z      | 4.7           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8e+04        |
| time/                   |               |
|    total_timesteps      | 1200500       |
| train/                  |               |
|    approx_kl            | 0.00015997037 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.001         |
|    loss                 | 9.54e+06      |
|    n_updates            | 5860          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 1.56          |
|    value_loss           | 6.02e+07      |
-------------------------------------------
Eval num_timesteps=1201000, episode_reward=-87190.99 +/- 24982.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3733459 |
|    mean velocity x | -0.401     |
|    mean velocity y | 0.185      |
|    mean velocity z | 3.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.72e+04  |
| time/              |            |
|    total_timesteps | 1201000    |
-----------------------------------
Eval num_timesteps=1201500, episode_reward=-85325.75 +/- 32620.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46770552 |
|    mean velocity x | -0.092      |
|    mean velocity y | 1.49        |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 1201500     |
------------------------------------
Eval num_timesteps=1202000, episode_reward=-107806.55 +/- 18982.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40344855 |
|    mean velocity x | 0.168       |
|    mean velocity y | 0.776       |
|    mean velocity z | 2.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1202000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 587     |
|    time_elapsed    | 48484   |
|    total_timesteps | 1202176 |
--------------------------------
Eval num_timesteps=1202500, episode_reward=-73475.57 +/- 26366.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46876943   |
|    mean velocity x      | -0.473        |
|    mean velocity y      | 0.783         |
|    mean velocity z      | 4.41          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.35e+04     |
| time/                   |               |
|    total_timesteps      | 1202500       |
| train/                  |               |
|    approx_kl            | 6.0338207e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.001         |
|    loss                 | 8.95e+06      |
|    n_updates            | 5870          |
|    policy_gradient_loss | -0.000375     |
|    std                  | 1.56          |
|    value_loss           | 5.89e+07      |
-------------------------------------------
Eval num_timesteps=1203000, episode_reward=-74865.13 +/- 47672.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37681672 |
|    mean velocity x | -0.178      |
|    mean velocity y | 0.422       |
|    mean velocity z | 1.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.49e+04   |
| time/              |             |
|    total_timesteps | 1203000     |
------------------------------------
Eval num_timesteps=1203500, episode_reward=-66767.42 +/- 35735.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58317983 |
|    mean velocity x | 0.655       |
|    mean velocity y | 2.09        |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 1203500     |
------------------------------------
Eval num_timesteps=1204000, episode_reward=-108619.07 +/- 15756.84
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.535944 |
|    mean velocity x | 0.265     |
|    mean velocity y | 1.74      |
|    mean velocity z | 3.58      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.09e+05 |
| time/              |           |
|    total_timesteps | 1204000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 588     |
|    time_elapsed    | 48565   |
|    total_timesteps | 1204224 |
--------------------------------
Eval num_timesteps=1204500, episode_reward=-95845.02 +/- 26064.74
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.41768408 |
|    mean velocity x      | -0.955      |
|    mean velocity y      | -0.196      |
|    mean velocity z      | 3.16        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.58e+04   |
| time/                   |             |
|    total_timesteps      | 1204500     |
| train/                  |             |
|    approx_kl            | 7.85185e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.001       |
|    loss                 | 1.37e+07    |
|    n_updates            | 5880        |
|    policy_gradient_loss | -0.000617   |
|    std                  | 1.56        |
|    value_loss           | 1.96e+07    |
-----------------------------------------
Eval num_timesteps=1205000, episode_reward=-81525.87 +/- 16488.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6343994 |
|    mean velocity x | -0.59      |
|    mean velocity y | 1.05       |
|    mean velocity z | 4.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 1205000    |
-----------------------------------
Eval num_timesteps=1205500, episode_reward=-64351.10 +/- 41569.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39843097 |
|    mean velocity x | -0.267      |
|    mean velocity y | 0.461       |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.44e+04   |
| time/              |             |
|    total_timesteps | 1205500     |
------------------------------------
Eval num_timesteps=1206000, episode_reward=-80654.34 +/- 41445.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3057784 |
|    mean velocity x | -0.067     |
|    mean velocity y | 0.488      |
|    mean velocity z | 1.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 1206000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 589     |
|    time_elapsed    | 48645   |
|    total_timesteps | 1206272 |
--------------------------------
Eval num_timesteps=1206500, episode_reward=-116942.37 +/- 25188.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39091095   |
|    mean velocity x      | -0.35         |
|    mean velocity y      | 0.704         |
|    mean velocity z      | 1.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.17e+05     |
| time/                   |               |
|    total_timesteps      | 1206500       |
| train/                  |               |
|    approx_kl            | 0.00014173091 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+07      |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.000587     |
|    std                  | 1.56          |
|    value_loss           | 3.44e+07      |
-------------------------------------------
Eval num_timesteps=1207000, episode_reward=-94190.00 +/- 39706.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3271934 |
|    mean velocity x | -0.127     |
|    mean velocity y | 0.572      |
|    mean velocity z | 2.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.42e+04  |
| time/              |            |
|    total_timesteps | 1207000    |
-----------------------------------
Eval num_timesteps=1207500, episode_reward=-21811.09 +/- 31766.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5108168 |
|    mean velocity x | -0.0236    |
|    mean velocity y | 1.63       |
|    mean velocity z | 4.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.18e+04  |
| time/              |            |
|    total_timesteps | 1207500    |
-----------------------------------
Eval num_timesteps=1208000, episode_reward=-82622.72 +/- 47087.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44395813 |
|    mean velocity x | -0.148      |
|    mean velocity y | 0.797       |
|    mean velocity z | 3.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.26e+04   |
| time/              |             |
|    total_timesteps | 1208000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 590     |
|    time_elapsed    | 48725   |
|    total_timesteps | 1208320 |
--------------------------------
Eval num_timesteps=1208500, episode_reward=-77385.86 +/- 23450.25
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47318786   |
|    mean velocity x      | -0.335        |
|    mean velocity y      | 0.749         |
|    mean velocity z      | 4.41          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.74e+04     |
| time/                   |               |
|    total_timesteps      | 1208500       |
| train/                  |               |
|    approx_kl            | 5.9633807e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.001         |
|    loss                 | 4.6e+07       |
|    n_updates            | 5900          |
|    policy_gradient_loss | -0.000562     |
|    std                  | 1.56          |
|    value_loss           | 6.59e+07      |
-------------------------------------------
Eval num_timesteps=1209000, episode_reward=-81371.77 +/- 28041.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47810277 |
|    mean velocity x | -0.282      |
|    mean velocity y | 0.832       |
|    mean velocity z | 4.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.14e+04   |
| time/              |             |
|    total_timesteps | 1209000     |
------------------------------------
Eval num_timesteps=1209500, episode_reward=-71292.22 +/- 30902.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3904334 |
|    mean velocity x | -0.0184    |
|    mean velocity y | 1.03       |
|    mean velocity z | 3.88       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.13e+04  |
| time/              |            |
|    total_timesteps | 1209500    |
-----------------------------------
Eval num_timesteps=1210000, episode_reward=-101433.86 +/- 28615.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32043493 |
|    mean velocity x | -0.199      |
|    mean velocity y | 0.356       |
|    mean velocity z | 4.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1210000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 591     |
|    time_elapsed    | 48806   |
|    total_timesteps | 1210368 |
--------------------------------
Eval num_timesteps=1210500, episode_reward=-69378.37 +/- 47894.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5238978   |
|    mean velocity x      | 0.282        |
|    mean velocity y      | 1.91         |
|    mean velocity z      | 4.04         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.94e+04    |
| time/                   |              |
|    total_timesteps      | 1210500      |
| train/                  |              |
|    approx_kl            | 6.336425e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.001        |
|    loss                 | 5.21e+07     |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.00046     |
|    std                  | 1.56         |
|    value_loss           | 8.51e+07     |
------------------------------------------
Eval num_timesteps=1211000, episode_reward=-84201.38 +/- 27896.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47994262 |
|    mean velocity x | -0.194      |
|    mean velocity y | 1.15        |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 1211000     |
------------------------------------
Eval num_timesteps=1211500, episode_reward=-52896.69 +/- 45792.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5057243 |
|    mean velocity x | -0.285     |
|    mean velocity y | 1.36       |
|    mean velocity z | 4.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.29e+04  |
| time/              |            |
|    total_timesteps | 1211500    |
-----------------------------------
Eval num_timesteps=1212000, episode_reward=-68276.49 +/- 43298.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49644896 |
|    mean velocity x | -0.291      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.83e+04   |
| time/              |             |
|    total_timesteps | 1212000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 592     |
|    time_elapsed    | 48886   |
|    total_timesteps | 1212416 |
--------------------------------
Eval num_timesteps=1212500, episode_reward=-96790.44 +/- 34088.95
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42579046  |
|    mean velocity x      | 0.168        |
|    mean velocity y      | 0.743        |
|    mean velocity z      | 2.7          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.68e+04    |
| time/                   |              |
|    total_timesteps      | 1212500      |
| train/                  |              |
|    approx_kl            | 3.213415e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.175        |
|    learning_rate        | 0.001        |
|    loss                 | 6.81e+07     |
|    n_updates            | 5920         |
|    policy_gradient_loss | -0.000417    |
|    std                  | 1.56         |
|    value_loss           | 7.48e+07     |
------------------------------------------
Eval num_timesteps=1213000, episode_reward=-90311.12 +/- 49115.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42363715 |
|    mean velocity x | -0.979      |
|    mean velocity y | -0.19       |
|    mean velocity z | 5.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 1213000     |
------------------------------------
Eval num_timesteps=1213500, episode_reward=-85136.24 +/- 38181.33
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4167368 |
|    mean velocity x | -0.325     |
|    mean velocity y | 0.795      |
|    mean velocity z | 4.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.51e+04  |
| time/              |            |
|    total_timesteps | 1213500    |
-----------------------------------
Eval num_timesteps=1214000, episode_reward=-84123.56 +/- 47689.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49180678 |
|    mean velocity x | -0.222      |
|    mean velocity y | 0.956       |
|    mean velocity z | 4.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.41e+04   |
| time/              |             |
|    total_timesteps | 1214000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 593     |
|    time_elapsed    | 48966   |
|    total_timesteps | 1214464 |
--------------------------------
Eval num_timesteps=1214500, episode_reward=-80675.19 +/- 37890.10
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4955595    |
|    mean velocity x      | -0.33         |
|    mean velocity y      | 1.35          |
|    mean velocity z      | 2.68          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.07e+04     |
| time/                   |               |
|    total_timesteps      | 1214500       |
| train/                  |               |
|    approx_kl            | 7.3226984e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.168         |
|    learning_rate        | 0.001         |
|    loss                 | 3.52e+07      |
|    n_updates            | 5930          |
|    policy_gradient_loss | -0.000237     |
|    std                  | 1.56          |
|    value_loss           | 8.77e+07      |
-------------------------------------------
Eval num_timesteps=1215000, episode_reward=-100148.85 +/- 28502.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44056538 |
|    mean velocity x | -0.845      |
|    mean velocity y | 0.298       |
|    mean velocity z | 3.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1215000     |
------------------------------------
Eval num_timesteps=1215500, episode_reward=-83268.01 +/- 27819.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29410172 |
|    mean velocity x | -0.232      |
|    mean velocity y | 0.581       |
|    mean velocity z | 0.659       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.33e+04   |
| time/              |             |
|    total_timesteps | 1215500     |
------------------------------------
Eval num_timesteps=1216000, episode_reward=-83158.41 +/- 56327.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5395424 |
|    mean velocity x | 0.496      |
|    mean velocity y | 1.73       |
|    mean velocity z | 3.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.32e+04  |
| time/              |            |
|    total_timesteps | 1216000    |
-----------------------------------
Eval num_timesteps=1216500, episode_reward=-101851.27 +/- 35141.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34325063 |
|    mean velocity x | 0.0715      |
|    mean velocity y | 1.24        |
|    mean velocity z | 4.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1216500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 594     |
|    time_elapsed    | 49066   |
|    total_timesteps | 1216512 |
--------------------------------
Eval num_timesteps=1217000, episode_reward=-86826.87 +/- 38839.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33770454   |
|    mean velocity x      | -0.243        |
|    mean velocity y      | 0.533         |
|    mean velocity z      | 4.11          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.68e+04     |
| time/                   |               |
|    total_timesteps      | 1217000       |
| train/                  |               |
|    approx_kl            | 2.6888389e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+07      |
|    n_updates            | 5940          |
|    policy_gradient_loss | -0.000496     |
|    std                  | 1.56          |
|    value_loss           | 6.46e+07      |
-------------------------------------------
Eval num_timesteps=1217500, episode_reward=-42380.55 +/- 47491.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31934455 |
|    mean velocity x | -0.27       |
|    mean velocity y | 1.01        |
|    mean velocity z | 1.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.24e+04   |
| time/              |             |
|    total_timesteps | 1217500     |
------------------------------------
Eval num_timesteps=1218000, episode_reward=-64274.62 +/- 55705.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.68598866 |
|    mean velocity x | 0.598       |
|    mean velocity y | 2.29        |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.43e+04   |
| time/              |             |
|    total_timesteps | 1218000     |
------------------------------------
Eval num_timesteps=1218500, episode_reward=-73648.29 +/- 56394.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42201126 |
|    mean velocity x | -0.75       |
|    mean velocity y | 0.48        |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1218500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 595     |
|    time_elapsed    | 49146   |
|    total_timesteps | 1218560 |
--------------------------------
Eval num_timesteps=1219000, episode_reward=-103531.19 +/- 22727.78
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4604574   |
|    mean velocity x      | -1.03        |
|    mean velocity y      | 0.164        |
|    mean velocity z      | 3.86         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.04e+05    |
| time/                   |              |
|    total_timesteps      | 1219000      |
| train/                  |              |
|    approx_kl            | 8.065836e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.001        |
|    loss                 | 1.93e+07     |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000208    |
|    std                  | 1.56         |
|    value_loss           | 2.09e+07     |
------------------------------------------
Eval num_timesteps=1219500, episode_reward=-75017.07 +/- 39191.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6069242 |
|    mean velocity x | -0.338     |
|    mean velocity y | 1.59       |
|    mean velocity z | 4.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.5e+04   |
| time/              |            |
|    total_timesteps | 1219500    |
-----------------------------------
Eval num_timesteps=1220000, episode_reward=-58459.02 +/- 42785.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3872843 |
|    mean velocity x | 0.0314     |
|    mean velocity y | 1          |
|    mean velocity z | 1.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.85e+04  |
| time/              |            |
|    total_timesteps | 1220000    |
-----------------------------------
Eval num_timesteps=1220500, episode_reward=-85645.74 +/- 46982.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43070084 |
|    mean velocity x | -0.379      |
|    mean velocity y | 0.325       |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 1220500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 596     |
|    time_elapsed    | 49227   |
|    total_timesteps | 1220608 |
--------------------------------
Eval num_timesteps=1221000, episode_reward=-79463.32 +/- 47370.49
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37875003   |
|    mean velocity x      | -1.16         |
|    mean velocity y      | 0.178         |
|    mean velocity z      | 3.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.95e+04     |
| time/                   |               |
|    total_timesteps      | 1221000       |
| train/                  |               |
|    approx_kl            | 2.6837311e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+05      |
|    n_updates            | 5960          |
|    policy_gradient_loss | -0.000292     |
|    std                  | 1.56          |
|    value_loss           | 5.98e+07      |
-------------------------------------------
Eval num_timesteps=1221500, episode_reward=-86334.43 +/- 31028.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5732122 |
|    mean velocity x | -0.772     |
|    mean velocity y | 0.756      |
|    mean velocity z | 4.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.63e+04  |
| time/              |            |
|    total_timesteps | 1221500    |
-----------------------------------
Eval num_timesteps=1222000, episode_reward=-34069.43 +/- 33122.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48081124 |
|    mean velocity x | -0.221      |
|    mean velocity y | 1.48        |
|    mean velocity z | 4.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.41e+04   |
| time/              |             |
|    total_timesteps | 1222000     |
------------------------------------
Eval num_timesteps=1222500, episode_reward=-82363.64 +/- 29540.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4960717 |
|    mean velocity x | -0.305     |
|    mean velocity y | 0.946      |
|    mean velocity z | 4.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 1222500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 597     |
|    time_elapsed    | 49307   |
|    total_timesteps | 1222656 |
--------------------------------
Eval num_timesteps=1223000, episode_reward=-68839.49 +/- 36377.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46247223   |
|    mean velocity x      | -0.411        |
|    mean velocity y      | 0.495         |
|    mean velocity z      | 4.65          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.88e+04     |
| time/                   |               |
|    total_timesteps      | 1223000       |
| train/                  |               |
|    approx_kl            | 2.0819774e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+07      |
|    n_updates            | 5970          |
|    policy_gradient_loss | -0.000515     |
|    std                  | 1.56          |
|    value_loss           | 9.57e+07      |
-------------------------------------------
Eval num_timesteps=1223500, episode_reward=-103747.84 +/- 41672.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5485377 |
|    mean velocity x | 0.0376     |
|    mean velocity y | 2.06       |
|    mean velocity z | 4.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1223500    |
-----------------------------------
Eval num_timesteps=1224000, episode_reward=-89377.87 +/- 47034.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53337246 |
|    mean velocity x | -0.636      |
|    mean velocity y | 0.46        |
|    mean velocity z | 4.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.94e+04   |
| time/              |             |
|    total_timesteps | 1224000     |
------------------------------------
Eval num_timesteps=1224500, episode_reward=-62261.27 +/- 43279.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30566528 |
|    mean velocity x | -1.55       |
|    mean velocity y | -1.42       |
|    mean velocity z | 6.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+04   |
| time/              |             |
|    total_timesteps | 1224500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 598     |
|    time_elapsed    | 49387   |
|    total_timesteps | 1224704 |
--------------------------------
Eval num_timesteps=1225000, episode_reward=-36517.15 +/- 37802.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6472896    |
|    mean velocity x      | -0.457        |
|    mean velocity y      | 1.38          |
|    mean velocity z      | 5.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.65e+04     |
| time/                   |               |
|    total_timesteps      | 1225000       |
| train/                  |               |
|    approx_kl            | 4.4701766e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.001         |
|    loss                 | 4.16e+07      |
|    n_updates            | 5980          |
|    policy_gradient_loss | -0.000534     |
|    std                  | 1.56          |
|    value_loss           | 5.87e+07      |
-------------------------------------------
Eval num_timesteps=1225500, episode_reward=-54583.33 +/- 27482.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4147691 |
|    mean velocity x | -0.475     |
|    mean velocity y | 0.635      |
|    mean velocity z | 3.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.46e+04  |
| time/              |            |
|    total_timesteps | 1225500    |
-----------------------------------
Eval num_timesteps=1226000, episode_reward=-91293.97 +/- 32317.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5463647 |
|    mean velocity x | -0.115     |
|    mean velocity y | 1.65       |
|    mean velocity z | 4.06       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.13e+04  |
| time/              |            |
|    total_timesteps | 1226000    |
-----------------------------------
Eval num_timesteps=1226500, episode_reward=-46052.94 +/- 32185.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32729322 |
|    mean velocity x | 0.316       |
|    mean velocity y | 0.462       |
|    mean velocity z | 1.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.61e+04   |
| time/              |             |
|    total_timesteps | 1226500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 599     |
|    time_elapsed    | 49468   |
|    total_timesteps | 1226752 |
--------------------------------
Eval num_timesteps=1227000, episode_reward=-76140.70 +/- 40511.49
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37224987   |
|    mean velocity x      | -0.172        |
|    mean velocity y      | 0.654         |
|    mean velocity z      | 2.22          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.61e+04     |
| time/                   |               |
|    total_timesteps      | 1227000       |
| train/                  |               |
|    approx_kl            | 2.5513698e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.001         |
|    loss                 | 1.52e+06      |
|    n_updates            | 5990          |
|    policy_gradient_loss | -0.000443     |
|    std                  | 1.56          |
|    value_loss           | 2.2e+07       |
-------------------------------------------
Eval num_timesteps=1227500, episode_reward=-72816.72 +/- 43790.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19946969 |
|    mean velocity x | -0.713      |
|    mean velocity y | -0.861      |
|    mean velocity z | 3.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 1227500     |
------------------------------------
Eval num_timesteps=1228000, episode_reward=-124250.21 +/- 10650.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51515007 |
|    mean velocity x | -0.196      |
|    mean velocity y | 1.4         |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.24e+05   |
| time/              |             |
|    total_timesteps | 1228000     |
------------------------------------
Eval num_timesteps=1228500, episode_reward=-77459.20 +/- 42599.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5220327 |
|    mean velocity x | -0.764     |
|    mean velocity y | -0.0157    |
|    mean velocity z | 6.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.75e+04  |
| time/              |            |
|    total_timesteps | 1228500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 600     |
|    time_elapsed    | 49548   |
|    total_timesteps | 1228800 |
--------------------------------
Eval num_timesteps=1229000, episode_reward=-72963.75 +/- 40929.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34308004   |
|    mean velocity x      | -0.78         |
|    mean velocity y      | 0.551         |
|    mean velocity z      | 3.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.3e+04      |
| time/                   |               |
|    total_timesteps      | 1229000       |
| train/                  |               |
|    approx_kl            | 3.0995026e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.298         |
|    learning_rate        | 0.001         |
|    loss                 | 5.49e+06      |
|    n_updates            | 6000          |
|    policy_gradient_loss | -0.000444     |
|    std                  | 1.56          |
|    value_loss           | 5.08e+07      |
-------------------------------------------
Eval num_timesteps=1229500, episode_reward=-122404.88 +/- 18517.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25280723 |
|    mean velocity x | -0.523      |
|    mean velocity y | 0.427       |
|    mean velocity z | 1.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.22e+05   |
| time/              |             |
|    total_timesteps | 1229500     |
------------------------------------
Eval num_timesteps=1230000, episode_reward=-78343.41 +/- 42646.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45438844 |
|    mean velocity x | -0.25       |
|    mean velocity y | 0.822       |
|    mean velocity z | 4.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.83e+04   |
| time/              |             |
|    total_timesteps | 1230000     |
------------------------------------
Eval num_timesteps=1230500, episode_reward=-71390.04 +/- 51741.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5029787 |
|    mean velocity x | 0.377      |
|    mean velocity y | 1.93       |
|    mean velocity z | 3.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.14e+04  |
| time/              |            |
|    total_timesteps | 1230500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 601     |
|    time_elapsed    | 49629   |
|    total_timesteps | 1230848 |
--------------------------------
Eval num_timesteps=1231000, episode_reward=-90826.20 +/- 29231.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32592988   |
|    mean velocity x      | -0.832        |
|    mean velocity y      | 0.572         |
|    mean velocity z      | 2.43          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.08e+04     |
| time/                   |               |
|    total_timesteps      | 1231000       |
| train/                  |               |
|    approx_kl            | 1.9899191e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 7.54e+06      |
|    n_updates            | 6010          |
|    policy_gradient_loss | -0.000309     |
|    std                  | 1.56          |
|    value_loss           | 4.3e+07       |
-------------------------------------------
Eval num_timesteps=1231500, episode_reward=-78736.56 +/- 46734.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47972313 |
|    mean velocity x | 0.0312      |
|    mean velocity y | 1.6         |
|    mean velocity z | 4.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.87e+04   |
| time/              |             |
|    total_timesteps | 1231500     |
------------------------------------
Eval num_timesteps=1232000, episode_reward=-95248.04 +/- 19923.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35120136 |
|    mean velocity x | -0.157      |
|    mean velocity y | 0.738       |
|    mean velocity z | 4.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.52e+04   |
| time/              |             |
|    total_timesteps | 1232000     |
------------------------------------
Eval num_timesteps=1232500, episode_reward=-90256.68 +/- 19997.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45947713 |
|    mean velocity x | -0.258      |
|    mean velocity y | 0.837       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 1232500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 602     |
|    time_elapsed    | 49709   |
|    total_timesteps | 1232896 |
--------------------------------
Eval num_timesteps=1233000, episode_reward=-112359.46 +/- 13844.19
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4472544   |
|    mean velocity x      | -0.253       |
|    mean velocity y      | 0.598        |
|    mean velocity z      | 4.13         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.12e+05    |
| time/                   |              |
|    total_timesteps      | 1233000      |
| train/                  |              |
|    approx_kl            | 2.466768e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.001        |
|    loss                 | 8.35e+07     |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.000402    |
|    std                  | 1.56         |
|    value_loss           | 1.08e+08     |
------------------------------------------
Eval num_timesteps=1233500, episode_reward=-71542.22 +/- 33137.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5067638 |
|    mean velocity x | -0.052     |
|    mean velocity y | 1.7        |
|    mean velocity z | 4.39       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.15e+04  |
| time/              |            |
|    total_timesteps | 1233500    |
-----------------------------------
Eval num_timesteps=1234000, episode_reward=-81967.58 +/- 43255.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18876919 |
|    mean velocity x | -0.23       |
|    mean velocity y | 0.591       |
|    mean velocity z | 0.451       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 1234000     |
------------------------------------
Eval num_timesteps=1234500, episode_reward=-72099.13 +/- 48359.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36333933 |
|    mean velocity x | -0.106      |
|    mean velocity y | 0.695       |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.21e+04   |
| time/              |             |
|    total_timesteps | 1234500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 603     |
|    time_elapsed    | 49789   |
|    total_timesteps | 1234944 |
--------------------------------
Eval num_timesteps=1235000, episode_reward=-97360.41 +/- 24633.47
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30948132   |
|    mean velocity x      | -0.111        |
|    mean velocity y      | 0.284         |
|    mean velocity z      | 3.87          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.74e+04     |
| time/                   |               |
|    total_timesteps      | 1235000       |
| train/                  |               |
|    approx_kl            | 1.6082719e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.001         |
|    loss                 | 4.63e+07      |
|    n_updates            | 6030          |
|    policy_gradient_loss | -0.000377     |
|    std                  | 1.56          |
|    value_loss           | 5.6e+07       |
-------------------------------------------
Eval num_timesteps=1235500, episode_reward=-88837.44 +/- 28653.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40682292 |
|    mean velocity x | 0.0816      |
|    mean velocity y | 1.48        |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.88e+04   |
| time/              |             |
|    total_timesteps | 1235500     |
------------------------------------
Eval num_timesteps=1236000, episode_reward=-58424.37 +/- 23152.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47362354 |
|    mean velocity x | -0.104      |
|    mean velocity y | 1.11        |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.84e+04   |
| time/              |             |
|    total_timesteps | 1236000     |
------------------------------------
Eval num_timesteps=1236500, episode_reward=-114670.48 +/- 15832.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46799153 |
|    mean velocity x | -0.309      |
|    mean velocity y | 0.694       |
|    mean velocity z | 4.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 1236500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 604     |
|    time_elapsed    | 49870   |
|    total_timesteps | 1236992 |
--------------------------------
Eval num_timesteps=1237000, episode_reward=-63125.76 +/- 51036.97
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38224936   |
|    mean velocity x      | 0.157         |
|    mean velocity y      | 1.29          |
|    mean velocity z      | 3.62          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.31e+04     |
| time/                   |               |
|    total_timesteps      | 1237000       |
| train/                  |               |
|    approx_kl            | 2.0932086e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.179         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+07      |
|    n_updates            | 6040          |
|    policy_gradient_loss | -0.000344     |
|    std                  | 1.56          |
|    value_loss           | 8.2e+07       |
-------------------------------------------
Eval num_timesteps=1237500, episode_reward=-76736.70 +/- 31745.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27284482 |
|    mean velocity x | -0.225      |
|    mean velocity y | 0.612       |
|    mean velocity z | 0.557       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.67e+04   |
| time/              |             |
|    total_timesteps | 1237500     |
------------------------------------
Eval num_timesteps=1238000, episode_reward=-107415.74 +/- 34433.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38332638 |
|    mean velocity x | -0.315      |
|    mean velocity y | 0.43        |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1238000     |
------------------------------------
Eval num_timesteps=1238500, episode_reward=-103684.25 +/- 25160.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33305165 |
|    mean velocity x | -0.639      |
|    mean velocity y | 0.245       |
|    mean velocity z | 3.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1238500     |
------------------------------------
Eval num_timesteps=1239000, episode_reward=-67512.24 +/- 46482.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28324017 |
|    mean velocity x | -0.169      |
|    mean velocity y | 0.56        |
|    mean velocity z | 0.397       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.75e+04   |
| time/              |             |
|    total_timesteps | 1239000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 605     |
|    time_elapsed    | 49969   |
|    total_timesteps | 1239040 |
--------------------------------
Eval num_timesteps=1239500, episode_reward=-80701.36 +/- 41074.13
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.45919073  |
|    mean velocity x      | -0.316       |
|    mean velocity y      | 0.6          |
|    mean velocity z      | 3.26         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.07e+04    |
| time/                   |              |
|    total_timesteps      | 1239500      |
| train/                  |              |
|    approx_kl            | 3.910315e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.188        |
|    learning_rate        | 0.001        |
|    loss                 | 4.43e+07     |
|    n_updates            | 6050         |
|    policy_gradient_loss | -0.000861    |
|    std                  | 1.56         |
|    value_loss           | 4.4e+07      |
------------------------------------------
Eval num_timesteps=1240000, episode_reward=-51780.01 +/- 37533.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3930663 |
|    mean velocity x | -0.256     |
|    mean velocity y | 0.582      |
|    mean velocity z | 3.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.18e+04  |
| time/              |            |
|    total_timesteps | 1240000    |
-----------------------------------
Eval num_timesteps=1240500, episode_reward=-95116.03 +/- 22977.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4491159 |
|    mean velocity x | 0.411      |
|    mean velocity y | 1.48       |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 1240500    |
-----------------------------------
Eval num_timesteps=1241000, episode_reward=-82929.61 +/- 43455.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5022521 |
|    mean velocity x | 0.0516     |
|    mean velocity y | 1.47       |
|    mean velocity z | 1.4        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.29e+04  |
| time/              |            |
|    total_timesteps | 1241000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 606     |
|    time_elapsed    | 50049   |
|    total_timesteps | 1241088 |
--------------------------------
Eval num_timesteps=1241500, episode_reward=-90466.18 +/- 29493.37
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39215502   |
|    mean velocity x      | -0.122        |
|    mean velocity y      | 0.632         |
|    mean velocity z      | 4.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.05e+04     |
| time/                   |               |
|    total_timesteps      | 1241500       |
| train/                  |               |
|    approx_kl            | 7.2380615e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.22          |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+07      |
|    n_updates            | 6060          |
|    policy_gradient_loss | -0.000735     |
|    std                  | 1.56          |
|    value_loss           | 4.08e+07      |
-------------------------------------------
Eval num_timesteps=1242000, episode_reward=-93555.44 +/- 14767.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49789068 |
|    mean velocity x | -0.789      |
|    mean velocity y | -0.118      |
|    mean velocity z | 4.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 1242000     |
------------------------------------
Eval num_timesteps=1242500, episode_reward=-86253.89 +/- 33832.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23406024 |
|    mean velocity x | 0.15        |
|    mean velocity y | 0.0564      |
|    mean velocity z | 1.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.63e+04   |
| time/              |             |
|    total_timesteps | 1242500     |
------------------------------------
Eval num_timesteps=1243000, episode_reward=-87794.91 +/- 44439.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5164789 |
|    mean velocity x | -0.406     |
|    mean velocity y | 0.95       |
|    mean velocity z | 4.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.78e+04  |
| time/              |            |
|    total_timesteps | 1243000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 607     |
|    time_elapsed    | 50130   |
|    total_timesteps | 1243136 |
--------------------------------
Eval num_timesteps=1243500, episode_reward=-108997.22 +/- 34788.39
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.182673     |
|    mean velocity x      | -0.879        |
|    mean velocity y      | -0.847        |
|    mean velocity z      | 3.95          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.09e+05     |
| time/                   |               |
|    total_timesteps      | 1243500       |
| train/                  |               |
|    approx_kl            | 4.9645518e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.001         |
|    loss                 | 4.31e+07      |
|    n_updates            | 6070          |
|    policy_gradient_loss | -0.000605     |
|    std                  | 1.56          |
|    value_loss           | 5.45e+07      |
-------------------------------------------
Eval num_timesteps=1244000, episode_reward=-84954.61 +/- 50767.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36911058 |
|    mean velocity x | 0.216       |
|    mean velocity y | 1.14        |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 1244000     |
------------------------------------
Eval num_timesteps=1244500, episode_reward=-73985.82 +/- 52953.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26605308 |
|    mean velocity x | -1.3        |
|    mean velocity y | -0.134      |
|    mean velocity z | 3.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.4e+04    |
| time/              |             |
|    total_timesteps | 1244500     |
------------------------------------
Eval num_timesteps=1245000, episode_reward=-100592.75 +/- 27683.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2383613 |
|    mean velocity x | -0.188     |
|    mean velocity y | -0.112     |
|    mean velocity z | 3.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 1245000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 608     |
|    time_elapsed    | 50210   |
|    total_timesteps | 1245184 |
--------------------------------
Eval num_timesteps=1245500, episode_reward=-104468.34 +/- 20311.81
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3679236   |
|    mean velocity x      | -0.943       |
|    mean velocity y      | -0.697       |
|    mean velocity z      | 4.16         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.04e+05    |
| time/                   |              |
|    total_timesteps      | 1245500      |
| train/                  |              |
|    approx_kl            | 8.303876e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.001        |
|    loss                 | 3.28e+07     |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.000339    |
|    std                  | 1.56         |
|    value_loss           | 4.33e+07     |
------------------------------------------
Eval num_timesteps=1246000, episode_reward=-92731.75 +/- 26313.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38284305 |
|    mean velocity x | -1.06       |
|    mean velocity y | 0.353       |
|    mean velocity z | 4.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 1246000     |
------------------------------------
Eval num_timesteps=1246500, episode_reward=-49042.39 +/- 33941.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7250836 |
|    mean velocity x | 0.463      |
|    mean velocity y | 1.74       |
|    mean velocity z | 7.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.9e+04   |
| time/              |            |
|    total_timesteps | 1246500    |
-----------------------------------
Eval num_timesteps=1247000, episode_reward=-69710.00 +/- 23780.65
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.520203 |
|    mean velocity x | -0.816    |
|    mean velocity y | 0.682     |
|    mean velocity z | 4.89      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.97e+04 |
| time/              |           |
|    total_timesteps | 1247000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 609     |
|    time_elapsed    | 50290   |
|    total_timesteps | 1247232 |
--------------------------------
Eval num_timesteps=1247500, episode_reward=-83794.29 +/- 15677.66
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46845943   |
|    mean velocity x      | -0.444        |
|    mean velocity y      | 0.639         |
|    mean velocity z      | 4.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.38e+04     |
| time/                   |               |
|    total_timesteps      | 1247500       |
| train/                  |               |
|    approx_kl            | 1.1908414e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 1.7e+07       |
|    n_updates            | 6090          |
|    policy_gradient_loss | -0.000235     |
|    std                  | 1.56          |
|    value_loss           | 7.75e+07      |
-------------------------------------------
Eval num_timesteps=1248000, episode_reward=-97719.68 +/- 20534.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42442182 |
|    mean velocity x | -0.208      |
|    mean velocity y | 0.664       |
|    mean velocity z | 4.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.77e+04   |
| time/              |             |
|    total_timesteps | 1248000     |
------------------------------------
Eval num_timesteps=1248500, episode_reward=-80189.02 +/- 39169.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3743476 |
|    mean velocity x | -0.571     |
|    mean velocity y | -0.131     |
|    mean velocity z | 4.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.02e+04  |
| time/              |            |
|    total_timesteps | 1248500    |
-----------------------------------
Eval num_timesteps=1249000, episode_reward=-73754.97 +/- 31097.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44031066 |
|    mean velocity x | -1.17       |
|    mean velocity y | 0.168       |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.38e+04   |
| time/              |             |
|    total_timesteps | 1249000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 610     |
|    time_elapsed    | 50371   |
|    total_timesteps | 1249280 |
--------------------------------
Eval num_timesteps=1249500, episode_reward=-93042.66 +/- 12888.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4329249    |
|    mean velocity x      | 0.07          |
|    mean velocity y      | 1.69          |
|    mean velocity z      | 4.28          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.3e+04      |
| time/                   |               |
|    total_timesteps      | 1249500       |
| train/                  |               |
|    approx_kl            | 2.8386537e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.219         |
|    learning_rate        | 0.001         |
|    loss                 | 5.55e+07      |
|    n_updates            | 6100          |
|    policy_gradient_loss | -0.000379     |
|    std                  | 1.56          |
|    value_loss           | 7.87e+07      |
-------------------------------------------
Eval num_timesteps=1250000, episode_reward=-68640.59 +/- 50370.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4392439 |
|    mean velocity x | -0.126     |
|    mean velocity y | 0.917      |
|    mean velocity z | 3.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.86e+04  |
| time/              |            |
|    total_timesteps | 1250000    |
-----------------------------------
Eval num_timesteps=1250500, episode_reward=-103610.92 +/- 26653.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3206582 |
|    mean velocity x | -0.463     |
|    mean velocity y | -0.102     |
|    mean velocity z | 3.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1250500    |
-----------------------------------
Eval num_timesteps=1251000, episode_reward=-34308.77 +/- 22819.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4576104 |
|    mean velocity x | -0.569     |
|    mean velocity y | 0.342      |
|    mean velocity z | 3.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.43e+04  |
| time/              |            |
|    total_timesteps | 1251000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 611     |
|    time_elapsed    | 50451   |
|    total_timesteps | 1251328 |
--------------------------------
Eval num_timesteps=1251500, episode_reward=-66014.70 +/- 33522.99
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4587547    |
|    mean velocity x      | -0.3          |
|    mean velocity y      | 0.758         |
|    mean velocity z      | 4.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.6e+04      |
| time/                   |               |
|    total_timesteps      | 1251500       |
| train/                  |               |
|    approx_kl            | 1.4535035e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.175         |
|    learning_rate        | 0.001         |
|    loss                 | 4.22e+07      |
|    n_updates            | 6110          |
|    policy_gradient_loss | -0.000392     |
|    std                  | 1.56          |
|    value_loss           | 7.97e+07      |
-------------------------------------------
Eval num_timesteps=1252000, episode_reward=-62727.57 +/- 37081.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5660758 |
|    mean velocity x | -0.331     |
|    mean velocity y | 1.38       |
|    mean velocity z | 4.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.27e+04  |
| time/              |            |
|    total_timesteps | 1252000    |
-----------------------------------
Eval num_timesteps=1252500, episode_reward=-96583.52 +/- 22473.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3075008 |
|    mean velocity x | -0.642     |
|    mean velocity y | 0.486      |
|    mean velocity z | 2.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.66e+04  |
| time/              |            |
|    total_timesteps | 1252500    |
-----------------------------------
Eval num_timesteps=1253000, episode_reward=-86532.23 +/- 41173.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40092224 |
|    mean velocity x | -0.135      |
|    mean velocity y | 0.7         |
|    mean velocity z | 3.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.65e+04   |
| time/              |             |
|    total_timesteps | 1253000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 612     |
|    time_elapsed    | 50532   |
|    total_timesteps | 1253376 |
--------------------------------
Eval num_timesteps=1253500, episode_reward=-93943.14 +/- 18660.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38849413  |
|    mean velocity x      | 0.405        |
|    mean velocity y      | 1.22         |
|    mean velocity z      | 3.65         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.39e+04    |
| time/                   |              |
|    total_timesteps      | 1253500      |
| train/                  |              |
|    approx_kl            | 9.273092e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.001        |
|    loss                 | 4.4e+07      |
|    n_updates            | 6120         |
|    policy_gradient_loss | -0.000235    |
|    std                  | 1.56         |
|    value_loss           | 5.23e+07     |
------------------------------------------
Eval num_timesteps=1254000, episode_reward=-81989.05 +/- 38691.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32538128 |
|    mean velocity x | -0.00102    |
|    mean velocity y | 0.759       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 1254000     |
------------------------------------
Eval num_timesteps=1254500, episode_reward=-44056.94 +/- 52767.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24641901 |
|    mean velocity x | -0.72       |
|    mean velocity y | -0.525      |
|    mean velocity z | 3.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.41e+04   |
| time/              |             |
|    total_timesteps | 1254500     |
------------------------------------
Eval num_timesteps=1255000, episode_reward=-90077.15 +/- 22322.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7458111 |
|    mean velocity x | 0.591      |
|    mean velocity y | 3          |
|    mean velocity z | 5.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.01e+04  |
| time/              |            |
|    total_timesteps | 1255000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 613     |
|    time_elapsed    | 50612   |
|    total_timesteps | 1255424 |
--------------------------------
Eval num_timesteps=1255500, episode_reward=-67584.97 +/- 43894.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49707413  |
|    mean velocity x      | -0.385       |
|    mean velocity y      | 0.805        |
|    mean velocity z      | 5.2          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.76e+04    |
| time/                   |              |
|    total_timesteps      | 1255500      |
| train/                  |              |
|    approx_kl            | 9.520736e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.001        |
|    loss                 | 8.15e+07     |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.000299    |
|    std                  | 1.56         |
|    value_loss           | 6.64e+07     |
------------------------------------------
Eval num_timesteps=1256000, episode_reward=-46916.87 +/- 35754.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43210143 |
|    mean velocity x | 0.0039      |
|    mean velocity y | 1.14        |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.69e+04   |
| time/              |             |
|    total_timesteps | 1256000     |
------------------------------------
Eval num_timesteps=1256500, episode_reward=-27131.68 +/- 39731.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37191987 |
|    mean velocity x | -0.761      |
|    mean velocity y | 0.117       |
|    mean velocity z | 2.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.71e+04   |
| time/              |             |
|    total_timesteps | 1256500     |
------------------------------------
Eval num_timesteps=1257000, episode_reward=-74367.69 +/- 36833.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3294978 |
|    mean velocity x | -0.0738    |
|    mean velocity y | 0.773      |
|    mean velocity z | 2.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.44e+04  |
| time/              |            |
|    total_timesteps | 1257000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 614     |
|    time_elapsed    | 50692   |
|    total_timesteps | 1257472 |
--------------------------------
Eval num_timesteps=1257500, episode_reward=-71896.28 +/- 31769.51
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44208363   |
|    mean velocity x      | -0.564        |
|    mean velocity y      | 0.154         |
|    mean velocity z      | 3.94          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.19e+04     |
| time/                   |               |
|    total_timesteps      | 1257500       |
| train/                  |               |
|    approx_kl            | 1.1110387e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.001         |
|    loss                 | 3.11e+06      |
|    n_updates            | 6140          |
|    policy_gradient_loss | -0.000282     |
|    std                  | 1.56          |
|    value_loss           | 4.15e+07      |
-------------------------------------------
Eval num_timesteps=1258000, episode_reward=-75705.83 +/- 38201.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4390827 |
|    mean velocity x | -1.12      |
|    mean velocity y | -0.571     |
|    mean velocity z | 6.18       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.57e+04  |
| time/              |            |
|    total_timesteps | 1258000    |
-----------------------------------
Eval num_timesteps=1258500, episode_reward=-95780.08 +/- 43569.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41573966 |
|    mean velocity x | -0.339      |
|    mean velocity y | 0.753       |
|    mean velocity z | 1.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.58e+04   |
| time/              |             |
|    total_timesteps | 1258500     |
------------------------------------
Eval num_timesteps=1259000, episode_reward=-83715.14 +/- 45849.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41783953 |
|    mean velocity x | 0.183       |
|    mean velocity y | 0.842       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 1259000     |
------------------------------------
Eval num_timesteps=1259500, episode_reward=-96808.25 +/- 47675.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4140077 |
|    mean velocity x | 0.39       |
|    mean velocity y | 0.937      |
|    mean velocity z | 2.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.68e+04  |
| time/              |            |
|    total_timesteps | 1259500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 615     |
|    time_elapsed    | 50791   |
|    total_timesteps | 1259520 |
--------------------------------
Eval num_timesteps=1260000, episode_reward=-114472.31 +/- 36162.82
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26424184  |
|    mean velocity x      | -0.104       |
|    mean velocity y      | 0.551        |
|    mean velocity z      | 0.576        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.14e+05    |
| time/                   |              |
|    total_timesteps      | 1260000      |
| train/                  |              |
|    approx_kl            | 9.648269e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+07     |
|    n_updates            | 6150         |
|    policy_gradient_loss | -0.000241    |
|    std                  | 1.56         |
|    value_loss           | 3.54e+07     |
------------------------------------------
Eval num_timesteps=1260500, episode_reward=-87643.31 +/- 33437.39
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.374736 |
|    mean velocity x | -0.175    |
|    mean velocity y | 0.705     |
|    mean velocity z | 1.32      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.76e+04 |
| time/              |           |
|    total_timesteps | 1260500   |
----------------------------------
Eval num_timesteps=1261000, episode_reward=-94462.24 +/- 38180.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43309444 |
|    mean velocity x | -0.293      |
|    mean velocity y | 0.939       |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.45e+04   |
| time/              |             |
|    total_timesteps | 1261000     |
------------------------------------
Eval num_timesteps=1261500, episode_reward=-83820.36 +/- 16247.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40444005 |
|    mean velocity x | -0.942      |
|    mean velocity y | -0.447      |
|    mean velocity z | 4.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.38e+04   |
| time/              |             |
|    total_timesteps | 1261500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 616     |
|    time_elapsed    | 50872   |
|    total_timesteps | 1261568 |
--------------------------------
Eval num_timesteps=1262000, episode_reward=-101389.04 +/- 16538.84
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6571061   |
|    mean velocity x      | -0.0216      |
|    mean velocity y      | 2.24         |
|    mean velocity z      | 5.38         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.01e+05    |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 1.284515e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.001        |
|    loss                 | 4.33e+06     |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.000356    |
|    std                  | 1.56         |
|    value_loss           | 3.42e+07     |
------------------------------------------
Eval num_timesteps=1262500, episode_reward=-56683.84 +/- 42502.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41141027 |
|    mean velocity x | -0.695      |
|    mean velocity y | 0.258       |
|    mean velocity z | 3.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.67e+04   |
| time/              |             |
|    total_timesteps | 1262500     |
------------------------------------
Eval num_timesteps=1263000, episode_reward=-62600.18 +/- 52167.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28712454 |
|    mean velocity x | -0.129      |
|    mean velocity y | 0.574       |
|    mean velocity z | 2.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.26e+04   |
| time/              |             |
|    total_timesteps | 1263000     |
------------------------------------
Eval num_timesteps=1263500, episode_reward=-59390.29 +/- 48875.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35729298 |
|    mean velocity x | -0.0897     |
|    mean velocity y | 0.576       |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.94e+04   |
| time/              |             |
|    total_timesteps | 1263500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 617     |
|    time_elapsed    | 50952   |
|    total_timesteps | 1263616 |
--------------------------------
Eval num_timesteps=1264000, episode_reward=-100406.15 +/- 24455.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49308258  |
|    mean velocity x      | -0.39        |
|    mean velocity y      | 1.07         |
|    mean velocity z      | 4.36         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1e+05       |
| time/                   |              |
|    total_timesteps      | 1264000      |
| train/                  |              |
|    approx_kl            | 4.000374e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.001        |
|    loss                 | 3.93e+07     |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.000413    |
|    std                  | 1.56         |
|    value_loss           | 5.05e+07     |
------------------------------------------
Eval num_timesteps=1264500, episode_reward=-58391.68 +/- 29845.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38311347 |
|    mean velocity x | -0.000585   |
|    mean velocity y | 0.788       |
|    mean velocity z | 0.854       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.84e+04   |
| time/              |             |
|    total_timesteps | 1264500     |
------------------------------------
Eval num_timesteps=1265000, episode_reward=-72402.71 +/- 38868.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38144323 |
|    mean velocity x | -0.48       |
|    mean velocity y | 0.313       |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.24e+04   |
| time/              |             |
|    total_timesteps | 1265000     |
------------------------------------
Eval num_timesteps=1265500, episode_reward=-78147.62 +/- 16375.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3780707 |
|    mean velocity x | 0.00157    |
|    mean velocity y | 0.637      |
|    mean velocity z | 3.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 1265500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 618     |
|    time_elapsed    | 51032   |
|    total_timesteps | 1265664 |
--------------------------------
Eval num_timesteps=1266000, episode_reward=-60815.38 +/- 41199.45
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.25176898  |
|    mean velocity x      | 0.0723       |
|    mean velocity y      | 0.328        |
|    mean velocity z      | 0.549        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.08e+04    |
| time/                   |              |
|    total_timesteps      | 1266000      |
| train/                  |              |
|    approx_kl            | 4.899336e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.217        |
|    learning_rate        | 0.001        |
|    loss                 | 2.84e+07     |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.00053     |
|    std                  | 1.56         |
|    value_loss           | 3.09e+07     |
------------------------------------------
Eval num_timesteps=1266500, episode_reward=-111729.62 +/- 9180.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5047173 |
|    mean velocity x | -0.313     |
|    mean velocity y | 0.955      |
|    mean velocity z | 4.4        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 1266500    |
-----------------------------------
Eval num_timesteps=1267000, episode_reward=-92279.79 +/- 43040.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48178717 |
|    mean velocity x | -0.38       |
|    mean velocity y | 0.89        |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 1267000     |
------------------------------------
Eval num_timesteps=1267500, episode_reward=-77037.85 +/- 45169.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36513522 |
|    mean velocity x | -0.388      |
|    mean velocity y | 0.0517      |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.7e+04    |
| time/              |             |
|    total_timesteps | 1267500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 619     |
|    time_elapsed    | 51113   |
|    total_timesteps | 1267712 |
--------------------------------
Eval num_timesteps=1268000, episode_reward=-28699.34 +/- 33632.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34286633   |
|    mean velocity x      | -0.146        |
|    mean velocity y      | 0.601         |
|    mean velocity z      | 4.72          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -2.87e+04     |
| time/                   |               |
|    total_timesteps      | 1268000       |
| train/                  |               |
|    approx_kl            | 5.3769705e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.166         |
|    learning_rate        | 0.001         |
|    loss                 | 7.07e+07      |
|    n_updates            | 6190          |
|    policy_gradient_loss | -0.000549     |
|    std                  | 1.56          |
|    value_loss           | 1.16e+08      |
-------------------------------------------
Eval num_timesteps=1268500, episode_reward=-91543.07 +/- 40497.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5064307 |
|    mean velocity x | -0.115     |
|    mean velocity y | 1.76       |
|    mean velocity z | 4.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.15e+04  |
| time/              |            |
|    total_timesteps | 1268500    |
-----------------------------------
Eval num_timesteps=1269000, episode_reward=-61748.05 +/- 32340.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45038155 |
|    mean velocity x | -0.0388     |
|    mean velocity y | 1.63        |
|    mean velocity z | 4.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+04   |
| time/              |             |
|    total_timesteps | 1269000     |
------------------------------------
Eval num_timesteps=1269500, episode_reward=-91736.68 +/- 49133.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38283277 |
|    mean velocity x | -1.34       |
|    mean velocity y | -0.659      |
|    mean velocity z | 4.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.17e+04   |
| time/              |             |
|    total_timesteps | 1269500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 620     |
|    time_elapsed    | 51193   |
|    total_timesteps | 1269760 |
--------------------------------
Eval num_timesteps=1270000, episode_reward=-102533.05 +/- 36516.37
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5162653   |
|    mean velocity x      | -0.271       |
|    mean velocity y      | 1.14         |
|    mean velocity z      | 4.53         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.03e+05    |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 5.435606e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+07     |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.000104    |
|    std                  | 1.56         |
|    value_loss           | 6.57e+07     |
------------------------------------------
Eval num_timesteps=1270500, episode_reward=-94888.14 +/- 18755.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33194345 |
|    mean velocity x | -0.444      |
|    mean velocity y | 0.0635      |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 1270500     |
------------------------------------
Eval num_timesteps=1271000, episode_reward=-75984.05 +/- 45995.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38845772 |
|    mean velocity x | -0.0966     |
|    mean velocity y | 0.647       |
|    mean velocity z | 3.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.6e+04    |
| time/              |             |
|    total_timesteps | 1271000     |
------------------------------------
Eval num_timesteps=1271500, episode_reward=-99873.58 +/- 21150.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38670594 |
|    mean velocity x | 0.025       |
|    mean velocity y | 0.915       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.99e+04   |
| time/              |             |
|    total_timesteps | 1271500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 621     |
|    time_elapsed    | 51273   |
|    total_timesteps | 1271808 |
--------------------------------
Eval num_timesteps=1272000, episode_reward=-95276.37 +/- 49115.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30894372   |
|    mean velocity x      | -0.431        |
|    mean velocity y      | 0.0848        |
|    mean velocity z      | 3.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.53e+04     |
| time/                   |               |
|    total_timesteps      | 1272000       |
| train/                  |               |
|    approx_kl            | 0.00012319707 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 3.84e+07      |
|    n_updates            | 6210          |
|    policy_gradient_loss | -0.000982     |
|    std                  | 1.56          |
|    value_loss           | 6.64e+07      |
-------------------------------------------
Eval num_timesteps=1272500, episode_reward=-49684.97 +/- 36555.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4655287 |
|    mean velocity x | 0.205      |
|    mean velocity y | 1.68       |
|    mean velocity z | 3.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.97e+04  |
| time/              |            |
|    total_timesteps | 1272500    |
-----------------------------------
Eval num_timesteps=1273000, episode_reward=-66091.69 +/- 48858.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45961592 |
|    mean velocity x | 0.33        |
|    mean velocity y | 1.23        |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.61e+04   |
| time/              |             |
|    total_timesteps | 1273000     |
------------------------------------
Eval num_timesteps=1273500, episode_reward=-95575.87 +/- 46030.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4434364 |
|    mean velocity x | -0.11      |
|    mean velocity y | 1.18       |
|    mean velocity z | 4.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.56e+04  |
| time/              |            |
|    total_timesteps | 1273500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 622     |
|    time_elapsed    | 51354   |
|    total_timesteps | 1273856 |
--------------------------------
Eval num_timesteps=1274000, episode_reward=-82831.36 +/- 40451.23
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20712933  |
|    mean velocity x      | -0.00402     |
|    mean velocity y      | 0.409        |
|    mean velocity z      | 0.348        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.28e+04    |
| time/                   |              |
|    total_timesteps      | 1274000      |
| train/                  |              |
|    approx_kl            | 0.0001329233 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.213        |
|    learning_rate        | 0.001        |
|    loss                 | 9.52e+06     |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.56         |
|    value_loss           | 5.17e+07     |
------------------------------------------
Eval num_timesteps=1274500, episode_reward=-83041.37 +/- 22776.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39005074 |
|    mean velocity x | -0.139      |
|    mean velocity y | 0.836       |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.3e+04    |
| time/              |             |
|    total_timesteps | 1274500     |
------------------------------------
Eval num_timesteps=1275000, episode_reward=-105929.74 +/- 17245.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43806344 |
|    mean velocity x | 0.525       |
|    mean velocity y | 1.45        |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1275000     |
------------------------------------
Eval num_timesteps=1275500, episode_reward=-112192.44 +/- 30423.72
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4372571 |
|    mean velocity x | -0.246     |
|    mean velocity y | 0.651      |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 1275500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 623     |
|    time_elapsed    | 51434   |
|    total_timesteps | 1275904 |
--------------------------------
Eval num_timesteps=1276000, episode_reward=-55505.77 +/- 33199.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.53342074  |
|    mean velocity x      | -0.36        |
|    mean velocity y      | 1.15         |
|    mean velocity z      | 4.93         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.55e+04    |
| time/                   |              |
|    total_timesteps      | 1276000      |
| train/                  |              |
|    approx_kl            | 7.925177e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.001        |
|    loss                 | 7.99e+07     |
|    n_updates            | 6230         |
|    policy_gradient_loss | -0.000306    |
|    std                  | 1.56         |
|    value_loss           | 9.67e+07     |
------------------------------------------
Eval num_timesteps=1276500, episode_reward=-79593.75 +/- 43232.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42404753 |
|    mean velocity x | -0.288      |
|    mean velocity y | 0.673       |
|    mean velocity z | 4.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.96e+04   |
| time/              |             |
|    total_timesteps | 1276500     |
------------------------------------
Eval num_timesteps=1277000, episode_reward=-73047.64 +/- 47083.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49246615 |
|    mean velocity x | -0.316      |
|    mean velocity y | 0.812       |
|    mean velocity z | 4.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.3e+04    |
| time/              |             |
|    total_timesteps | 1277000     |
------------------------------------
Eval num_timesteps=1277500, episode_reward=-39819.51 +/- 45110.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29726914 |
|    mean velocity x | -0.264      |
|    mean velocity y | 0.127       |
|    mean velocity z | 3.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.98e+04   |
| time/              |             |
|    total_timesteps | 1277500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 624     |
|    time_elapsed    | 51514   |
|    total_timesteps | 1277952 |
--------------------------------
Eval num_timesteps=1278000, episode_reward=-77793.97 +/- 28885.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3262313    |
|    mean velocity x      | -0.596        |
|    mean velocity y      | -0.0721       |
|    mean velocity z      | 3.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.78e+04     |
| time/                   |               |
|    total_timesteps      | 1278000       |
| train/                  |               |
|    approx_kl            | 3.9602717e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.001         |
|    loss                 | 8.76e+06      |
|    n_updates            | 6240          |
|    policy_gradient_loss | -0.000638     |
|    std                  | 1.56          |
|    value_loss           | 6.95e+07      |
-------------------------------------------
Eval num_timesteps=1278500, episode_reward=-73949.66 +/- 44308.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51104397 |
|    mean velocity x | -0.292      |
|    mean velocity y | 1.27        |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.39e+04   |
| time/              |             |
|    total_timesteps | 1278500     |
------------------------------------
Eval num_timesteps=1279000, episode_reward=-85138.78 +/- 46347.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32955757 |
|    mean velocity x | -0.276      |
|    mean velocity y | 0.679       |
|    mean velocity z | 1.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.51e+04   |
| time/              |             |
|    total_timesteps | 1279000     |
------------------------------------
Eval num_timesteps=1279500, episode_reward=-82944.50 +/- 40044.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32334462 |
|    mean velocity x | -0.0213     |
|    mean velocity y | 0.899       |
|    mean velocity z | 0.788       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 1279500     |
------------------------------------
Eval num_timesteps=1280000, episode_reward=-74338.69 +/- 58403.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4193015 |
|    mean velocity x | -0.281     |
|    mean velocity y | 0.486      |
|    mean velocity z | 4.9        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.43e+04  |
| time/              |            |
|    total_timesteps | 1280000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 625     |
|    time_elapsed    | 51614   |
|    total_timesteps | 1280000 |
--------------------------------
Eval num_timesteps=1280500, episode_reward=-81838.13 +/- 44660.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38616097   |
|    mean velocity x      | -0.342        |
|    mean velocity y      | 0.911         |
|    mean velocity z      | 2.96          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.18e+04     |
| time/                   |               |
|    total_timesteps      | 1280500       |
| train/                  |               |
|    approx_kl            | 6.3572836e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.001         |
|    loss                 | 1.52e+07      |
|    n_updates            | 6250          |
|    policy_gradient_loss | -0.000538     |
|    std                  | 1.56          |
|    value_loss           | 6.14e+07      |
-------------------------------------------
Eval num_timesteps=1281000, episode_reward=-118187.43 +/- 28638.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58429295 |
|    mean velocity x | -0.69       |
|    mean velocity y | 0.916       |
|    mean velocity z | 5.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 1281000     |
------------------------------------
Eval num_timesteps=1281500, episode_reward=-82429.84 +/- 62058.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3279336 |
|    mean velocity x | -0.502     |
|    mean velocity y | 0.542      |
|    mean velocity z | 2.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 1281500    |
-----------------------------------
Eval num_timesteps=1282000, episode_reward=-63946.40 +/- 27798.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26235297 |
|    mean velocity x | -1.07       |
|    mean velocity y | 0.0869      |
|    mean velocity z | 2.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.39e+04   |
| time/              |             |
|    total_timesteps | 1282000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 626     |
|    time_elapsed    | 51694   |
|    total_timesteps | 1282048 |
--------------------------------
Eval num_timesteps=1282500, episode_reward=-92363.12 +/- 29759.08
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43929505   |
|    mean velocity x      | 0.00245       |
|    mean velocity y      | 1.31          |
|    mean velocity z      | 3.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.24e+04     |
| time/                   |               |
|    total_timesteps      | 1282500       |
| train/                  |               |
|    approx_kl            | 1.0284188e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.001         |
|    loss                 | 2.59e+07      |
|    n_updates            | 6260          |
|    policy_gradient_loss | -0.000238     |
|    std                  | 1.56          |
|    value_loss           | 4.81e+07      |
-------------------------------------------
Eval num_timesteps=1283000, episode_reward=-82102.65 +/- 19024.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49363884 |
|    mean velocity x | -0.283      |
|    mean velocity y | 1.08        |
|    mean velocity z | 4.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.21e+04   |
| time/              |             |
|    total_timesteps | 1283000     |
------------------------------------
Eval num_timesteps=1283500, episode_reward=-94949.49 +/- 36461.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54522413 |
|    mean velocity x | 0.114       |
|    mean velocity y | 1.3         |
|    mean velocity z | 3.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 1283500     |
------------------------------------
Eval num_timesteps=1284000, episode_reward=-62995.49 +/- 43832.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4183066 |
|    mean velocity x | -0.151     |
|    mean velocity y | 0.749      |
|    mean velocity z | 3.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.3e+04   |
| time/              |            |
|    total_timesteps | 1284000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 627     |
|    time_elapsed    | 51774   |
|    total_timesteps | 1284096 |
--------------------------------
Eval num_timesteps=1284500, episode_reward=-105782.12 +/- 26897.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38698748  |
|    mean velocity x      | 0.0697       |
|    mean velocity y      | 0.622        |
|    mean velocity z      | 0.987        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.06e+05    |
| time/                   |              |
|    total_timesteps      | 1284500      |
| train/                  |              |
|    approx_kl            | 7.867356e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.001        |
|    loss                 | 6.2e+06      |
|    n_updates            | 6270         |
|    policy_gradient_loss | -0.00058     |
|    std                  | 1.56         |
|    value_loss           | 6.28e+07     |
------------------------------------------
Eval num_timesteps=1285000, episode_reward=-70195.01 +/- 35378.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32559323 |
|    mean velocity x | -0.339      |
|    mean velocity y | -0.0637     |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.02e+04   |
| time/              |             |
|    total_timesteps | 1285000     |
------------------------------------
Eval num_timesteps=1285500, episode_reward=-59959.95 +/- 47864.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6265147 |
|    mean velocity x | -0.329     |
|    mean velocity y | 1.62       |
|    mean velocity z | 4.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6e+04     |
| time/              |            |
|    total_timesteps | 1285500    |
-----------------------------------
Eval num_timesteps=1286000, episode_reward=-77738.04 +/- 32127.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49509302 |
|    mean velocity x | -0.323      |
|    mean velocity y | 0.928       |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 1286000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 628     |
|    time_elapsed    | 51855   |
|    total_timesteps | 1286144 |
--------------------------------
Eval num_timesteps=1286500, episode_reward=-96467.85 +/- 19392.40
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20368631  |
|    mean velocity x      | -1.17        |
|    mean velocity y      | -0.88        |
|    mean velocity z      | 4.5          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.65e+04    |
| time/                   |              |
|    total_timesteps      | 1286500      |
| train/                  |              |
|    approx_kl            | 9.289975e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.001        |
|    loss                 | 4.48e+07     |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.000996    |
|    std                  | 1.56         |
|    value_loss           | 5.88e+07     |
------------------------------------------
Eval num_timesteps=1287000, episode_reward=-32648.23 +/- 22928.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42808267 |
|    mean velocity x | -0.958      |
|    mean velocity y | 0.237       |
|    mean velocity z | 3.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.26e+04   |
| time/              |             |
|    total_timesteps | 1287000     |
------------------------------------
Eval num_timesteps=1287500, episode_reward=-58077.51 +/- 32793.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35119146 |
|    mean velocity x | -0.175      |
|    mean velocity y | 0.337       |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.81e+04   |
| time/              |             |
|    total_timesteps | 1287500     |
------------------------------------
Eval num_timesteps=1288000, episode_reward=-50561.44 +/- 41658.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3874829 |
|    mean velocity x | 0.0862     |
|    mean velocity y | 0.538      |
|    mean velocity z | 1.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.06e+04  |
| time/              |            |
|    total_timesteps | 1288000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 629     |
|    time_elapsed    | 51935   |
|    total_timesteps | 1288192 |
--------------------------------
Eval num_timesteps=1288500, episode_reward=-78018.50 +/- 28144.84
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5059524    |
|    mean velocity x      | 0.0817        |
|    mean velocity y      | 0.987         |
|    mean velocity z      | 1.93          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.8e+04      |
| time/                   |               |
|    total_timesteps      | 1288500       |
| train/                  |               |
|    approx_kl            | 2.1145446e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 7.57e+06      |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.000345     |
|    std                  | 1.56          |
|    value_loss           | 3.2e+07       |
-------------------------------------------
Eval num_timesteps=1289000, episode_reward=-72249.41 +/- 35943.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36621577 |
|    mean velocity x | -0.103      |
|    mean velocity y | 0.597       |
|    mean velocity z | 3.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.22e+04   |
| time/              |             |
|    total_timesteps | 1289000     |
------------------------------------
Eval num_timesteps=1289500, episode_reward=-94725.94 +/- 37212.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44717395 |
|    mean velocity x | 0.0308      |
|    mean velocity y | 1.41        |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.47e+04   |
| time/              |             |
|    total_timesteps | 1289500     |
------------------------------------
Eval num_timesteps=1290000, episode_reward=-109662.72 +/- 24237.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47242972 |
|    mean velocity x | 0.293       |
|    mean velocity y | 1.38        |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 1290000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 630     |
|    time_elapsed    | 52016   |
|    total_timesteps | 1290240 |
--------------------------------
Eval num_timesteps=1290500, episode_reward=-75833.72 +/- 40352.49
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36560827  |
|    mean velocity x      | -0.337       |
|    mean velocity y      | 0.807        |
|    mean velocity z      | 1.69         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.58e+04    |
| time/                   |              |
|    total_timesteps      | 1290500      |
| train/                  |              |
|    approx_kl            | 4.892089e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+07     |
|    n_updates            | 6300         |
|    policy_gradient_loss | -0.000573    |
|    std                  | 1.56         |
|    value_loss           | 3.44e+07     |
------------------------------------------
Eval num_timesteps=1291000, episode_reward=-102006.66 +/- 37123.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27016243 |
|    mean velocity x | -0.141      |
|    mean velocity y | 0.422       |
|    mean velocity z | 0.372       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1291000     |
------------------------------------
Eval num_timesteps=1291500, episode_reward=-69565.68 +/- 48552.80
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.292402 |
|    mean velocity x | -0.105    |
|    mean velocity y | 0.511     |
|    mean velocity z | 0.589     |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.96e+04 |
| time/              |           |
|    total_timesteps | 1291500   |
----------------------------------
Eval num_timesteps=1292000, episode_reward=-37268.90 +/- 34144.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49306393 |
|    mean velocity x | 0.527       |
|    mean velocity y | 1.48        |
|    mean velocity z | 3.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.73e+04   |
| time/              |             |
|    total_timesteps | 1292000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 631     |
|    time_elapsed    | 52096   |
|    total_timesteps | 1292288 |
--------------------------------
Eval num_timesteps=1292500, episode_reward=-73977.45 +/- 26934.79
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48932353   |
|    mean velocity x      | 0.373         |
|    mean velocity y      | 1.44          |
|    mean velocity z      | 3.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.4e+04      |
| time/                   |               |
|    total_timesteps      | 1292500       |
| train/                  |               |
|    approx_kl            | 0.00087380584 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.411         |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+07      |
|    n_updates            | 6310          |
|    policy_gradient_loss | -0.00129      |
|    std                  | 1.55          |
|    value_loss           | 1.54e+07      |
-------------------------------------------
Eval num_timesteps=1293000, episode_reward=-80394.39 +/- 41013.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3552106 |
|    mean velocity x | 0.129      |
|    mean velocity y | 1.14       |
|    mean velocity z | 4.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 1293000    |
-----------------------------------
Eval num_timesteps=1293500, episode_reward=-66888.19 +/- 54696.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4106252 |
|    mean velocity x | 0.32       |
|    mean velocity y | 1.25       |
|    mean velocity z | 3.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.69e+04  |
| time/              |            |
|    total_timesteps | 1293500    |
-----------------------------------
Eval num_timesteps=1294000, episode_reward=-62723.47 +/- 40717.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36463746 |
|    mean velocity x | -0.543      |
|    mean velocity y | 1.05        |
|    mean velocity z | 1.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.27e+04   |
| time/              |             |
|    total_timesteps | 1294000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 632     |
|    time_elapsed    | 52176   |
|    total_timesteps | 1294336 |
--------------------------------
Eval num_timesteps=1294500, episode_reward=-87534.01 +/- 39621.23
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.36700973    |
|    mean velocity x      | 0.235          |
|    mean velocity y      | 0.689          |
|    mean velocity z      | 2.63           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -8.75e+04      |
| time/                   |                |
|    total_timesteps      | 1294500        |
| train/                  |                |
|    approx_kl            | 0.000121905556 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.26           |
|    learning_rate        | 0.001          |
|    loss                 | 1.8e+07        |
|    n_updates            | 6320           |
|    policy_gradient_loss | -0.000559      |
|    std                  | 1.55           |
|    value_loss           | 4.29e+07       |
--------------------------------------------
Eval num_timesteps=1295000, episode_reward=-56011.05 +/- 40224.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4420501 |
|    mean velocity x | -0.206     |
|    mean velocity y | 0.782      |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.6e+04   |
| time/              |            |
|    total_timesteps | 1295000    |
-----------------------------------
Eval num_timesteps=1295500, episode_reward=-85837.14 +/- 62508.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47131306 |
|    mean velocity x | 0.541       |
|    mean velocity y | 1.6         |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.58e+04   |
| time/              |             |
|    total_timesteps | 1295500     |
------------------------------------
Eval num_timesteps=1296000, episode_reward=-83233.44 +/- 35149.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3906156 |
|    mean velocity x | -1.41      |
|    mean velocity y | -0.433     |
|    mean velocity z | 4.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.32e+04  |
| time/              |            |
|    total_timesteps | 1296000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 633     |
|    time_elapsed    | 52257   |
|    total_timesteps | 1296384 |
--------------------------------
Eval num_timesteps=1296500, episode_reward=-102437.25 +/- 28798.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4901406    |
|    mean velocity x      | -0.296        |
|    mean velocity y      | 0.858         |
|    mean velocity z      | 3.85          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 1296500       |
| train/                  |               |
|    approx_kl            | 2.7227943e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 4.96e+07      |
|    n_updates            | 6330          |
|    policy_gradient_loss | -0.000337     |
|    std                  | 1.55          |
|    value_loss           | 5.31e+07      |
-------------------------------------------
Eval num_timesteps=1297000, episode_reward=-119805.01 +/- 37072.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48844618 |
|    mean velocity x | 0.382       |
|    mean velocity y | 1.7         |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 1297000     |
------------------------------------
Eval num_timesteps=1297500, episode_reward=-78111.74 +/- 43376.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3082198 |
|    mean velocity x | 0.321      |
|    mean velocity y | 1.56       |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 1297500    |
-----------------------------------
Eval num_timesteps=1298000, episode_reward=-78225.22 +/- 43463.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29004094 |
|    mean velocity x | -1.04       |
|    mean velocity y | -0.0122     |
|    mean velocity z | 2.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 1298000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 634     |
|    time_elapsed    | 52337   |
|    total_timesteps | 1298432 |
--------------------------------
Eval num_timesteps=1298500, episode_reward=-85783.87 +/- 45407.84
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3498885   |
|    mean velocity x      | 0.0986       |
|    mean velocity y      | 0.699        |
|    mean velocity z      | 1.06         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.58e+04    |
| time/                   |              |
|    total_timesteps      | 1298500      |
| train/                  |              |
|    approx_kl            | 0.0001485807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.001        |
|    loss                 | 8.93e+06     |
|    n_updates            | 6340         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.55         |
|    value_loss           | 2.71e+07     |
------------------------------------------
Eval num_timesteps=1299000, episode_reward=-83858.22 +/- 44487.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7102987 |
|    mean velocity x | -0.288     |
|    mean velocity y | 1.61       |
|    mean velocity z | 6.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.39e+04  |
| time/              |            |
|    total_timesteps | 1299000    |
-----------------------------------
Eval num_timesteps=1299500, episode_reward=-84313.11 +/- 23357.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5343008 |
|    mean velocity x | -0.464     |
|    mean velocity y | 0.919      |
|    mean velocity z | 5.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 1299500    |
-----------------------------------
Eval num_timesteps=1300000, episode_reward=-89444.22 +/- 19815.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5777283 |
|    mean velocity x | -0.644     |
|    mean velocity y | 1.18       |
|    mean velocity z | 3.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.94e+04  |
| time/              |            |
|    total_timesteps | 1300000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 635     |
|    time_elapsed    | 52417   |
|    total_timesteps | 1300480 |
--------------------------------
Eval num_timesteps=1300500, episode_reward=-121789.31 +/- 66272.49
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.5137909     |
|    mean velocity x      | -0.125         |
|    mean velocity y      | 1.23           |
|    mean velocity z      | 4.33           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1.22e+05      |
| time/                   |                |
|    total_timesteps      | 1300500        |
| train/                  |                |
|    approx_kl            | 0.000102145044 |
|    clip_fraction        | 4.88e-05       |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.288          |
|    learning_rate        | 0.001          |
|    loss                 | 3.64e+07       |
|    n_updates            | 6350           |
|    policy_gradient_loss | -0.000925      |
|    std                  | 1.55           |
|    value_loss           | 8.01e+07       |
--------------------------------------------
Eval num_timesteps=1301000, episode_reward=-53433.90 +/- 46952.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20570366 |
|    mean velocity x | -2.88       |
|    mean velocity y | -2.57       |
|    mean velocity z | 8.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.34e+04   |
| time/              |             |
|    total_timesteps | 1301000     |
------------------------------------
Eval num_timesteps=1301500, episode_reward=-91304.58 +/- 22488.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50048345 |
|    mean velocity x | -0.503      |
|    mean velocity y | 0.399       |
|    mean velocity z | 4.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.13e+04   |
| time/              |             |
|    total_timesteps | 1301500     |
------------------------------------
Eval num_timesteps=1302000, episode_reward=-116207.02 +/- 33179.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44589674 |
|    mean velocity x | -0.381      |
|    mean velocity y | 0.912       |
|    mean velocity z | 2.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 1302000     |
------------------------------------
Eval num_timesteps=1302500, episode_reward=-76870.53 +/- 61235.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40845653 |
|    mean velocity x | -0.172      |
|    mean velocity y | 0.699       |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 1302500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 636     |
|    time_elapsed    | 52517   |
|    total_timesteps | 1302528 |
--------------------------------
Eval num_timesteps=1303000, episode_reward=-79324.90 +/- 45637.82
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43531197  |
|    mean velocity x      | -0.703       |
|    mean velocity y      | 0.63         |
|    mean velocity z      | 3.43         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.93e+04    |
| time/                   |              |
|    total_timesteps      | 1303000      |
| train/                  |              |
|    approx_kl            | 5.781796e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.001        |
|    loss                 | 3.88e+07     |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.00038     |
|    std                  | 1.55         |
|    value_loss           | 7.17e+07     |
------------------------------------------
Eval num_timesteps=1303500, episode_reward=-85731.85 +/- 22737.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4504723 |
|    mean velocity x | 0.621      |
|    mean velocity y | 1.56       |
|    mean velocity z | 3.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.57e+04  |
| time/              |            |
|    total_timesteps | 1303500    |
-----------------------------------
Eval num_timesteps=1304000, episode_reward=-94193.71 +/- 14760.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5391746 |
|    mean velocity x | -0.316     |
|    mean velocity y | 1.17       |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.42e+04  |
| time/              |            |
|    total_timesteps | 1304000    |
-----------------------------------
Eval num_timesteps=1304500, episode_reward=-68118.52 +/- 36591.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30896765 |
|    mean velocity x | -0.849      |
|    mean velocity y | -0.532      |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1304500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 637     |
|    time_elapsed    | 52597   |
|    total_timesteps | 1304576 |
--------------------------------
Eval num_timesteps=1305000, episode_reward=-93021.02 +/- 22865.55
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41545033   |
|    mean velocity x      | -0.235        |
|    mean velocity y      | 0.771         |
|    mean velocity z      | 2.9           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.3e+04      |
| time/                   |               |
|    total_timesteps      | 1305000       |
| train/                  |               |
|    approx_kl            | 3.9953826e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+07      |
|    n_updates            | 6370          |
|    policy_gradient_loss | -0.00062      |
|    std                  | 1.55          |
|    value_loss           | 4.16e+07      |
-------------------------------------------
Eval num_timesteps=1305500, episode_reward=-64802.83 +/- 41195.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4064702 |
|    mean velocity x | -0.356     |
|    mean velocity y | 1.05       |
|    mean velocity z | 3.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.48e+04  |
| time/              |            |
|    total_timesteps | 1305500    |
-----------------------------------
Eval num_timesteps=1306000, episode_reward=-47757.34 +/- 42376.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5297035 |
|    mean velocity x | 0.158      |
|    mean velocity y | 1.43       |
|    mean velocity z | 3.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.78e+04  |
| time/              |            |
|    total_timesteps | 1306000    |
-----------------------------------
Eval num_timesteps=1306500, episode_reward=-80470.49 +/- 64953.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40858182 |
|    mean velocity x | 0.154       |
|    mean velocity y | 1.52        |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 1306500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 638     |
|    time_elapsed    | 52677   |
|    total_timesteps | 1306624 |
--------------------------------
Eval num_timesteps=1307000, episode_reward=-117461.48 +/- 17783.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3396989    |
|    mean velocity x      | 0.203         |
|    mean velocity y      | 0.443         |
|    mean velocity z      | 1.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.17e+05     |
| time/                   |               |
|    total_timesteps      | 1307000       |
| train/                  |               |
|    approx_kl            | 1.9241357e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.001         |
|    loss                 | 3.49e+07      |
|    n_updates            | 6380          |
|    policy_gradient_loss | -0.000471     |
|    std                  | 1.55          |
|    value_loss           | 4.4e+07       |
-------------------------------------------
Eval num_timesteps=1307500, episode_reward=-79079.41 +/- 41720.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4956905 |
|    mean velocity x | -0.123     |
|    mean velocity y | 1.48       |
|    mean velocity z | 3.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.91e+04  |
| time/              |            |
|    total_timesteps | 1307500    |
-----------------------------------
Eval num_timesteps=1308000, episode_reward=-103425.49 +/- 18979.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34512472 |
|    mean velocity x | -0.164      |
|    mean velocity y | 0.344       |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1308000     |
------------------------------------
Eval num_timesteps=1308500, episode_reward=-110705.00 +/- 21356.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3428443 |
|    mean velocity x | -0.57      |
|    mean velocity y | 0.555      |
|    mean velocity z | 3.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.11e+05  |
| time/              |            |
|    total_timesteps | 1308500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 639     |
|    time_elapsed    | 52758   |
|    total_timesteps | 1308672 |
--------------------------------
Eval num_timesteps=1309000, episode_reward=-67549.29 +/- 54138.62
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46767256   |
|    mean velocity x      | -0.102        |
|    mean velocity y      | 1.06          |
|    mean velocity z      | 3.95          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.75e+04     |
| time/                   |               |
|    total_timesteps      | 1309000       |
| train/                  |               |
|    approx_kl            | 1.0892632e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.001         |
|    loss                 | 6.32e+07      |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000203     |
|    std                  | 1.55          |
|    value_loss           | 6.66e+07      |
-------------------------------------------
Eval num_timesteps=1309500, episode_reward=-77065.27 +/- 46562.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5008831 |
|    mean velocity x | 0.624      |
|    mean velocity y | 1.94       |
|    mean velocity z | 3.98       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.71e+04  |
| time/              |            |
|    total_timesteps | 1309500    |
-----------------------------------
Eval num_timesteps=1310000, episode_reward=-94283.26 +/- 34937.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41682297 |
|    mean velocity x | -0.121      |
|    mean velocity y | 0.703       |
|    mean velocity z | 3.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.43e+04   |
| time/              |             |
|    total_timesteps | 1310000     |
------------------------------------
Eval num_timesteps=1310500, episode_reward=-99532.51 +/- 37258.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44803193 |
|    mean velocity x | -0.251      |
|    mean velocity y | 0.765       |
|    mean velocity z | 4.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 1310500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 640     |
|    time_elapsed    | 52838   |
|    total_timesteps | 1310720 |
--------------------------------
Eval num_timesteps=1311000, episode_reward=-106099.03 +/- 27678.91
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.23537804  |
|    mean velocity x      | 0.173        |
|    mean velocity y      | 0.445        |
|    mean velocity z      | 3.68         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.06e+05    |
| time/                   |              |
|    total_timesteps      | 1311000      |
| train/                  |              |
|    approx_kl            | 1.790392e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.229        |
|    learning_rate        | 0.001        |
|    loss                 | 6.77e+07     |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.00033     |
|    std                  | 1.55         |
|    value_loss           | 6.41e+07     |
------------------------------------------
Eval num_timesteps=1311500, episode_reward=-62754.88 +/- 31810.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34811938 |
|    mean velocity x | 0.188       |
|    mean velocity y | 1.2         |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 1311500     |
------------------------------------
Eval num_timesteps=1312000, episode_reward=-73713.50 +/- 28410.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4274349 |
|    mean velocity x | 0.403      |
|    mean velocity y | 1.59       |
|    mean velocity z | 3.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.37e+04  |
| time/              |            |
|    total_timesteps | 1312000    |
-----------------------------------
Eval num_timesteps=1312500, episode_reward=-92537.43 +/- 22254.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3748193 |
|    mean velocity x | -0.132     |
|    mean velocity y | 0.559      |
|    mean velocity z | 4.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.25e+04  |
| time/              |            |
|    total_timesteps | 1312500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 641     |
|    time_elapsed    | 52919   |
|    total_timesteps | 1312768 |
--------------------------------
Eval num_timesteps=1313000, episode_reward=-91640.81 +/- 23993.38
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2843371    |
|    mean velocity x      | -0.219        |
|    mean velocity y      | 0.558         |
|    mean velocity z      | 0.316         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.16e+04     |
| time/                   |               |
|    total_timesteps      | 1313000       |
| train/                  |               |
|    approx_kl            | 4.0378014e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.001         |
|    loss                 | 2.49e+07      |
|    n_updates            | 6410          |
|    policy_gradient_loss | -0.000442     |
|    std                  | 1.55          |
|    value_loss           | 5.64e+07      |
-------------------------------------------
Eval num_timesteps=1313500, episode_reward=-71989.37 +/- 13313.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.61914784 |
|    mean velocity x | 0.56        |
|    mean velocity y | 1.93        |
|    mean velocity z | 3.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 1313500     |
------------------------------------
Eval num_timesteps=1314000, episode_reward=-90782.10 +/- 37121.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2993352 |
|    mean velocity x | -0.804     |
|    mean velocity y | -0.152     |
|    mean velocity z | 3.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.08e+04  |
| time/              |            |
|    total_timesteps | 1314000    |
-----------------------------------
Eval num_timesteps=1314500, episode_reward=-74994.49 +/- 48340.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50034285 |
|    mean velocity x | 0.207       |
|    mean velocity y | 1.39        |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.5e+04    |
| time/              |             |
|    total_timesteps | 1314500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 642     |
|    time_elapsed    | 52999   |
|    total_timesteps | 1314816 |
--------------------------------
Eval num_timesteps=1315000, episode_reward=-82334.00 +/- 38851.23
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47031984   |
|    mean velocity x      | -0.388        |
|    mean velocity y      | 1.01          |
|    mean velocity z      | 4.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.23e+04     |
| time/                   |               |
|    total_timesteps      | 1315000       |
| train/                  |               |
|    approx_kl            | 2.8236245e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+07      |
|    n_updates            | 6420          |
|    policy_gradient_loss | -0.000109     |
|    std                  | 1.55          |
|    value_loss           | 4.57e+07      |
-------------------------------------------
Eval num_timesteps=1315500, episode_reward=-57749.55 +/- 31602.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33314857 |
|    mean velocity x | 0.26        |
|    mean velocity y | 1.76        |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.77e+04   |
| time/              |             |
|    total_timesteps | 1315500     |
------------------------------------
Eval num_timesteps=1316000, episode_reward=-64255.79 +/- 47310.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48157126 |
|    mean velocity x | 0.0259      |
|    mean velocity y | 1.53        |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.43e+04   |
| time/              |             |
|    total_timesteps | 1316000     |
------------------------------------
Eval num_timesteps=1316500, episode_reward=-90978.11 +/- 54131.56
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5714918 |
|    mean velocity x | -0.433     |
|    mean velocity y | 0.941      |
|    mean velocity z | 4.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.1e+04   |
| time/              |            |
|    total_timesteps | 1316500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 643     |
|    time_elapsed    | 53079   |
|    total_timesteps | 1316864 |
--------------------------------
Eval num_timesteps=1317000, episode_reward=-83044.82 +/- 29849.09
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49733892   |
|    mean velocity x      | -0.298        |
|    mean velocity y      | 1.37          |
|    mean velocity z      | 1.03          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.3e+04      |
| time/                   |               |
|    total_timesteps      | 1317000       |
| train/                  |               |
|    approx_kl            | 1.3999292e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.001         |
|    loss                 | 1.99e+07      |
|    n_updates            | 6430          |
|    policy_gradient_loss | -0.000267     |
|    std                  | 1.55          |
|    value_loss           | 6.58e+07      |
-------------------------------------------
Eval num_timesteps=1317500, episode_reward=-78010.38 +/- 35182.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41663566 |
|    mean velocity x | -2.24       |
|    mean velocity y | -1.12       |
|    mean velocity z | 7.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.8e+04    |
| time/              |             |
|    total_timesteps | 1317500     |
------------------------------------
Eval num_timesteps=1318000, episode_reward=-70540.04 +/- 38787.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48159447 |
|    mean velocity x | 0.578       |
|    mean velocity y | 1.7         |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 1318000     |
------------------------------------
Eval num_timesteps=1318500, episode_reward=-69821.15 +/- 39616.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5157699 |
|    mean velocity x | -0.417     |
|    mean velocity y | 1.05       |
|    mean velocity z | 3.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.98e+04  |
| time/              |            |
|    total_timesteps | 1318500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 644     |
|    time_elapsed    | 53160   |
|    total_timesteps | 1318912 |
--------------------------------
Eval num_timesteps=1319000, episode_reward=-102333.51 +/- 11655.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5433085    |
|    mean velocity x      | -0.475        |
|    mean velocity y      | 1.3           |
|    mean velocity z      | 3.94          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 1319000       |
| train/                  |               |
|    approx_kl            | 1.7711398e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.387         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+07      |
|    n_updates            | 6440          |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.55          |
|    value_loss           | 4.17e+07      |
-------------------------------------------
Eval num_timesteps=1319500, episode_reward=-64088.56 +/- 20795.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44924885 |
|    mean velocity x | -0.149      |
|    mean velocity y | 1.4         |
|    mean velocity z | 4.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.41e+04   |
| time/              |             |
|    total_timesteps | 1319500     |
------------------------------------
Eval num_timesteps=1320000, episode_reward=-93835.85 +/- 41790.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34337056 |
|    mean velocity x | -0.388      |
|    mean velocity y | 0.728       |
|    mean velocity z | 0.552       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.38e+04   |
| time/              |             |
|    total_timesteps | 1320000     |
------------------------------------
Eval num_timesteps=1320500, episode_reward=-99121.78 +/- 33329.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43858254 |
|    mean velocity x | 0.281       |
|    mean velocity y | 1.47        |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.91e+04   |
| time/              |             |
|    total_timesteps | 1320500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 645     |
|    time_elapsed    | 53240   |
|    total_timesteps | 1320960 |
--------------------------------
Eval num_timesteps=1321000, episode_reward=-87494.78 +/- 31470.99
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5166554    |
|    mean velocity x      | -0.0596       |
|    mean velocity y      | 1.53          |
|    mean velocity z      | 3.66          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.75e+04     |
| time/                   |               |
|    total_timesteps      | 1321000       |
| train/                  |               |
|    approx_kl            | 2.3400207e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+07      |
|    n_updates            | 6450          |
|    policy_gradient_loss | -0.000403     |
|    std                  | 1.55          |
|    value_loss           | 4.15e+07      |
-------------------------------------------
Eval num_timesteps=1321500, episode_reward=-84479.18 +/- 46502.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3775129 |
|    mean velocity x | -0.139     |
|    mean velocity y | 0.879      |
|    mean velocity z | 4.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.45e+04  |
| time/              |            |
|    total_timesteps | 1321500    |
-----------------------------------
Eval num_timesteps=1322000, episode_reward=-44882.18 +/- 32299.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32882413 |
|    mean velocity x | -0.0727     |
|    mean velocity y | 0.524       |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.49e+04   |
| time/              |             |
|    total_timesteps | 1322000     |
------------------------------------
Eval num_timesteps=1322500, episode_reward=-76322.69 +/- 42302.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49570072 |
|    mean velocity x | -0.0933     |
|    mean velocity y | 1.68        |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.63e+04   |
| time/              |             |
|    total_timesteps | 1322500     |
------------------------------------
Eval num_timesteps=1323000, episode_reward=-64393.90 +/- 40511.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26312053 |
|    mean velocity x | -0.4        |
|    mean velocity y | -0.129      |
|    mean velocity z | 3.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.44e+04   |
| time/              |             |
|    total_timesteps | 1323000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 646     |
|    time_elapsed    | 53339   |
|    total_timesteps | 1323008 |
--------------------------------
Eval num_timesteps=1323500, episode_reward=-110570.13 +/- 50548.11
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37237155  |
|    mean velocity x      | -0.757       |
|    mean velocity y      | -0.0474      |
|    mean velocity z      | 3.35         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.11e+05    |
| time/                   |              |
|    total_timesteps      | 1323500      |
| train/                  |              |
|    approx_kl            | 6.265406e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.193        |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+07     |
|    n_updates            | 6460         |
|    policy_gradient_loss | -0.000638    |
|    std                  | 1.55         |
|    value_loss           | 9.47e+07     |
------------------------------------------
Eval num_timesteps=1324000, episode_reward=-70838.20 +/- 53915.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4640324 |
|    mean velocity x | 0.416      |
|    mean velocity y | 1.6        |
|    mean velocity z | 3.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.08e+04  |
| time/              |            |
|    total_timesteps | 1324000    |
-----------------------------------
Eval num_timesteps=1324500, episode_reward=-111190.30 +/- 17129.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44657728 |
|    mean velocity x | -0.49       |
|    mean velocity y | 0.351       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1324500     |
------------------------------------
Eval num_timesteps=1325000, episode_reward=-83551.14 +/- 40091.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23983899 |
|    mean velocity x | -0.153      |
|    mean velocity y | 0.681       |
|    mean velocity z | 0.364       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.36e+04   |
| time/              |             |
|    total_timesteps | 1325000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 647     |
|    time_elapsed    | 53420   |
|    total_timesteps | 1325056 |
--------------------------------
Eval num_timesteps=1325500, episode_reward=-89883.92 +/- 33149.02
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.36844185 |
|    mean velocity x      | -0.375      |
|    mean velocity y      | 0.246       |
|    mean velocity z      | 4.03        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.99e+04   |
| time/                   |             |
|    total_timesteps      | 1325500     |
| train/                  |             |
|    approx_kl            | 1.88759e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.001       |
|    loss                 | 3.78e+07    |
|    n_updates            | 6470        |
|    policy_gradient_loss | -0.000318   |
|    std                  | 1.55        |
|    value_loss           | 5.47e+07    |
-----------------------------------------
Eval num_timesteps=1326000, episode_reward=-75662.53 +/- 38748.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5070583 |
|    mean velocity x | -0.391     |
|    mean velocity y | 0.909      |
|    mean velocity z | 4.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.57e+04  |
| time/              |            |
|    total_timesteps | 1326000    |
-----------------------------------
Eval num_timesteps=1326500, episode_reward=-60103.94 +/- 39739.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49958795 |
|    mean velocity x | 0.447       |
|    mean velocity y | 1.66        |
|    mean velocity z | 3.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.01e+04   |
| time/              |             |
|    total_timesteps | 1326500     |
------------------------------------
Eval num_timesteps=1327000, episode_reward=-78643.08 +/- 39108.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51691663 |
|    mean velocity x | -0.718      |
|    mean velocity y | 0.449       |
|    mean velocity z | 4.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.86e+04   |
| time/              |             |
|    total_timesteps | 1327000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 648     |
|    time_elapsed    | 53500   |
|    total_timesteps | 1327104 |
--------------------------------
Eval num_timesteps=1327500, episode_reward=-99911.19 +/- 26401.35
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33253816   |
|    mean velocity x      | 0.17          |
|    mean velocity y      | 0.34          |
|    mean velocity z      | 1.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.99e+04     |
| time/                   |               |
|    total_timesteps      | 1327500       |
| train/                  |               |
|    approx_kl            | 1.2537377e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+07      |
|    n_updates            | 6480          |
|    policy_gradient_loss | -0.000227     |
|    std                  | 1.55          |
|    value_loss           | 5.58e+07      |
-------------------------------------------
Eval num_timesteps=1328000, episode_reward=-84118.69 +/- 29956.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44172758 |
|    mean velocity x | -0.82       |
|    mean velocity y | 0.37        |
|    mean velocity z | 3.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.41e+04   |
| time/              |             |
|    total_timesteps | 1328000     |
------------------------------------
Eval num_timesteps=1328500, episode_reward=-95509.98 +/- 53374.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48457822 |
|    mean velocity x | -0.316      |
|    mean velocity y | 0.716       |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.55e+04   |
| time/              |             |
|    total_timesteps | 1328500     |
------------------------------------
Eval num_timesteps=1329000, episode_reward=-83262.28 +/- 23950.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4912762 |
|    mean velocity x | 0.504      |
|    mean velocity y | 1.4        |
|    mean velocity z | 3.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.33e+04  |
| time/              |            |
|    total_timesteps | 1329000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 649     |
|    time_elapsed    | 53581   |
|    total_timesteps | 1329152 |
--------------------------------
Eval num_timesteps=1329500, episode_reward=-72545.28 +/- 23307.30
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5005637    |
|    mean velocity x      | 0.671         |
|    mean velocity y      | 1.54          |
|    mean velocity z      | 3.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.25e+04     |
| time/                   |               |
|    total_timesteps      | 1329500       |
| train/                  |               |
|    approx_kl            | 3.4863857e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.298         |
|    learning_rate        | 0.001         |
|    loss                 | 3.31e+07      |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000419     |
|    std                  | 1.55          |
|    value_loss           | 4.24e+07      |
-------------------------------------------
Eval num_timesteps=1330000, episode_reward=-86858.23 +/- 43556.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4566388 |
|    mean velocity x | -0.205     |
|    mean velocity y | 0.777      |
|    mean velocity z | 2.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 1330000    |
-----------------------------------
Eval num_timesteps=1330500, episode_reward=-58009.72 +/- 26564.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34149182 |
|    mean velocity x | 0.148       |
|    mean velocity y | 0.51        |
|    mean velocity z | 1.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.8e+04    |
| time/              |             |
|    total_timesteps | 1330500     |
------------------------------------
Eval num_timesteps=1331000, episode_reward=-96068.72 +/- 42628.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4280601 |
|    mean velocity x | -0.282     |
|    mean velocity y | 0.733      |
|    mean velocity z | 4.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.61e+04  |
| time/              |            |
|    total_timesteps | 1331000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 650     |
|    time_elapsed    | 53661   |
|    total_timesteps | 1331200 |
--------------------------------
Eval num_timesteps=1331500, episode_reward=-93606.13 +/- 27554.64
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39173526   |
|    mean velocity x      | -0.116        |
|    mean velocity y      | 0.584         |
|    mean velocity z      | 3.97          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.36e+04     |
| time/                   |               |
|    total_timesteps      | 1331500       |
| train/                  |               |
|    approx_kl            | 7.3397823e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.001         |
|    loss                 | 5.65e+07      |
|    n_updates            | 6500          |
|    policy_gradient_loss | -0.000201     |
|    std                  | 1.55          |
|    value_loss           | 6.08e+07      |
-------------------------------------------
Eval num_timesteps=1332000, episode_reward=-101295.85 +/- 29960.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41708192 |
|    mean velocity x | -0.1        |
|    mean velocity y | 0.506       |
|    mean velocity z | 1.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1332000     |
------------------------------------
Eval num_timesteps=1332500, episode_reward=-77915.90 +/- 42933.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4034224 |
|    mean velocity x | -0.203     |
|    mean velocity y | 0.709      |
|    mean velocity z | 3.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.79e+04  |
| time/              |            |
|    total_timesteps | 1332500    |
-----------------------------------
Eval num_timesteps=1333000, episode_reward=-63644.66 +/- 36127.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20225385 |
|    mean velocity x | -0.581      |
|    mean velocity y | -0.422      |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.36e+04   |
| time/              |             |
|    total_timesteps | 1333000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 651     |
|    time_elapsed    | 53741   |
|    total_timesteps | 1333248 |
--------------------------------
Eval num_timesteps=1333500, episode_reward=-68448.19 +/- 60989.23
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40262145   |
|    mean velocity x      | -0.0153       |
|    mean velocity y      | 0.967         |
|    mean velocity z      | 4.31          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.84e+04     |
| time/                   |               |
|    total_timesteps      | 1333500       |
| train/                  |               |
|    approx_kl            | 3.1409785e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+07      |
|    n_updates            | 6510          |
|    policy_gradient_loss | -0.000314     |
|    std                  | 1.55          |
|    value_loss           | 4.53e+07      |
-------------------------------------------
Eval num_timesteps=1334000, episode_reward=-91169.47 +/- 16755.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2624481 |
|    mean velocity x | -0.178     |
|    mean velocity y | 0.401      |
|    mean velocity z | 0.756      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.12e+04  |
| time/              |            |
|    total_timesteps | 1334000    |
-----------------------------------
Eval num_timesteps=1334500, episode_reward=-98570.96 +/- 20424.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21100323 |
|    mean velocity x | -0.123      |
|    mean velocity y | 0.367       |
|    mean velocity z | 0.649       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.86e+04   |
| time/              |             |
|    total_timesteps | 1334500     |
------------------------------------
Eval num_timesteps=1335000, episode_reward=-83194.76 +/- 36427.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55435926 |
|    mean velocity x | 0.256       |
|    mean velocity y | 1.63        |
|    mean velocity z | 3.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.32e+04   |
| time/              |             |
|    total_timesteps | 1335000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 652     |
|    time_elapsed    | 53822   |
|    total_timesteps | 1335296 |
--------------------------------
Eval num_timesteps=1335500, episode_reward=-103119.00 +/- 23102.87
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34009388   |
|    mean velocity x      | -0.343        |
|    mean velocity y      | 0.0472        |
|    mean velocity z      | 3.81          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 1335500       |
| train/                  |               |
|    approx_kl            | 0.00014114307 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.001         |
|    loss                 | 5.95e+06      |
|    n_updates            | 6520          |
|    policy_gradient_loss | -0.000686     |
|    std                  | 1.55          |
|    value_loss           | 3.58e+07      |
-------------------------------------------
Eval num_timesteps=1336000, episode_reward=-78940.67 +/- 11836.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38340575 |
|    mean velocity x | 0.153       |
|    mean velocity y | 1.03        |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.89e+04   |
| time/              |             |
|    total_timesteps | 1336000     |
------------------------------------
Eval num_timesteps=1336500, episode_reward=-96612.19 +/- 36361.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40782726 |
|    mean velocity x | 0.264       |
|    mean velocity y | 1.07        |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.66e+04   |
| time/              |             |
|    total_timesteps | 1336500     |
------------------------------------
Eval num_timesteps=1337000, episode_reward=-64811.18 +/- 46827.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38589558 |
|    mean velocity x | -0.251      |
|    mean velocity y | 0.691       |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 1337000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 653     |
|    time_elapsed    | 53902   |
|    total_timesteps | 1337344 |
--------------------------------
Eval num_timesteps=1337500, episode_reward=-94652.07 +/- 50840.28
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41192803   |
|    mean velocity x      | -0.243        |
|    mean velocity y      | 0.478         |
|    mean velocity z      | 4.14          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.47e+04     |
| time/                   |               |
|    total_timesteps      | 1337500       |
| train/                  |               |
|    approx_kl            | 2.5197602e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.001         |
|    loss                 | 7.02e+07      |
|    n_updates            | 6530          |
|    policy_gradient_loss | -0.000614     |
|    std                  | 1.55          |
|    value_loss           | 9.42e+07      |
-------------------------------------------
Eval num_timesteps=1338000, episode_reward=-78052.29 +/- 37141.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0788079 |
|    mean velocity x | -0.0122    |
|    mean velocity y | 0.29       |
|    mean velocity z | 0.389      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 1338000    |
-----------------------------------
Eval num_timesteps=1338500, episode_reward=-80451.85 +/- 24591.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48385337 |
|    mean velocity x | -0.113      |
|    mean velocity y | 0.804       |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 1338500     |
------------------------------------
Eval num_timesteps=1339000, episode_reward=-43543.68 +/- 24760.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49479473 |
|    mean velocity x | -0.295      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.35e+04   |
| time/              |             |
|    total_timesteps | 1339000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 654     |
|    time_elapsed    | 53982   |
|    total_timesteps | 1339392 |
--------------------------------
Eval num_timesteps=1339500, episode_reward=-61282.44 +/- 44464.58
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.64818764   |
|    mean velocity x      | 0.912         |
|    mean velocity y      | 2.05          |
|    mean velocity z      | 4.57          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.13e+04     |
| time/                   |               |
|    total_timesteps      | 1339500       |
| train/                  |               |
|    approx_kl            | 1.7990853e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+07      |
|    n_updates            | 6540          |
|    policy_gradient_loss | -0.000358     |
|    std                  | 1.55          |
|    value_loss           | 4.67e+07      |
-------------------------------------------
Eval num_timesteps=1340000, episode_reward=-74924.17 +/- 46250.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.448547 |
|    mean velocity x | 0.306     |
|    mean velocity y | 1.11      |
|    mean velocity z | 3.38      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.49e+04 |
| time/              |           |
|    total_timesteps | 1340000   |
----------------------------------
Eval num_timesteps=1340500, episode_reward=-75898.50 +/- 46650.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46387428 |
|    mean velocity x | -1.03       |
|    mean velocity y | 0.137       |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 1340500     |
------------------------------------
Eval num_timesteps=1341000, episode_reward=-107316.17 +/- 39637.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27279943 |
|    mean velocity x | 0.016       |
|    mean velocity y | 0.402       |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1341000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 655     |
|    time_elapsed    | 54063   |
|    total_timesteps | 1341440 |
--------------------------------
Eval num_timesteps=1341500, episode_reward=-78773.22 +/- 37594.10
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46799225   |
|    mean velocity x      | -0.4          |
|    mean velocity y      | 0.859         |
|    mean velocity z      | 3.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.88e+04     |
| time/                   |               |
|    total_timesteps      | 1341500       |
| train/                  |               |
|    approx_kl            | 1.6423874e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.001         |
|    loss                 | 4.03e+07      |
|    n_updates            | 6550          |
|    policy_gradient_loss | -0.000273     |
|    std                  | 1.55          |
|    value_loss           | 5.14e+07      |
-------------------------------------------
Eval num_timesteps=1342000, episode_reward=-75494.86 +/- 34323.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27546817 |
|    mean velocity x | 0.018       |
|    mean velocity y | 0.248       |
|    mean velocity z | 0.886       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 1342000     |
------------------------------------
Eval num_timesteps=1342500, episode_reward=-109007.04 +/- 39183.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47951832 |
|    mean velocity x | 0.137       |
|    mean velocity y | 1.58        |
|    mean velocity z | 4.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1342500     |
------------------------------------
Eval num_timesteps=1343000, episode_reward=-55093.15 +/- 39800.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46753994 |
|    mean velocity x | -0.158      |
|    mean velocity y | 0.734       |
|    mean velocity z | 2.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.51e+04   |
| time/              |             |
|    total_timesteps | 1343000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 656     |
|    time_elapsed    | 54143   |
|    total_timesteps | 1343488 |
--------------------------------
Eval num_timesteps=1343500, episode_reward=-89730.55 +/- 12420.12
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.427656     |
|    mean velocity x      | 0.367         |
|    mean velocity y      | 1.15          |
|    mean velocity z      | 3.45          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.97e+04     |
| time/                   |               |
|    total_timesteps      | 1343500       |
| train/                  |               |
|    approx_kl            | 2.3488828e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.001         |
|    loss                 | 8.24e+06      |
|    n_updates            | 6560          |
|    policy_gradient_loss | -0.000417     |
|    std                  | 1.56          |
|    value_loss           | 3.21e+07      |
-------------------------------------------
Eval num_timesteps=1344000, episode_reward=-80185.60 +/- 29441.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5244386 |
|    mean velocity x | -0.477     |
|    mean velocity y | 0.852      |
|    mean velocity z | 5.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.02e+04  |
| time/              |            |
|    total_timesteps | 1344000    |
-----------------------------------
Eval num_timesteps=1344500, episode_reward=-67520.24 +/- 50768.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4401488 |
|    mean velocity x | -0.173     |
|    mean velocity y | 0.908      |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.75e+04  |
| time/              |            |
|    total_timesteps | 1344500    |
-----------------------------------
Eval num_timesteps=1345000, episode_reward=-102108.58 +/- 36522.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3467228 |
|    mean velocity x | -0.39      |
|    mean velocity y | 0.407      |
|    mean velocity z | 1.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 1345000    |
-----------------------------------
Eval num_timesteps=1345500, episode_reward=-78685.55 +/- 68732.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4268493 |
|    mean velocity x | -0.789     |
|    mean velocity y | 0.347      |
|    mean velocity z | 3.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.87e+04  |
| time/              |            |
|    total_timesteps | 1345500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 657     |
|    time_elapsed    | 54242   |
|    total_timesteps | 1345536 |
--------------------------------
Eval num_timesteps=1346000, episode_reward=-76766.46 +/- 39661.12
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35410008   |
|    mean velocity x      | -0.168        |
|    mean velocity y      | 0.635         |
|    mean velocity z      | 0.457         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.68e+04     |
| time/                   |               |
|    total_timesteps      | 1346000       |
| train/                  |               |
|    approx_kl            | 7.2406954e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.305         |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+06      |
|    n_updates            | 6570          |
|    policy_gradient_loss | -0.000506     |
|    std                  | 1.55          |
|    value_loss           | 5.43e+07      |
-------------------------------------------
Eval num_timesteps=1346500, episode_reward=-69944.03 +/- 39541.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43337193 |
|    mean velocity x | 0.603       |
|    mean velocity y | 1.51        |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.99e+04   |
| time/              |             |
|    total_timesteps | 1346500     |
------------------------------------
Eval num_timesteps=1347000, episode_reward=-67084.82 +/- 31731.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47476855 |
|    mean velocity x | 0.326       |
|    mean velocity y | 0.914       |
|    mean velocity z | 2.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 1347000     |
------------------------------------
Eval num_timesteps=1347500, episode_reward=-68197.96 +/- 41910.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39083946 |
|    mean velocity x | 0.148       |
|    mean velocity y | 0.939       |
|    mean velocity z | 3.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.82e+04   |
| time/              |             |
|    total_timesteps | 1347500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 658     |
|    time_elapsed    | 54323   |
|    total_timesteps | 1347584 |
--------------------------------
Eval num_timesteps=1348000, episode_reward=-67055.47 +/- 19384.57
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4180176   |
|    mean velocity x      | -0.523       |
|    mean velocity y      | 0.884        |
|    mean velocity z      | 2.1          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.71e+04    |
| time/                   |              |
|    total_timesteps      | 1348000      |
| train/                  |              |
|    approx_kl            | 6.852325e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.328        |
|    learning_rate        | 0.001        |
|    loss                 | 3.08e+07     |
|    n_updates            | 6580         |
|    policy_gradient_loss | -0.000548    |
|    std                  | 1.55         |
|    value_loss           | 2.98e+07     |
------------------------------------------
Eval num_timesteps=1348500, episode_reward=-47536.87 +/- 46042.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22877401 |
|    mean velocity x | 0.055       |
|    mean velocity y | 0.278       |
|    mean velocity z | 1.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.75e+04   |
| time/              |             |
|    total_timesteps | 1348500     |
------------------------------------
Eval num_timesteps=1349000, episode_reward=-59053.34 +/- 32304.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26430315 |
|    mean velocity x | -0.014      |
|    mean velocity y | 0.443       |
|    mean velocity z | 0.421       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.91e+04   |
| time/              |             |
|    total_timesteps | 1349000     |
------------------------------------
Eval num_timesteps=1349500, episode_reward=-55283.69 +/- 34554.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46227655 |
|    mean velocity x | -0.556      |
|    mean velocity y | 1.12        |
|    mean velocity z | 2.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.53e+04   |
| time/              |             |
|    total_timesteps | 1349500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 659     |
|    time_elapsed    | 54403   |
|    total_timesteps | 1349632 |
--------------------------------
Eval num_timesteps=1350000, episode_reward=-78047.67 +/- 42373.47
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.56364024   |
|    mean velocity x      | -0.391        |
|    mean velocity y      | 1.03          |
|    mean velocity z      | 5.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.8e+04      |
| time/                   |               |
|    total_timesteps      | 1350000       |
| train/                  |               |
|    approx_kl            | 0.00023077981 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 2.56e+07      |
|    n_updates            | 6590          |
|    policy_gradient_loss | -0.00112      |
|    std                  | 1.55          |
|    value_loss           | 4.33e+07      |
-------------------------------------------
Eval num_timesteps=1350500, episode_reward=-86072.91 +/- 28905.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42075536 |
|    mean velocity x | -0.123      |
|    mean velocity y | 1.48        |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.61e+04   |
| time/              |             |
|    total_timesteps | 1350500     |
------------------------------------
Eval num_timesteps=1351000, episode_reward=-86352.43 +/- 70209.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45589805 |
|    mean velocity x | -0.264      |
|    mean velocity y | 0.865       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 1351000     |
------------------------------------
Eval num_timesteps=1351500, episode_reward=-51838.98 +/- 41518.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49824268 |
|    mean velocity x | 0.505       |
|    mean velocity y | 1.65        |
|    mean velocity z | 3.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.18e+04   |
| time/              |             |
|    total_timesteps | 1351500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 660     |
|    time_elapsed    | 54483   |
|    total_timesteps | 1351680 |
--------------------------------
Eval num_timesteps=1352000, episode_reward=-94898.15 +/- 20453.32
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.57555825   |
|    mean velocity x      | 0.198         |
|    mean velocity y      | 1.82          |
|    mean velocity z      | 3.84          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.49e+04     |
| time/                   |               |
|    total_timesteps      | 1352000       |
| train/                  |               |
|    approx_kl            | 3.4385797e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 4.35e+07      |
|    n_updates            | 6600          |
|    policy_gradient_loss | -0.00042      |
|    std                  | 1.56          |
|    value_loss           | 7.11e+07      |
-------------------------------------------
Eval num_timesteps=1352500, episode_reward=-98804.98 +/- 28235.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38441816 |
|    mean velocity x | -0.0536     |
|    mean velocity y | 0.62        |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.88e+04   |
| time/              |             |
|    total_timesteps | 1352500     |
------------------------------------
Eval num_timesteps=1353000, episode_reward=-106020.73 +/- 22771.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2756673 |
|    mean velocity x | -0.141     |
|    mean velocity y | 0.541      |
|    mean velocity z | 0.443      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 1353000    |
-----------------------------------
Eval num_timesteps=1353500, episode_reward=-122722.12 +/- 10287.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37611297 |
|    mean velocity x | -0.259      |
|    mean velocity y | 0.34        |
|    mean velocity z | 3.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 1353500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 661     |
|    time_elapsed    | 54573   |
|    total_timesteps | 1353728 |
--------------------------------
Eval num_timesteps=1354000, episode_reward=-76186.89 +/- 32542.68
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5419792    |
|    mean velocity x      | -0.51         |
|    mean velocity y      | 0.949         |
|    mean velocity z      | 4.56          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.62e+04     |
| time/                   |               |
|    total_timesteps      | 1354000       |
| train/                  |               |
|    approx_kl            | 4.5512657e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 6.22e+06      |
|    n_updates            | 6610          |
|    policy_gradient_loss | -0.000512     |
|    std                  | 1.56          |
|    value_loss           | 5.56e+07      |
-------------------------------------------
Eval num_timesteps=1354500, episode_reward=-77219.77 +/- 31912.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40019825 |
|    mean velocity x | -1.41       |
|    mean velocity y | -0.364      |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 1354500     |
------------------------------------
Eval num_timesteps=1355000, episode_reward=-102625.16 +/- 15069.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21879368 |
|    mean velocity x | -0.156      |
|    mean velocity y | 0.345       |
|    mean velocity z | 0.394       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1355000     |
------------------------------------
Eval num_timesteps=1355500, episode_reward=-88399.55 +/- 44714.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50133866 |
|    mean velocity x | 0.22        |
|    mean velocity y | 1.15        |
|    mean velocity z | 3.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 1355500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 662     |
|    time_elapsed    | 54653   |
|    total_timesteps | 1355776 |
--------------------------------
Eval num_timesteps=1356000, episode_reward=-76409.29 +/- 46906.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47321513   |
|    mean velocity x      | -0.159        |
|    mean velocity y      | 1.27          |
|    mean velocity z      | 4.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.64e+04     |
| time/                   |               |
|    total_timesteps      | 1356000       |
| train/                  |               |
|    approx_kl            | 0.00014061198 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.001         |
|    loss                 | 9.04e+06      |
|    n_updates            | 6620          |
|    policy_gradient_loss | -0.000682     |
|    std                  | 1.56          |
|    value_loss           | 4.66e+07      |
-------------------------------------------
Eval num_timesteps=1356500, episode_reward=-84568.74 +/- 32660.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46405604 |
|    mean velocity x | 0.541       |
|    mean velocity y | 1.58        |
|    mean velocity z | 3.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.46e+04   |
| time/              |             |
|    total_timesteps | 1356500     |
------------------------------------
Eval num_timesteps=1357000, episode_reward=-90593.23 +/- 33393.92
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.571133 |
|    mean velocity x | -0.378    |
|    mean velocity y | 1.02      |
|    mean velocity z | 4.96      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.06e+04 |
| time/              |           |
|    total_timesteps | 1357000   |
----------------------------------
Eval num_timesteps=1357500, episode_reward=-111884.78 +/- 6454.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40458262 |
|    mean velocity x | -0.415      |
|    mean velocity y | 0.481       |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 1357500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 663     |
|    time_elapsed    | 54734   |
|    total_timesteps | 1357824 |
--------------------------------
Eval num_timesteps=1358000, episode_reward=-72630.99 +/- 32345.84
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5494846    |
|    mean velocity x      | -0.089        |
|    mean velocity y      | 1.53          |
|    mean velocity z      | 4.24          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.26e+04     |
| time/                   |               |
|    total_timesteps      | 1358000       |
| train/                  |               |
|    approx_kl            | 1.9287108e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.263         |
|    learning_rate        | 0.001         |
|    loss                 | 2.98e+07      |
|    n_updates            | 6630          |
|    policy_gradient_loss | -0.000427     |
|    std                  | 1.56          |
|    value_loss           | 7.57e+07      |
-------------------------------------------
Eval num_timesteps=1358500, episode_reward=-61221.18 +/- 35267.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40390745 |
|    mean velocity x | -0.341      |
|    mean velocity y | 0.657       |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.12e+04   |
| time/              |             |
|    total_timesteps | 1358500     |
------------------------------------
Eval num_timesteps=1359000, episode_reward=-70514.93 +/- 36863.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42339668 |
|    mean velocity x | -0.138      |
|    mean velocity y | 0.755       |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 1359000     |
------------------------------------
Eval num_timesteps=1359500, episode_reward=-85599.40 +/- 39449.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49105892 |
|    mean velocity x | -0.624      |
|    mean velocity y | 0.0242      |
|    mean velocity z | 6.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 1359500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 664     |
|    time_elapsed    | 54814   |
|    total_timesteps | 1359872 |
--------------------------------
Eval num_timesteps=1360000, episode_reward=-94494.45 +/- 23201.26
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.42990336 |
|    mean velocity x      | 0.252       |
|    mean velocity y      | 1.37        |
|    mean velocity z      | 3.71        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.45e+04   |
| time/                   |             |
|    total_timesteps      | 1360000     |
| train/                  |             |
|    approx_kl            | 8.69316e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.001       |
|    loss                 | 2.01e+07    |
|    n_updates            | 6640        |
|    policy_gradient_loss | -0.000409   |
|    std                  | 1.56        |
|    value_loss           | 6.96e+07    |
-----------------------------------------
Eval num_timesteps=1360500, episode_reward=-65518.96 +/- 38589.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4926713 |
|    mean velocity x | -0.169     |
|    mean velocity y | 1.35       |
|    mean velocity z | 3.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.55e+04  |
| time/              |            |
|    total_timesteps | 1360500    |
-----------------------------------
Eval num_timesteps=1361000, episode_reward=-86469.26 +/- 13048.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46856806 |
|    mean velocity x | -0.883      |
|    mean velocity y | 0.326       |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.65e+04   |
| time/              |             |
|    total_timesteps | 1361000     |
------------------------------------
Eval num_timesteps=1361500, episode_reward=-90820.17 +/- 50029.57
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.535575 |
|    mean velocity x | -0.401    |
|    mean velocity y | 1.06      |
|    mean velocity z | 4.37      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.08e+04 |
| time/              |           |
|    total_timesteps | 1361500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 665     |
|    time_elapsed    | 54894   |
|    total_timesteps | 1361920 |
--------------------------------
Eval num_timesteps=1362000, episode_reward=-115042.17 +/- 17579.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.25263566  |
|    mean velocity x      | 0.224        |
|    mean velocity y      | 0.245        |
|    mean velocity z      | 1.05         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.15e+05    |
| time/                   |              |
|    total_timesteps      | 1362000      |
| train/                  |              |
|    approx_kl            | 0.0002723322 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.001        |
|    loss                 | 5.92e+07     |
|    n_updates            | 6650         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.56         |
|    value_loss           | 4.92e+07     |
------------------------------------------
Eval num_timesteps=1362500, episode_reward=-66039.83 +/- 38228.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44735876 |
|    mean velocity x | 0.0301      |
|    mean velocity y | 0.787       |
|    mean velocity z | 1.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.6e+04    |
| time/              |             |
|    total_timesteps | 1362500     |
------------------------------------
Eval num_timesteps=1363000, episode_reward=-116361.87 +/- 24970.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31845066 |
|    mean velocity x | -0.105      |
|    mean velocity y | 0.469       |
|    mean velocity z | 0.633       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 1363000     |
------------------------------------
Eval num_timesteps=1363500, episode_reward=-65715.29 +/- 29955.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37599292 |
|    mean velocity x | 0.166       |
|    mean velocity y | 1.31        |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.57e+04   |
| time/              |             |
|    total_timesteps | 1363500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 666     |
|    time_elapsed    | 54975   |
|    total_timesteps | 1363968 |
--------------------------------
Eval num_timesteps=1364000, episode_reward=-85101.15 +/- 14923.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.53044516   |
|    mean velocity x      | 0.132         |
|    mean velocity y      | 1.28          |
|    mean velocity z      | 3.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.51e+04     |
| time/                   |               |
|    total_timesteps      | 1364000       |
| train/                  |               |
|    approx_kl            | 0.00022130439 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.282         |
|    learning_rate        | 0.001         |
|    loss                 | 9.93e+06      |
|    n_updates            | 6660          |
|    policy_gradient_loss | -0.00129      |
|    std                  | 1.56          |
|    value_loss           | 3.72e+07      |
-------------------------------------------
Eval num_timesteps=1364500, episode_reward=-72821.06 +/- 39365.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25399572 |
|    mean velocity x | -0.738      |
|    mean velocity y | -0.32       |
|    mean velocity z | 2.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 1364500     |
------------------------------------
Eval num_timesteps=1365000, episode_reward=-87773.95 +/- 18574.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51686317 |
|    mean velocity x | 0.543       |
|    mean velocity y | 1.66        |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 1365000     |
------------------------------------
Eval num_timesteps=1365500, episode_reward=-66127.70 +/- 46027.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41793793 |
|    mean velocity x | -0.226      |
|    mean velocity y | 0.572       |
|    mean velocity z | 4.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.61e+04   |
| time/              |             |
|    total_timesteps | 1365500     |
------------------------------------
Eval num_timesteps=1366000, episode_reward=-89922.42 +/- 25472.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4309181 |
|    mean velocity x | -1.1       |
|    mean velocity y | 0.292      |
|    mean velocity z | 3.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.99e+04  |
| time/              |            |
|    total_timesteps | 1366000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 667     |
|    time_elapsed    | 55074   |
|    total_timesteps | 1366016 |
--------------------------------
Eval num_timesteps=1366500, episode_reward=-99105.32 +/- 53019.23
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24166925  |
|    mean velocity x      | -0.0633      |
|    mean velocity y      | 0.559        |
|    mean velocity z      | 0.357        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.91e+04    |
| time/                   |              |
|    total_timesteps      | 1366500      |
| train/                  |              |
|    approx_kl            | 9.614261e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.001        |
|    loss                 | 1.94e+07     |
|    n_updates            | 6670         |
|    policy_gradient_loss | -0.000984    |
|    std                  | 1.56         |
|    value_loss           | 5.02e+07     |
------------------------------------------
Eval num_timesteps=1367000, episode_reward=-47034.94 +/- 37184.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3283966 |
|    mean velocity x | -0.108     |
|    mean velocity y | 0.437      |
|    mean velocity z | 1.96       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.7e+04   |
| time/              |            |
|    total_timesteps | 1367000    |
-----------------------------------
Eval num_timesteps=1367500, episode_reward=-100986.24 +/- 26044.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47399184 |
|    mean velocity x | -0.205      |
|    mean velocity y | 1.22        |
|    mean velocity z | 4.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1367500     |
------------------------------------
Eval num_timesteps=1368000, episode_reward=-107174.13 +/- 17202.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2328065 |
|    mean velocity x | 0.00912    |
|    mean velocity y | 0.384      |
|    mean velocity z | 0.532      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 1368000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 668     |
|    time_elapsed    | 55155   |
|    total_timesteps | 1368064 |
--------------------------------
Eval num_timesteps=1368500, episode_reward=-75159.12 +/- 49914.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40707934   |
|    mean velocity x      | -1.27         |
|    mean velocity y      | -0.833        |
|    mean velocity z      | 6.03          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.52e+04     |
| time/                   |               |
|    total_timesteps      | 1368500       |
| train/                  |               |
|    approx_kl            | 0.00010358944 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.222         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+07       |
|    n_updates            | 6680          |
|    policy_gradient_loss | -0.000663     |
|    std                  | 1.56          |
|    value_loss           | 4.73e+07      |
-------------------------------------------
Eval num_timesteps=1369000, episode_reward=-64694.58 +/- 37192.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37037078 |
|    mean velocity x | -0.665      |
|    mean velocity y | 0.109       |
|    mean velocity z | 3.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.47e+04   |
| time/              |             |
|    total_timesteps | 1369000     |
------------------------------------
Eval num_timesteps=1369500, episode_reward=-94591.26 +/- 46589.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39033943 |
|    mean velocity x | -0.487      |
|    mean velocity y | 0.106       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.46e+04   |
| time/              |             |
|    total_timesteps | 1369500     |
------------------------------------
Eval num_timesteps=1370000, episode_reward=-75972.77 +/- 35272.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4637705 |
|    mean velocity x | -0.168     |
|    mean velocity y | 0.911      |
|    mean velocity z | 4.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.6e+04   |
| time/              |            |
|    total_timesteps | 1370000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 669     |
|    time_elapsed    | 55235   |
|    total_timesteps | 1370112 |
--------------------------------
Eval num_timesteps=1370500, episode_reward=-78935.64 +/- 44467.68
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4944692    |
|    mean velocity x      | 0.351         |
|    mean velocity y      | 1.6           |
|    mean velocity z      | 3.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.89e+04     |
| time/                   |               |
|    total_timesteps      | 1370500       |
| train/                  |               |
|    approx_kl            | 5.7142664e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.27          |
|    learning_rate        | 0.001         |
|    loss                 | 6.18e+07      |
|    n_updates            | 6690          |
|    policy_gradient_loss | -0.000608     |
|    std                  | 1.56          |
|    value_loss           | 7.83e+07      |
-------------------------------------------
Eval num_timesteps=1371000, episode_reward=-89652.05 +/- 28727.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6431744 |
|    mean velocity x | 0.52       |
|    mean velocity y | 2.32       |
|    mean velocity z | 4.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.97e+04  |
| time/              |            |
|    total_timesteps | 1371000    |
-----------------------------------
Eval num_timesteps=1371500, episode_reward=-86328.64 +/- 20552.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40107977 |
|    mean velocity x | -0.0752     |
|    mean velocity y | 0.82        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.63e+04   |
| time/              |             |
|    total_timesteps | 1371500     |
------------------------------------
Eval num_timesteps=1372000, episode_reward=-71424.27 +/- 38399.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48221004 |
|    mean velocity x | 0.408       |
|    mean velocity y | 1.34        |
|    mean velocity z | 3.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.14e+04   |
| time/              |             |
|    total_timesteps | 1372000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 670     |
|    time_elapsed    | 55315   |
|    total_timesteps | 1372160 |
--------------------------------
Eval num_timesteps=1372500, episode_reward=-62645.46 +/- 39090.61
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45841816   |
|    mean velocity x      | -0.806        |
|    mean velocity y      | 0.705         |
|    mean velocity z      | 2.62          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.26e+04     |
| time/                   |               |
|    total_timesteps      | 1372500       |
| train/                  |               |
|    approx_kl            | 3.4832512e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.302         |
|    learning_rate        | 0.001         |
|    loss                 | 5.59e+06      |
|    n_updates            | 6700          |
|    policy_gradient_loss | -0.000536     |
|    std                  | 1.56          |
|    value_loss           | 4.74e+07      |
-------------------------------------------
Eval num_timesteps=1373000, episode_reward=-90475.23 +/- 22727.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5650748 |
|    mean velocity x | 0.261      |
|    mean velocity y | 1.61       |
|    mean velocity z | 3.85       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.05e+04  |
| time/              |            |
|    total_timesteps | 1373000    |
-----------------------------------
Eval num_timesteps=1373500, episode_reward=-87185.03 +/- 40250.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.69047475 |
|    mean velocity x | 0.321       |
|    mean velocity y | 2.6         |
|    mean velocity z | 5.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.72e+04   |
| time/              |             |
|    total_timesteps | 1373500     |
------------------------------------
Eval num_timesteps=1374000, episode_reward=-104958.17 +/- 20143.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40499824 |
|    mean velocity x | 0.224       |
|    mean velocity y | 0.713       |
|    mean velocity z | 2.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1374000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 671     |
|    time_elapsed    | 55396   |
|    total_timesteps | 1374208 |
--------------------------------
Eval num_timesteps=1374500, episode_reward=-88988.39 +/- 22694.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.29029405   |
|    mean velocity x      | 0.226         |
|    mean velocity y      | 0.803         |
|    mean velocity z      | 3.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.9e+04      |
| time/                   |               |
|    total_timesteps      | 1374500       |
| train/                  |               |
|    approx_kl            | 1.1990196e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.358         |
|    learning_rate        | 0.001         |
|    loss                 | 2.86e+07      |
|    n_updates            | 6710          |
|    policy_gradient_loss | -0.000333     |
|    std                  | 1.56          |
|    value_loss           | 4.27e+07      |
-------------------------------------------
Eval num_timesteps=1375000, episode_reward=-56320.62 +/- 28572.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4888287 |
|    mean velocity x | -1.01      |
|    mean velocity y | 0.307      |
|    mean velocity z | 4.44       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.63e+04  |
| time/              |            |
|    total_timesteps | 1375000    |
-----------------------------------
Eval num_timesteps=1375500, episode_reward=-56300.48 +/- 41741.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4944958 |
|    mean velocity x | -0.0346    |
|    mean velocity y | 1.62       |
|    mean velocity z | 3.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.63e+04  |
| time/              |            |
|    total_timesteps | 1375500    |
-----------------------------------
Eval num_timesteps=1376000, episode_reward=-76184.74 +/- 47489.51
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5475078 |
|    mean velocity x | 0.115      |
|    mean velocity y | 0.969      |
|    mean velocity z | 2.62       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.62e+04  |
| time/              |            |
|    total_timesteps | 1376000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 672     |
|    time_elapsed    | 55476   |
|    total_timesteps | 1376256 |
--------------------------------
Eval num_timesteps=1376500, episode_reward=-92681.82 +/- 36670.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5237795   |
|    mean velocity x      | -0.442       |
|    mean velocity y      | 0.908        |
|    mean velocity z      | 4.6          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.27e+04    |
| time/                   |              |
|    total_timesteps      | 1376500      |
| train/                  |              |
|    approx_kl            | 1.975926e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+07        |
|    n_updates            | 6720         |
|    policy_gradient_loss | -0.000342    |
|    std                  | 1.56         |
|    value_loss           | 5.78e+07     |
------------------------------------------
Eval num_timesteps=1377000, episode_reward=-78699.69 +/- 40337.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44291228 |
|    mean velocity x | 0.0222      |
|    mean velocity y | 1.5         |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.87e+04   |
| time/              |             |
|    total_timesteps | 1377000     |
------------------------------------
Eval num_timesteps=1377500, episode_reward=-32234.43 +/- 39207.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45272288 |
|    mean velocity x | 0.0437      |
|    mean velocity y | 1.41        |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.22e+04   |
| time/              |             |
|    total_timesteps | 1377500     |
------------------------------------
Eval num_timesteps=1378000, episode_reward=-83655.20 +/- 50872.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45289972 |
|    mean velocity x | -0.18       |
|    mean velocity y | 1.4         |
|    mean velocity z | 4.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 1378000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 673     |
|    time_elapsed    | 55556   |
|    total_timesteps | 1378304 |
--------------------------------
Eval num_timesteps=1378500, episode_reward=-92369.22 +/- 47058.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.52883124   |
|    mean velocity x      | -0.446        |
|    mean velocity y      | 0.885         |
|    mean velocity z      | 4.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.24e+04     |
| time/                   |               |
|    total_timesteps      | 1378500       |
| train/                  |               |
|    approx_kl            | 4.7340174e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.219         |
|    learning_rate        | 0.001         |
|    loss                 | 6.58e+07      |
|    n_updates            | 6730          |
|    policy_gradient_loss | -0.000593     |
|    std                  | 1.56          |
|    value_loss           | 1.02e+08      |
-------------------------------------------
Eval num_timesteps=1379000, episode_reward=-106711.16 +/- 20593.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50746614 |
|    mean velocity x | 0.532       |
|    mean velocity y | 1.83        |
|    mean velocity z | 3.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1379000     |
------------------------------------
Eval num_timesteps=1379500, episode_reward=-57869.87 +/- 35433.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3056716 |
|    mean velocity x | -0.183     |
|    mean velocity y | 0.453      |
|    mean velocity z | 4.4        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.79e+04  |
| time/              |            |
|    total_timesteps | 1379500    |
-----------------------------------
Eval num_timesteps=1380000, episode_reward=-68948.88 +/- 35458.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4520122 |
|    mean velocity x | 0.011      |
|    mean velocity y | 1.36       |
|    mean velocity z | 3.78       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.89e+04  |
| time/              |            |
|    total_timesteps | 1380000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 674     |
|    time_elapsed    | 55637   |
|    total_timesteps | 1380352 |
--------------------------------
Eval num_timesteps=1380500, episode_reward=-54037.89 +/- 49054.79
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.11516236   |
|    mean velocity x      | -0.257        |
|    mean velocity y      | 0.24          |
|    mean velocity z      | 0.258         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.4e+04      |
| time/                   |               |
|    total_timesteps      | 1380500       |
| train/                  |               |
|    approx_kl            | 3.2870885e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+06      |
|    n_updates            | 6740          |
|    policy_gradient_loss | -0.00051      |
|    std                  | 1.56          |
|    value_loss           | 4.95e+07      |
-------------------------------------------
Eval num_timesteps=1381000, episode_reward=-63199.97 +/- 36724.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4015157 |
|    mean velocity x | -0.267     |
|    mean velocity y | 0.824      |
|    mean velocity z | 4.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.32e+04  |
| time/              |            |
|    total_timesteps | 1381000    |
-----------------------------------
Eval num_timesteps=1381500, episode_reward=-100796.09 +/- 28496.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34966543 |
|    mean velocity x | -0.0662     |
|    mean velocity y | 0.858       |
|    mean velocity z | 4.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1381500     |
------------------------------------
Eval num_timesteps=1382000, episode_reward=-68644.73 +/- 46879.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5552096 |
|    mean velocity x | -0.22      |
|    mean velocity y | 1.72       |
|    mean velocity z | 4.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.86e+04  |
| time/              |            |
|    total_timesteps | 1382000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 675     |
|    time_elapsed    | 55717   |
|    total_timesteps | 1382400 |
--------------------------------
Eval num_timesteps=1382500, episode_reward=-64964.73 +/- 46102.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.21748802   |
|    mean velocity x      | -0.571        |
|    mean velocity y      | 0.574         |
|    mean velocity z      | 1.16          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.5e+04      |
| time/                   |               |
|    total_timesteps      | 1382500       |
| train/                  |               |
|    approx_kl            | 2.2687484e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.001         |
|    loss                 | 5.36e+07      |
|    n_updates            | 6750          |
|    policy_gradient_loss | -0.000577     |
|    std                  | 1.56          |
|    value_loss           | 9.08e+07      |
-------------------------------------------
Eval num_timesteps=1383000, episode_reward=-25273.06 +/- 41049.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38332722 |
|    mean velocity x | 0.151       |
|    mean velocity y | 0.927       |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -2.53e+04   |
| time/              |             |
|    total_timesteps | 1383000     |
------------------------------------
Eval num_timesteps=1383500, episode_reward=-76711.51 +/- 26499.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42591476 |
|    mean velocity x | -0.427      |
|    mean velocity y | 0.858       |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.67e+04   |
| time/              |             |
|    total_timesteps | 1383500     |
------------------------------------
Eval num_timesteps=1384000, episode_reward=-70411.94 +/- 51005.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37545753 |
|    mean velocity x | -0.818      |
|    mean velocity y | -0.473      |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.04e+04   |
| time/              |             |
|    total_timesteps | 1384000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 676     |
|    time_elapsed    | 55797   |
|    total_timesteps | 1384448 |
--------------------------------
Eval num_timesteps=1384500, episode_reward=-111337.59 +/- 27962.10
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37452394   |
|    mean velocity x      | -0.275        |
|    mean velocity y      | 0.539         |
|    mean velocity z      | 0.664         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.11e+05     |
| time/                   |               |
|    total_timesteps      | 1384500       |
| train/                  |               |
|    approx_kl            | 1.9873522e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+07      |
|    n_updates            | 6760          |
|    policy_gradient_loss | -0.000299     |
|    std                  | 1.56          |
|    value_loss           | 3.31e+07      |
-------------------------------------------
Eval num_timesteps=1385000, episode_reward=-61723.30 +/- 46635.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49184117 |
|    mean velocity x | 0.0581      |
|    mean velocity y | 1.04        |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+04   |
| time/              |             |
|    total_timesteps | 1385000     |
------------------------------------
Eval num_timesteps=1385500, episode_reward=-90941.14 +/- 22578.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43719175 |
|    mean velocity x | -0.304      |
|    mean velocity y | 0.729       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 1385500     |
------------------------------------
Eval num_timesteps=1386000, episode_reward=-68697.66 +/- 40467.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47974023 |
|    mean velocity x | -0.245      |
|    mean velocity y | 0.758       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.87e+04   |
| time/              |             |
|    total_timesteps | 1386000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 677     |
|    time_elapsed    | 55878   |
|    total_timesteps | 1386496 |
--------------------------------
Eval num_timesteps=1386500, episode_reward=-100579.00 +/- 12803.44
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38520923  |
|    mean velocity x      | -0.371       |
|    mean velocity y      | 0.64         |
|    mean velocity z      | 2.98         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.01e+05    |
| time/                   |              |
|    total_timesteps      | 1386500      |
| train/                  |              |
|    approx_kl            | 0.0001292078 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.201        |
|    learning_rate        | 0.001        |
|    loss                 | 7.42e+07     |
|    n_updates            | 6770         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.56         |
|    value_loss           | 7.53e+07     |
------------------------------------------
Eval num_timesteps=1387000, episode_reward=-115529.41 +/- 17081.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3310769 |
|    mean velocity x | 0.254      |
|    mean velocity y | 0.871      |
|    mean velocity z | 2.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.16e+05  |
| time/              |            |
|    total_timesteps | 1387000    |
-----------------------------------
Eval num_timesteps=1387500, episode_reward=-101867.37 +/- 27512.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38226864 |
|    mean velocity x | -0.171      |
|    mean velocity y | 0.555       |
|    mean velocity z | 4.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1387500     |
------------------------------------
Eval num_timesteps=1388000, episode_reward=-86675.50 +/- 43427.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19095623 |
|    mean velocity x | -0.0631     |
|    mean velocity y | 0.443       |
|    mean velocity z | 0.318       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.67e+04   |
| time/              |             |
|    total_timesteps | 1388000     |
------------------------------------
Eval num_timesteps=1388500, episode_reward=-71145.11 +/- 36009.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51188457 |
|    mean velocity x | -0.392      |
|    mean velocity y | 1.09        |
|    mean velocity z | 2.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.11e+04   |
| time/              |             |
|    total_timesteps | 1388500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 678     |
|    time_elapsed    | 55977   |
|    total_timesteps | 1388544 |
--------------------------------
Eval num_timesteps=1389000, episode_reward=-85674.21 +/- 33385.71
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43518126   |
|    mean velocity x      | -0.344        |
|    mean velocity y      | 0.847         |
|    mean velocity z      | 3.32          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.57e+04     |
| time/                   |               |
|    total_timesteps      | 1389000       |
| train/                  |               |
|    approx_kl            | 1.2415461e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.216         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+07      |
|    n_updates            | 6780          |
|    policy_gradient_loss | -0.000337     |
|    std                  | 1.56          |
|    value_loss           | 4.95e+07      |
-------------------------------------------
Eval num_timesteps=1389500, episode_reward=-65207.84 +/- 41132.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50792086 |
|    mean velocity x | -0.132      |
|    mean velocity y | 1.47        |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.52e+04   |
| time/              |             |
|    total_timesteps | 1389500     |
------------------------------------
Eval num_timesteps=1390000, episode_reward=-89063.90 +/- 31438.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46347418 |
|    mean velocity x | -0.227      |
|    mean velocity y | 1.42        |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.91e+04   |
| time/              |             |
|    total_timesteps | 1390000     |
------------------------------------
Eval num_timesteps=1390500, episode_reward=-88343.93 +/- 17634.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2986337 |
|    mean velocity x | -1.43      |
|    mean velocity y | -0.763     |
|    mean velocity z | 4.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.83e+04  |
| time/              |            |
|    total_timesteps | 1390500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 679     |
|    time_elapsed    | 56057   |
|    total_timesteps | 1390592 |
--------------------------------
Eval num_timesteps=1391000, episode_reward=-92448.41 +/- 50815.16
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43544856  |
|    mean velocity x      | -1.33        |
|    mean velocity y      | -0.531       |
|    mean velocity z      | 5.45         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.24e+04    |
| time/                   |              |
|    total_timesteps      | 1391000      |
| train/                  |              |
|    approx_kl            | 1.765677e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.357        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+07     |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.000357    |
|    std                  | 1.56         |
|    value_loss           | 4.29e+07     |
------------------------------------------
Eval num_timesteps=1391500, episode_reward=-75466.06 +/- 39459.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45899945 |
|    mean velocity x | -0.118      |
|    mean velocity y | 1.38        |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 1391500     |
------------------------------------
Eval num_timesteps=1392000, episode_reward=-81475.38 +/- 23657.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33480358 |
|    mean velocity x | -0.246      |
|    mean velocity y | 0.519       |
|    mean velocity z | 1.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.15e+04   |
| time/              |             |
|    total_timesteps | 1392000     |
------------------------------------
Eval num_timesteps=1392500, episode_reward=-84902.96 +/- 42263.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3456766 |
|    mean velocity x | -0.211     |
|    mean velocity y | 0.72       |
|    mean velocity z | 1.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.49e+04  |
| time/              |            |
|    total_timesteps | 1392500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 680     |
|    time_elapsed    | 56138   |
|    total_timesteps | 1392640 |
--------------------------------
Eval num_timesteps=1393000, episode_reward=-87467.80 +/- 27119.80
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34975603   |
|    mean velocity x      | 0.0236        |
|    mean velocity y      | 0.959         |
|    mean velocity z      | 4.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.75e+04     |
| time/                   |               |
|    total_timesteps      | 1393000       |
| train/                  |               |
|    approx_kl            | 0.00011981424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.001         |
|    loss                 | 2.59e+07      |
|    n_updates            | 6800          |
|    policy_gradient_loss | -0.000782     |
|    std                  | 1.56          |
|    value_loss           | 6.2e+07       |
-------------------------------------------
Eval num_timesteps=1393500, episode_reward=-54866.41 +/- 41618.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4510776 |
|    mean velocity x | -0.388     |
|    mean velocity y | 0.937      |
|    mean velocity z | 2.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.49e+04  |
| time/              |            |
|    total_timesteps | 1393500    |
-----------------------------------
Eval num_timesteps=1394000, episode_reward=-51488.50 +/- 17109.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24298503 |
|    mean velocity x | -1.37       |
|    mean velocity y | -0.453      |
|    mean velocity z | 3.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.15e+04   |
| time/              |             |
|    total_timesteps | 1394000     |
------------------------------------
Eval num_timesteps=1394500, episode_reward=-81045.52 +/- 32646.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4591546 |
|    mean velocity x | 0.595      |
|    mean velocity y | 1.58       |
|    mean velocity z | 3.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 1394500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 681     |
|    time_elapsed    | 56218   |
|    total_timesteps | 1394688 |
--------------------------------
Eval num_timesteps=1395000, episode_reward=-76480.60 +/- 41428.12
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.7379569    |
|    mean velocity x      | 0.379         |
|    mean velocity y      | 2.68          |
|    mean velocity z      | 4.98          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.65e+04     |
| time/                   |               |
|    total_timesteps      | 1395000       |
| train/                  |               |
|    approx_kl            | 3.1555246e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.518         |
|    learning_rate        | 0.001         |
|    loss                 | 3.84e+06      |
|    n_updates            | 6810          |
|    policy_gradient_loss | -0.000348     |
|    std                  | 1.56          |
|    value_loss           | 1.71e+07      |
-------------------------------------------
Eval num_timesteps=1395500, episode_reward=-98447.36 +/- 14140.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38458562 |
|    mean velocity x | -0.127      |
|    mean velocity y | 1.27        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.84e+04   |
| time/              |             |
|    total_timesteps | 1395500     |
------------------------------------
Eval num_timesteps=1396000, episode_reward=-48186.78 +/- 40269.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47556245 |
|    mean velocity x | -0.365      |
|    mean velocity y | 0.683       |
|    mean velocity z | 3.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.82e+04   |
| time/              |             |
|    total_timesteps | 1396000     |
------------------------------------
Eval num_timesteps=1396500, episode_reward=-100260.42 +/- 31785.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2937604 |
|    mean velocity x | -1.67      |
|    mean velocity y | -1.22      |
|    mean velocity z | 5.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1e+05     |
| time/              |            |
|    total_timesteps | 1396500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 682     |
|    time_elapsed    | 56298   |
|    total_timesteps | 1396736 |
--------------------------------
Eval num_timesteps=1397000, episode_reward=-66715.49 +/- 39004.19
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4671681    |
|    mean velocity x      | 0.327         |
|    mean velocity y      | 1.58          |
|    mean velocity z      | 3.75          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.67e+04     |
| time/                   |               |
|    total_timesteps      | 1397000       |
| train/                  |               |
|    approx_kl            | 5.1058916e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.299         |
|    learning_rate        | 0.001         |
|    loss                 | 3.79e+07      |
|    n_updates            | 6820          |
|    policy_gradient_loss | -0.000418     |
|    std                  | 1.56          |
|    value_loss           | 6.9e+07       |
-------------------------------------------
Eval num_timesteps=1397500, episode_reward=-82290.59 +/- 19302.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43331724 |
|    mean velocity x | -0.871      |
|    mean velocity y | -0.0554     |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1397500     |
------------------------------------
Eval num_timesteps=1398000, episode_reward=-51999.28 +/- 42364.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37051603 |
|    mean velocity x | -0.00157    |
|    mean velocity y | 0.893       |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.2e+04    |
| time/              |             |
|    total_timesteps | 1398000     |
------------------------------------
Eval num_timesteps=1398500, episode_reward=-85578.86 +/- 28115.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20812689 |
|    mean velocity x | -0.252      |
|    mean velocity y | 0.371       |
|    mean velocity z | 0.479       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 1398500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 683     |
|    time_elapsed    | 56379   |
|    total_timesteps | 1398784 |
--------------------------------
Eval num_timesteps=1399000, episode_reward=-68703.45 +/- 35290.30
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46693456   |
|    mean velocity x      | 0.139         |
|    mean velocity y      | 1.55          |
|    mean velocity z      | 3.8           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.87e+04     |
| time/                   |               |
|    total_timesteps      | 1399000       |
| train/                  |               |
|    approx_kl            | 1.8016755e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.001         |
|    loss                 | 7.41e+06      |
|    n_updates            | 6830          |
|    policy_gradient_loss | -0.000337     |
|    std                  | 1.56          |
|    value_loss           | 5.13e+07      |
-------------------------------------------
Eval num_timesteps=1399500, episode_reward=-56889.85 +/- 40036.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43959603 |
|    mean velocity x | -0.205      |
|    mean velocity y | 1.22        |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 1399500     |
------------------------------------
Eval num_timesteps=1400000, episode_reward=-105070.36 +/- 46297.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24630548 |
|    mean velocity x | -0.261      |
|    mean velocity y | 0.376       |
|    mean velocity z | 0.427       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1400000     |
------------------------------------
Eval num_timesteps=1400500, episode_reward=-80580.19 +/- 39085.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.276188 |
|    mean velocity x | -0.192    |
|    mean velocity y | 0.417     |
|    mean velocity z | 0.785     |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.06e+04 |
| time/              |           |
|    total_timesteps | 1400500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 684     |
|    time_elapsed    | 56459   |
|    total_timesteps | 1400832 |
--------------------------------
Eval num_timesteps=1401000, episode_reward=-43006.15 +/- 31041.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.50486726   |
|    mean velocity x      | -0.143        |
|    mean velocity y      | 1.67          |
|    mean velocity z      | 4.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.3e+04      |
| time/                   |               |
|    total_timesteps      | 1401000       |
| train/                  |               |
|    approx_kl            | 4.0059676e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.001         |
|    loss                 | 2.29e+07      |
|    n_updates            | 6840          |
|    policy_gradient_loss | -0.0002       |
|    std                  | 1.56          |
|    value_loss           | 3.96e+07      |
-------------------------------------------
Eval num_timesteps=1401500, episode_reward=-68417.82 +/- 53575.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4347109 |
|    mean velocity x | -1.04      |
|    mean velocity y | 0.182      |
|    mean velocity z | 3.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.84e+04  |
| time/              |            |
|    total_timesteps | 1401500    |
-----------------------------------
Eval num_timesteps=1402000, episode_reward=-39799.89 +/- 47006.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4090619 |
|    mean velocity x | 0.394      |
|    mean velocity y | 0.881      |
|    mean velocity z | 2.98       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.98e+04  |
| time/              |            |
|    total_timesteps | 1402000    |
-----------------------------------
Eval num_timesteps=1402500, episode_reward=-82209.69 +/- 32218.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59261626 |
|    mean velocity x | -0.619      |
|    mean velocity y | 0.833       |
|    mean velocity z | 5.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.22e+04   |
| time/              |             |
|    total_timesteps | 1402500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 685     |
|    time_elapsed    | 56539   |
|    total_timesteps | 1402880 |
--------------------------------
Eval num_timesteps=1403000, episode_reward=-94358.31 +/- 38591.87
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48359016  |
|    mean velocity x      | -0.333       |
|    mean velocity y      | 0.925        |
|    mean velocity z      | 4.55         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.44e+04    |
| time/                   |              |
|    total_timesteps      | 1403000      |
| train/                  |              |
|    approx_kl            | 5.713661e-05 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+07     |
|    n_updates            | 6850         |
|    policy_gradient_loss | -0.00081     |
|    std                  | 1.56         |
|    value_loss           | 6.83e+07     |
------------------------------------------
Eval num_timesteps=1403500, episode_reward=-88522.95 +/- 17770.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17659849 |
|    mean velocity x | -0.539      |
|    mean velocity y | 0.496       |
|    mean velocity z | 1.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 1403500     |
------------------------------------
Eval num_timesteps=1404000, episode_reward=-111615.65 +/- 20466.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3136769 |
|    mean velocity x | -0.338     |
|    mean velocity y | 0.489      |
|    mean velocity z | 0.798      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 1404000    |
-----------------------------------
Eval num_timesteps=1404500, episode_reward=-61304.64 +/- 50743.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4147842 |
|    mean velocity x | -0.64      |
|    mean velocity y | 0.807      |
|    mean velocity z | 3.01       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.13e+04  |
| time/              |            |
|    total_timesteps | 1404500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 686     |
|    time_elapsed    | 56619   |
|    total_timesteps | 1404928 |
--------------------------------
Eval num_timesteps=1405000, episode_reward=-112370.19 +/- 63886.34
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4458003    |
|    mean velocity x      | -0.394        |
|    mean velocity y      | 0.935         |
|    mean velocity z      | 1.16          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.12e+05     |
| time/                   |               |
|    total_timesteps      | 1405000       |
| train/                  |               |
|    approx_kl            | 3.8369064e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.282         |
|    learning_rate        | 0.001         |
|    loss                 | 8.35e+05      |
|    n_updates            | 6860          |
|    policy_gradient_loss | -0.00042      |
|    std                  | 1.56          |
|    value_loss           | 8.7e+06       |
-------------------------------------------
Eval num_timesteps=1405500, episode_reward=-86141.17 +/- 27750.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22442597 |
|    mean velocity x | -0.233      |
|    mean velocity y | 0.533       |
|    mean velocity z | 0.605       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.61e+04   |
| time/              |             |
|    total_timesteps | 1405500     |
------------------------------------
Eval num_timesteps=1406000, episode_reward=-88588.48 +/- 37061.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34164283 |
|    mean velocity x | 0.221       |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.86e+04   |
| time/              |             |
|    total_timesteps | 1406000     |
------------------------------------
Eval num_timesteps=1406500, episode_reward=-102904.60 +/- 17763.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3571563 |
|    mean velocity x | -0.37      |
|    mean velocity y | 0.802      |
|    mean velocity z | 0.789      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1406500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 687     |
|    time_elapsed    | 56700   |
|    total_timesteps | 1406976 |
--------------------------------
Eval num_timesteps=1407000, episode_reward=-94401.61 +/- 14861.18
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36583158   |
|    mean velocity x      | -0.283        |
|    mean velocity y      | 0.516         |
|    mean velocity z      | 1.28          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.44e+04     |
| time/                   |               |
|    total_timesteps      | 1407000       |
| train/                  |               |
|    approx_kl            | 0.00031971678 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.279         |
|    learning_rate        | 0.001         |
|    loss                 | 9.41e+06      |
|    n_updates            | 6870          |
|    policy_gradient_loss | -0.00173      |
|    std                  | 1.56          |
|    value_loss           | 1.74e+07      |
-------------------------------------------
Eval num_timesteps=1407500, episode_reward=-54721.43 +/- 31057.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5108063 |
|    mean velocity x | -0.23      |
|    mean velocity y | 0.841      |
|    mean velocity z | 4.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.47e+04  |
| time/              |            |
|    total_timesteps | 1407500    |
-----------------------------------
Eval num_timesteps=1408000, episode_reward=-54454.12 +/- 38370.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41364428 |
|    mean velocity x | -0.104      |
|    mean velocity y | 0.796       |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.45e+04   |
| time/              |             |
|    total_timesteps | 1408000     |
------------------------------------
Eval num_timesteps=1408500, episode_reward=-77690.00 +/- 46357.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42994776 |
|    mean velocity x | -0.109      |
|    mean velocity y | 0.674       |
|    mean velocity z | 2.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 1408500     |
------------------------------------
Eval num_timesteps=1409000, episode_reward=-90962.97 +/- 10279.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35265958 |
|    mean velocity x | 0.0793      |
|    mean velocity y | 0.845       |
|    mean velocity z | 3.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 1409000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 688     |
|    time_elapsed    | 56799   |
|    total_timesteps | 1409024 |
--------------------------------
Eval num_timesteps=1409500, episode_reward=-62495.60 +/- 31615.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47266728   |
|    mean velocity x      | -1.03         |
|    mean velocity y      | -0.451        |
|    mean velocity z      | 7.07          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.25e+04     |
| time/                   |               |
|    total_timesteps      | 1409500       |
| train/                  |               |
|    approx_kl            | 4.1860185e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.001         |
|    loss                 | 2.65e+07      |
|    n_updates            | 6880          |
|    policy_gradient_loss | -0.000461     |
|    std                  | 1.56          |
|    value_loss           | 7.6e+07       |
-------------------------------------------
Eval num_timesteps=1410000, episode_reward=-59151.96 +/- 40417.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5125429 |
|    mean velocity x | -0.44      |
|    mean velocity y | 0.987      |
|    mean velocity z | 4.55       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.92e+04  |
| time/              |            |
|    total_timesteps | 1410000    |
-----------------------------------
Eval num_timesteps=1410500, episode_reward=-89603.70 +/- 13039.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48788047 |
|    mean velocity x | -0.258      |
|    mean velocity y | 0.831       |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 1410500     |
------------------------------------
Eval num_timesteps=1411000, episode_reward=-80598.88 +/- 46549.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39033192 |
|    mean velocity x | -0.208      |
|    mean velocity y | 0.821       |
|    mean velocity z | 1.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 1411000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 689     |
|    time_elapsed    | 56880   |
|    total_timesteps | 1411072 |
--------------------------------
Eval num_timesteps=1411500, episode_reward=-75418.02 +/- 44984.62
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5918432    |
|    mean velocity x      | 0.743         |
|    mean velocity y      | 2.24          |
|    mean velocity z      | 4.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.54e+04     |
| time/                   |               |
|    total_timesteps      | 1411500       |
| train/                  |               |
|    approx_kl            | 1.1796859e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+07      |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.00032      |
|    std                  | 1.56          |
|    value_loss           | 7.69e+07      |
-------------------------------------------
Eval num_timesteps=1412000, episode_reward=-97889.14 +/- 38194.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33151677 |
|    mean velocity x | -0.832      |
|    mean velocity y | 0.268       |
|    mean velocity z | 2.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.79e+04   |
| time/              |             |
|    total_timesteps | 1412000     |
------------------------------------
Eval num_timesteps=1412500, episode_reward=-67861.98 +/- 30937.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40457124 |
|    mean velocity x | -0.0964     |
|    mean velocity y | 0.755       |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.79e+04   |
| time/              |             |
|    total_timesteps | 1412500     |
------------------------------------
Eval num_timesteps=1413000, episode_reward=-70594.06 +/- 48138.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43370166 |
|    mean velocity x | -1.09       |
|    mean velocity y | 0.209       |
|    mean velocity z | 2.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.06e+04   |
| time/              |             |
|    total_timesteps | 1413000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 690     |
|    time_elapsed    | 56960   |
|    total_timesteps | 1413120 |
--------------------------------
Eval num_timesteps=1413500, episode_reward=-73068.28 +/- 52242.66
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42936268   |
|    mean velocity x      | -0.144        |
|    mean velocity y      | 1.11          |
|    mean velocity z      | 3.52          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.31e+04     |
| time/                   |               |
|    total_timesteps      | 1413500       |
| train/                  |               |
|    approx_kl            | 4.6152563e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.001         |
|    loss                 | 1.3e+07       |
|    n_updates            | 6900          |
|    policy_gradient_loss | -0.000197     |
|    std                  | 1.56          |
|    value_loss           | 4.96e+07      |
-------------------------------------------
Eval num_timesteps=1414000, episode_reward=-57926.59 +/- 45969.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36717454 |
|    mean velocity x | -0.209      |
|    mean velocity y | 0.778       |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.79e+04   |
| time/              |             |
|    total_timesteps | 1414000     |
------------------------------------
Eval num_timesteps=1414500, episode_reward=-58438.34 +/- 42703.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4419385 |
|    mean velocity x | -0.26      |
|    mean velocity y | 0.834      |
|    mean velocity z | 3.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.84e+04  |
| time/              |            |
|    total_timesteps | 1414500    |
-----------------------------------
Eval num_timesteps=1415000, episode_reward=-105290.86 +/- 19977.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39726415 |
|    mean velocity x | 0.0239      |
|    mean velocity y | 1.36        |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1415000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 691     |
|    time_elapsed    | 57040   |
|    total_timesteps | 1415168 |
--------------------------------
Eval num_timesteps=1415500, episode_reward=-95155.94 +/- 29107.42
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37807658  |
|    mean velocity x      | -0.642       |
|    mean velocity y      | 0.224        |
|    mean velocity z      | 4.06         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.52e+04    |
| time/                   |              |
|    total_timesteps      | 1415500      |
| train/                  |              |
|    approx_kl            | 0.0004001617 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.001        |
|    loss                 | 3.9e+07      |
|    n_updates            | 6910         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.56         |
|    value_loss           | 6.96e+07     |
------------------------------------------
Eval num_timesteps=1416000, episode_reward=-81442.27 +/- 46722.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.05447251 |
|    mean velocity x | 0.0116     |
|    mean velocity y | -0.282     |
|    mean velocity z | 0.0597     |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.14e+04  |
| time/              |            |
|    total_timesteps | 1416000    |
-----------------------------------
Eval num_timesteps=1416500, episode_reward=-80662.04 +/- 49592.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47899148 |
|    mean velocity x | 0.307       |
|    mean velocity y | 1.22        |
|    mean velocity z | 3.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 1416500     |
------------------------------------
Eval num_timesteps=1417000, episode_reward=-81453.26 +/- 23055.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4645838 |
|    mean velocity x | -0.262     |
|    mean velocity y | 0.626      |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 1417000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 692     |
|    time_elapsed    | 57120   |
|    total_timesteps | 1417216 |
--------------------------------
Eval num_timesteps=1417500, episode_reward=-82364.84 +/- 46405.19
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5069675   |
|    mean velocity x      | -0.0645      |
|    mean velocity y      | 1.41         |
|    mean velocity z      | 3.86         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.24e+04    |
| time/                   |              |
|    total_timesteps      | 1417500      |
| train/                  |              |
|    approx_kl            | 6.069342e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.001        |
|    loss                 | 6.78e+06     |
|    n_updates            | 6920         |
|    policy_gradient_loss | -0.00028     |
|    std                  | 1.56         |
|    value_loss           | 4.47e+07     |
------------------------------------------
Eval num_timesteps=1418000, episode_reward=-51073.64 +/- 36649.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40358132 |
|    mean velocity x | -0.0241     |
|    mean velocity y | 0.885       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.11e+04   |
| time/              |             |
|    total_timesteps | 1418000     |
------------------------------------
Eval num_timesteps=1418500, episode_reward=-82555.09 +/- 41828.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47013575 |
|    mean velocity x | 0.215       |
|    mean velocity y | 1.71        |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.26e+04   |
| time/              |             |
|    total_timesteps | 1418500     |
------------------------------------
Eval num_timesteps=1419000, episode_reward=-94183.42 +/- 34275.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5844773 |
|    mean velocity x | -0.426     |
|    mean velocity y | 1.12       |
|    mean velocity z | 5.18       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.42e+04  |
| time/              |            |
|    total_timesteps | 1419000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 693     |
|    time_elapsed    | 57201   |
|    total_timesteps | 1419264 |
--------------------------------
Eval num_timesteps=1419500, episode_reward=-97680.33 +/- 38026.51
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.27429408 |
|    mean velocity x      | -0.386      |
|    mean velocity y      | 0.0852      |
|    mean velocity z      | 3.46        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.77e+04   |
| time/                   |             |
|    total_timesteps      | 1419500     |
| train/                  |             |
|    approx_kl            | 5.82129e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.001       |
|    loss                 | 4.3e+06     |
|    n_updates            | 6930        |
|    policy_gradient_loss | -0.000163   |
|    std                  | 1.56        |
|    value_loss           | 5.88e+07    |
-----------------------------------------
Eval num_timesteps=1420000, episode_reward=-75875.58 +/- 51698.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38896313 |
|    mean velocity x | -0.19       |
|    mean velocity y | 0.469       |
|    mean velocity z | 4.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 1420000     |
------------------------------------
Eval num_timesteps=1420500, episode_reward=-89420.68 +/- 27492.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5132418 |
|    mean velocity x | 0.38       |
|    mean velocity y | 1.57       |
|    mean velocity z | 3.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.94e+04  |
| time/              |            |
|    total_timesteps | 1420500    |
-----------------------------------
Eval num_timesteps=1421000, episode_reward=-105282.29 +/- 14804.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26740143 |
|    mean velocity x | -0.336      |
|    mean velocity y | 0.487       |
|    mean velocity z | 1.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1421000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 694     |
|    time_elapsed    | 57281   |
|    total_timesteps | 1421312 |
--------------------------------
Eval num_timesteps=1421500, episode_reward=-105102.76 +/- 30870.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43687296   |
|    mean velocity x      | 0.458         |
|    mean velocity y      | 1.54          |
|    mean velocity z      | 3.78          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 1421500       |
| train/                  |               |
|    approx_kl            | 4.5801076e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+07      |
|    n_updates            | 6940          |
|    policy_gradient_loss | -0.000946     |
|    std                  | 1.56          |
|    value_loss           | 3.73e+07      |
-------------------------------------------
Eval num_timesteps=1422000, episode_reward=-85712.19 +/- 52120.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38494706 |
|    mean velocity x | -0.793      |
|    mean velocity y | 0.508       |
|    mean velocity z | 3.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.57e+04   |
| time/              |             |
|    total_timesteps | 1422000     |
------------------------------------
Eval num_timesteps=1422500, episode_reward=-94202.56 +/- 28114.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5118044 |
|    mean velocity x | -0.994     |
|    mean velocity y | 0.629      |
|    mean velocity z | 3.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.42e+04  |
| time/              |            |
|    total_timesteps | 1422500    |
-----------------------------------
Eval num_timesteps=1423000, episode_reward=-62268.48 +/- 42171.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28825682 |
|    mean velocity x | -0.295      |
|    mean velocity y | 0.51        |
|    mean velocity z | 0.385       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+04   |
| time/              |             |
|    total_timesteps | 1423000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 695     |
|    time_elapsed    | 57361   |
|    total_timesteps | 1423360 |
--------------------------------
Eval num_timesteps=1423500, episode_reward=-83396.43 +/- 47966.95
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43671584   |
|    mean velocity x      | -0.238        |
|    mean velocity y      | 0.907         |
|    mean velocity z      | 3.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.34e+04     |
| time/                   |               |
|    total_timesteps      | 1423500       |
| train/                  |               |
|    approx_kl            | 2.0180276e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.31          |
|    learning_rate        | 0.001         |
|    loss                 | 2.19e+06      |
|    n_updates            | 6950          |
|    policy_gradient_loss | -0.000235     |
|    std                  | 1.56          |
|    value_loss           | 3.26e+07      |
-------------------------------------------
Eval num_timesteps=1424000, episode_reward=-80049.39 +/- 37201.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45112032 |
|    mean velocity x | -0.329      |
|    mean velocity y | 0.713       |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8e+04      |
| time/              |             |
|    total_timesteps | 1424000     |
------------------------------------
Eval num_timesteps=1424500, episode_reward=-116107.40 +/- 17999.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34113893 |
|    mean velocity x | -0.0026     |
|    mean velocity y | 0.803       |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 1424500     |
------------------------------------
Eval num_timesteps=1425000, episode_reward=-75920.40 +/- 38573.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25432366 |
|    mean velocity x | -0.751      |
|    mean velocity y | -0.323      |
|    mean velocity z | 3.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 1425000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 696     |
|    time_elapsed    | 57442   |
|    total_timesteps | 1425408 |
--------------------------------
Eval num_timesteps=1425500, episode_reward=-60410.81 +/- 37279.52
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41706732  |
|    mean velocity x      | -0.749       |
|    mean velocity y      | -0.0816      |
|    mean velocity z      | 3.38         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.04e+04    |
| time/                   |              |
|    total_timesteps      | 1425500      |
| train/                  |              |
|    approx_kl            | 4.855517e-05 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.262        |
|    learning_rate        | 0.001        |
|    loss                 | 3.85e+07     |
|    n_updates            | 6960         |
|    policy_gradient_loss | -0.000476    |
|    std                  | 1.56         |
|    value_loss           | 5.65e+07     |
------------------------------------------
Eval num_timesteps=1426000, episode_reward=-85331.08 +/- 39473.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29886726 |
|    mean velocity x | -0.151      |
|    mean velocity y | 0.704       |
|    mean velocity z | 0.747       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 1426000     |
------------------------------------
Eval num_timesteps=1426500, episode_reward=-72922.89 +/- 22677.08
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.541178 |
|    mean velocity x | 0.11      |
|    mean velocity y | 1.46      |
|    mean velocity z | 3.15      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.29e+04 |
| time/              |           |
|    total_timesteps | 1426500   |
----------------------------------
Eval num_timesteps=1427000, episode_reward=-110246.53 +/- 39645.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4791097 |
|    mean velocity x | -0.51      |
|    mean velocity y | 0.32       |
|    mean velocity z | 4.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 1427000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 697     |
|    time_elapsed    | 57522   |
|    total_timesteps | 1427456 |
--------------------------------
Eval num_timesteps=1427500, episode_reward=-78986.88 +/- 39652.77
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34444952  |
|    mean velocity x      | -0.525       |
|    mean velocity y      | 0.385        |
|    mean velocity z      | 3.18         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.9e+04     |
| time/                   |              |
|    total_timesteps      | 1427500      |
| train/                  |              |
|    approx_kl            | 4.790607e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+07     |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.000184    |
|    std                  | 1.56         |
|    value_loss           | 3.75e+07     |
------------------------------------------
Eval num_timesteps=1428000, episode_reward=-89599.84 +/- 28687.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20039997 |
|    mean velocity x | -0.167      |
|    mean velocity y | 0.543       |
|    mean velocity z | 0.441       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 1428000     |
------------------------------------
Eval num_timesteps=1428500, episode_reward=-90619.67 +/- 43812.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57148844 |
|    mean velocity x | -0.542      |
|    mean velocity y | 0.871       |
|    mean velocity z | 5.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.06e+04   |
| time/              |             |
|    total_timesteps | 1428500     |
------------------------------------
Eval num_timesteps=1429000, episode_reward=-101425.35 +/- 23666.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3984779 |
|    mean velocity x | -0.728     |
|    mean velocity y | 0.424      |
|    mean velocity z | 2.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 1429000    |
-----------------------------------
Eval num_timesteps=1429500, episode_reward=-64780.96 +/- 38985.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4893775 |
|    mean velocity x | -0.579     |
|    mean velocity y | 0.55       |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.48e+04  |
| time/              |            |
|    total_timesteps | 1429500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 698     |
|    time_elapsed    | 57622   |
|    total_timesteps | 1429504 |
--------------------------------
Eval num_timesteps=1430000, episode_reward=-75574.94 +/- 47132.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.22081448   |
|    mean velocity x      | -0.319        |
|    mean velocity y      | 0.639         |
|    mean velocity z      | 0.791         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.56e+04     |
| time/                   |               |
|    total_timesteps      | 1430000       |
| train/                  |               |
|    approx_kl            | 1.1048658e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.001         |
|    loss                 | 1.51e+07      |
|    n_updates            | 6980          |
|    policy_gradient_loss | -0.000363     |
|    std                  | 1.56          |
|    value_loss           | 4.96e+07      |
-------------------------------------------
Eval num_timesteps=1430500, episode_reward=-68219.34 +/- 43987.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48644698 |
|    mean velocity x | -0.375      |
|    mean velocity y | 0.766       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.82e+04   |
| time/              |             |
|    total_timesteps | 1430500     |
------------------------------------
Eval num_timesteps=1431000, episode_reward=-60338.62 +/- 47403.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43369466 |
|    mean velocity x | -0.0786     |
|    mean velocity y | 1.17        |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.03e+04   |
| time/              |             |
|    total_timesteps | 1431000     |
------------------------------------
Eval num_timesteps=1431500, episode_reward=-71134.96 +/- 36366.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4120335 |
|    mean velocity x | -0.578     |
|    mean velocity y | 0.27       |
|    mean velocity z | 3.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.11e+04  |
| time/              |            |
|    total_timesteps | 1431500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 699     |
|    time_elapsed    | 57702   |
|    total_timesteps | 1431552 |
--------------------------------
Eval num_timesteps=1432000, episode_reward=-39548.26 +/- 41152.90
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40388942   |
|    mean velocity x      | -0.676        |
|    mean velocity y      | -0.000613     |
|    mean velocity z      | 3.9           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.95e+04     |
| time/                   |               |
|    total_timesteps      | 1432000       |
| train/                  |               |
|    approx_kl            | 4.8239686e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.001         |
|    loss                 | 4.63e+07      |
|    n_updates            | 6990          |
|    policy_gradient_loss | -0.000384     |
|    std                  | 1.56          |
|    value_loss           | 8.01e+07      |
-------------------------------------------
Eval num_timesteps=1432500, episode_reward=-79355.24 +/- 45121.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4507776 |
|    mean velocity x | -0.5       |
|    mean velocity y | 0.493      |
|    mean velocity z | 4.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.94e+04  |
| time/              |            |
|    total_timesteps | 1432500    |
-----------------------------------
Eval num_timesteps=1433000, episode_reward=-88057.45 +/- 36368.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26631346 |
|    mean velocity x | -0.1        |
|    mean velocity y | 0.555       |
|    mean velocity z | 0.238       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.81e+04   |
| time/              |             |
|    total_timesteps | 1433000     |
------------------------------------
Eval num_timesteps=1433500, episode_reward=-80337.75 +/- 10110.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39392236 |
|    mean velocity x | 0.132       |
|    mean velocity y | 1.31        |
|    mean velocity z | 4.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.03e+04   |
| time/              |             |
|    total_timesteps | 1433500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 700     |
|    time_elapsed    | 57782   |
|    total_timesteps | 1433600 |
--------------------------------
Eval num_timesteps=1434000, episode_reward=-66539.19 +/- 7372.09
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.641328     |
|    mean velocity x      | -0.479        |
|    mean velocity y      | 0.751         |
|    mean velocity z      | 5.44          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.65e+04     |
| time/                   |               |
|    total_timesteps      | 1434000       |
| train/                  |               |
|    approx_kl            | 1.4719175e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+07      |
|    n_updates            | 7000          |
|    policy_gradient_loss | -0.000382     |
|    std                  | 1.56          |
|    value_loss           | 5.67e+07      |
-------------------------------------------
Eval num_timesteps=1434500, episode_reward=-99687.44 +/- 32123.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30280048 |
|    mean velocity x | -0.181      |
|    mean velocity y | 0.285       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.97e+04   |
| time/              |             |
|    total_timesteps | 1434500     |
------------------------------------
Eval num_timesteps=1435000, episode_reward=-99930.78 +/- 22857.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52586716 |
|    mean velocity x | -0.0858     |
|    mean velocity y | 1.69        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.99e+04   |
| time/              |             |
|    total_timesteps | 1435000     |
------------------------------------
Eval num_timesteps=1435500, episode_reward=-65824.58 +/- 42102.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33017576 |
|    mean velocity x | 0.142       |
|    mean velocity y | 0.373       |
|    mean velocity z | 2.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.58e+04   |
| time/              |             |
|    total_timesteps | 1435500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 701     |
|    time_elapsed    | 57863   |
|    total_timesteps | 1435648 |
--------------------------------
Eval num_timesteps=1436000, episode_reward=-80646.72 +/- 45161.93
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3592372    |
|    mean velocity x      | -0.257        |
|    mean velocity y      | 0.509         |
|    mean velocity z      | 2.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.06e+04     |
| time/                   |               |
|    total_timesteps      | 1436000       |
| train/                  |               |
|    approx_kl            | 2.5103654e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.001         |
|    loss                 | 5.81e+06      |
|    n_updates            | 7010          |
|    policy_gradient_loss | -0.000432     |
|    std                  | 1.56          |
|    value_loss           | 4.36e+07      |
-------------------------------------------
Eval num_timesteps=1436500, episode_reward=-59947.18 +/- 50592.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48811466 |
|    mean velocity x | -0.629      |
|    mean velocity y | 0.707       |
|    mean velocity z | 4.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.99e+04   |
| time/              |             |
|    total_timesteps | 1436500     |
------------------------------------
Eval num_timesteps=1437000, episode_reward=-72225.59 +/- 34630.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3230816 |
|    mean velocity x | 0.273      |
|    mean velocity y | 1.08       |
|    mean velocity z | 3.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.22e+04  |
| time/              |            |
|    total_timesteps | 1437000    |
-----------------------------------
Eval num_timesteps=1437500, episode_reward=-97940.12 +/- 24119.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42970887 |
|    mean velocity x | -1.88       |
|    mean velocity y | -0.24       |
|    mean velocity z | 6.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.79e+04   |
| time/              |             |
|    total_timesteps | 1437500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 702     |
|    time_elapsed    | 57943   |
|    total_timesteps | 1437696 |
--------------------------------
Eval num_timesteps=1438000, episode_reward=-50173.42 +/- 39981.39
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4440171    |
|    mean velocity x      | -0.419        |
|    mean velocity y      | 0.796         |
|    mean velocity z      | 4.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.02e+04     |
| time/                   |               |
|    total_timesteps      | 1438000       |
| train/                  |               |
|    approx_kl            | 4.3548353e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+07      |
|    n_updates            | 7020          |
|    policy_gradient_loss | -0.000133     |
|    std                  | 1.56          |
|    value_loss           | 6.8e+07       |
-------------------------------------------
Eval num_timesteps=1438500, episode_reward=-99187.55 +/- 27036.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56468576 |
|    mean velocity x | -0.0897     |
|    mean velocity y | 1.8         |
|    mean velocity z | 4.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.92e+04   |
| time/              |             |
|    total_timesteps | 1438500     |
------------------------------------
Eval num_timesteps=1439000, episode_reward=-51023.84 +/- 49451.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4502885 |
|    mean velocity x | -0.318     |
|    mean velocity y | 0.84       |
|    mean velocity z | 4.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.1e+04   |
| time/              |            |
|    total_timesteps | 1439000    |
-----------------------------------
Eval num_timesteps=1439500, episode_reward=-61436.78 +/- 34996.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30475557 |
|    mean velocity x | -0.0393     |
|    mean velocity y | 0.372       |
|    mean velocity z | 3.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.14e+04   |
| time/              |             |
|    total_timesteps | 1439500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 703     |
|    time_elapsed    | 58023   |
|    total_timesteps | 1439744 |
--------------------------------
Eval num_timesteps=1440000, episode_reward=-72778.29 +/- 41786.61
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47048932   |
|    mean velocity x      | -0.151        |
|    mean velocity y      | 1.3           |
|    mean velocity z      | 4.52          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.28e+04     |
| time/                   |               |
|    total_timesteps      | 1440000       |
| train/                  |               |
|    approx_kl            | 4.8884365e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.222         |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+07      |
|    n_updates            | 7030          |
|    policy_gradient_loss | -0.000574     |
|    std                  | 1.56          |
|    value_loss           | 7.93e+07      |
-------------------------------------------
Eval num_timesteps=1440500, episode_reward=-84527.51 +/- 20915.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48178887 |
|    mean velocity x | 0.16        |
|    mean velocity y | 1.19        |
|    mean velocity z | 2.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.45e+04   |
| time/              |             |
|    total_timesteps | 1440500     |
------------------------------------
Eval num_timesteps=1441000, episode_reward=-88151.64 +/- 28003.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17478418 |
|    mean velocity x | -0.671      |
|    mean velocity y | -0.617      |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.82e+04   |
| time/              |             |
|    total_timesteps | 1441000     |
------------------------------------
Eval num_timesteps=1441500, episode_reward=-59942.90 +/- 32719.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4547344 |
|    mean velocity x | -0.535     |
|    mean velocity y | 0.447      |
|    mean velocity z | 3.79       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.99e+04  |
| time/              |            |
|    total_timesteps | 1441500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 704     |
|    time_elapsed    | 58104   |
|    total_timesteps | 1441792 |
--------------------------------
Eval num_timesteps=1442000, episode_reward=-52495.95 +/- 27037.61
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4074632   |
|    mean velocity x      | -0.185       |
|    mean velocity y      | 0.957        |
|    mean velocity z      | 3.78         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.25e+04    |
| time/                   |              |
|    total_timesteps      | 1442000      |
| train/                  |              |
|    approx_kl            | 6.556307e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.001        |
|    loss                 | 6.23e+06     |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.000264    |
|    std                  | 1.56         |
|    value_loss           | 4.85e+07     |
------------------------------------------
Eval num_timesteps=1442500, episode_reward=-75629.38 +/- 40261.10
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4825833 |
|    mean velocity x | 0.485      |
|    mean velocity y | 1.57       |
|    mean velocity z | 3.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.56e+04  |
| time/              |            |
|    total_timesteps | 1442500    |
-----------------------------------
Eval num_timesteps=1443000, episode_reward=-118077.23 +/- 13726.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4003196 |
|    mean velocity x | -0.479     |
|    mean velocity y | 0.084      |
|    mean velocity z | 4.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 1443000    |
-----------------------------------
Eval num_timesteps=1443500, episode_reward=-105822.47 +/- 17950.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38563317 |
|    mean velocity x | 0.38        |
|    mean velocity y | 1           |
|    mean velocity z | 3.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1443500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 705     |
|    time_elapsed    | 58184   |
|    total_timesteps | 1443840 |
--------------------------------
Eval num_timesteps=1444000, episode_reward=-98169.78 +/- 9920.31
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34692252   |
|    mean velocity x      | 0.0256        |
|    mean velocity y      | 0.451         |
|    mean velocity z      | 1.29          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.82e+04     |
| time/                   |               |
|    total_timesteps      | 1444000       |
| train/                  |               |
|    approx_kl            | 1.8761464e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.001         |
|    loss                 | 2.69e+07      |
|    n_updates            | 7050          |
|    policy_gradient_loss | -0.000475     |
|    std                  | 1.56          |
|    value_loss           | 3.61e+07      |
-------------------------------------------
Eval num_timesteps=1444500, episode_reward=-73995.64 +/- 50440.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3737517 |
|    mean velocity x | -1.25      |
|    mean velocity y | -0.521     |
|    mean velocity z | 3.37       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.4e+04   |
| time/              |            |
|    total_timesteps | 1444500    |
-----------------------------------
Eval num_timesteps=1445000, episode_reward=-60016.92 +/- 48532.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44854945 |
|    mean velocity x | -0.0781     |
|    mean velocity y | 1.36        |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6e+04      |
| time/              |             |
|    total_timesteps | 1445000     |
------------------------------------
Eval num_timesteps=1445500, episode_reward=-52504.46 +/- 25783.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50028783 |
|    mean velocity x | -0.309      |
|    mean velocity y | 0.82        |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.25e+04   |
| time/              |             |
|    total_timesteps | 1445500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 706     |
|    time_elapsed    | 58265   |
|    total_timesteps | 1445888 |
--------------------------------
Eval num_timesteps=1446000, episode_reward=-101064.01 +/- 48807.23
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4624849    |
|    mean velocity x      | 0.432         |
|    mean velocity y      | 1.07          |
|    mean velocity z      | 2.91          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+05     |
| time/                   |               |
|    total_timesteps      | 1446000       |
| train/                  |               |
|    approx_kl            | 6.4479274e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.299         |
|    learning_rate        | 0.001         |
|    loss                 | 5.29e+06      |
|    n_updates            | 7060          |
|    policy_gradient_loss | -0.000374     |
|    std                  | 1.56          |
|    value_loss           | 4.26e+07      |
-------------------------------------------
Eval num_timesteps=1446500, episode_reward=-97212.05 +/- 36278.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36284727 |
|    mean velocity x | -0.304      |
|    mean velocity y | 0.94        |
|    mean velocity z | 2.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.72e+04   |
| time/              |             |
|    total_timesteps | 1446500     |
------------------------------------
Eval num_timesteps=1447000, episode_reward=-77257.70 +/- 14575.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28818378 |
|    mean velocity x | -0.00657    |
|    mean velocity y | 0.388       |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.73e+04   |
| time/              |             |
|    total_timesteps | 1447000     |
------------------------------------
Eval num_timesteps=1447500, episode_reward=-69731.42 +/- 41301.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45585248 |
|    mean velocity x | 0.12        |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.97e+04   |
| time/              |             |
|    total_timesteps | 1447500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 707     |
|    time_elapsed    | 58345   |
|    total_timesteps | 1447936 |
--------------------------------
Eval num_timesteps=1448000, episode_reward=-100046.08 +/- 22374.36
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5469867   |
|    mean velocity x      | -0.329       |
|    mean velocity y      | 1.17         |
|    mean velocity z      | 4.75         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1e+05       |
| time/                   |              |
|    total_timesteps      | 1448000      |
| train/                  |              |
|    approx_kl            | 2.719299e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+07     |
|    n_updates            | 7070         |
|    policy_gradient_loss | -0.000376    |
|    std                  | 1.56         |
|    value_loss           | 6.55e+07     |
------------------------------------------
Eval num_timesteps=1448500, episode_reward=-87291.67 +/- 45465.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46775568 |
|    mean velocity x | -0.388      |
|    mean velocity y | 0.732       |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.73e+04   |
| time/              |             |
|    total_timesteps | 1448500     |
------------------------------------
Eval num_timesteps=1449000, episode_reward=-90644.44 +/- 11528.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38773394 |
|    mean velocity x | -0.161      |
|    mean velocity y | 0.539       |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.06e+04   |
| time/              |             |
|    total_timesteps | 1449000     |
------------------------------------
Eval num_timesteps=1449500, episode_reward=-61133.61 +/- 39056.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44460538 |
|    mean velocity x | -0.06       |
|    mean velocity y | 1.08        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.11e+04   |
| time/              |             |
|    total_timesteps | 1449500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 708     |
|    time_elapsed    | 58425   |
|    total_timesteps | 1449984 |
--------------------------------
Eval num_timesteps=1450000, episode_reward=-94104.65 +/- 33316.59
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34626463   |
|    mean velocity x      | -0.628        |
|    mean velocity y      | -0.361        |
|    mean velocity z      | 3.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.41e+04     |
| time/                   |               |
|    total_timesteps      | 1450000       |
| train/                  |               |
|    approx_kl            | 2.0410633e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.001         |
|    loss                 | 4.13e+07      |
|    n_updates            | 7080          |
|    policy_gradient_loss | -0.000339     |
|    std                  | 1.56          |
|    value_loss           | 8.93e+07      |
-------------------------------------------
Eval num_timesteps=1450500, episode_reward=-72378.69 +/- 59197.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4584743 |
|    mean velocity x | -0.458     |
|    mean velocity y | 1          |
|    mean velocity z | 2.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.24e+04  |
| time/              |            |
|    total_timesteps | 1450500    |
-----------------------------------
Eval num_timesteps=1451000, episode_reward=-89302.31 +/- 45963.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44580126 |
|    mean velocity x | -0.354      |
|    mean velocity y | 0.803       |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1451000     |
------------------------------------
Eval num_timesteps=1451500, episode_reward=-77340.68 +/- 47630.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44180718 |
|    mean velocity x | -0.347      |
|    mean velocity y | 0.847       |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.73e+04   |
| time/              |             |
|    total_timesteps | 1451500     |
------------------------------------
Eval num_timesteps=1452000, episode_reward=-58821.47 +/- 42085.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47359258 |
|    mean velocity x | -0.247      |
|    mean velocity y | 0.968       |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.88e+04   |
| time/              |             |
|    total_timesteps | 1452000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 709     |
|    time_elapsed    | 58525   |
|    total_timesteps | 1452032 |
--------------------------------
Eval num_timesteps=1452500, episode_reward=-92155.37 +/- 33447.83
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33751592   |
|    mean velocity x      | -1.1          |
|    mean velocity y      | -0.541        |
|    mean velocity z      | 4.81          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.22e+04     |
| time/                   |               |
|    total_timesteps      | 1452500       |
| train/                  |               |
|    approx_kl            | 9.1124093e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+07      |
|    n_updates            | 7090          |
|    policy_gradient_loss | -7.43e-05     |
|    std                  | 1.56          |
|    value_loss           | 7.28e+07      |
-------------------------------------------
Eval num_timesteps=1453000, episode_reward=-70153.65 +/- 44728.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3788264 |
|    mean velocity x | -0.0316    |
|    mean velocity y | 1.26       |
|    mean velocity z | 4.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.02e+04  |
| time/              |            |
|    total_timesteps | 1453000    |
-----------------------------------
Eval num_timesteps=1453500, episode_reward=-92463.77 +/- 15295.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46492034 |
|    mean velocity x | -0.364      |
|    mean velocity y | 0.785       |
|    mean velocity z | 4.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.25e+04   |
| time/              |             |
|    total_timesteps | 1453500     |
------------------------------------
Eval num_timesteps=1454000, episode_reward=-54411.63 +/- 24035.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57045126 |
|    mean velocity x | 0.147       |
|    mean velocity y | 1.48        |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.44e+04   |
| time/              |             |
|    total_timesteps | 1454000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 710     |
|    time_elapsed    | 58605   |
|    total_timesteps | 1454080 |
--------------------------------
Eval num_timesteps=1454500, episode_reward=-99070.79 +/- 31217.70
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.22924112   |
|    mean velocity x      | -0.232        |
|    mean velocity y      | 0.488         |
|    mean velocity z      | 1.06          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.91e+04     |
| time/                   |               |
|    total_timesteps      | 1454500       |
| train/                  |               |
|    approx_kl            | 1.0375021e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 3.82e+07      |
|    n_updates            | 7100          |
|    policy_gradient_loss | -0.000259     |
|    std                  | 1.56          |
|    value_loss           | 7.54e+07      |
-------------------------------------------
Eval num_timesteps=1455000, episode_reward=-94306.61 +/- 36132.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3218434 |
|    mean velocity x | -0.861     |
|    mean velocity y | 0.4        |
|    mean velocity z | 2.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.43e+04  |
| time/              |            |
|    total_timesteps | 1455000    |
-----------------------------------
Eval num_timesteps=1455500, episode_reward=-60530.35 +/- 50838.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47571364 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.816       |
|    mean velocity z | 4.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.05e+04   |
| time/              |             |
|    total_timesteps | 1455500     |
------------------------------------
Eval num_timesteps=1456000, episode_reward=-101849.42 +/- 19418.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5900424 |
|    mean velocity x | -0.595     |
|    mean velocity y | 0.848      |
|    mean velocity z | 4.9        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 1456000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 711     |
|    time_elapsed    | 58686   |
|    total_timesteps | 1456128 |
--------------------------------
Eval num_timesteps=1456500, episode_reward=-75979.07 +/- 35369.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4461919    |
|    mean velocity x      | -0.129        |
|    mean velocity y      | 0.918         |
|    mean velocity z      | 2.87          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.6e+04      |
| time/                   |               |
|    total_timesteps      | 1456500       |
| train/                  |               |
|    approx_kl            | 2.6575872e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+07      |
|    n_updates            | 7110          |
|    policy_gradient_loss | -0.000351     |
|    std                  | 1.56          |
|    value_loss           | 5.77e+07      |
-------------------------------------------
Eval num_timesteps=1457000, episode_reward=-78137.03 +/- 48751.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48602715 |
|    mean velocity x | 0.182       |
|    mean velocity y | 1.44        |
|    mean velocity z | 3.78        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.81e+04   |
| time/              |             |
|    total_timesteps | 1457000     |
------------------------------------
Eval num_timesteps=1457500, episode_reward=-90769.39 +/- 25701.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5237798 |
|    mean velocity x | -0.847     |
|    mean velocity y | -0.0932    |
|    mean velocity z | 6.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.08e+04  |
| time/              |            |
|    total_timesteps | 1457500    |
-----------------------------------
Eval num_timesteps=1458000, episode_reward=-60857.07 +/- 35311.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5052632 |
|    mean velocity x | -0.171     |
|    mean velocity y | 1.34       |
|    mean velocity z | 3.66       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.09e+04  |
| time/              |            |
|    total_timesteps | 1458000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 712     |
|    time_elapsed    | 58766   |
|    total_timesteps | 1458176 |
--------------------------------
Eval num_timesteps=1458500, episode_reward=-106537.26 +/- 20946.35
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4037341    |
|    mean velocity x      | -0.345        |
|    mean velocity y      | 1.06          |
|    mean velocity z      | 2.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 1458500       |
| train/                  |               |
|    approx_kl            | 7.5988646e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.001         |
|    loss                 | 2.21e+07      |
|    n_updates            | 7120          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 1.56          |
|    value_loss           | 4.23e+07      |
-------------------------------------------
Eval num_timesteps=1459000, episode_reward=-51258.06 +/- 48186.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39956397 |
|    mean velocity x | -0.674      |
|    mean velocity y | 0.0423      |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.13e+04   |
| time/              |             |
|    total_timesteps | 1459000     |
------------------------------------
Eval num_timesteps=1459500, episode_reward=-56772.41 +/- 41764.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34552372 |
|    mean velocity x | -0.031      |
|    mean velocity y | 0.808       |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.68e+04   |
| time/              |             |
|    total_timesteps | 1459500     |
------------------------------------
Eval num_timesteps=1460000, episode_reward=-63076.51 +/- 27947.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4644223 |
|    mean velocity x | -0.149     |
|    mean velocity y | 1.14       |
|    mean velocity z | 3.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.31e+04  |
| time/              |            |
|    total_timesteps | 1460000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 713     |
|    time_elapsed    | 58846   |
|    total_timesteps | 1460224 |
--------------------------------
Eval num_timesteps=1460500, episode_reward=-115106.84 +/- 54506.70
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5103656    |
|    mean velocity x      | -0.576        |
|    mean velocity y      | 0.867         |
|    mean velocity z      | 4.81          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.15e+05     |
| time/                   |               |
|    total_timesteps      | 1460500       |
| train/                  |               |
|    approx_kl            | 4.7179055e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.001         |
|    loss                 | 3.98e+07      |
|    n_updates            | 7130          |
|    policy_gradient_loss | -0.000164     |
|    std                  | 1.56          |
|    value_loss           | 7.74e+07      |
-------------------------------------------
Eval num_timesteps=1461000, episode_reward=-90172.73 +/- 29157.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4228993 |
|    mean velocity x | -0.104     |
|    mean velocity y | 1.26       |
|    mean velocity z | 4.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.02e+04  |
| time/              |            |
|    total_timesteps | 1461000    |
-----------------------------------
Eval num_timesteps=1461500, episode_reward=-60990.97 +/- 41953.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4310543 |
|    mean velocity x | -0.159     |
|    mean velocity y | 0.699      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.1e+04   |
| time/              |            |
|    total_timesteps | 1461500    |
-----------------------------------
Eval num_timesteps=1462000, episode_reward=-131123.58 +/- 45551.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33422956 |
|    mean velocity x | -0.353      |
|    mean velocity y | 0.38        |
|    mean velocity z | 3.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.31e+05   |
| time/              |             |
|    total_timesteps | 1462000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 714     |
|    time_elapsed    | 58927   |
|    total_timesteps | 1462272 |
--------------------------------
Eval num_timesteps=1462500, episode_reward=-110652.42 +/- 18958.47
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50275666  |
|    mean velocity x      | 0.206        |
|    mean velocity y      | 1.41         |
|    mean velocity z      | 3.51         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.11e+05    |
| time/                   |              |
|    total_timesteps      | 1462500      |
| train/                  |              |
|    approx_kl            | 6.023125e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+07     |
|    n_updates            | 7140         |
|    policy_gradient_loss | -0.000236    |
|    std                  | 1.56         |
|    value_loss           | 8.49e+07     |
------------------------------------------
Eval num_timesteps=1463000, episode_reward=-67573.41 +/- 37262.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42998052 |
|    mean velocity x | 0.18        |
|    mean velocity y | 1.15        |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 1463000     |
------------------------------------
Eval num_timesteps=1463500, episode_reward=-77701.42 +/- 53734.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31797692 |
|    mean velocity x | -0.49       |
|    mean velocity y | 0.552       |
|    mean velocity z | 1.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 1463500     |
------------------------------------
Eval num_timesteps=1464000, episode_reward=-44014.51 +/- 50270.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48045358 |
|    mean velocity x | -0.489      |
|    mean velocity y | 0.752       |
|    mean velocity z | 4.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.4e+04    |
| time/              |             |
|    total_timesteps | 1464000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 715     |
|    time_elapsed    | 59007   |
|    total_timesteps | 1464320 |
--------------------------------
Eval num_timesteps=1464500, episode_reward=-60596.09 +/- 29963.00
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.39507812 |
|    mean velocity x      | -0.0182     |
|    mean velocity y      | 0.905       |
|    mean velocity z      | 3.89        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.06e+04   |
| time/                   |             |
|    total_timesteps      | 1464500     |
| train/                  |             |
|    approx_kl            | 2.02925e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.001       |
|    loss                 | 5.58e+07    |
|    n_updates            | 7150        |
|    policy_gradient_loss | -0.000388   |
|    std                  | 1.56        |
|    value_loss           | 5.96e+07    |
-----------------------------------------
Eval num_timesteps=1465000, episode_reward=-61905.16 +/- 30418.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45489347 |
|    mean velocity x | -0.396      |
|    mean velocity y | 0.73        |
|    mean velocity z | 3.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+04   |
| time/              |             |
|    total_timesteps | 1465000     |
------------------------------------
Eval num_timesteps=1465500, episode_reward=-78202.84 +/- 27888.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44964764 |
|    mean velocity x | -0.469      |
|    mean velocity y | 0.658       |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 1465500     |
------------------------------------
Eval num_timesteps=1466000, episode_reward=-86150.35 +/- 46133.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47934687 |
|    mean velocity x | 0.346       |
|    mean velocity y | 1.52        |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 1466000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 716     |
|    time_elapsed    | 59087   |
|    total_timesteps | 1466368 |
--------------------------------
Eval num_timesteps=1466500, episode_reward=-67537.37 +/- 33920.27
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5036976    |
|    mean velocity x      | -0.383        |
|    mean velocity y      | 1             |
|    mean velocity z      | 5.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.75e+04     |
| time/                   |               |
|    total_timesteps      | 1466500       |
| train/                  |               |
|    approx_kl            | 1.3239711e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 3.44e+07      |
|    n_updates            | 7160          |
|    policy_gradient_loss | -0.000307     |
|    std                  | 1.56          |
|    value_loss           | 8.04e+07      |
-------------------------------------------
Eval num_timesteps=1467000, episode_reward=-77206.13 +/- 48419.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41794178 |
|    mean velocity x | -0.201      |
|    mean velocity y | 1.2         |
|    mean velocity z | 4.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 1467000     |
------------------------------------
Eval num_timesteps=1467500, episode_reward=-75164.04 +/- 18331.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31445566 |
|    mean velocity x | -0.184      |
|    mean velocity y | 0.635       |
|    mean velocity z | 0.502       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.52e+04   |
| time/              |             |
|    total_timesteps | 1467500     |
------------------------------------
Eval num_timesteps=1468000, episode_reward=-77437.35 +/- 34506.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37496766 |
|    mean velocity x | 0.598       |
|    mean velocity y | 1.51        |
|    mean velocity z | 3.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.74e+04   |
| time/              |             |
|    total_timesteps | 1468000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 717     |
|    time_elapsed    | 59168   |
|    total_timesteps | 1468416 |
--------------------------------
Eval num_timesteps=1468500, episode_reward=-143216.51 +/- 19749.11
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.28105694    |
|    mean velocity x      | -0.195         |
|    mean velocity y      | 0.231          |
|    mean velocity z      | 3.67           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1.43e+05      |
| time/                   |                |
|    total_timesteps      | 1468500        |
| train/                  |                |
|    approx_kl            | 1.14084105e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.58          |
|    explained_variance   | 0.232          |
|    learning_rate        | 0.001          |
|    loss                 | 5e+07          |
|    n_updates            | 7170           |
|    policy_gradient_loss | -0.000304      |
|    std                  | 1.56           |
|    value_loss           | 5.89e+07       |
--------------------------------------------
Eval num_timesteps=1469000, episode_reward=-37334.22 +/- 27247.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43026263 |
|    mean velocity x | 0.14        |
|    mean velocity y | 0.828       |
|    mean velocity z | 2.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.73e+04   |
| time/              |             |
|    total_timesteps | 1469000     |
------------------------------------
Eval num_timesteps=1469500, episode_reward=-91748.20 +/- 22232.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48841444 |
|    mean velocity x | -0.181      |
|    mean velocity y | 1.35        |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.17e+04   |
| time/              |             |
|    total_timesteps | 1469500     |
------------------------------------
Eval num_timesteps=1470000, episode_reward=-67397.30 +/- 35566.61
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.402665 |
|    mean velocity x | -1.02     |
|    mean velocity y | 0.27      |
|    mean velocity z | 3.22      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.74e+04 |
| time/              |           |
|    total_timesteps | 1470000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 718     |
|    time_elapsed    | 59248   |
|    total_timesteps | 1470464 |
--------------------------------
Eval num_timesteps=1470500, episode_reward=-85935.24 +/- 31864.90
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4373763    |
|    mean velocity x      | -0.444        |
|    mean velocity y      | 0.805         |
|    mean velocity z      | 4.8           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.59e+04     |
| time/                   |               |
|    total_timesteps      | 1470500       |
| train/                  |               |
|    approx_kl            | 9.4311545e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.277         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+07      |
|    n_updates            | 7180          |
|    policy_gradient_loss | -0.000336     |
|    std                  | 1.56          |
|    value_loss           | 5.13e+07      |
-------------------------------------------
Eval num_timesteps=1471000, episode_reward=-73053.88 +/- 42226.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3855085 |
|    mean velocity x | -0.447     |
|    mean velocity y | 0.251      |
|    mean velocity z | 3.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 1471000    |
-----------------------------------
Eval num_timesteps=1471500, episode_reward=-106689.86 +/- 44778.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3629061 |
|    mean velocity x | -0.485     |
|    mean velocity y | 0.779      |
|    mean velocity z | 1.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 1471500    |
-----------------------------------
Eval num_timesteps=1472000, episode_reward=-50600.43 +/- 25490.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27021286 |
|    mean velocity x | -0.057      |
|    mean velocity y | 0.457       |
|    mean velocity z | 0.484       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 1472000     |
------------------------------------
Eval num_timesteps=1472500, episode_reward=-39162.26 +/- 40209.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2949575 |
|    mean velocity x | -1.39      |
|    mean velocity y | -0.724     |
|    mean velocity z | 4.88       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -3.92e+04  |
| time/              |            |
|    total_timesteps | 1472500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 719     |
|    time_elapsed    | 59348   |
|    total_timesteps | 1472512 |
--------------------------------
Eval num_timesteps=1473000, episode_reward=-62590.75 +/- 30876.39
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32007745  |
|    mean velocity x      | 0.0721       |
|    mean velocity y      | 0.362        |
|    mean velocity z      | 1.99         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.26e+04    |
| time/                   |              |
|    total_timesteps      | 1473000      |
| train/                  |              |
|    approx_kl            | 9.924639e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.333        |
|    learning_rate        | 0.001        |
|    loss                 | 3.35e+07     |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000268    |
|    std                  | 1.56         |
|    value_loss           | 2.86e+07     |
------------------------------------------
Eval num_timesteps=1473500, episode_reward=-73566.00 +/- 41635.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39040336 |
|    mean velocity x | 0.189       |
|    mean velocity y | 1.4         |
|    mean velocity z | 3.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1473500     |
------------------------------------
Eval num_timesteps=1474000, episode_reward=-75099.07 +/- 21951.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43540493 |
|    mean velocity x | -0.528      |
|    mean velocity y | 0.0944      |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1474000     |
------------------------------------
Eval num_timesteps=1474500, episode_reward=-97223.56 +/- 27700.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40515023 |
|    mean velocity x | -0.869      |
|    mean velocity y | 0.226       |
|    mean velocity z | 3.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.72e+04   |
| time/              |             |
|    total_timesteps | 1474500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 720     |
|    time_elapsed    | 59428   |
|    total_timesteps | 1474560 |
--------------------------------
Eval num_timesteps=1475000, episode_reward=-87211.52 +/- 31219.67
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.55910575   |
|    mean velocity x      | 0.293         |
|    mean velocity y      | 1.42          |
|    mean velocity z      | 3.51          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.72e+04     |
| time/                   |               |
|    total_timesteps      | 1475000       |
| train/                  |               |
|    approx_kl            | 1.5364814e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.326         |
|    learning_rate        | 0.001         |
|    loss                 | 1.99e+07      |
|    n_updates            | 7200          |
|    policy_gradient_loss | -0.000395     |
|    std                  | 1.56          |
|    value_loss           | 5.19e+07      |
-------------------------------------------
Eval num_timesteps=1475500, episode_reward=-81243.69 +/- 44247.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41834563 |
|    mean velocity x | -0.381      |
|    mean velocity y | 0.778       |
|    mean velocity z | 4.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 1475500     |
------------------------------------
Eval num_timesteps=1476000, episode_reward=-83792.78 +/- 33913.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42855993 |
|    mean velocity x | -0.222      |
|    mean velocity y | 0.972       |
|    mean velocity z | 4.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.38e+04   |
| time/              |             |
|    total_timesteps | 1476000     |
------------------------------------
Eval num_timesteps=1476500, episode_reward=-87044.89 +/- 44355.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49299446 |
|    mean velocity x | -0.518      |
|    mean velocity y | 0.638       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.7e+04    |
| time/              |             |
|    total_timesteps | 1476500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 721     |
|    time_elapsed    | 59508   |
|    total_timesteps | 1476608 |
--------------------------------
Eval num_timesteps=1477000, episode_reward=-59627.08 +/- 29618.20
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40585336   |
|    mean velocity x      | 0.27          |
|    mean velocity y      | 0.76          |
|    mean velocity z      | 2.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.96e+04     |
| time/                   |               |
|    total_timesteps      | 1477000       |
| train/                  |               |
|    approx_kl            | 2.5167654e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.001         |
|    loss                 | 2.85e+07      |
|    n_updates            | 7210          |
|    policy_gradient_loss | -0.000272     |
|    std                  | 1.56          |
|    value_loss           | 8.78e+07      |
-------------------------------------------
Eval num_timesteps=1477500, episode_reward=-74654.77 +/- 60929.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25121462 |
|    mean velocity x | 0.0139      |
|    mean velocity y | 0.516       |
|    mean velocity z | 0.892       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.47e+04   |
| time/              |             |
|    total_timesteps | 1477500     |
------------------------------------
Eval num_timesteps=1478000, episode_reward=-117150.12 +/- 52304.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4861375 |
|    mean velocity x | 0.28       |
|    mean velocity y | 1.27       |
|    mean velocity z | 3.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.17e+05  |
| time/              |            |
|    total_timesteps | 1478000    |
-----------------------------------
Eval num_timesteps=1478500, episode_reward=-82321.19 +/- 43970.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40529236 |
|    mean velocity x | 0.416       |
|    mean velocity y | 1.02        |
|    mean velocity z | 2.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1478500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 722     |
|    time_elapsed    | 59589   |
|    total_timesteps | 1478656 |
--------------------------------
Eval num_timesteps=1479000, episode_reward=-75893.60 +/- 28086.60
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2839529    |
|    mean velocity x      | -0.0961       |
|    mean velocity y      | 0.521         |
|    mean velocity z      | 0.473         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.59e+04     |
| time/                   |               |
|    total_timesteps      | 1479000       |
| train/                  |               |
|    approx_kl            | 6.7440764e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.374         |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+06      |
|    n_updates            | 7220          |
|    policy_gradient_loss | -0.00036      |
|    std                  | 1.56          |
|    value_loss           | 1.76e+07      |
-------------------------------------------
Eval num_timesteps=1479500, episode_reward=-68857.87 +/- 34617.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30477422 |
|    mean velocity x | -0.0515     |
|    mean velocity y | 0.486       |
|    mean velocity z | 3.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.89e+04   |
| time/              |             |
|    total_timesteps | 1479500     |
------------------------------------
Eval num_timesteps=1480000, episode_reward=-81735.76 +/- 17784.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3822518 |
|    mean velocity x | -1.23      |
|    mean velocity y | -0.638     |
|    mean velocity z | 4.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.17e+04  |
| time/              |            |
|    total_timesteps | 1480000    |
-----------------------------------
Eval num_timesteps=1480500, episode_reward=-84304.75 +/- 31321.72
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5023334 |
|    mean velocity x | -0.486     |
|    mean velocity y | 0.646      |
|    mean velocity z | 3.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 1480500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 723     |
|    time_elapsed    | 59669   |
|    total_timesteps | 1480704 |
--------------------------------
Eval num_timesteps=1481000, episode_reward=-69047.35 +/- 34129.64
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5284618    |
|    mean velocity x      | 0.0164        |
|    mean velocity y      | 1.3           |
|    mean velocity z      | 3.54          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.9e+04      |
| time/                   |               |
|    total_timesteps      | 1481000       |
| train/                  |               |
|    approx_kl            | 0.00010805324 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+07      |
|    n_updates            | 7230          |
|    policy_gradient_loss | -0.000867     |
|    std                  | 1.56          |
|    value_loss           | 4.26e+07      |
-------------------------------------------
Eval num_timesteps=1481500, episode_reward=-133270.78 +/- 10749.70
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.444084 |
|    mean velocity x | 0.273     |
|    mean velocity y | 1.39      |
|    mean velocity z | 3.49      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.33e+05 |
| time/              |           |
|    total_timesteps | 1481500   |
----------------------------------
Eval num_timesteps=1482000, episode_reward=-56278.58 +/- 45356.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51966494 |
|    mean velocity x | -0.258      |
|    mean velocity y | 0.735       |
|    mean velocity z | 5.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.63e+04   |
| time/              |             |
|    total_timesteps | 1482000     |
------------------------------------
Eval num_timesteps=1482500, episode_reward=-58050.94 +/- 37379.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4246072 |
|    mean velocity x | -0.421     |
|    mean velocity y | 0.488      |
|    mean velocity z | 4.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.81e+04  |
| time/              |            |
|    total_timesteps | 1482500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 724     |
|    time_elapsed    | 59749   |
|    total_timesteps | 1482752 |
--------------------------------
Eval num_timesteps=1483000, episode_reward=-71399.86 +/- 40895.10
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33124444   |
|    mean velocity x      | -0.166        |
|    mean velocity y      | 0.429         |
|    mean velocity z      | 2.5           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.14e+04     |
| time/                   |               |
|    total_timesteps      | 1483000       |
| train/                  |               |
|    approx_kl            | 0.00020158503 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+07      |
|    n_updates            | 7240          |
|    policy_gradient_loss | -0.00128      |
|    std                  | 1.56          |
|    value_loss           | 7.39e+07      |
-------------------------------------------
Eval num_timesteps=1483500, episode_reward=-78918.16 +/- 40029.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3860165 |
|    mean velocity x | -0.298     |
|    mean velocity y | 0.483      |
|    mean velocity z | 2.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 1483500    |
-----------------------------------
Eval num_timesteps=1484000, episode_reward=-70855.18 +/- 39679.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40771756 |
|    mean velocity x | -0.326      |
|    mean velocity y | 0.909       |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 1484000     |
------------------------------------
Eval num_timesteps=1484500, episode_reward=-81571.31 +/- 45901.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44996312 |
|    mean velocity x | -0.606      |
|    mean velocity y | 0.47        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 1484500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 725     |
|    time_elapsed    | 59830   |
|    total_timesteps | 1484800 |
--------------------------------
Eval num_timesteps=1485000, episode_reward=-83459.95 +/- 50201.26
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44087756   |
|    mean velocity x      | 0.159         |
|    mean velocity y      | 1.16          |
|    mean velocity z      | 3.66          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.35e+04     |
| time/                   |               |
|    total_timesteps      | 1485000       |
| train/                  |               |
|    approx_kl            | 3.4712197e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.58         |
|    explained_variance   | 0.27          |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+07      |
|    n_updates            | 7250          |
|    policy_gradient_loss | -0.000361     |
|    std                  | 1.56          |
|    value_loss           | 6.1e+07       |
-------------------------------------------
Eval num_timesteps=1485500, episode_reward=-94341.14 +/- 24752.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25882623 |
|    mean velocity x | -0.127      |
|    mean velocity y | 0.65        |
|    mean velocity z | 0.504       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.43e+04   |
| time/              |             |
|    total_timesteps | 1485500     |
------------------------------------
Eval num_timesteps=1486000, episode_reward=-108553.56 +/- 23061.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2492733 |
|    mean velocity x | -2.28      |
|    mean velocity y | -1.02      |
|    mean velocity z | 4.95       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 1486000    |
-----------------------------------
Eval num_timesteps=1486500, episode_reward=-94112.14 +/- 30779.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3946983 |
|    mean velocity x | -0.18      |
|    mean velocity y | 0.842      |
|    mean velocity z | 4.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.41e+04  |
| time/              |            |
|    total_timesteps | 1486500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 726     |
|    time_elapsed    | 59910   |
|    total_timesteps | 1486848 |
--------------------------------
Eval num_timesteps=1487000, episode_reward=-75998.63 +/- 33839.11
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.537935     |
|    mean velocity x      | 0.263         |
|    mean velocity y      | 1.51          |
|    mean velocity z      | 3.71          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.6e+04      |
| time/                   |               |
|    total_timesteps      | 1487000       |
| train/                  |               |
|    approx_kl            | 4.1683757e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.337         |
|    learning_rate        | 0.001         |
|    loss                 | 5.19e+07      |
|    n_updates            | 7260          |
|    policy_gradient_loss | -0.000468     |
|    std                  | 1.56          |
|    value_loss           | 4.72e+07      |
-------------------------------------------
Eval num_timesteps=1487500, episode_reward=-87766.39 +/- 10243.54
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.363541 |
|    mean velocity x | -0.179    |
|    mean velocity y | 0.573     |
|    mean velocity z | 2.28      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.78e+04 |
| time/              |           |
|    total_timesteps | 1487500   |
----------------------------------
Eval num_timesteps=1488000, episode_reward=-98936.83 +/- 19075.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51955867 |
|    mean velocity x | 0.026       |
|    mean velocity y | 0.97        |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.89e+04   |
| time/              |             |
|    total_timesteps | 1488000     |
------------------------------------
Eval num_timesteps=1488500, episode_reward=-88946.66 +/- 44634.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38481897 |
|    mean velocity x | 0.0615      |
|    mean velocity y | 0.691       |
|    mean velocity z | 3.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.89e+04   |
| time/              |             |
|    total_timesteps | 1488500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 727     |
|    time_elapsed    | 59990   |
|    total_timesteps | 1488896 |
--------------------------------
Eval num_timesteps=1489000, episode_reward=-67753.68 +/- 42053.92
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4429105    |
|    mean velocity x      | -0.405        |
|    mean velocity y      | 0.965         |
|    mean velocity z      | 4.62          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.78e+04     |
| time/                   |               |
|    total_timesteps      | 1489000       |
| train/                  |               |
|    approx_kl            | 5.3179683e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+07      |
|    n_updates            | 7270          |
|    policy_gradient_loss | -0.000213     |
|    std                  | 1.56          |
|    value_loss           | 4.82e+07      |
-------------------------------------------
Eval num_timesteps=1489500, episode_reward=-89477.82 +/- 41489.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48457915 |
|    mean velocity x | -1.01       |
|    mean velocity y | 0.498       |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.95e+04   |
| time/              |             |
|    total_timesteps | 1489500     |
------------------------------------
Eval num_timesteps=1490000, episode_reward=-59293.95 +/- 38370.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33346987 |
|    mean velocity x | -2.27       |
|    mean velocity y | -1.01       |
|    mean velocity z | 5.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.93e+04   |
| time/              |             |
|    total_timesteps | 1490000     |
------------------------------------
Eval num_timesteps=1490500, episode_reward=-104990.08 +/- 27937.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35329613 |
|    mean velocity x | 0.0365      |
|    mean velocity y | 1.22        |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1490500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 728     |
|    time_elapsed    | 60071   |
|    total_timesteps | 1490944 |
--------------------------------
Eval num_timesteps=1491000, episode_reward=-72191.66 +/- 43939.17
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3534749   |
|    mean velocity x      | -1.03        |
|    mean velocity y      | -0.638       |
|    mean velocity z      | 3.56         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.22e+04    |
| time/                   |              |
|    total_timesteps      | 1491000      |
| train/                  |              |
|    approx_kl            | 8.048752e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.001        |
|    loss                 | 1.83e+07     |
|    n_updates            | 7280         |
|    policy_gradient_loss | -0.000161    |
|    std                  | 1.56         |
|    value_loss           | 3.86e+07     |
------------------------------------------
Eval num_timesteps=1491500, episode_reward=-84164.83 +/- 45125.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3593539 |
|    mean velocity x | -0.369     |
|    mean velocity y | 0.447      |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.42e+04  |
| time/              |            |
|    total_timesteps | 1491500    |
-----------------------------------
Eval num_timesteps=1492000, episode_reward=-81421.54 +/- 18754.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39069498 |
|    mean velocity x | -0.208      |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.14e+04   |
| time/              |             |
|    total_timesteps | 1492000     |
------------------------------------
Eval num_timesteps=1492500, episode_reward=-84342.60 +/- 49697.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48843005 |
|    mean velocity x | -0.565      |
|    mean velocity y | 0.802       |
|    mean velocity z | 4.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.43e+04   |
| time/              |             |
|    total_timesteps | 1492500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 729     |
|    time_elapsed    | 60151   |
|    total_timesteps | 1492992 |
--------------------------------
Eval num_timesteps=1493000, episode_reward=-78739.41 +/- 48099.96
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47768044   |
|    mean velocity x      | -0.229        |
|    mean velocity y      | 0.782         |
|    mean velocity z      | 4.27          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.87e+04     |
| time/                   |               |
|    total_timesteps      | 1493000       |
| train/                  |               |
|    approx_kl            | 4.6438654e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.233         |
|    learning_rate        | 0.001         |
|    loss                 | 6.63e+07      |
|    n_updates            | 7290          |
|    policy_gradient_loss | -0.000189     |
|    std                  | 1.56          |
|    value_loss           | 8.79e+07      |
-------------------------------------------
Eval num_timesteps=1493500, episode_reward=-98512.18 +/- 38086.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46873292 |
|    mean velocity x | -0.367      |
|    mean velocity y | 1.07        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.85e+04   |
| time/              |             |
|    total_timesteps | 1493500     |
------------------------------------
Eval num_timesteps=1494000, episode_reward=-103698.23 +/- 31322.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35092404 |
|    mean velocity x | 0.623       |
|    mean velocity y | 1.41        |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1494000     |
------------------------------------
Eval num_timesteps=1494500, episode_reward=-59802.38 +/- 58937.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3154895 |
|    mean velocity x | 0.0976     |
|    mean velocity y | 0.568      |
|    mean velocity z | 3.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.98e+04  |
| time/              |            |
|    total_timesteps | 1494500    |
-----------------------------------
Eval num_timesteps=1495000, episode_reward=-90974.23 +/- 21815.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21616195 |
|    mean velocity x | -0.395      |
|    mean velocity y | 0.463       |
|    mean velocity z | 0.853       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 1495000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 730     |
|    time_elapsed    | 60251   |
|    total_timesteps | 1495040 |
--------------------------------
Eval num_timesteps=1495500, episode_reward=-62857.92 +/- 45297.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5208827    |
|    mean velocity x      | -0.744        |
|    mean velocity y      | 0.259         |
|    mean velocity z      | 4.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.29e+04     |
| time/                   |               |
|    total_timesteps      | 1495500       |
| train/                  |               |
|    approx_kl            | 1.2676872e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+07      |
|    n_updates            | 7300          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 1.56          |
|    value_loss           | 5.01e+07      |
-------------------------------------------
Eval num_timesteps=1496000, episode_reward=-40349.93 +/- 29083.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4208924 |
|    mean velocity x | 0.511      |
|    mean velocity y | 1.09       |
|    mean velocity z | 2.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.03e+04  |
| time/              |            |
|    total_timesteps | 1496000    |
-----------------------------------
Eval num_timesteps=1496500, episode_reward=-118118.09 +/- 35853.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36204702 |
|    mean velocity x | -0.353      |
|    mean velocity y | 0.804       |
|    mean velocity z | 1.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 1496500     |
------------------------------------
Eval num_timesteps=1497000, episode_reward=-84153.07 +/- 21403.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35910264 |
|    mean velocity x | -0.85       |
|    mean velocity y | -0.446      |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 1497000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 731     |
|    time_elapsed    | 60331   |
|    total_timesteps | 1497088 |
--------------------------------
Eval num_timesteps=1497500, episode_reward=-89307.83 +/- 11242.38
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38628125   |
|    mean velocity x      | -0.177        |
|    mean velocity y      | 0.794         |
|    mean velocity z      | 4.65          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.93e+04     |
| time/                   |               |
|    total_timesteps      | 1497500       |
| train/                  |               |
|    approx_kl            | 1.7154089e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 9.55e+06      |
|    n_updates            | 7310          |
|    policy_gradient_loss | -0.000325     |
|    std                  | 1.56          |
|    value_loss           | 3.63e+07      |
-------------------------------------------
Eval num_timesteps=1498000, episode_reward=-91664.89 +/- 35184.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4585205 |
|    mean velocity x | -0.457     |
|    mean velocity y | 0.61       |
|    mean velocity z | 4.39       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.17e+04  |
| time/              |            |
|    total_timesteps | 1498000    |
-----------------------------------
Eval num_timesteps=1498500, episode_reward=-38637.28 +/- 37174.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46371797 |
|    mean velocity x | -0.411      |
|    mean velocity y | 0.662       |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.86e+04   |
| time/              |             |
|    total_timesteps | 1498500     |
------------------------------------
Eval num_timesteps=1499000, episode_reward=-86593.35 +/- 35063.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36609247 |
|    mean velocity x | -1.25       |
|    mean velocity y | 0.0335      |
|    mean velocity z | 2.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 1499000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 732     |
|    time_elapsed    | 60411   |
|    total_timesteps | 1499136 |
--------------------------------
Eval num_timesteps=1499500, episode_reward=-73242.64 +/- 38750.63
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4497309   |
|    mean velocity x      | -0.155       |
|    mean velocity y      | 1.39         |
|    mean velocity z      | 4.52         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.32e+04    |
| time/                   |              |
|    total_timesteps      | 1499500      |
| train/                  |              |
|    approx_kl            | 1.127689e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+07     |
|    n_updates            | 7320         |
|    policy_gradient_loss | -0.00041     |
|    std                  | 1.55         |
|    value_loss           | 8.9e+07      |
------------------------------------------
Eval num_timesteps=1500000, episode_reward=-59023.06 +/- 41810.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41651943 |
|    mean velocity x | -0.62       |
|    mean velocity y | 0.343       |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.9e+04    |
| time/              |             |
|    total_timesteps | 1500000     |
------------------------------------
Eval num_timesteps=1500500, episode_reward=-98757.28 +/- 28449.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33852994 |
|    mean velocity x | -4.18       |
|    mean velocity y | -3.27       |
|    mean velocity z | 11.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.88e+04   |
| time/              |             |
|    total_timesteps | 1500500     |
------------------------------------
Eval num_timesteps=1501000, episode_reward=-76972.63 +/- 42549.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12080648 |
|    mean velocity x | 0.0942      |
|    mean velocity y | 0.0262      |
|    mean velocity z | 0.421       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.7e+04    |
| time/              |             |
|    total_timesteps | 1501000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 733     |
|    time_elapsed    | 60491   |
|    total_timesteps | 1501184 |
--------------------------------
Eval num_timesteps=1501500, episode_reward=-36466.30 +/- 25261.58
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35125092   |
|    mean velocity x      | -0.123        |
|    mean velocity y      | 0.942         |
|    mean velocity z      | 4.38          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.65e+04     |
| time/                   |               |
|    total_timesteps      | 1501500       |
| train/                  |               |
|    approx_kl            | 2.8865441e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.349         |
|    learning_rate        | 0.001         |
|    loss                 | 4.38e+07      |
|    n_updates            | 7330          |
|    policy_gradient_loss | -0.000314     |
|    std                  | 1.55          |
|    value_loss           | 6.3e+07       |
-------------------------------------------
Eval num_timesteps=1502000, episode_reward=-60690.67 +/- 41880.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37565953 |
|    mean velocity x | -0.337      |
|    mean velocity y | 0.686       |
|    mean velocity z | 4.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.07e+04   |
| time/              |             |
|    total_timesteps | 1502000     |
------------------------------------
Eval num_timesteps=1502500, episode_reward=-112013.75 +/- 27815.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30922467 |
|    mean velocity x | -0.101      |
|    mean velocity y | 0.601       |
|    mean velocity z | 0.828       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 1502500     |
------------------------------------
Eval num_timesteps=1503000, episode_reward=-67749.01 +/- 43291.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27149352 |
|    mean velocity x | -0.805      |
|    mean velocity y | -0.136      |
|    mean velocity z | 3.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.77e+04   |
| time/              |             |
|    total_timesteps | 1503000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 734     |
|    time_elapsed    | 60572   |
|    total_timesteps | 1503232 |
--------------------------------
Eval num_timesteps=1503500, episode_reward=-43471.23 +/- 27894.70
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.45164597  |
|    mean velocity x      | 0.118        |
|    mean velocity y      | 1.47         |
|    mean velocity z      | 3.58         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.35e+04    |
| time/                   |              |
|    total_timesteps      | 1503500      |
| train/                  |              |
|    approx_kl            | 1.063128e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.001        |
|    loss                 | 9.31e+06     |
|    n_updates            | 7340         |
|    policy_gradient_loss | -0.000329    |
|    std                  | 1.55         |
|    value_loss           | 4.62e+07     |
------------------------------------------
Eval num_timesteps=1504000, episode_reward=-62724.29 +/- 43779.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28365847 |
|    mean velocity x | 0.0178      |
|    mean velocity y | 0.588       |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.27e+04   |
| time/              |             |
|    total_timesteps | 1504000     |
------------------------------------
Eval num_timesteps=1504500, episode_reward=-50107.54 +/- 40141.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4053743 |
|    mean velocity x | -0.227     |
|    mean velocity y | 0.95       |
|    mean velocity z | 2.96       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.01e+04  |
| time/              |            |
|    total_timesteps | 1504500    |
-----------------------------------
Eval num_timesteps=1505000, episode_reward=-80146.21 +/- 6054.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44934967 |
|    mean velocity x | -0.541      |
|    mean velocity y | 0.455       |
|    mean velocity z | 3.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 1505000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 735     |
|    time_elapsed    | 60652   |
|    total_timesteps | 1505280 |
--------------------------------
Eval num_timesteps=1505500, episode_reward=-116240.64 +/- 13332.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47760704   |
|    mean velocity x      | -0.34         |
|    mean velocity y      | 1.23          |
|    mean velocity z      | 4.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.16e+05     |
| time/                   |               |
|    total_timesteps      | 1505500       |
| train/                  |               |
|    approx_kl            | 1.2004923e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 5.11e+06      |
|    n_updates            | 7350          |
|    policy_gradient_loss | -0.000354     |
|    std                  | 1.55          |
|    value_loss           | 6.79e+07      |
-------------------------------------------
Eval num_timesteps=1506000, episode_reward=-75059.25 +/- 27415.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44751763 |
|    mean velocity x | -0.536      |
|    mean velocity y | 1.02        |
|    mean velocity z | 2.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1506000     |
------------------------------------
Eval num_timesteps=1506500, episode_reward=-77181.27 +/- 38086.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.132619 |
|    mean velocity x | -0.0889   |
|    mean velocity y | 0.388     |
|    mean velocity z | 0.243     |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.72e+04 |
| time/              |           |
|    total_timesteps | 1506500   |
----------------------------------
Eval num_timesteps=1507000, episode_reward=-63000.47 +/- 26142.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3600519 |
|    mean velocity x | 0.108      |
|    mean velocity y | 1.19       |
|    mean velocity z | 3.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.3e+04   |
| time/              |            |
|    total_timesteps | 1507000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 736     |
|    time_elapsed    | 60732   |
|    total_timesteps | 1507328 |
--------------------------------
Eval num_timesteps=1507500, episode_reward=-72061.23 +/- 39535.39
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.4316095  |
|    mean velocity x      | -1.49       |
|    mean velocity y      | 0.152       |
|    mean velocity z      | 3.55        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -7.21e+04   |
| time/                   |             |
|    total_timesteps      | 1507500     |
| train/                  |             |
|    approx_kl            | 6.20583e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.001       |
|    loss                 | 1.29e+06    |
|    n_updates            | 7360        |
|    policy_gradient_loss | -0.000547   |
|    std                  | 1.56        |
|    value_loss           | 3.42e+07    |
-----------------------------------------
Eval num_timesteps=1508000, episode_reward=-77450.84 +/- 39631.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40640205 |
|    mean velocity x | -0.339      |
|    mean velocity y | 0.614       |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.75e+04   |
| time/              |             |
|    total_timesteps | 1508000     |
------------------------------------
Eval num_timesteps=1508500, episode_reward=-84527.47 +/- 34120.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47065845 |
|    mean velocity x | -0.153      |
|    mean velocity y | 0.964       |
|    mean velocity z | 2.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.45e+04   |
| time/              |             |
|    total_timesteps | 1508500     |
------------------------------------
Eval num_timesteps=1509000, episode_reward=-83837.32 +/- 46740.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4445692 |
|    mean velocity x | -0.211     |
|    mean velocity y | 0.775      |
|    mean velocity z | 3.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.38e+04  |
| time/              |            |
|    total_timesteps | 1509000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 737     |
|    time_elapsed    | 60812   |
|    total_timesteps | 1509376 |
--------------------------------
Eval num_timesteps=1509500, episode_reward=-94486.27 +/- 40916.07
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5287377    |
|    mean velocity x      | -0.479        |
|    mean velocity y      | 0.805         |
|    mean velocity z      | 4.2           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.45e+04     |
| time/                   |               |
|    total_timesteps      | 1509500       |
| train/                  |               |
|    approx_kl            | 0.00052674033 |
|    clip_fraction        | 0.0021        |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.001         |
|    loss                 | 4.26e+07      |
|    n_updates            | 7370          |
|    policy_gradient_loss | -0.00279      |
|    std                  | 1.55          |
|    value_loss           | 8.57e+07      |
-------------------------------------------
Eval num_timesteps=1510000, episode_reward=-90581.65 +/- 29954.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.56039196 |
|    mean velocity x | -0.692      |
|    mean velocity y | 0.752       |
|    mean velocity z | 4.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.06e+04   |
| time/              |             |
|    total_timesteps | 1510000     |
------------------------------------
Eval num_timesteps=1510500, episode_reward=-76852.90 +/- 44854.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47473884 |
|    mean velocity x | -0.427      |
|    mean velocity y | 0.86        |
|    mean velocity z | 2.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 1510500     |
------------------------------------
Eval num_timesteps=1511000, episode_reward=-76439.45 +/- 42772.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5270974 |
|    mean velocity x | -0.288     |
|    mean velocity y | 1.57       |
|    mean velocity z | 4.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.64e+04  |
| time/              |            |
|    total_timesteps | 1511000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 738     |
|    time_elapsed    | 60893   |
|    total_timesteps | 1511424 |
--------------------------------
Eval num_timesteps=1511500, episode_reward=-107225.95 +/- 16215.43
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5061571    |
|    mean velocity x      | -0.854        |
|    mean velocity y      | 0.274         |
|    mean velocity z      | 4.61          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 1511500       |
| train/                  |               |
|    approx_kl            | 1.1871569e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.358         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+07      |
|    n_updates            | 7380          |
|    policy_gradient_loss | -0.000231     |
|    std                  | 1.56          |
|    value_loss           | 4.75e+07      |
-------------------------------------------
Eval num_timesteps=1512000, episode_reward=-84128.87 +/- 17670.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.022035878 |
|    mean velocity x | -0.888      |
|    mean velocity y | -1.45       |
|    mean velocity z | 4.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.41e+04   |
| time/              |             |
|    total_timesteps | 1512000     |
------------------------------------
Eval num_timesteps=1512500, episode_reward=-88948.09 +/- 30971.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34404743 |
|    mean velocity x | -0.603      |
|    mean velocity y | 0.73        |
|    mean velocity z | 2.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.89e+04   |
| time/              |             |
|    total_timesteps | 1512500     |
------------------------------------
Eval num_timesteps=1513000, episode_reward=-81115.98 +/- 48889.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4118868 |
|    mean velocity x | -0.207     |
|    mean velocity y | 0.992      |
|    mean velocity z | 4.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.11e+04  |
| time/              |            |
|    total_timesteps | 1513000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 739     |
|    time_elapsed    | 60973   |
|    total_timesteps | 1513472 |
--------------------------------
Eval num_timesteps=1513500, episode_reward=-77104.13 +/- 55355.82
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5000937   |
|    mean velocity x      | -0.36        |
|    mean velocity y      | 0.654        |
|    mean velocity z      | 3.92         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.71e+04    |
| time/                   |              |
|    total_timesteps      | 1513500      |
| train/                  |              |
|    approx_kl            | 4.958507e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.001        |
|    loss                 | 4.12e+07     |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.000149    |
|    std                  | 1.56         |
|    value_loss           | 5.01e+07     |
------------------------------------------
Eval num_timesteps=1514000, episode_reward=-57514.01 +/- 30686.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5115808 |
|    mean velocity x | -0.383     |
|    mean velocity y | 1.02       |
|    mean velocity z | 4.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.75e+04  |
| time/              |            |
|    total_timesteps | 1514000    |
-----------------------------------
Eval num_timesteps=1514500, episode_reward=-95900.31 +/- 43300.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34544986 |
|    mean velocity x | -0.238      |
|    mean velocity y | 0.338       |
|    mean velocity z | 4.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.59e+04   |
| time/              |             |
|    total_timesteps | 1514500     |
------------------------------------
Eval num_timesteps=1515000, episode_reward=-67288.24 +/- 38546.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2254661 |
|    mean velocity x | 0.0248     |
|    mean velocity y | 0.499      |
|    mean velocity z | 0.857      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.73e+04  |
| time/              |            |
|    total_timesteps | 1515000    |
-----------------------------------
Eval num_timesteps=1515500, episode_reward=-85861.64 +/- 46248.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5029893 |
|    mean velocity x | -0.611     |
|    mean velocity y | 0.726      |
|    mean velocity z | 4.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.59e+04  |
| time/              |            |
|    total_timesteps | 1515500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 740     |
|    time_elapsed    | 61072   |
|    total_timesteps | 1515520 |
--------------------------------
Eval num_timesteps=1516000, episode_reward=-83183.89 +/- 44833.04
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3831074    |
|    mean velocity x      | -0.232        |
|    mean velocity y      | 0.695         |
|    mean velocity z      | 3.97          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.32e+04     |
| time/                   |               |
|    total_timesteps      | 1516000       |
| train/                  |               |
|    approx_kl            | 3.4388446e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+07      |
|    n_updates            | 7400          |
|    policy_gradient_loss | -0.000605     |
|    std                  | 1.56          |
|    value_loss           | 1.07e+08      |
-------------------------------------------
Eval num_timesteps=1516500, episode_reward=-73419.96 +/- 44340.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4554798 |
|    mean velocity x | -0.435     |
|    mean velocity y | 0.626      |
|    mean velocity z | 4.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.34e+04  |
| time/              |            |
|    total_timesteps | 1516500    |
-----------------------------------
Eval num_timesteps=1517000, episode_reward=-86858.95 +/- 17272.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2541419 |
|    mean velocity x | -0.114     |
|    mean velocity y | 0.227      |
|    mean velocity z | 1.39       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 1517000    |
-----------------------------------
Eval num_timesteps=1517500, episode_reward=-85492.51 +/- 49216.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37848696 |
|    mean velocity x | 0.0542      |
|    mean velocity y | 0.934       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.55e+04   |
| time/              |             |
|    total_timesteps | 1517500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 741     |
|    time_elapsed    | 61153   |
|    total_timesteps | 1517568 |
--------------------------------
Eval num_timesteps=1518000, episode_reward=-95726.74 +/- 30612.46
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40866444   |
|    mean velocity x      | -1.56         |
|    mean velocity y      | -0.204        |
|    mean velocity z      | 4.99          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.57e+04     |
| time/                   |               |
|    total_timesteps      | 1518000       |
| train/                  |               |
|    approx_kl            | 1.0777527e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.001         |
|    loss                 | 6.94e+07      |
|    n_updates            | 7410          |
|    policy_gradient_loss | -0.000301     |
|    std                  | 1.56          |
|    value_loss           | 5.36e+07      |
-------------------------------------------
Eval num_timesteps=1518500, episode_reward=-100518.90 +/- 21667.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26279902 |
|    mean velocity x | 0.0128      |
|    mean velocity y | 0.187       |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1518500     |
------------------------------------
Eval num_timesteps=1519000, episode_reward=-128873.66 +/- 46641.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31052586 |
|    mean velocity x | -0.124      |
|    mean velocity y | 0.346       |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.29e+05   |
| time/              |             |
|    total_timesteps | 1519000     |
------------------------------------
Eval num_timesteps=1519500, episode_reward=-99194.91 +/- 99005.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32201645 |
|    mean velocity x | -0.687      |
|    mean velocity y | 0.806       |
|    mean velocity z | 1.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.92e+04   |
| time/              |             |
|    total_timesteps | 1519500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 742     |
|    time_elapsed    | 61233   |
|    total_timesteps | 1519616 |
--------------------------------
Eval num_timesteps=1520000, episode_reward=-89758.60 +/- 17708.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34902313   |
|    mean velocity x      | -0.0833       |
|    mean velocity y      | 0.498         |
|    mean velocity z      | 3.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.98e+04     |
| time/                   |               |
|    total_timesteps      | 1520000       |
| train/                  |               |
|    approx_kl            | 3.6599813e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.258         |
|    learning_rate        | 0.001         |
|    loss                 | 6.05e+07      |
|    n_updates            | 7420          |
|    policy_gradient_loss | -0.000112     |
|    std                  | 1.55          |
|    value_loss           | 6.59e+07      |
-------------------------------------------
Eval num_timesteps=1520500, episode_reward=-63490.34 +/- 27286.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33168456 |
|    mean velocity x | -0.642      |
|    mean velocity y | 0.188       |
|    mean velocity z | 2.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+04   |
| time/              |             |
|    total_timesteps | 1520500     |
------------------------------------
Eval num_timesteps=1521000, episode_reward=-119609.41 +/- 11236.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42546052 |
|    mean velocity x | -0.0817     |
|    mean velocity y | 0.934       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 1521000     |
------------------------------------
Eval num_timesteps=1521500, episode_reward=-71886.14 +/- 39145.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36667785 |
|    mean velocity x | -0.578      |
|    mean velocity y | -0.203      |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.19e+04   |
| time/              |             |
|    total_timesteps | 1521500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 743     |
|    time_elapsed    | 61313   |
|    total_timesteps | 1521664 |
--------------------------------
Eval num_timesteps=1522000, episode_reward=-79282.34 +/- 51120.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30780706   |
|    mean velocity x      | -1.11         |
|    mean velocity y      | -0.532        |
|    mean velocity z      | 3.27          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.93e+04     |
| time/                   |               |
|    total_timesteps      | 1522000       |
| train/                  |               |
|    approx_kl            | 2.1198153e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+07      |
|    n_updates            | 7430          |
|    policy_gradient_loss | -0.000319     |
|    std                  | 1.56          |
|    value_loss           | 5.97e+07      |
-------------------------------------------
Eval num_timesteps=1522500, episode_reward=-55620.43 +/- 45263.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42384243 |
|    mean velocity x | -0.161      |
|    mean velocity y | 1.31        |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.56e+04   |
| time/              |             |
|    total_timesteps | 1522500     |
------------------------------------
Eval num_timesteps=1523000, episode_reward=-54626.98 +/- 40507.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49215564 |
|    mean velocity x | -0.219      |
|    mean velocity y | 1.28        |
|    mean velocity z | 2.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.46e+04   |
| time/              |             |
|    total_timesteps | 1523000     |
------------------------------------
Eval num_timesteps=1523500, episode_reward=-81263.06 +/- 40843.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4769718 |
|    mean velocity x | -0.464     |
|    mean velocity y | 0.488      |
|    mean velocity z | 4.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.13e+04  |
| time/              |            |
|    total_timesteps | 1523500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 744     |
|    time_elapsed    | 61394   |
|    total_timesteps | 1523712 |
--------------------------------
Eval num_timesteps=1524000, episode_reward=-64472.50 +/- 34377.64
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39813885   |
|    mean velocity x      | -0.0585       |
|    mean velocity y      | 1.17          |
|    mean velocity z      | 4.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.45e+04     |
| time/                   |               |
|    total_timesteps      | 1524000       |
| train/                  |               |
|    approx_kl            | 1.0003278e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.001         |
|    loss                 | 4.42e+07      |
|    n_updates            | 7440          |
|    policy_gradient_loss | -0.000267     |
|    std                  | 1.56          |
|    value_loss           | 6.94e+07      |
-------------------------------------------
Eval num_timesteps=1524500, episode_reward=-88759.44 +/- 20601.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1957217 |
|    mean velocity x | -0.547     |
|    mean velocity y | 0.422      |
|    mean velocity z | 1.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.88e+04  |
| time/              |            |
|    total_timesteps | 1524500    |
-----------------------------------
Eval num_timesteps=1525000, episode_reward=-37832.80 +/- 26603.93
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.40304  |
|    mean velocity x | -0.808    |
|    mean velocity y | 0.828     |
|    mean velocity z | 2.17      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.78e+04 |
| time/              |           |
|    total_timesteps | 1525000   |
----------------------------------
Eval num_timesteps=1525500, episode_reward=-104397.68 +/- 11590.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4157733 |
|    mean velocity x | -0.119     |
|    mean velocity y | 1.34       |
|    mean velocity z | 4.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1525500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 745     |
|    time_elapsed    | 61474   |
|    total_timesteps | 1525760 |
--------------------------------
Eval num_timesteps=1526000, episode_reward=-100490.71 +/- 19971.02
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35778484   |
|    mean velocity x      | -0.809        |
|    mean velocity y      | -0.207        |
|    mean velocity z      | 3.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1e+05        |
| time/                   |               |
|    total_timesteps      | 1526000       |
| train/                  |               |
|    approx_kl            | 2.3437402e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.258         |
|    learning_rate        | 0.001         |
|    loss                 | 1.74e+07      |
|    n_updates            | 7450          |
|    policy_gradient_loss | -0.000286     |
|    std                  | 1.56          |
|    value_loss           | 3.52e+07      |
-------------------------------------------
Eval num_timesteps=1526500, episode_reward=-65339.19 +/- 40981.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38432366 |
|    mean velocity x | 0.0708      |
|    mean velocity y | 0.793       |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.53e+04   |
| time/              |             |
|    total_timesteps | 1526500     |
------------------------------------
Eval num_timesteps=1527000, episode_reward=-90411.50 +/- 27587.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40432453 |
|    mean velocity x | -0.23       |
|    mean velocity y | 0.739       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.04e+04   |
| time/              |             |
|    total_timesteps | 1527000     |
------------------------------------
Eval num_timesteps=1527500, episode_reward=-53540.33 +/- 36164.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4592304 |
|    mean velocity x | -0.319     |
|    mean velocity y | 0.912      |
|    mean velocity z | 4.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.35e+04  |
| time/              |            |
|    total_timesteps | 1527500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 746     |
|    time_elapsed    | 61554   |
|    total_timesteps | 1527808 |
--------------------------------
Eval num_timesteps=1528000, episode_reward=-62706.00 +/- 33704.84
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.1789984    |
|    mean velocity x      | 0.242         |
|    mean velocity y      | 0.233         |
|    mean velocity z      | 0.398         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.27e+04     |
| time/                   |               |
|    total_timesteps      | 1528000       |
| train/                  |               |
|    approx_kl            | 1.0252901e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 7.25e+07      |
|    n_updates            | 7460          |
|    policy_gradient_loss | -0.00021      |
|    std                  | 1.56          |
|    value_loss           | 8.57e+07      |
-------------------------------------------
Eval num_timesteps=1528500, episode_reward=-63980.58 +/- 47244.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4640474 |
|    mean velocity x | 0.179      |
|    mean velocity y | 1.38       |
|    mean velocity z | 4.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.4e+04   |
| time/              |            |
|    total_timesteps | 1528500    |
-----------------------------------
Eval num_timesteps=1529000, episode_reward=-89095.96 +/- 21355.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28157273 |
|    mean velocity x | -0.558      |
|    mean velocity y | 0.399       |
|    mean velocity z | 1.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.91e+04   |
| time/              |             |
|    total_timesteps | 1529000     |
------------------------------------
Eval num_timesteps=1529500, episode_reward=-109414.18 +/- 15783.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44667816 |
|    mean velocity x | -0.203      |
|    mean velocity y | 0.857       |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1529500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 747     |
|    time_elapsed    | 61635   |
|    total_timesteps | 1529856 |
--------------------------------
Eval num_timesteps=1530000, episode_reward=-80341.65 +/- 40981.07
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38439488  |
|    mean velocity x      | 0.242        |
|    mean velocity y      | 1.29         |
|    mean velocity z      | 3.15         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.03e+04    |
| time/                   |              |
|    total_timesteps      | 1530000      |
| train/                  |              |
|    approx_kl            | 5.054899e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.282        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+07     |
|    n_updates            | 7470         |
|    policy_gradient_loss | -0.000156    |
|    std                  | 1.56         |
|    value_loss           | 4.76e+07     |
------------------------------------------
Eval num_timesteps=1530500, episode_reward=-71541.85 +/- 48504.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20958279 |
|    mean velocity x | -0.114      |
|    mean velocity y | 0.415       |
|    mean velocity z | 0.408       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.15e+04   |
| time/              |             |
|    total_timesteps | 1530500     |
------------------------------------
Eval num_timesteps=1531000, episode_reward=-68391.14 +/- 22700.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44827726 |
|    mean velocity x | -0.0413     |
|    mean velocity y | 1.33        |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 1531000     |
------------------------------------
Eval num_timesteps=1531500, episode_reward=-60117.49 +/- 46280.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41873407 |
|    mean velocity x | -0.419      |
|    mean velocity y | 0.564       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.01e+04   |
| time/              |             |
|    total_timesteps | 1531500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 748     |
|    time_elapsed    | 61715   |
|    total_timesteps | 1531904 |
--------------------------------
Eval num_timesteps=1532000, episode_reward=-94073.25 +/- 30220.25
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3747316    |
|    mean velocity x      | 0.23          |
|    mean velocity y      | 1.49          |
|    mean velocity z      | 3.51          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.41e+04     |
| time/                   |               |
|    total_timesteps      | 1532000       |
| train/                  |               |
|    approx_kl            | 5.9496087e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.244         |
|    learning_rate        | 0.001         |
|    loss                 | 4.81e+06      |
|    n_updates            | 7480          |
|    policy_gradient_loss | -0.000178     |
|    std                  | 1.56          |
|    value_loss           | 5.69e+07      |
-------------------------------------------
Eval num_timesteps=1532500, episode_reward=-94115.59 +/- 28097.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43167007 |
|    mean velocity x | -0.601      |
|    mean velocity y | 0.0545      |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.41e+04   |
| time/              |             |
|    total_timesteps | 1532500     |
------------------------------------
Eval num_timesteps=1533000, episode_reward=-77113.40 +/- 26471.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.57329357 |
|    mean velocity x | 0.193       |
|    mean velocity y | 1.62        |
|    mean velocity z | 3.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 1533000     |
------------------------------------
Eval num_timesteps=1533500, episode_reward=-69759.43 +/- 36390.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3208398 |
|    mean velocity x | -1.25      |
|    mean velocity y | -0.299     |
|    mean velocity z | 3.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.98e+04  |
| time/              |            |
|    total_timesteps | 1533500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 749     |
|    time_elapsed    | 61795   |
|    total_timesteps | 1533952 |
--------------------------------
Eval num_timesteps=1534000, episode_reward=-110297.06 +/- 19529.73
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3120769   |
|    mean velocity x      | -0.305       |
|    mean velocity y      | 0.647        |
|    mean velocity z      | 1.25         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.1e+05     |
| time/                   |              |
|    total_timesteps      | 1534000      |
| train/                  |              |
|    approx_kl            | 1.684454e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.369        |
|    learning_rate        | 0.001        |
|    loss                 | 5.13e+06     |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.000323    |
|    std                  | 1.56         |
|    value_loss           | 3.2e+07      |
------------------------------------------
Eval num_timesteps=1534500, episode_reward=-52910.35 +/- 36921.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40316615 |
|    mean velocity x | 0.0558      |
|    mean velocity y | 1.1         |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.29e+04   |
| time/              |             |
|    total_timesteps | 1534500     |
------------------------------------
Eval num_timesteps=1535000, episode_reward=-86710.73 +/- 31211.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3628119 |
|    mean velocity x | -0.0792    |
|    mean velocity y | 0.536      |
|    mean velocity z | 3.21       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.67e+04  |
| time/              |            |
|    total_timesteps | 1535000    |
-----------------------------------
Eval num_timesteps=1535500, episode_reward=-77010.42 +/- 37618.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48592174 |
|    mean velocity x | -0.418      |
|    mean velocity y | 0.645       |
|    mean velocity z | 5.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.7e+04    |
| time/              |             |
|    total_timesteps | 1535500     |
------------------------------------
Eval num_timesteps=1536000, episode_reward=-90336.16 +/- 20267.57
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4328116 |
|    mean velocity x | -0.206     |
|    mean velocity y | 0.614      |
|    mean velocity z | 4.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.03e+04  |
| time/              |            |
|    total_timesteps | 1536000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 750     |
|    time_elapsed    | 61895   |
|    total_timesteps | 1536000 |
--------------------------------
Eval num_timesteps=1536500, episode_reward=-69153.85 +/- 37594.20
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43776676   |
|    mean velocity x      | -0.46         |
|    mean velocity y      | 0.657         |
|    mean velocity z      | 3.76          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.92e+04     |
| time/                   |               |
|    total_timesteps      | 1536500       |
| train/                  |               |
|    approx_kl            | 9.1702095e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.001         |
|    loss                 | 8.58e+07      |
|    n_updates            | 7500          |
|    policy_gradient_loss | -0.000174     |
|    std                  | 1.56          |
|    value_loss           | 8.96e+07      |
-------------------------------------------
Eval num_timesteps=1537000, episode_reward=-49956.49 +/- 41428.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47848168 |
|    mean velocity x | -0.0881     |
|    mean velocity y | 0.802       |
|    mean velocity z | 2.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5e+04      |
| time/              |             |
|    total_timesteps | 1537000     |
------------------------------------
Eval num_timesteps=1537500, episode_reward=-93676.88 +/- 23421.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46709228 |
|    mean velocity x | 0.126       |
|    mean velocity y | 1.24        |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.37e+04   |
| time/              |             |
|    total_timesteps | 1537500     |
------------------------------------
Eval num_timesteps=1538000, episode_reward=-59607.52 +/- 35859.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42880476 |
|    mean velocity x | -0.225      |
|    mean velocity y | 1.11        |
|    mean velocity z | 2           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.96e+04   |
| time/              |             |
|    total_timesteps | 1538000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 751     |
|    time_elapsed    | 61975   |
|    total_timesteps | 1538048 |
--------------------------------
Eval num_timesteps=1538500, episode_reward=-91896.53 +/- 41233.33
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.52924925    |
|    mean velocity x      | -0.871         |
|    mean velocity y      | 0.92           |
|    mean velocity z      | 2.69           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -9.19e+04      |
| time/                   |                |
|    total_timesteps      | 1538500        |
| train/                  |                |
|    approx_kl            | 1.51467975e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.57          |
|    explained_variance   | 0.318          |
|    learning_rate        | 0.001          |
|    loss                 | 1.07e+07       |
|    n_updates            | 7510           |
|    policy_gradient_loss | -0.000562      |
|    std                  | 1.56           |
|    value_loss           | 3.15e+07       |
--------------------------------------------
Eval num_timesteps=1539000, episode_reward=-77147.98 +/- 40208.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37974495 |
|    mean velocity x | 0.149       |
|    mean velocity y | 0.438       |
|    mean velocity z | 1.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 1539000     |
------------------------------------
Eval num_timesteps=1539500, episode_reward=-80400.45 +/- 34919.46
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4166221 |
|    mean velocity x | -0.17      |
|    mean velocity y | 0.838      |
|    mean velocity z | 4.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 1539500    |
-----------------------------------
Eval num_timesteps=1540000, episode_reward=-60677.34 +/- 46873.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46186796 |
|    mean velocity x | -0.104      |
|    mean velocity y | 0.877       |
|    mean velocity z | 3.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.07e+04   |
| time/              |             |
|    total_timesteps | 1540000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 752     |
|    time_elapsed    | 62055   |
|    total_timesteps | 1540096 |
--------------------------------
Eval num_timesteps=1540500, episode_reward=-92675.28 +/- 30215.34
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46859276   |
|    mean velocity x      | -0.629        |
|    mean velocity y      | 0.51          |
|    mean velocity z      | 3.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.27e+04     |
| time/                   |               |
|    total_timesteps      | 1540500       |
| train/                  |               |
|    approx_kl            | 2.2246531e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.001         |
|    loss                 | 3.35e+07      |
|    n_updates            | 7520          |
|    policy_gradient_loss | -0.000116     |
|    std                  | 1.56          |
|    value_loss           | 5.1e+07       |
-------------------------------------------
Eval num_timesteps=1541000, episode_reward=-98988.89 +/- 8057.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4723743 |
|    mean velocity x | 0.384      |
|    mean velocity y | 1.46       |
|    mean velocity z | 3.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.9e+04   |
| time/              |            |
|    total_timesteps | 1541000    |
-----------------------------------
Eval num_timesteps=1541500, episode_reward=-51754.80 +/- 38282.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35730246 |
|    mean velocity x | -0.342      |
|    mean velocity y | 0.663       |
|    mean velocity z | 0.904       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.18e+04   |
| time/              |             |
|    total_timesteps | 1541500     |
------------------------------------
Eval num_timesteps=1542000, episode_reward=-70457.21 +/- 42229.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5723291 |
|    mean velocity x | 0.432      |
|    mean velocity y | 1.75       |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.05e+04  |
| time/              |            |
|    total_timesteps | 1542000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 753     |
|    time_elapsed    | 62136   |
|    total_timesteps | 1542144 |
--------------------------------
Eval num_timesteps=1542500, episode_reward=-95732.31 +/- 19317.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4850501   |
|    mean velocity x      | 0.152        |
|    mean velocity y      | 1.64         |
|    mean velocity z      | 4.07         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.57e+04    |
| time/                   |              |
|    total_timesteps      | 1542500      |
| train/                  |              |
|    approx_kl            | 2.750766e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.001        |
|    loss                 | 2.88e+06     |
|    n_updates            | 7530         |
|    policy_gradient_loss | -0.000383    |
|    std                  | 1.55         |
|    value_loss           | 2.8e+07      |
------------------------------------------
Eval num_timesteps=1543000, episode_reward=-71013.43 +/- 24639.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3783727 |
|    mean velocity x | -1.07      |
|    mean velocity y | 0.0609     |
|    mean velocity z | 3.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.1e+04   |
| time/              |            |
|    total_timesteps | 1543000    |
-----------------------------------
Eval num_timesteps=1543500, episode_reward=-109605.87 +/- 25156.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36763996 |
|    mean velocity x | -0.234      |
|    mean velocity y | 1.04        |
|    mean velocity z | 4.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 1543500     |
------------------------------------
Eval num_timesteps=1544000, episode_reward=-94781.79 +/- 33093.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48129657 |
|    mean velocity x | -0.385      |
|    mean velocity y | 0.778       |
|    mean velocity z | 4.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.48e+04   |
| time/              |             |
|    total_timesteps | 1544000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 754     |
|    time_elapsed    | 62216   |
|    total_timesteps | 1544192 |
--------------------------------
Eval num_timesteps=1544500, episode_reward=-108251.40 +/- 41027.03
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40999246   |
|    mean velocity x      | -0.327        |
|    mean velocity y      | 0.778         |
|    mean velocity z      | 4.02          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.08e+05     |
| time/                   |               |
|    total_timesteps      | 1544500       |
| train/                  |               |
|    approx_kl            | 0.00010488811 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 2.74e+07      |
|    n_updates            | 7540          |
|    policy_gradient_loss | -0.000799     |
|    std                  | 1.55          |
|    value_loss           | 9.92e+07      |
-------------------------------------------
Eval num_timesteps=1545000, episode_reward=-83118.11 +/- 33901.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46012175 |
|    mean velocity x | -0.385      |
|    mean velocity y | 0.939       |
|    mean velocity z | 4.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 1545000     |
------------------------------------
Eval num_timesteps=1545500, episode_reward=-82893.73 +/- 27805.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48922434 |
|    mean velocity x | 0.379       |
|    mean velocity y | 1.56        |
|    mean velocity z | 3.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 1545500     |
------------------------------------
Eval num_timesteps=1546000, episode_reward=-61325.13 +/- 28653.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39124542 |
|    mean velocity x | -0.365      |
|    mean velocity y | 0.78        |
|    mean velocity z | 1.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.13e+04   |
| time/              |             |
|    total_timesteps | 1546000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 755     |
|    time_elapsed    | 62296   |
|    total_timesteps | 1546240 |
--------------------------------
Eval num_timesteps=1546500, episode_reward=-82326.44 +/- 48313.10
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.458989    |
|    mean velocity x      | -0.857       |
|    mean velocity y      | 0.216        |
|    mean velocity z      | 3.61         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.23e+04    |
| time/                   |              |
|    total_timesteps      | 1546500      |
| train/                  |              |
|    approx_kl            | 8.804898e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.001        |
|    loss                 | 4.63e+06     |
|    n_updates            | 7550         |
|    policy_gradient_loss | -0.000302    |
|    std                  | 1.55         |
|    value_loss           | 3.75e+07     |
------------------------------------------
Eval num_timesteps=1547000, episode_reward=-74633.73 +/- 46846.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4005919 |
|    mean velocity x | 0.282      |
|    mean velocity y | 1.54       |
|    mean velocity z | 3.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.46e+04  |
| time/              |            |
|    total_timesteps | 1547000    |
-----------------------------------
Eval num_timesteps=1547500, episode_reward=-65817.27 +/- 41739.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45265773 |
|    mean velocity x | 0.159       |
|    mean velocity y | 1.44        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.58e+04   |
| time/              |             |
|    total_timesteps | 1547500     |
------------------------------------
Eval num_timesteps=1548000, episode_reward=-78602.33 +/- 46668.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46456367 |
|    mean velocity x | -0.37       |
|    mean velocity y | 0.698       |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.86e+04   |
| time/              |             |
|    total_timesteps | 1548000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 756     |
|    time_elapsed    | 62377   |
|    total_timesteps | 1548288 |
--------------------------------
Eval num_timesteps=1548500, episode_reward=-57858.92 +/- 24637.24
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41567546   |
|    mean velocity x      | -0.249        |
|    mean velocity y      | 0.689         |
|    mean velocity z      | 4.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.79e+04     |
| time/                   |               |
|    total_timesteps      | 1548500       |
| train/                  |               |
|    approx_kl            | 3.4205645e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.001         |
|    loss                 | 7.12e+07      |
|    n_updates            | 7560          |
|    policy_gradient_loss | -0.000348     |
|    std                  | 1.55          |
|    value_loss           | 8.23e+07      |
-------------------------------------------
Eval num_timesteps=1549000, episode_reward=-55323.08 +/- 49441.33
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.44274  |
|    mean velocity x | -0.066    |
|    mean velocity y | 1.14      |
|    mean velocity z | 4.01      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.53e+04 |
| time/              |           |
|    total_timesteps | 1549000   |
----------------------------------
Eval num_timesteps=1549500, episode_reward=-92726.63 +/- 7700.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34900847 |
|    mean velocity x | -0.826      |
|    mean velocity y | -0.0425     |
|    mean velocity z | 2.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 1549500     |
------------------------------------
Eval num_timesteps=1550000, episode_reward=-85868.83 +/- 26563.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2664152 |
|    mean velocity x | -0.211     |
|    mean velocity y | 0.452      |
|    mean velocity z | 0.316      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.59e+04  |
| time/              |            |
|    total_timesteps | 1550000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 757     |
|    time_elapsed    | 62457   |
|    total_timesteps | 1550336 |
--------------------------------
Eval num_timesteps=1550500, episode_reward=-104978.90 +/- 35449.52
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35652113  |
|    mean velocity x      | -0.237       |
|    mean velocity y      | 0.506        |
|    mean velocity z      | 2.59         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 1550500      |
| train/                  |              |
|    approx_kl            | 9.551196e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.001        |
|    loss                 | 6.81e+06     |
|    n_updates            | 7570         |
|    policy_gradient_loss | -0.000856    |
|    std                  | 1.55         |
|    value_loss           | 3.07e+07     |
------------------------------------------
Eval num_timesteps=1551000, episode_reward=-61513.44 +/- 41129.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41157544 |
|    mean velocity x | -0.392      |
|    mean velocity y | 0.384       |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.15e+04   |
| time/              |             |
|    total_timesteps | 1551000     |
------------------------------------
Eval num_timesteps=1551500, episode_reward=-90045.60 +/- 19570.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5112293 |
|    mean velocity x | -0.347     |
|    mean velocity y | 1.37       |
|    mean velocity z | 4.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9e+04     |
| time/              |            |
|    total_timesteps | 1551500    |
-----------------------------------
Eval num_timesteps=1552000, episode_reward=-78621.08 +/- 43037.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52050805 |
|    mean velocity x | -0.225      |
|    mean velocity y | 1.37        |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.86e+04   |
| time/              |             |
|    total_timesteps | 1552000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 758     |
|    time_elapsed    | 62538   |
|    total_timesteps | 1552384 |
--------------------------------
Eval num_timesteps=1552500, episode_reward=-107928.76 +/- 15820.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.29533252   |
|    mean velocity x      | 0.0181        |
|    mean velocity y      | 0.442         |
|    mean velocity z      | 2.13          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.08e+05     |
| time/                   |               |
|    total_timesteps      | 1552500       |
| train/                  |               |
|    approx_kl            | 2.4011213e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.001         |
|    loss                 | 2.91e+07      |
|    n_updates            | 7580          |
|    policy_gradient_loss | -0.000424     |
|    std                  | 1.56          |
|    value_loss           | 6.89e+07      |
-------------------------------------------
Eval num_timesteps=1553000, episode_reward=-63504.86 +/- 38642.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45555884 |
|    mean velocity x | -0.306      |
|    mean velocity y | 0.802       |
|    mean velocity z | 4.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+04   |
| time/              |             |
|    total_timesteps | 1553000     |
------------------------------------
Eval num_timesteps=1553500, episode_reward=-52465.27 +/- 46295.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5323381 |
|    mean velocity x | 0.402      |
|    mean velocity y | 1.53       |
|    mean velocity z | 3.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.25e+04  |
| time/              |            |
|    total_timesteps | 1553500    |
-----------------------------------
Eval num_timesteps=1554000, episode_reward=-100922.43 +/- 19391.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39095348 |
|    mean velocity x | -0.21       |
|    mean velocity y | 0.51        |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1554000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 759     |
|    time_elapsed    | 62618   |
|    total_timesteps | 1554432 |
--------------------------------
Eval num_timesteps=1554500, episode_reward=-91124.78 +/- 17401.39
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.4753749  |
|    mean velocity x      | -0.639      |
|    mean velocity y      | 0.478       |
|    mean velocity z      | 4.02        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -9.11e+04   |
| time/                   |             |
|    total_timesteps      | 1554500     |
| train/                  |             |
|    approx_kl            | 5.53453e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.001       |
|    loss                 | 4.86e+06    |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.000237   |
|    std                  | 1.56        |
|    value_loss           | 6.01e+07    |
-----------------------------------------
Eval num_timesteps=1555000, episode_reward=-47990.36 +/- 34175.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3408523 |
|    mean velocity x | -0.98      |
|    mean velocity y | 0.0502     |
|    mean velocity z | 3.15       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.8e+04   |
| time/              |            |
|    total_timesteps | 1555000    |
-----------------------------------
Eval num_timesteps=1555500, episode_reward=-91396.64 +/- 24752.59
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.517267 |
|    mean velocity x | 0.0623    |
|    mean velocity y | 1.47      |
|    mean velocity z | 3.39      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.14e+04 |
| time/              |           |
|    total_timesteps | 1555500   |
----------------------------------
Eval num_timesteps=1556000, episode_reward=-74901.61 +/- 37970.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4948458 |
|    mean velocity x | -0.26      |
|    mean velocity y | 1.34       |
|    mean velocity z | 2.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.49e+04  |
| time/              |            |
|    total_timesteps | 1556000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 760     |
|    time_elapsed    | 62698   |
|    total_timesteps | 1556480 |
--------------------------------
Eval num_timesteps=1556500, episode_reward=-83211.09 +/- 22289.87
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4541463    |
|    mean velocity x      | 0.182         |
|    mean velocity y      | 1.24          |
|    mean velocity z      | 3.33          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.32e+04     |
| time/                   |               |
|    total_timesteps      | 1556500       |
| train/                  |               |
|    approx_kl            | 3.4631084e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.383         |
|    learning_rate        | 0.001         |
|    loss                 | 4.4e+06       |
|    n_updates            | 7600          |
|    policy_gradient_loss | -0.00038      |
|    std                  | 1.56          |
|    value_loss           | 2.41e+07      |
-------------------------------------------
Eval num_timesteps=1557000, episode_reward=-120073.14 +/- 15379.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41232377 |
|    mean velocity x | -0.257      |
|    mean velocity y | 0.867       |
|    mean velocity z | 4.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 1557000     |
------------------------------------
Eval num_timesteps=1557500, episode_reward=-97972.62 +/- 22237.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35223874 |
|    mean velocity x | -0.642      |
|    mean velocity y | 0.499       |
|    mean velocity z | 1.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.8e+04    |
| time/              |             |
|    total_timesteps | 1557500     |
------------------------------------
Eval num_timesteps=1558000, episode_reward=-72469.50 +/- 40776.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42077848 |
|    mean velocity x | -0.0667     |
|    mean velocity y | 0.799       |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.25e+04   |
| time/              |             |
|    total_timesteps | 1558000     |
------------------------------------
Eval num_timesteps=1558500, episode_reward=-108084.83 +/- 48814.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28071687 |
|    mean velocity x | -0.889      |
|    mean velocity y | -0.425      |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1558500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 761     |
|    time_elapsed    | 62798   |
|    total_timesteps | 1558528 |
--------------------------------
Eval num_timesteps=1559000, episode_reward=-89123.84 +/- 27473.27
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46072155  |
|    mean velocity x      | -0.0365      |
|    mean velocity y      | 0.919        |
|    mean velocity z      | 3.7          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.91e+04    |
| time/                   |              |
|    total_timesteps      | 1559000      |
| train/                  |              |
|    approx_kl            | 6.803253e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.001        |
|    loss                 | 4.01e+07     |
|    n_updates            | 7610         |
|    policy_gradient_loss | -0.000128    |
|    std                  | 1.56         |
|    value_loss           | 8.39e+07     |
------------------------------------------
Eval num_timesteps=1559500, episode_reward=-95321.66 +/- 17674.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44066694 |
|    mean velocity x | -0.3        |
|    mean velocity y | 1.13        |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.53e+04   |
| time/              |             |
|    total_timesteps | 1559500     |
------------------------------------
Eval num_timesteps=1560000, episode_reward=-66669.02 +/- 52584.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31663898 |
|    mean velocity x | -0.285      |
|    mean velocity y | 0.564       |
|    mean velocity z | 0.441       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.67e+04   |
| time/              |             |
|    total_timesteps | 1560000     |
------------------------------------
Eval num_timesteps=1560500, episode_reward=-94027.00 +/- 19468.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39561749 |
|    mean velocity x | -0.217      |
|    mean velocity y | 0.992       |
|    mean velocity z | 4.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.4e+04    |
| time/              |             |
|    total_timesteps | 1560500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 762     |
|    time_elapsed    | 62878   |
|    total_timesteps | 1560576 |
--------------------------------
Eval num_timesteps=1561000, episode_reward=-67344.98 +/- 35553.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3735873    |
|    mean velocity x      | -1.03         |
|    mean velocity y      | -0.659        |
|    mean velocity z      | 5.1           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.73e+04     |
| time/                   |               |
|    total_timesteps      | 1561000       |
| train/                  |               |
|    approx_kl            | 1.6555015e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+07      |
|    n_updates            | 7620          |
|    policy_gradient_loss | -0.000346     |
|    std                  | 1.55          |
|    value_loss           | 6.96e+07      |
-------------------------------------------
Eval num_timesteps=1561500, episode_reward=-33972.04 +/- 44852.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34243277 |
|    mean velocity x | -0.178      |
|    mean velocity y | 0.742       |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.4e+04    |
| time/              |             |
|    total_timesteps | 1561500     |
------------------------------------
Eval num_timesteps=1562000, episode_reward=-102497.16 +/- 22559.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24441403 |
|    mean velocity x | -0.145      |
|    mean velocity y | 0.366       |
|    mean velocity z | 2.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1562000     |
------------------------------------
Eval num_timesteps=1562500, episode_reward=-99371.09 +/- 19367.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4353737 |
|    mean velocity x | -0.962     |
|    mean velocity y | 0.283      |
|    mean velocity z | 3.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.94e+04  |
| time/              |            |
|    total_timesteps | 1562500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 763     |
|    time_elapsed    | 62959   |
|    total_timesteps | 1562624 |
--------------------------------
Eval num_timesteps=1563000, episode_reward=-95306.70 +/- 47877.17
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.38615656    |
|    mean velocity x      | -1.1           |
|    mean velocity y      | 0.0763         |
|    mean velocity z      | 2.9            |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -9.53e+04      |
| time/                   |                |
|    total_timesteps      | 1563000        |
| train/                  |                |
|    approx_kl            | 1.45091035e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.57          |
|    explained_variance   | 0.292          |
|    learning_rate        | 0.001          |
|    loss                 | 3.71e+07       |
|    n_updates            | 7630           |
|    policy_gradient_loss | -0.000151      |
|    std                  | 1.55           |
|    value_loss           | 4.73e+07       |
--------------------------------------------
Eval num_timesteps=1563500, episode_reward=-49712.25 +/- 38644.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54628205 |
|    mean velocity x | -0.296      |
|    mean velocity y | 1.43        |
|    mean velocity z | 4.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.97e+04   |
| time/              |             |
|    total_timesteps | 1563500     |
------------------------------------
Eval num_timesteps=1564000, episode_reward=-34950.28 +/- 39346.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19567698 |
|    mean velocity x | -0.354      |
|    mean velocity y | 0.503       |
|    mean velocity z | 0.674       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.5e+04    |
| time/              |             |
|    total_timesteps | 1564000     |
------------------------------------
Eval num_timesteps=1564500, episode_reward=-101340.64 +/- 27421.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5551709 |
|    mean velocity x | 0.214      |
|    mean velocity y | 1.79       |
|    mean velocity z | 3.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 1564500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 764     |
|    time_elapsed    | 63039   |
|    total_timesteps | 1564672 |
--------------------------------
Eval num_timesteps=1565000, episode_reward=-71575.69 +/- 40262.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36988622   |
|    mean velocity x      | -0.59         |
|    mean velocity y      | -0.139        |
|    mean velocity z      | 3.63          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.16e+04     |
| time/                   |               |
|    total_timesteps      | 1565000       |
| train/                  |               |
|    approx_kl            | 1.1718308e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.001         |
|    loss                 | 5.83e+07      |
|    n_updates            | 7640          |
|    policy_gradient_loss | -0.000383     |
|    std                  | 1.55          |
|    value_loss           | 5.64e+07      |
-------------------------------------------
Eval num_timesteps=1565500, episode_reward=-102646.79 +/- 22986.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47027576 |
|    mean velocity x | -0.257      |
|    mean velocity y | 0.699       |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1565500     |
------------------------------------
Eval num_timesteps=1566000, episode_reward=-102754.97 +/- 26111.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16977894 |
|    mean velocity x | -0.126      |
|    mean velocity y | 0.424       |
|    mean velocity z | 0.352       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1566000     |
------------------------------------
Eval num_timesteps=1566500, episode_reward=-36244.36 +/- 40796.25
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.466259 |
|    mean velocity x | -0.489    |
|    mean velocity y | 0.639     |
|    mean velocity z | 4.59      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -3.62e+04 |
| time/              |           |
|    total_timesteps | 1566500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 765     |
|    time_elapsed    | 63119   |
|    total_timesteps | 1566720 |
--------------------------------
Eval num_timesteps=1567000, episode_reward=-79863.97 +/- 49906.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4090187   |
|    mean velocity x      | -0.352       |
|    mean velocity y      | 0.772        |
|    mean velocity z      | 4.3          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.99e+04    |
| time/                   |              |
|    total_timesteps      | 1567000      |
| train/                  |              |
|    approx_kl            | 2.448025e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+07     |
|    n_updates            | 7650         |
|    policy_gradient_loss | -0.000335    |
|    std                  | 1.56         |
|    value_loss           | 7.12e+07     |
------------------------------------------
Eval num_timesteps=1567500, episode_reward=-85029.88 +/- 37322.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4331983 |
|    mean velocity x | -0.321     |
|    mean velocity y | 0.638      |
|    mean velocity z | 3.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.5e+04   |
| time/              |            |
|    total_timesteps | 1567500    |
-----------------------------------
Eval num_timesteps=1568000, episode_reward=-56648.43 +/- 33439.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4774363 |
|    mean velocity x | -0.191     |
|    mean velocity y | 1.18       |
|    mean velocity z | 4.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.66e+04  |
| time/              |            |
|    total_timesteps | 1568000    |
-----------------------------------
Eval num_timesteps=1568500, episode_reward=-81548.01 +/- 40392.26
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4276315 |
|    mean velocity x | -0.498     |
|    mean velocity y | 0.791      |
|    mean velocity z | 2.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 1568500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 766     |
|    time_elapsed    | 63200   |
|    total_timesteps | 1568768 |
--------------------------------
Eval num_timesteps=1569000, episode_reward=-39973.63 +/- 40091.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32931408   |
|    mean velocity x      | -0.0049       |
|    mean velocity y      | 0.58          |
|    mean velocity z      | 3.97          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4e+04        |
| time/                   |               |
|    total_timesteps      | 1569000       |
| train/                  |               |
|    approx_kl            | 3.6913494e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.202         |
|    learning_rate        | 0.001         |
|    loss                 | 3.48e+07      |
|    n_updates            | 7660          |
|    policy_gradient_loss | -0.000405     |
|    std                  | 1.56          |
|    value_loss           | 7.83e+07      |
-------------------------------------------
Eval num_timesteps=1569500, episode_reward=-71855.36 +/- 30067.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28633934 |
|    mean velocity x | 0.0121      |
|    mean velocity y | 0.352       |
|    mean velocity z | 3.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.19e+04   |
| time/              |             |
|    total_timesteps | 1569500     |
------------------------------------
Eval num_timesteps=1570000, episode_reward=-63047.74 +/- 47409.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42566705 |
|    mean velocity x | -0.754      |
|    mean velocity y | 0.361       |
|    mean velocity z | 3.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.3e+04    |
| time/              |             |
|    total_timesteps | 1570000     |
------------------------------------
Eval num_timesteps=1570500, episode_reward=-81509.14 +/- 42517.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5877737 |
|    mean velocity x | 0.517      |
|    mean velocity y | 1.73       |
|    mean velocity z | 4.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 1570500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 767     |
|    time_elapsed    | 63280   |
|    total_timesteps | 1570816 |
--------------------------------
Eval num_timesteps=1571000, episode_reward=-82118.81 +/- 55966.68
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3776024   |
|    mean velocity x      | -0.439       |
|    mean velocity y      | 0.224        |
|    mean velocity z      | 3.78         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.21e+04    |
| time/                   |              |
|    total_timesteps      | 1571000      |
| train/                  |              |
|    approx_kl            | 0.0001335277 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.001        |
|    loss                 | 2.95e+07     |
|    n_updates            | 7670         |
|    policy_gradient_loss | -0.000967    |
|    std                  | 1.56         |
|    value_loss           | 5.22e+07     |
------------------------------------------
Eval num_timesteps=1571500, episode_reward=-76252.72 +/- 58564.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3799905 |
|    mean velocity x | -0.286     |
|    mean velocity y | 0.693      |
|    mean velocity z | 0.925      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 1571500    |
-----------------------------------
Eval num_timesteps=1572000, episode_reward=-87772.75 +/- 31048.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47704828 |
|    mean velocity x | -0.00459    |
|    mean velocity y | 1.36        |
|    mean velocity z | 3.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 1572000     |
------------------------------------
Eval num_timesteps=1572500, episode_reward=-93793.43 +/- 36945.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35985342 |
|    mean velocity x | -0.524      |
|    mean velocity y | 0.031       |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.38e+04   |
| time/              |             |
|    total_timesteps | 1572500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 768     |
|    time_elapsed    | 63360   |
|    total_timesteps | 1572864 |
--------------------------------
Eval num_timesteps=1573000, episode_reward=-62035.80 +/- 48191.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45580974   |
|    mean velocity x      | -1.16         |
|    mean velocity y      | 0.363         |
|    mean velocity z      | 3.4           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.2e+04      |
| time/                   |               |
|    total_timesteps      | 1573000       |
| train/                  |               |
|    approx_kl            | 1.4444115e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.001         |
|    loss                 | 2.61e+07      |
|    n_updates            | 7680          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 1.56          |
|    value_loss           | 2.69e+07      |
-------------------------------------------
Eval num_timesteps=1573500, episode_reward=-68412.14 +/- 34138.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3488267 |
|    mean velocity x | -0.839     |
|    mean velocity y | 0.414      |
|    mean velocity z | 2.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.84e+04  |
| time/              |            |
|    total_timesteps | 1573500    |
-----------------------------------
Eval num_timesteps=1574000, episode_reward=-95566.38 +/- 18640.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3895144 |
|    mean velocity x | -0.218     |
|    mean velocity y | 0.648      |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.56e+04  |
| time/              |            |
|    total_timesteps | 1574000    |
-----------------------------------
Eval num_timesteps=1574500, episode_reward=-53726.23 +/- 53479.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29232666 |
|    mean velocity x | -0.267      |
|    mean velocity y | 0.0906      |
|    mean velocity z | 2.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.37e+04   |
| time/              |             |
|    total_timesteps | 1574500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 769     |
|    time_elapsed    | 63441   |
|    total_timesteps | 1574912 |
--------------------------------
Eval num_timesteps=1575000, episode_reward=-80216.40 +/- 39561.01
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3100722    |
|    mean velocity x      | -0.157        |
|    mean velocity y      | 0.647         |
|    mean velocity z      | 3.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.02e+04     |
| time/                   |               |
|    total_timesteps      | 1575000       |
| train/                  |               |
|    approx_kl            | 0.00016962129 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.001         |
|    loss                 | 2.67e+07      |
|    n_updates            | 7690          |
|    policy_gradient_loss | -0.0017       |
|    std                  | 1.56          |
|    value_loss           | 6.72e+07      |
-------------------------------------------
Eval num_timesteps=1575500, episode_reward=-101222.69 +/- 28708.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45123515 |
|    mean velocity x | -0.161      |
|    mean velocity y | 0.558       |
|    mean velocity z | 2.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1575500     |
------------------------------------
Eval num_timesteps=1576000, episode_reward=-59165.37 +/- 39664.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36395356 |
|    mean velocity x | -0.463      |
|    mean velocity y | 0.572       |
|    mean velocity z | 1.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.92e+04   |
| time/              |             |
|    total_timesteps | 1576000     |
------------------------------------
Eval num_timesteps=1576500, episode_reward=-65578.80 +/- 35283.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48974624 |
|    mean velocity x | -0.525      |
|    mean velocity y | 0.855       |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.56e+04   |
| time/              |             |
|    total_timesteps | 1576500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 770     |
|    time_elapsed    | 63521   |
|    total_timesteps | 1576960 |
--------------------------------
Eval num_timesteps=1577000, episode_reward=-72821.28 +/- 27796.67
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3190873    |
|    mean velocity x      | -0.28         |
|    mean velocity y      | 0.057         |
|    mean velocity z      | 3.25          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.28e+04     |
| time/                   |               |
|    total_timesteps      | 1577000       |
| train/                  |               |
|    approx_kl            | 1.0765943e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.298         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+06      |
|    n_updates            | 7700          |
|    policy_gradient_loss | -0.00023      |
|    std                  | 1.56          |
|    value_loss           | 3.32e+07      |
-------------------------------------------
Eval num_timesteps=1577500, episode_reward=-51346.55 +/- 25287.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43525568 |
|    mean velocity x | -0.81       |
|    mean velocity y | 0.135       |
|    mean velocity z | 3.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.13e+04   |
| time/              |             |
|    total_timesteps | 1577500     |
------------------------------------
Eval num_timesteps=1578000, episode_reward=-79593.93 +/- 42801.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32032362 |
|    mean velocity x | -0.78       |
|    mean velocity y | 0.365       |
|    mean velocity z | 2.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.96e+04   |
| time/              |             |
|    total_timesteps | 1578000     |
------------------------------------
Eval num_timesteps=1578500, episode_reward=-85589.08 +/- 38507.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42856506 |
|    mean velocity x | -0.0178     |
|    mean velocity y | 1.2         |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 1578500     |
------------------------------------
Eval num_timesteps=1579000, episode_reward=-85970.92 +/- 31924.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3935917 |
|    mean velocity x | -0.838     |
|    mean velocity y | 0.627      |
|    mean velocity z | 3.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.6e+04   |
| time/              |            |
|    total_timesteps | 1579000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 771     |
|    time_elapsed    | 63621   |
|    total_timesteps | 1579008 |
--------------------------------
Eval num_timesteps=1579500, episode_reward=-88795.70 +/- 27267.95
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.33339807 |
|    mean velocity x      | -0.907      |
|    mean velocity y      | 0.199       |
|    mean velocity z      | 2.47        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.88e+04   |
| time/                   |             |
|    total_timesteps      | 1579500     |
| train/                  |             |
|    approx_kl            | 7.74223e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.57       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.001       |
|    loss                 | 2e+07       |
|    n_updates            | 7710        |
|    policy_gradient_loss | -0.000224   |
|    std                  | 1.56        |
|    value_loss           | 3.88e+07    |
-----------------------------------------
Eval num_timesteps=1580000, episode_reward=-62822.77 +/- 35533.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44563875 |
|    mean velocity x | -0.853      |
|    mean velocity y | 0.316       |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 1580000     |
------------------------------------
Eval num_timesteps=1580500, episode_reward=-79812.92 +/- 41290.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.66172105 |
|    mean velocity x | 0.569       |
|    mean velocity y | 2.63        |
|    mean velocity z | 5.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.98e+04   |
| time/              |             |
|    total_timesteps | 1580500     |
------------------------------------
Eval num_timesteps=1581000, episode_reward=-91817.38 +/- 37027.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4403715 |
|    mean velocity x | -0.225     |
|    mean velocity y | 1.02       |
|    mean velocity z | 4.5        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.18e+04  |
| time/              |            |
|    total_timesteps | 1581000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 772     |
|    time_elapsed    | 63701   |
|    total_timesteps | 1581056 |
--------------------------------
Eval num_timesteps=1581500, episode_reward=-82872.73 +/- 45887.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.56248784   |
|    mean velocity x      | -0.768        |
|    mean velocity y      | 0.727         |
|    mean velocity z      | 7.02          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.29e+04     |
| time/                   |               |
|    total_timesteps      | 1581500       |
| train/                  |               |
|    approx_kl            | 2.9388699e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.329         |
|    learning_rate        | 0.001         |
|    loss                 | 9.48e+06      |
|    n_updates            | 7720          |
|    policy_gradient_loss | -0.000264     |
|    std                  | 1.56          |
|    value_loss           | 6.84e+07      |
-------------------------------------------
Eval num_timesteps=1582000, episode_reward=-91540.79 +/- 44785.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49869666 |
|    mean velocity x | -1.27       |
|    mean velocity y | -0.528      |
|    mean velocity z | 7.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.15e+04   |
| time/              |             |
|    total_timesteps | 1582000     |
------------------------------------
Eval num_timesteps=1582500, episode_reward=-70489.27 +/- 42797.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46002197 |
|    mean velocity x | -0.979      |
|    mean velocity y | 0.224       |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 1582500     |
------------------------------------
Eval num_timesteps=1583000, episode_reward=-64894.08 +/- 37946.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4712449 |
|    mean velocity x | -0.483     |
|    mean velocity y | 0.241      |
|    mean velocity z | 4.18       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.49e+04  |
| time/              |            |
|    total_timesteps | 1583000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 773     |
|    time_elapsed    | 63781   |
|    total_timesteps | 1583104 |
--------------------------------
Eval num_timesteps=1583500, episode_reward=-69849.18 +/- 26826.46
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42553508  |
|    mean velocity x      | -0.379       |
|    mean velocity y      | 0.751        |
|    mean velocity z      | 4.59         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.98e+04    |
| time/                   |              |
|    total_timesteps      | 1583500      |
| train/                  |              |
|    approx_kl            | 9.107956e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.001        |
|    loss                 | 2.76e+07     |
|    n_updates            | 7730         |
|    policy_gradient_loss | -0.00032     |
|    std                  | 1.56         |
|    value_loss           | 9.67e+07     |
------------------------------------------
Eval num_timesteps=1584000, episode_reward=-57562.45 +/- 51443.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41797557 |
|    mean velocity x | -0.11       |
|    mean velocity y | 1.07        |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.76e+04   |
| time/              |             |
|    total_timesteps | 1584000     |
------------------------------------
Eval num_timesteps=1584500, episode_reward=-72816.12 +/- 18888.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26571637 |
|    mean velocity x | -0.156      |
|    mean velocity y | 0.523       |
|    mean velocity z | 0.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 1584500     |
------------------------------------
Eval num_timesteps=1585000, episode_reward=-39334.86 +/- 42488.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40874046 |
|    mean velocity x | -1.72       |
|    mean velocity y | -0.109      |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.93e+04   |
| time/              |             |
|    total_timesteps | 1585000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 774     |
|    time_elapsed    | 63862   |
|    total_timesteps | 1585152 |
--------------------------------
Eval num_timesteps=1585500, episode_reward=-78002.88 +/- 42412.61
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.36179328    |
|    mean velocity x      | -0.72          |
|    mean velocity y      | -0.284         |
|    mean velocity z      | 3.4            |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -7.8e+04       |
| time/                   |                |
|    total_timesteps      | 1585500        |
| train/                  |                |
|    approx_kl            | 1.46739185e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.57          |
|    explained_variance   | 0.311          |
|    learning_rate        | 0.001          |
|    loss                 | 3.57e+07       |
|    n_updates            | 7740           |
|    policy_gradient_loss | -0.000331      |
|    std                  | 1.56           |
|    value_loss           | 4.02e+07       |
--------------------------------------------
Eval num_timesteps=1586000, episode_reward=-78548.42 +/- 38609.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5762872 |
|    mean velocity x | -0.544     |
|    mean velocity y | 0.888      |
|    mean velocity z | 5.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.85e+04  |
| time/              |            |
|    total_timesteps | 1586000    |
-----------------------------------
Eval num_timesteps=1586500, episode_reward=-85441.76 +/- 17080.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3797689 |
|    mean velocity x | -0.636     |
|    mean velocity y | 0.431      |
|    mean velocity z | 4.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.54e+04  |
| time/              |            |
|    total_timesteps | 1586500    |
-----------------------------------
Eval num_timesteps=1587000, episode_reward=-94733.26 +/- 42460.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53364575 |
|    mean velocity x | -0.19       |
|    mean velocity y | 1.44        |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.47e+04   |
| time/              |             |
|    total_timesteps | 1587000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 775     |
|    time_elapsed    | 63942   |
|    total_timesteps | 1587200 |
--------------------------------
Eval num_timesteps=1587500, episode_reward=-80262.27 +/- 15710.97
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44565848   |
|    mean velocity x      | -0.166        |
|    mean velocity y      | 1.54          |
|    mean velocity z      | 3.98          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.03e+04     |
| time/                   |               |
|    total_timesteps      | 1587500       |
| train/                  |               |
|    approx_kl            | 1.2779521e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.001         |
|    loss                 | 3.8e+07       |
|    n_updates            | 7750          |
|    policy_gradient_loss | -0.00047      |
|    std                  | 1.56          |
|    value_loss           | 6.2e+07       |
-------------------------------------------
Eval num_timesteps=1588000, episode_reward=-67145.44 +/- 34347.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35059515 |
|    mean velocity x | 0.118       |
|    mean velocity y | 0.567       |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 1588000     |
------------------------------------
Eval num_timesteps=1588500, episode_reward=-64906.49 +/- 44097.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2195033 |
|    mean velocity x | -0.121     |
|    mean velocity y | 0.386      |
|    mean velocity z | 0.596      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.49e+04  |
| time/              |            |
|    total_timesteps | 1588500    |
-----------------------------------
Eval num_timesteps=1589000, episode_reward=-97758.13 +/- 45914.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32161358 |
|    mean velocity x | -0.834      |
|    mean velocity y | -0.253      |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.78e+04   |
| time/              |             |
|    total_timesteps | 1589000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 776     |
|    time_elapsed    | 64023   |
|    total_timesteps | 1589248 |
--------------------------------
Eval num_timesteps=1589500, episode_reward=-42841.26 +/- 43500.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40388197   |
|    mean velocity x      | 0.00396       |
|    mean velocity y      | 1.2           |
|    mean velocity z      | 4.02          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.28e+04     |
| time/                   |               |
|    total_timesteps      | 1589500       |
| train/                  |               |
|    approx_kl            | 2.6572234e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+07      |
|    n_updates            | 7760          |
|    policy_gradient_loss | -0.000637     |
|    std                  | 1.55          |
|    value_loss           | 4.2e+07       |
-------------------------------------------
Eval num_timesteps=1590000, episode_reward=-89605.31 +/- 23247.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47048587 |
|    mean velocity x | -1.17       |
|    mean velocity y | 0.431       |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 1590000     |
------------------------------------
Eval num_timesteps=1590500, episode_reward=-71312.20 +/- 32859.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42927167 |
|    mean velocity x | -0.215      |
|    mean velocity y | 0.926       |
|    mean velocity z | 4.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 1590500     |
------------------------------------
Eval num_timesteps=1591000, episode_reward=-74777.28 +/- 25655.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46906647 |
|    mean velocity x | -0.332      |
|    mean velocity y | 1.12        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.48e+04   |
| time/              |             |
|    total_timesteps | 1591000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 777     |
|    time_elapsed    | 64103   |
|    total_timesteps | 1591296 |
--------------------------------
Eval num_timesteps=1591500, episode_reward=-75333.05 +/- 29869.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4481471    |
|    mean velocity x      | -1.03         |
|    mean velocity y      | 0.241         |
|    mean velocity z      | 3.85          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.53e+04     |
| time/                   |               |
|    total_timesteps      | 1591500       |
| train/                  |               |
|    approx_kl            | 4.6485075e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.285         |
|    learning_rate        | 0.001         |
|    loss                 | 9.21e+07      |
|    n_updates            | 7770          |
|    policy_gradient_loss | -0.000453     |
|    std                  | 1.55          |
|    value_loss           | 8.21e+07      |
-------------------------------------------
Eval num_timesteps=1592000, episode_reward=-92228.56 +/- 10553.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4574759 |
|    mean velocity x | -0.329     |
|    mean velocity y | 0.846      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.22e+04  |
| time/              |            |
|    total_timesteps | 1592000    |
-----------------------------------
Eval num_timesteps=1592500, episode_reward=-77231.71 +/- 41978.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27332798 |
|    mean velocity x | -1.04       |
|    mean velocity y | 0.11        |
|    mean velocity z | 2.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 1592500     |
------------------------------------
Eval num_timesteps=1593000, episode_reward=-105536.70 +/- 29173.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40251982 |
|    mean velocity x | -0.419      |
|    mean velocity y | 0.413       |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1593000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 778     |
|    time_elapsed    | 64183   |
|    total_timesteps | 1593344 |
--------------------------------
Eval num_timesteps=1593500, episode_reward=-48943.51 +/- 47196.21
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48745793  |
|    mean velocity x      | 0.356        |
|    mean velocity y      | 1.75         |
|    mean velocity z      | 3.71         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.89e+04    |
| time/                   |              |
|    total_timesteps      | 1593500      |
| train/                  |              |
|    approx_kl            | 9.027505e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.001        |
|    loss                 | 3.61e+07     |
|    n_updates            | 7780         |
|    policy_gradient_loss | -0.000806    |
|    std                  | 1.56         |
|    value_loss           | 6.91e+07     |
------------------------------------------
Eval num_timesteps=1594000, episode_reward=-99650.19 +/- 12266.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37299415 |
|    mean velocity x | -0.753      |
|    mean velocity y | 0.772       |
|    mean velocity z | 1.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.97e+04   |
| time/              |             |
|    total_timesteps | 1594000     |
------------------------------------
Eval num_timesteps=1594500, episode_reward=-112704.88 +/- 16182.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40004417 |
|    mean velocity x | -0.349      |
|    mean velocity y | 0.756       |
|    mean velocity z | 4.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 1594500     |
------------------------------------
Eval num_timesteps=1595000, episode_reward=-62064.57 +/- 43648.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45947522 |
|    mean velocity x | -0.355      |
|    mean velocity y | 0.897       |
|    mean velocity z | 4.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+04   |
| time/              |             |
|    total_timesteps | 1595000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 779     |
|    time_elapsed    | 64264   |
|    total_timesteps | 1595392 |
--------------------------------
Eval num_timesteps=1595500, episode_reward=-55735.08 +/- 45951.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44260737   |
|    mean velocity x      | -0.479        |
|    mean velocity y      | 0.75          |
|    mean velocity z      | 4.08          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.57e+04     |
| time/                   |               |
|    total_timesteps      | 1595500       |
| train/                  |               |
|    approx_kl            | 7.6013384e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 7.61e+07      |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.000192     |
|    std                  | 1.55          |
|    value_loss           | 8.53e+07      |
-------------------------------------------
Eval num_timesteps=1596000, episode_reward=-62499.27 +/- 33479.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41457018 |
|    mean velocity x | -0.404      |
|    mean velocity y | 0.359       |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 1596000     |
------------------------------------
Eval num_timesteps=1596500, episode_reward=-112811.64 +/- 15184.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36041623 |
|    mean velocity x | 0.0965      |
|    mean velocity y | 0.534       |
|    mean velocity z | 3.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 1596500     |
------------------------------------
Eval num_timesteps=1597000, episode_reward=-77949.59 +/- 40023.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5280167 |
|    mean velocity x | -0.0364    |
|    mean velocity y | 1.77       |
|    mean velocity z | 4.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.79e+04  |
| time/              |            |
|    total_timesteps | 1597000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 780     |
|    time_elapsed    | 64344   |
|    total_timesteps | 1597440 |
--------------------------------
Eval num_timesteps=1597500, episode_reward=-70558.03 +/- 41512.62
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4468912    |
|    mean velocity x      | -0.886        |
|    mean velocity y      | 0.17          |
|    mean velocity z      | 3.73          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.06e+04     |
| time/                   |               |
|    total_timesteps      | 1597500       |
| train/                  |               |
|    approx_kl            | 1.5325699e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.284         |
|    learning_rate        | 0.001         |
|    loss                 | 6.43e+06      |
|    n_updates            | 7800          |
|    policy_gradient_loss | -0.000221     |
|    std                  | 1.56          |
|    value_loss           | 6.12e+07      |
-------------------------------------------
Eval num_timesteps=1598000, episode_reward=-73996.74 +/- 37086.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35556477 |
|    mean velocity x | -0.133      |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.4e+04    |
| time/              |             |
|    total_timesteps | 1598000     |
------------------------------------
Eval num_timesteps=1598500, episode_reward=-64664.27 +/- 32761.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50300413 |
|    mean velocity x | -0.807      |
|    mean velocity y | 0.589       |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.47e+04   |
| time/              |             |
|    total_timesteps | 1598500     |
------------------------------------
Eval num_timesteps=1599000, episode_reward=-91049.13 +/- 68775.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5758374 |
|    mean velocity x | 0.833      |
|    mean velocity y | 1.94       |
|    mean velocity z | 3.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.1e+04   |
| time/              |            |
|    total_timesteps | 1599000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 781     |
|    time_elapsed    | 64425   |
|    total_timesteps | 1599488 |
--------------------------------
Eval num_timesteps=1599500, episode_reward=-97638.73 +/- 53782.08
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49137947   |
|    mean velocity x      | -0.346        |
|    mean velocity y      | 0.958         |
|    mean velocity z      | 4.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.76e+04     |
| time/                   |               |
|    total_timesteps      | 1599500       |
| train/                  |               |
|    approx_kl            | 3.0938565e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.001         |
|    loss                 | 2.63e+07      |
|    n_updates            | 7810          |
|    policy_gradient_loss | -0.000467     |
|    std                  | 1.55          |
|    value_loss           | 7.02e+07      |
-------------------------------------------
Eval num_timesteps=1600000, episode_reward=-108860.73 +/- 28029.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3844065 |
|    mean velocity x | 0.158      |
|    mean velocity y | 1.05       |
|    mean velocity z | 3.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 1600000    |
-----------------------------------
Eval num_timesteps=1600500, episode_reward=-103257.15 +/- 16136.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.9620379 |
|    mean velocity x | 0.728      |
|    mean velocity y | 3.41       |
|    mean velocity z | 8.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1600500    |
-----------------------------------
Eval num_timesteps=1601000, episode_reward=-83190.16 +/- 42487.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43332416 |
|    mean velocity x | -0.319      |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.32e+04   |
| time/              |             |
|    total_timesteps | 1601000     |
------------------------------------
Eval num_timesteps=1601500, episode_reward=-69257.51 +/- 35018.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4363496 |
|    mean velocity x | 0.609      |
|    mean velocity y | 1.65       |
|    mean velocity z | 3.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.93e+04  |
| time/              |            |
|    total_timesteps | 1601500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 782     |
|    time_elapsed    | 64524   |
|    total_timesteps | 1601536 |
--------------------------------
Eval num_timesteps=1602000, episode_reward=-105524.89 +/- 53357.83
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36406434  |
|    mean velocity x      | -0.562       |
|    mean velocity y      | 0.107        |
|    mean velocity z      | 4.05         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.06e+05    |
| time/                   |              |
|    total_timesteps      | 1602000      |
| train/                  |              |
|    approx_kl            | 4.568894e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.001        |
|    loss                 | 1.75e+07     |
|    n_updates            | 7820         |
|    policy_gradient_loss | -0.000132    |
|    std                  | 1.55         |
|    value_loss           | 7.04e+07     |
------------------------------------------
Eval num_timesteps=1602500, episode_reward=-61012.78 +/- 18038.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42788905 |
|    mean velocity x | -0.258      |
|    mean velocity y | 0.618       |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.1e+04    |
| time/              |             |
|    total_timesteps | 1602500     |
------------------------------------
Eval num_timesteps=1603000, episode_reward=-97816.21 +/- 30447.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36398733 |
|    mean velocity x | 0.0776      |
|    mean velocity y | 0.879       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.78e+04   |
| time/              |             |
|    total_timesteps | 1603000     |
------------------------------------
Eval num_timesteps=1603500, episode_reward=-111209.80 +/- 25017.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45167738 |
|    mean velocity x | 0.0802      |
|    mean velocity y | 1.17        |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1603500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 783     |
|    time_elapsed    | 64604   |
|    total_timesteps | 1603584 |
--------------------------------
Eval num_timesteps=1604000, episode_reward=-98809.14 +/- 32315.78
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40239057   |
|    mean velocity x      | -0.319        |
|    mean velocity y      | 0.765         |
|    mean velocity z      | 4.35          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.88e+04     |
| time/                   |               |
|    total_timesteps      | 1604000       |
| train/                  |               |
|    approx_kl            | 7.1491813e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.001         |
|    loss                 | 5.28e+07      |
|    n_updates            | 7830          |
|    policy_gradient_loss | -0.000264     |
|    std                  | 1.55          |
|    value_loss           | 7.29e+07      |
-------------------------------------------
Eval num_timesteps=1604500, episode_reward=-96636.39 +/- 27276.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52033985 |
|    mean velocity x | -0.425      |
|    mean velocity y | 0.97        |
|    mean velocity z | 4.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.66e+04   |
| time/              |             |
|    total_timesteps | 1604500     |
------------------------------------
Eval num_timesteps=1605000, episode_reward=-108890.82 +/- 62150.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36671728 |
|    mean velocity x | -1.23       |
|    mean velocity y | -0.21       |
|    mean velocity z | 3.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1605000     |
------------------------------------
Eval num_timesteps=1605500, episode_reward=-89370.91 +/- 25702.75
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5597111 |
|    mean velocity x | -0.778     |
|    mean velocity y | 0.741      |
|    mean velocity z | 4.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.94e+04  |
| time/              |            |
|    total_timesteps | 1605500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 784     |
|    time_elapsed    | 64685   |
|    total_timesteps | 1605632 |
--------------------------------
Eval num_timesteps=1606000, episode_reward=-91078.41 +/- 34924.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43685      |
|    mean velocity x      | -1.61         |
|    mean velocity y      | -0.351        |
|    mean velocity z      | 3.76          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.11e+04     |
| time/                   |               |
|    total_timesteps      | 1606000       |
| train/                  |               |
|    approx_kl            | 1.2259028e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.313         |
|    learning_rate        | 0.001         |
|    loss                 | 4.24e+07      |
|    n_updates            | 7840          |
|    policy_gradient_loss | -0.000302     |
|    std                  | 1.55          |
|    value_loss           | 5.96e+07      |
-------------------------------------------
Eval num_timesteps=1606500, episode_reward=-101476.53 +/- 36270.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43804348 |
|    mean velocity x | -0.333      |
|    mean velocity y | 1.15        |
|    mean velocity z | 4.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1606500     |
------------------------------------
Eval num_timesteps=1607000, episode_reward=-83807.02 +/- 42704.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5090398 |
|    mean velocity x | -0.584     |
|    mean velocity y | 0.757      |
|    mean velocity z | 4.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.38e+04  |
| time/              |            |
|    total_timesteps | 1607000    |
-----------------------------------
Eval num_timesteps=1607500, episode_reward=-72325.99 +/- 29389.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35189468 |
|    mean velocity x | -0.481      |
|    mean velocity y | 0.238       |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 1607500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 785     |
|    time_elapsed    | 64765   |
|    total_timesteps | 1607680 |
--------------------------------
Eval num_timesteps=1608000, episode_reward=-90462.68 +/- 24921.37
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4668232    |
|    mean velocity x      | -0.473        |
|    mean velocity y      | 0.84          |
|    mean velocity z      | 3.92          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.05e+04     |
| time/                   |               |
|    total_timesteps      | 1608000       |
| train/                  |               |
|    approx_kl            | 1.5064143e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 3.35e+06      |
|    n_updates            | 7850          |
|    policy_gradient_loss | -0.00012      |
|    std                  | 1.55          |
|    value_loss           | 8.13e+07      |
-------------------------------------------
Eval num_timesteps=1608500, episode_reward=-82315.16 +/- 34583.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49683332 |
|    mean velocity x | 0.549       |
|    mean velocity y | 1.34        |
|    mean velocity z | 3.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1608500     |
------------------------------------
Eval num_timesteps=1609000, episode_reward=-58673.12 +/- 28081.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46801913 |
|    mean velocity x | -0.915      |
|    mean velocity y | 0.0898      |
|    mean velocity z | 3.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.87e+04   |
| time/              |             |
|    total_timesteps | 1609000     |
------------------------------------
Eval num_timesteps=1609500, episode_reward=-86046.42 +/- 33980.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39962786 |
|    mean velocity x | -0.606      |
|    mean velocity y | 0.113       |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.6e+04    |
| time/              |             |
|    total_timesteps | 1609500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 786     |
|    time_elapsed    | 64846   |
|    total_timesteps | 1609728 |
--------------------------------
Eval num_timesteps=1610000, episode_reward=-74082.55 +/- 60822.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4666751   |
|    mean velocity x      | 0.148        |
|    mean velocity y      | 1.41         |
|    mean velocity z      | 3.56         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.41e+04    |
| time/                   |              |
|    total_timesteps      | 1610000      |
| train/                  |              |
|    approx_kl            | 5.998416e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+07     |
|    n_updates            | 7860         |
|    policy_gradient_loss | -0.000222    |
|    std                  | 1.55         |
|    value_loss           | 4.05e+07     |
------------------------------------------
Eval num_timesteps=1610500, episode_reward=-93071.47 +/- 28165.11
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4692235 |
|    mean velocity x | -0.969     |
|    mean velocity y | 0.452      |
|    mean velocity z | 3.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.31e+04  |
| time/              |            |
|    total_timesteps | 1610500    |
-----------------------------------
Eval num_timesteps=1611000, episode_reward=-78026.61 +/- 54700.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3912442 |
|    mean velocity x | 0.0796     |
|    mean velocity y | 1.21       |
|    mean velocity z | 3.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.8e+04   |
| time/              |            |
|    total_timesteps | 1611000    |
-----------------------------------
Eval num_timesteps=1611500, episode_reward=-75036.77 +/- 9973.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43183142 |
|    mean velocity x | -0.127      |
|    mean velocity y | 1.34        |
|    mean velocity z | 4.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.5e+04    |
| time/              |             |
|    total_timesteps | 1611500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 787     |
|    time_elapsed    | 64926   |
|    total_timesteps | 1611776 |
--------------------------------
Eval num_timesteps=1612000, episode_reward=-54413.73 +/- 28952.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5196376    |
|    mean velocity x      | -0.532        |
|    mean velocity y      | 0.855         |
|    mean velocity z      | 4.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.44e+04     |
| time/                   |               |
|    total_timesteps      | 1612000       |
| train/                  |               |
|    approx_kl            | 6.1541505e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+07      |
|    n_updates            | 7870          |
|    policy_gradient_loss | -0.000179     |
|    std                  | 1.55          |
|    value_loss           | 8.71e+07      |
-------------------------------------------
Eval num_timesteps=1612500, episode_reward=-100112.88 +/- 27529.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44678932 |
|    mean velocity x | -0.745      |
|    mean velocity y | -0.11       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1612500     |
------------------------------------
Eval num_timesteps=1613000, episode_reward=-76547.98 +/- 50767.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40516943 |
|    mean velocity x | -0.243      |
|    mean velocity y | 0.727       |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.65e+04   |
| time/              |             |
|    total_timesteps | 1613000     |
------------------------------------
Eval num_timesteps=1613500, episode_reward=-90469.34 +/- 22694.43
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.363524 |
|    mean velocity x | 0.233     |
|    mean velocity y | 0.853     |
|    mean velocity z | 2.8       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.05e+04 |
| time/              |           |
|    total_timesteps | 1613500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 788     |
|    time_elapsed    | 65006   |
|    total_timesteps | 1613824 |
--------------------------------
Eval num_timesteps=1614000, episode_reward=-59373.84 +/- 46650.34
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32616603  |
|    mean velocity x      | -1.34        |
|    mean velocity y      | -0.87        |
|    mean velocity z      | 5.23         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.94e+04    |
| time/                   |              |
|    total_timesteps      | 1614000      |
| train/                  |              |
|    approx_kl            | 4.582689e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.001        |
|    loss                 | 6.02e+07     |
|    n_updates            | 7880         |
|    policy_gradient_loss | -0.000128    |
|    std                  | 1.55         |
|    value_loss           | 6.47e+07     |
------------------------------------------
Eval num_timesteps=1614500, episode_reward=-72217.60 +/- 34456.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41928798 |
|    mean velocity x | -0.583      |
|    mean velocity y | 0.569       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.22e+04   |
| time/              |             |
|    total_timesteps | 1614500     |
------------------------------------
Eval num_timesteps=1615000, episode_reward=-74374.19 +/- 40587.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45460474 |
|    mean velocity x | -1.05       |
|    mean velocity y | 0.346       |
|    mean velocity z | 4.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.44e+04   |
| time/              |             |
|    total_timesteps | 1615000     |
------------------------------------
Eval num_timesteps=1615500, episode_reward=-92660.25 +/- 20179.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36326513 |
|    mean velocity x | -0.284      |
|    mean velocity y | 0.699       |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 1615500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 789     |
|    time_elapsed    | 65087   |
|    total_timesteps | 1615872 |
--------------------------------
Eval num_timesteps=1616000, episode_reward=-70744.66 +/- 47149.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3995659    |
|    mean velocity x      | -0.177        |
|    mean velocity y      | 0.741         |
|    mean velocity z      | 1.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.07e+04     |
| time/                   |               |
|    total_timesteps      | 1616000       |
| train/                  |               |
|    approx_kl            | 2.9383227e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+07      |
|    n_updates            | 7890          |
|    policy_gradient_loss | -0.000105     |
|    std                  | 1.55          |
|    value_loss           | 5.6e+07       |
-------------------------------------------
Eval num_timesteps=1616500, episode_reward=-55176.43 +/- 44337.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34303644 |
|    mean velocity x | -0.634      |
|    mean velocity y | -0.0421     |
|    mean velocity z | 3.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.52e+04   |
| time/              |             |
|    total_timesteps | 1616500     |
------------------------------------
Eval num_timesteps=1617000, episode_reward=-79443.06 +/- 11175.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4893839 |
|    mean velocity x | -0.18      |
|    mean velocity y | 1.12       |
|    mean velocity z | 4.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.94e+04  |
| time/              |            |
|    total_timesteps | 1617000    |
-----------------------------------
Eval num_timesteps=1617500, episode_reward=-69724.77 +/- 25079.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47335055 |
|    mean velocity x | -0.41       |
|    mean velocity y | 1.08        |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.97e+04   |
| time/              |             |
|    total_timesteps | 1617500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 790     |
|    time_elapsed    | 65167   |
|    total_timesteps | 1617920 |
--------------------------------
Eval num_timesteps=1618000, episode_reward=-107950.31 +/- 52629.18
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27275446   |
|    mean velocity x      | -0.805        |
|    mean velocity y      | -0.769        |
|    mean velocity z      | 3.8           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.08e+05     |
| time/                   |               |
|    total_timesteps      | 1618000       |
| train/                  |               |
|    approx_kl            | 2.7234782e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.284         |
|    learning_rate        | 0.001         |
|    loss                 | 6.46e+07      |
|    n_updates            | 7900          |
|    policy_gradient_loss | -0.000149     |
|    std                  | 1.55          |
|    value_loss           | 5.45e+07      |
-------------------------------------------
Eval num_timesteps=1618500, episode_reward=-82925.69 +/- 24783.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26598656 |
|    mean velocity x | -0.00639    |
|    mean velocity y | 0.236       |
|    mean velocity z | 1.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 1618500     |
------------------------------------
Eval num_timesteps=1619000, episode_reward=-84825.47 +/- 24694.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62422246 |
|    mean velocity x | 0.454       |
|    mean velocity y | 2.52        |
|    mean velocity z | 5.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.48e+04   |
| time/              |             |
|    total_timesteps | 1619000     |
------------------------------------
Eval num_timesteps=1619500, episode_reward=-87070.61 +/- 37056.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3633051 |
|    mean velocity x | -0.644     |
|    mean velocity y | -0.12      |
|    mean velocity z | 3.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.71e+04  |
| time/              |            |
|    total_timesteps | 1619500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 791     |
|    time_elapsed    | 65248   |
|    total_timesteps | 1619968 |
--------------------------------
Eval num_timesteps=1620000, episode_reward=-93078.39 +/- 47877.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35422328   |
|    mean velocity x      | -0.434        |
|    mean velocity y      | 0.438         |
|    mean velocity z      | 3.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.31e+04     |
| time/                   |               |
|    total_timesteps      | 1620000       |
| train/                  |               |
|    approx_kl            | 1.5741272e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.323         |
|    learning_rate        | 0.001         |
|    loss                 | 3.57e+07      |
|    n_updates            | 7910          |
|    policy_gradient_loss | -0.000172     |
|    std                  | 1.55          |
|    value_loss           | 3.91e+07      |
-------------------------------------------
Eval num_timesteps=1620500, episode_reward=-73744.48 +/- 40050.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40582532 |
|    mean velocity x | -0.243      |
|    mean velocity y | 0.795       |
|    mean velocity z | 4.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 1620500     |
------------------------------------
Eval num_timesteps=1621000, episode_reward=-83629.87 +/- 26622.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30058643 |
|    mean velocity x | 0.0734      |
|    mean velocity y | 0.704       |
|    mean velocity z | 0.642       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.36e+04   |
| time/              |             |
|    total_timesteps | 1621000     |
------------------------------------
Eval num_timesteps=1621500, episode_reward=-59923.24 +/- 43968.53
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.220578 |
|    mean velocity x | 0.4       |
|    mean velocity y | 0.74      |
|    mean velocity z | 3.3       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.99e+04 |
| time/              |           |
|    total_timesteps | 1621500   |
----------------------------------
Eval num_timesteps=1622000, episode_reward=-103844.76 +/- 3921.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49539146 |
|    mean velocity x | -0.546      |
|    mean velocity y | 0.973       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1622000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 792     |
|    time_elapsed    | 65347   |
|    total_timesteps | 1622016 |
--------------------------------
Eval num_timesteps=1622500, episode_reward=-98654.13 +/- 20035.62
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38897717   |
|    mean velocity x      | -0.246        |
|    mean velocity y      | 0.573         |
|    mean velocity z      | 4.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.87e+04     |
| time/                   |               |
|    total_timesteps      | 1622500       |
| train/                  |               |
|    approx_kl            | 6.8812224e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.001         |
|    loss                 | 3.99e+07      |
|    n_updates            | 7920          |
|    policy_gradient_loss | -0.000239     |
|    std                  | 1.55          |
|    value_loss           | 7.83e+07      |
-------------------------------------------
Eval num_timesteps=1623000, episode_reward=-88577.05 +/- 8663.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42303896 |
|    mean velocity x | -0.248      |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.86e+04   |
| time/              |             |
|    total_timesteps | 1623000     |
------------------------------------
Eval num_timesteps=1623500, episode_reward=-91974.30 +/- 31124.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48265618 |
|    mean velocity x | -0.596      |
|    mean velocity y | 0.555       |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.2e+04    |
| time/              |             |
|    total_timesteps | 1623500     |
------------------------------------
Eval num_timesteps=1624000, episode_reward=-105298.92 +/- 17534.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39185885 |
|    mean velocity x | -2.33       |
|    mean velocity y | -0.747      |
|    mean velocity z | 4.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1624000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 793     |
|    time_elapsed    | 65427   |
|    total_timesteps | 1624064 |
--------------------------------
Eval num_timesteps=1624500, episode_reward=-101693.67 +/- 13418.05
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41086847   |
|    mean velocity x      | -0.323        |
|    mean velocity y      | 1.08          |
|    mean velocity z      | 4.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 1624500       |
| train/                  |               |
|    approx_kl            | 7.5155403e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.298         |
|    learning_rate        | 0.001         |
|    loss                 | 3.77e+07      |
|    n_updates            | 7930          |
|    policy_gradient_loss | -0.000128     |
|    std                  | 1.55          |
|    value_loss           | 7.01e+07      |
-------------------------------------------
Eval num_timesteps=1625000, episode_reward=-44792.22 +/- 47921.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4760107 |
|    mean velocity x | -0.25      |
|    mean velocity y | 0.781      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.48e+04  |
| time/              |            |
|    total_timesteps | 1625000    |
-----------------------------------
Eval num_timesteps=1625500, episode_reward=-53207.08 +/- 26079.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39777252 |
|    mean velocity x | -0.0113     |
|    mean velocity y | 0.751       |
|    mean velocity z | 3.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.32e+04   |
| time/              |             |
|    total_timesteps | 1625500     |
------------------------------------
Eval num_timesteps=1626000, episode_reward=-111071.43 +/- 17769.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34718284 |
|    mean velocity x | -0.00324    |
|    mean velocity y | 0.885       |
|    mean velocity z | 4.02        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1626000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 794     |
|    time_elapsed    | 65508   |
|    total_timesteps | 1626112 |
--------------------------------
Eval num_timesteps=1626500, episode_reward=-29981.38 +/- 23980.58
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2578644    |
|    mean velocity x      | -0.218        |
|    mean velocity y      | 0.168         |
|    mean velocity z      | 2.23          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3e+04        |
| time/                   |               |
|    total_timesteps      | 1626500       |
| train/                  |               |
|    approx_kl            | 1.6065023e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.218         |
|    learning_rate        | 0.001         |
|    loss                 | 4.57e+07      |
|    n_updates            | 7940          |
|    policy_gradient_loss | -0.000143     |
|    std                  | 1.55          |
|    value_loss           | 7.02e+07      |
-------------------------------------------
Eval num_timesteps=1627000, episode_reward=-103824.62 +/- 15337.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4360797 |
|    mean velocity x | -0.458     |
|    mean velocity y | 0.514      |
|    mean velocity z | 4.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1627000    |
-----------------------------------
Eval num_timesteps=1627500, episode_reward=-73204.74 +/- 32807.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21030883 |
|    mean velocity x | 0.0579      |
|    mean velocity y | 0.192       |
|    mean velocity z | 0.439       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.32e+04   |
| time/              |             |
|    total_timesteps | 1627500     |
------------------------------------
Eval num_timesteps=1628000, episode_reward=-90879.41 +/- 36466.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31712097 |
|    mean velocity x | -0.00882    |
|    mean velocity y | 0.668       |
|    mean velocity z | 4.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 1628000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 795     |
|    time_elapsed    | 65588   |
|    total_timesteps | 1628160 |
--------------------------------
Eval num_timesteps=1628500, episode_reward=-57485.74 +/- 33160.36
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4482701   |
|    mean velocity x      | -0.126       |
|    mean velocity y      | 1.2          |
|    mean velocity z      | 3.24         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.75e+04    |
| time/                   |              |
|    total_timesteps      | 1628500      |
| train/                  |              |
|    approx_kl            | 6.630202e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.001        |
|    loss                 | 1.94e+07     |
|    n_updates            | 7950         |
|    policy_gradient_loss | -0.000246    |
|    std                  | 1.55         |
|    value_loss           | 6.25e+07     |
------------------------------------------
Eval num_timesteps=1629000, episode_reward=-57739.22 +/- 38289.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4381214 |
|    mean velocity x | -0.129     |
|    mean velocity y | 0.713      |
|    mean velocity z | 3.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.77e+04  |
| time/              |            |
|    total_timesteps | 1629000    |
-----------------------------------
Eval num_timesteps=1629500, episode_reward=-65044.18 +/- 36586.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2272015 |
|    mean velocity x | -0.192     |
|    mean velocity y | 0.243      |
|    mean velocity z | 0.953      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.5e+04   |
| time/              |            |
|    total_timesteps | 1629500    |
-----------------------------------
Eval num_timesteps=1630000, episode_reward=-61151.45 +/- 43902.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50399524 |
|    mean velocity x | -0.599      |
|    mean velocity y | 0.839       |
|    mean velocity z | 3.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.12e+04   |
| time/              |             |
|    total_timesteps | 1630000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 796     |
|    time_elapsed    | 65669   |
|    total_timesteps | 1630208 |
--------------------------------
Eval num_timesteps=1630500, episode_reward=-73846.86 +/- 35111.57
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.412303     |
|    mean velocity x      | 0.0567        |
|    mean velocity y      | 0.977         |
|    mean velocity z      | 3.31          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.38e+04     |
| time/                   |               |
|    total_timesteps      | 1630500       |
| train/                  |               |
|    approx_kl            | 2.9578776e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.001         |
|    loss                 | 1.69e+07      |
|    n_updates            | 7960          |
|    policy_gradient_loss | -0.000454     |
|    std                  | 1.55          |
|    value_loss           | 4.08e+07      |
-------------------------------------------
Eval num_timesteps=1631000, episode_reward=-88144.61 +/- 33527.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44475353 |
|    mean velocity x | -0.412      |
|    mean velocity y | 0.605       |
|    mean velocity z | 4.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.81e+04   |
| time/              |             |
|    total_timesteps | 1631000     |
------------------------------------
Eval num_timesteps=1631500, episode_reward=-71646.45 +/- 19649.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30243272 |
|    mean velocity x | -0.175      |
|    mean velocity y | 0.56        |
|    mean velocity z | 2.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.16e+04   |
| time/              |             |
|    total_timesteps | 1631500     |
------------------------------------
Eval num_timesteps=1632000, episode_reward=-89135.60 +/- 25621.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2639638 |
|    mean velocity x | -1.86      |
|    mean velocity y | -1.18      |
|    mean velocity z | 5.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.91e+04  |
| time/              |            |
|    total_timesteps | 1632000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 797     |
|    time_elapsed    | 65749   |
|    total_timesteps | 1632256 |
--------------------------------
Eval num_timesteps=1632500, episode_reward=-50498.41 +/- 33909.95
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38147017  |
|    mean velocity x      | -0.199       |
|    mean velocity y      | 0.889        |
|    mean velocity z      | 3.54         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.05e+04    |
| time/                   |              |
|    total_timesteps      | 1632500      |
| train/                  |              |
|    approx_kl            | 3.999559e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.001        |
|    loss                 | 8.05e+06     |
|    n_updates            | 7970         |
|    policy_gradient_loss | -0.000559    |
|    std                  | 1.55         |
|    value_loss           | 5.61e+07     |
------------------------------------------
Eval num_timesteps=1633000, episode_reward=-76884.96 +/- 23592.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44115075 |
|    mean velocity x | -0.14       |
|    mean velocity y | 0.767       |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 1633000     |
------------------------------------
Eval num_timesteps=1633500, episode_reward=-57056.56 +/- 51886.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32586077 |
|    mean velocity x | -0.911      |
|    mean velocity y | 0.613       |
|    mean velocity z | 1.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.71e+04   |
| time/              |             |
|    total_timesteps | 1633500     |
------------------------------------
Eval num_timesteps=1634000, episode_reward=-108500.74 +/- 16500.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30147555 |
|    mean velocity x | -1.26       |
|    mean velocity y | -0.0719     |
|    mean velocity z | 2.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1634000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 798     |
|    time_elapsed    | 65829   |
|    total_timesteps | 1634304 |
--------------------------------
Eval num_timesteps=1634500, episode_reward=-58826.75 +/- 17527.82
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46690476  |
|    mean velocity x      | -0.256       |
|    mean velocity y      | 1.2          |
|    mean velocity z      | 4.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.88e+04    |
| time/                   |              |
|    total_timesteps      | 1634500      |
| train/                  |              |
|    approx_kl            | 6.993301e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.001        |
|    loss                 | 3.65e+07     |
|    n_updates            | 7980         |
|    policy_gradient_loss | -0.000181    |
|    std                  | 1.56         |
|    value_loss           | 4.73e+07     |
------------------------------------------
Eval num_timesteps=1635000, episode_reward=-88527.37 +/- 11417.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30353045 |
|    mean velocity x | 0.271       |
|    mean velocity y | 0.931       |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.85e+04   |
| time/              |             |
|    total_timesteps | 1635000     |
------------------------------------
Eval num_timesteps=1635500, episode_reward=-65052.71 +/- 54061.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48497838 |
|    mean velocity x | -0.224      |
|    mean velocity y | 1.26        |
|    mean velocity z | 4.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.51e+04   |
| time/              |             |
|    total_timesteps | 1635500     |
------------------------------------
Eval num_timesteps=1636000, episode_reward=-94941.02 +/- 41368.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31139457 |
|    mean velocity x | 0.0208      |
|    mean velocity y | 0.395       |
|    mean velocity z | 0.905       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 1636000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 799     |
|    time_elapsed    | 65910   |
|    total_timesteps | 1636352 |
--------------------------------
Eval num_timesteps=1636500, episode_reward=-67409.66 +/- 44246.59
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5514866   |
|    mean velocity x      | -0.207       |
|    mean velocity y      | 1.75         |
|    mean velocity z      | 4.35         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.74e+04    |
| time/                   |              |
|    total_timesteps      | 1636500      |
| train/                  |              |
|    approx_kl            | 7.637485e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.266        |
|    learning_rate        | 0.001        |
|    loss                 | 9.7e+07      |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.000323    |
|    std                  | 1.55         |
|    value_loss           | 6.77e+07     |
------------------------------------------
Eval num_timesteps=1637000, episode_reward=-114268.76 +/- 13859.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32929116 |
|    mean velocity x | -0.522      |
|    mean velocity y | 0.864       |
|    mean velocity z | 1.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 1637000     |
------------------------------------
Eval num_timesteps=1637500, episode_reward=-86322.11 +/- 47807.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6334564 |
|    mean velocity x | 0.509      |
|    mean velocity y | 2.26       |
|    mean velocity z | 4.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.63e+04  |
| time/              |            |
|    total_timesteps | 1637500    |
-----------------------------------
Eval num_timesteps=1638000, episode_reward=-99446.21 +/- 33450.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1663034 |
|    mean velocity x | -0.603     |
|    mean velocity y | 0.202      |
|    mean velocity z | 1.89       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.94e+04  |
| time/              |            |
|    total_timesteps | 1638000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 800     |
|    time_elapsed    | 65990   |
|    total_timesteps | 1638400 |
--------------------------------
Eval num_timesteps=1638500, episode_reward=-83264.10 +/- 51604.09
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6118393    |
|    mean velocity x      | 0.728         |
|    mean velocity y      | 2.08          |
|    mean velocity z      | 3.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.33e+04     |
| time/                   |               |
|    total_timesteps      | 1638500       |
| train/                  |               |
|    approx_kl            | 3.1085685e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.559         |
|    learning_rate        | 0.001         |
|    loss                 | 6.06e+06      |
|    n_updates            | 8000          |
|    policy_gradient_loss | -0.000441     |
|    std                  | 1.55          |
|    value_loss           | 1.13e+07      |
-------------------------------------------
Eval num_timesteps=1639000, episode_reward=-85201.55 +/- 43690.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3266618 |
|    mean velocity x | -0.213     |
|    mean velocity y | 0.555      |
|    mean velocity z | 2.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.52e+04  |
| time/              |            |
|    total_timesteps | 1639000    |
-----------------------------------
Eval num_timesteps=1639500, episode_reward=-52268.17 +/- 45082.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4861355 |
|    mean velocity x | -0.492     |
|    mean velocity y | 0.824      |
|    mean velocity z | 4.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.23e+04  |
| time/              |            |
|    total_timesteps | 1639500    |
-----------------------------------
Eval num_timesteps=1640000, episode_reward=-81585.02 +/- 41030.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4670636 |
|    mean velocity x | -0.104     |
|    mean velocity y | 0.737      |
|    mean velocity z | 1.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.16e+04  |
| time/              |            |
|    total_timesteps | 1640000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 801     |
|    time_elapsed    | 66070   |
|    total_timesteps | 1640448 |
--------------------------------
Eval num_timesteps=1640500, episode_reward=-74883.51 +/- 45761.30
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42926314  |
|    mean velocity x      | -0.237       |
|    mean velocity y      | 0.907        |
|    mean velocity z      | 4.26         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.49e+04    |
| time/                   |              |
|    total_timesteps      | 1640500      |
| train/                  |              |
|    approx_kl            | 9.105279e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.001        |
|    loss                 | 8.3e+07      |
|    n_updates            | 8010         |
|    policy_gradient_loss | -0.00024     |
|    std                  | 1.56         |
|    value_loss           | 5.83e+07     |
------------------------------------------
Eval num_timesteps=1641000, episode_reward=-102945.83 +/- 17917.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3864522 |
|    mean velocity x | 0.00313    |
|    mean velocity y | 1          |
|    mean velocity z | 4.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1641000    |
-----------------------------------
Eval num_timesteps=1641500, episode_reward=-93867.18 +/- 37925.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39203998 |
|    mean velocity x | -0.364      |
|    mean velocity y | 0.272       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.39e+04   |
| time/              |             |
|    total_timesteps | 1641500     |
------------------------------------
Eval num_timesteps=1642000, episode_reward=-115968.93 +/- 14768.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4626048 |
|    mean velocity x | -0.267     |
|    mean velocity y | 1.08       |
|    mean velocity z | 4.49       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.16e+05  |
| time/              |            |
|    total_timesteps | 1642000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 802     |
|    time_elapsed    | 66151   |
|    total_timesteps | 1642496 |
--------------------------------
Eval num_timesteps=1642500, episode_reward=-76378.59 +/- 39290.46
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4158473    |
|    mean velocity x      | -0.694        |
|    mean velocity y      | 0.434         |
|    mean velocity z      | 3.35          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.64e+04     |
| time/                   |               |
|    total_timesteps      | 1642500       |
| train/                  |               |
|    approx_kl            | 2.3882458e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+07      |
|    n_updates            | 8020          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 1.55          |
|    value_loss           | 9.74e+07      |
-------------------------------------------
Eval num_timesteps=1643000, episode_reward=-73619.16 +/- 24957.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4306547 |
|    mean velocity x | -0.343     |
|    mean velocity y | 0.611      |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.36e+04  |
| time/              |            |
|    total_timesteps | 1643000    |
-----------------------------------
Eval num_timesteps=1643500, episode_reward=-126915.92 +/- 44739.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36937407 |
|    mean velocity x | -0.233      |
|    mean velocity y | 1.15        |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.27e+05   |
| time/              |             |
|    total_timesteps | 1643500     |
------------------------------------
Eval num_timesteps=1644000, episode_reward=-74300.79 +/- 16758.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3130163 |
|    mean velocity x | -1.03      |
|    mean velocity y | 0.204      |
|    mean velocity z | 2.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.43e+04  |
| time/              |            |
|    total_timesteps | 1644000    |
-----------------------------------
Eval num_timesteps=1644500, episode_reward=-87536.35 +/- 36436.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42096347 |
|    mean velocity x | -0.583      |
|    mean velocity y | 0.0378      |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.75e+04   |
| time/              |             |
|    total_timesteps | 1644500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 803     |
|    time_elapsed    | 66250   |
|    total_timesteps | 1644544 |
--------------------------------
Eval num_timesteps=1645000, episode_reward=-75754.59 +/- 49645.87
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42998004   |
|    mean velocity x      | -0.337        |
|    mean velocity y      | 0.948         |
|    mean velocity z      | 4.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.58e+04     |
| time/                   |               |
|    total_timesteps      | 1645000       |
| train/                  |               |
|    approx_kl            | 2.2273161e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.001         |
|    loss                 | 2.59e+07      |
|    n_updates            | 8030          |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.55          |
|    value_loss           | 9.51e+07      |
-------------------------------------------
Eval num_timesteps=1645500, episode_reward=-85923.52 +/- 18348.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51790464 |
|    mean velocity x | 0.376       |
|    mean velocity y | 1.34        |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.59e+04   |
| time/              |             |
|    total_timesteps | 1645500     |
------------------------------------
Eval num_timesteps=1646000, episode_reward=-92028.38 +/- 26836.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4236664 |
|    mean velocity x | 0.0394     |
|    mean velocity y | 1.37       |
|    mean velocity z | 4.11       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.2e+04   |
| time/              |            |
|    total_timesteps | 1646000    |
-----------------------------------
Eval num_timesteps=1646500, episode_reward=-98074.79 +/- 23688.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36225322 |
|    mean velocity x | -0.101      |
|    mean velocity y | 1           |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.81e+04   |
| time/              |             |
|    total_timesteps | 1646500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 804     |
|    time_elapsed    | 66330   |
|    total_timesteps | 1646592 |
--------------------------------
Eval num_timesteps=1647000, episode_reward=-80639.15 +/- 36523.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29098648  |
|    mean velocity x      | -0.791       |
|    mean velocity y      | -0.195       |
|    mean velocity z      | 2.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.06e+04    |
| time/                   |              |
|    total_timesteps      | 1647000      |
| train/                  |              |
|    approx_kl            | 8.375966e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.001        |
|    loss                 | 1.22e+07     |
|    n_updates            | 8040         |
|    policy_gradient_loss | -0.000188    |
|    std                  | 1.55         |
|    value_loss           | 6.18e+07     |
------------------------------------------
Eval num_timesteps=1647500, episode_reward=-102767.13 +/- 72523.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32271796 |
|    mean velocity x | -1.06       |
|    mean velocity y | 0.115       |
|    mean velocity z | 2.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1647500     |
------------------------------------
Eval num_timesteps=1648000, episode_reward=-92345.09 +/- 18627.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48850626 |
|    mean velocity x | -0.945      |
|    mean velocity y | 0.536       |
|    mean velocity z | 3.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 1648000     |
------------------------------------
Eval num_timesteps=1648500, episode_reward=-76215.71 +/- 15986.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46823686 |
|    mean velocity x | -0.281      |
|    mean velocity y | 1.09        |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.62e+04   |
| time/              |             |
|    total_timesteps | 1648500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 805     |
|    time_elapsed    | 66411   |
|    total_timesteps | 1648640 |
--------------------------------
Eval num_timesteps=1649000, episode_reward=-53738.56 +/- 37526.11
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40024254  |
|    mean velocity x      | -0.341       |
|    mean velocity y      | 0.478        |
|    mean velocity z      | 4.05         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.37e+04    |
| time/                   |              |
|    total_timesteps      | 1649000      |
| train/                  |              |
|    approx_kl            | 9.894517e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.001        |
|    loss                 | 5.77e+07     |
|    n_updates            | 8050         |
|    policy_gradient_loss | -0.000298    |
|    std                  | 1.55         |
|    value_loss           | 6.08e+07     |
------------------------------------------
Eval num_timesteps=1649500, episode_reward=-74838.81 +/- 40628.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37890163 |
|    mean velocity x | -0.611      |
|    mean velocity y | 0.402       |
|    mean velocity z | 2.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.48e+04   |
| time/              |             |
|    total_timesteps | 1649500     |
------------------------------------
Eval num_timesteps=1650000, episode_reward=-63917.80 +/- 52315.58
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5432379 |
|    mean velocity x | 0.632      |
|    mean velocity y | 1.93       |
|    mean velocity z | 3.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.39e+04  |
| time/              |            |
|    total_timesteps | 1650000    |
-----------------------------------
Eval num_timesteps=1650500, episode_reward=-62908.23 +/- 41417.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52332735 |
|    mean velocity x | -0.568      |
|    mean velocity y | 0.803       |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.29e+04   |
| time/              |             |
|    total_timesteps | 1650500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 806     |
|    time_elapsed    | 66491   |
|    total_timesteps | 1650688 |
--------------------------------
Eval num_timesteps=1651000, episode_reward=-58792.35 +/- 39928.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36392725  |
|    mean velocity x      | -0.0405      |
|    mean velocity y      | 0.619        |
|    mean velocity z      | 3.82         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.88e+04    |
| time/                   |              |
|    total_timesteps      | 1651000      |
| train/                  |              |
|    approx_kl            | 4.796282e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.001        |
|    loss                 | 9.95e+06     |
|    n_updates            | 8060         |
|    policy_gradient_loss | -0.000147    |
|    std                  | 1.55         |
|    value_loss           | 4.14e+07     |
------------------------------------------
Eval num_timesteps=1651500, episode_reward=-86639.30 +/- 21865.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46771026 |
|    mean velocity x | -0.965      |
|    mean velocity y | 0.465       |
|    mean velocity z | 2.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 1651500     |
------------------------------------
Eval num_timesteps=1652000, episode_reward=-60431.65 +/- 50226.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37573823 |
|    mean velocity x | -0.25       |
|    mean velocity y | 0.571       |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.04e+04   |
| time/              |             |
|    total_timesteps | 1652000     |
------------------------------------
Eval num_timesteps=1652500, episode_reward=-75015.38 +/- 17230.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30939028 |
|    mean velocity x | 0.246       |
|    mean velocity y | 0.712       |
|    mean velocity z | 2.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.5e+04    |
| time/              |             |
|    total_timesteps | 1652500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 807     |
|    time_elapsed    | 66572   |
|    total_timesteps | 1652736 |
--------------------------------
Eval num_timesteps=1653000, episode_reward=-75126.47 +/- 23288.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39849353   |
|    mean velocity x      | -0.202        |
|    mean velocity y      | 1.13          |
|    mean velocity z      | 4.39          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.51e+04     |
| time/                   |               |
|    total_timesteps      | 1653000       |
| train/                  |               |
|    approx_kl            | 1.3557321e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+07      |
|    n_updates            | 8070          |
|    policy_gradient_loss | -0.000265     |
|    std                  | 1.55          |
|    value_loss           | 5.47e+07      |
-------------------------------------------
Eval num_timesteps=1653500, episode_reward=-74955.12 +/- 33081.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4802485 |
|    mean velocity x | -0.388     |
|    mean velocity y | 0.769      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.5e+04   |
| time/              |            |
|    total_timesteps | 1653500    |
-----------------------------------
Eval num_timesteps=1654000, episode_reward=-82030.43 +/- 31654.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30442736 |
|    mean velocity x | -0.416      |
|    mean velocity y | -0.182      |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 1654000     |
------------------------------------
Eval num_timesteps=1654500, episode_reward=-83186.60 +/- 39851.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46446785 |
|    mean velocity x | 0.425       |
|    mean velocity y | 1.45        |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.32e+04   |
| time/              |             |
|    total_timesteps | 1654500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 808     |
|    time_elapsed    | 66652   |
|    total_timesteps | 1654784 |
--------------------------------
Eval num_timesteps=1655000, episode_reward=-90328.16 +/- 21013.00
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3943856    |
|    mean velocity x      | -0.00014      |
|    mean velocity y      | 0.845         |
|    mean velocity z      | 3.36          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.03e+04     |
| time/                   |               |
|    total_timesteps      | 1655000       |
| train/                  |               |
|    approx_kl            | 0.00010465155 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.001         |
|    loss                 | 6.08e+07      |
|    n_updates            | 8080          |
|    policy_gradient_loss | -0.000767     |
|    std                  | 1.55          |
|    value_loss           | 7.28e+07      |
-------------------------------------------
Eval num_timesteps=1655500, episode_reward=-86019.94 +/- 25273.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2521261 |
|    mean velocity x | -2.67      |
|    mean velocity y | -2.62      |
|    mean velocity z | 8.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.6e+04   |
| time/              |            |
|    total_timesteps | 1655500    |
-----------------------------------
Eval num_timesteps=1656000, episode_reward=-39009.49 +/- 36641.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45269096 |
|    mean velocity x | -0.257      |
|    mean velocity y | 0.812       |
|    mean velocity z | 4.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.9e+04    |
| time/              |             |
|    total_timesteps | 1656000     |
------------------------------------
Eval num_timesteps=1656500, episode_reward=-68328.24 +/- 31628.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6312459 |
|    mean velocity x | -0.805     |
|    mean velocity y | 0.115      |
|    mean velocity z | 5.06       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.83e+04  |
| time/              |            |
|    total_timesteps | 1656500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 809     |
|    time_elapsed    | 66732   |
|    total_timesteps | 1656832 |
--------------------------------
Eval num_timesteps=1657000, episode_reward=-82180.24 +/- 33448.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5524145    |
|    mean velocity x      | -0.448        |
|    mean velocity y      | 0.862         |
|    mean velocity z      | 4.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.22e+04     |
| time/                   |               |
|    total_timesteps      | 1657000       |
| train/                  |               |
|    approx_kl            | 6.6953653e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.334         |
|    learning_rate        | 0.001         |
|    loss                 | 2.01e+07      |
|    n_updates            | 8090          |
|    policy_gradient_loss | -0.000189     |
|    std                  | 1.55          |
|    value_loss           | 7.98e+07      |
-------------------------------------------
Eval num_timesteps=1657500, episode_reward=-65203.48 +/- 46592.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36954722 |
|    mean velocity x | 0.0582      |
|    mean velocity y | 0.606       |
|    mean velocity z | 2.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.52e+04   |
| time/              |             |
|    total_timesteps | 1657500     |
------------------------------------
Eval num_timesteps=1658000, episode_reward=-101475.63 +/- 20437.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44359687 |
|    mean velocity x | -0.41       |
|    mean velocity y | 0.358       |
|    mean velocity z | 4.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1658000     |
------------------------------------
Eval num_timesteps=1658500, episode_reward=-83469.90 +/- 41888.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3514225 |
|    mean velocity x | 0.168      |
|    mean velocity y | 0.806      |
|    mean velocity z | 3.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.35e+04  |
| time/              |            |
|    total_timesteps | 1658500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 810     |
|    time_elapsed    | 66813   |
|    total_timesteps | 1658880 |
--------------------------------
Eval num_timesteps=1659000, episode_reward=-96410.31 +/- 18594.49
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38512513   |
|    mean velocity x      | -0.0113       |
|    mean velocity y      | 1.04          |
|    mean velocity z      | 4.19          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.64e+04     |
| time/                   |               |
|    total_timesteps      | 1659000       |
| train/                  |               |
|    approx_kl            | 2.2559543e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+07      |
|    n_updates            | 8100          |
|    policy_gradient_loss | -6.86e-05     |
|    std                  | 1.55          |
|    value_loss           | 7.41e+07      |
-------------------------------------------
Eval num_timesteps=1659500, episode_reward=-62614.22 +/- 29379.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43094075 |
|    mean velocity x | -0.465      |
|    mean velocity y | 0.832       |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.26e+04   |
| time/              |             |
|    total_timesteps | 1659500     |
------------------------------------
Eval num_timesteps=1660000, episode_reward=-62831.30 +/- 33567.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41842157 |
|    mean velocity x | 0.188       |
|    mean velocity y | 0.913       |
|    mean velocity z | 2.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 1660000     |
------------------------------------
Eval num_timesteps=1660500, episode_reward=-104501.08 +/- 22259.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5011591 |
|    mean velocity x | -0.409     |
|    mean velocity y | 0.858      |
|    mean velocity z | 4.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.05e+05  |
| time/              |            |
|    total_timesteps | 1660500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 811     |
|    time_elapsed    | 66893   |
|    total_timesteps | 1660928 |
--------------------------------
Eval num_timesteps=1661000, episode_reward=-90736.17 +/- 48665.28
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.56270885   |
|    mean velocity x      | 0.465         |
|    mean velocity y      | 1.84          |
|    mean velocity z      | 3.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.07e+04     |
| time/                   |               |
|    total_timesteps      | 1661000       |
| train/                  |               |
|    approx_kl            | 3.9170292e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.302         |
|    learning_rate        | 0.001         |
|    loss                 | 3.69e+07      |
|    n_updates            | 8110          |
|    policy_gradient_loss | -0.000432     |
|    std                  | 1.55          |
|    value_loss           | 5.65e+07      |
-------------------------------------------
Eval num_timesteps=1661500, episode_reward=-99068.77 +/- 27828.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3682418 |
|    mean velocity x | -1.85      |
|    mean velocity y | -1.36      |
|    mean velocity z | 6.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.91e+04  |
| time/              |            |
|    total_timesteps | 1661500    |
-----------------------------------
Eval num_timesteps=1662000, episode_reward=-69812.69 +/- 41262.43
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3976126 |
|    mean velocity x | 0.0684     |
|    mean velocity y | 1.09       |
|    mean velocity z | 3.85       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.98e+04  |
| time/              |            |
|    total_timesteps | 1662000    |
-----------------------------------
Eval num_timesteps=1662500, episode_reward=-77416.67 +/- 36659.61
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.493275 |
|    mean velocity x | -0.204    |
|    mean velocity y | 1.07      |
|    mean velocity z | 4.31      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.74e+04 |
| time/              |           |
|    total_timesteps | 1662500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 812     |
|    time_elapsed    | 66973   |
|    total_timesteps | 1662976 |
--------------------------------
Eval num_timesteps=1663000, episode_reward=-58478.13 +/- 28145.52
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49009773  |
|    mean velocity x      | -0.0435      |
|    mean velocity y      | 1.11         |
|    mean velocity z      | 3.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.85e+04    |
| time/                   |              |
|    total_timesteps      | 1663000      |
| train/                  |              |
|    approx_kl            | 2.412562e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.001        |
|    loss                 | 3.82e+07     |
|    n_updates            | 8120         |
|    policy_gradient_loss | -0.000119    |
|    std                  | 1.55         |
|    value_loss           | 6.51e+07     |
------------------------------------------
Eval num_timesteps=1663500, episode_reward=-83442.87 +/- 28288.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45535007 |
|    mean velocity x | 0.3         |
|    mean velocity y | 1.59        |
|    mean velocity z | 3.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.34e+04   |
| time/              |             |
|    total_timesteps | 1663500     |
------------------------------------
Eval num_timesteps=1664000, episode_reward=-74282.21 +/- 41172.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47583503 |
|    mean velocity x | 0.034       |
|    mean velocity y | 1.54        |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 1664000     |
------------------------------------
Eval num_timesteps=1664500, episode_reward=-86122.05 +/- 18685.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33303085 |
|    mean velocity x | -0.571      |
|    mean velocity y | 0.2         |
|    mean velocity z | 2.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.61e+04   |
| time/              |             |
|    total_timesteps | 1664500     |
------------------------------------
Eval num_timesteps=1665000, episode_reward=-88678.76 +/- 24995.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26120448 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.436       |
|    mean velocity z | 0.394       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.87e+04   |
| time/              |             |
|    total_timesteps | 1665000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 813     |
|    time_elapsed    | 67073   |
|    total_timesteps | 1665024 |
--------------------------------
Eval num_timesteps=1665500, episode_reward=-82621.51 +/- 45610.60
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.23951334   |
|    mean velocity x      | -0.13         |
|    mean velocity y      | 0.379         |
|    mean velocity z      | 0.468         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.26e+04     |
| time/                   |               |
|    total_timesteps      | 1665500       |
| train/                  |               |
|    approx_kl            | 5.9675076e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+07      |
|    n_updates            | 8130          |
|    policy_gradient_loss | -0.000209     |
|    std                  | 1.55          |
|    value_loss           | 2.88e+07      |
-------------------------------------------
Eval num_timesteps=1666000, episode_reward=-89564.76 +/- 16960.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40677953 |
|    mean velocity x | 0.0379      |
|    mean velocity y | 0.534       |
|    mean velocity z | 2.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 1666000     |
------------------------------------
Eval num_timesteps=1666500, episode_reward=-104614.81 +/- 22430.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46391943 |
|    mean velocity x | -0.711      |
|    mean velocity y | 0.552       |
|    mean velocity z | 3.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1666500     |
------------------------------------
Eval num_timesteps=1667000, episode_reward=-59503.89 +/- 46590.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4182878 |
|    mean velocity x | -0.269     |
|    mean velocity y | 0.733      |
|    mean velocity z | 4.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.95e+04  |
| time/              |            |
|    total_timesteps | 1667000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 814     |
|    time_elapsed    | 67153   |
|    total_timesteps | 1667072 |
--------------------------------
Eval num_timesteps=1667500, episode_reward=-64048.35 +/- 57348.91
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27730975   |
|    mean velocity x      | 0.119         |
|    mean velocity y      | 0.348         |
|    mean velocity z      | 2.48          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.4e+04      |
| time/                   |               |
|    total_timesteps      | 1667500       |
| train/                  |               |
|    approx_kl            | 2.5296671e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.258         |
|    learning_rate        | 0.001         |
|    loss                 | 4.33e+07      |
|    n_updates            | 8140          |
|    policy_gradient_loss | -0.000345     |
|    std                  | 1.55          |
|    value_loss           | 4.94e+07      |
-------------------------------------------
Eval num_timesteps=1668000, episode_reward=-36128.65 +/- 41174.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.59867704 |
|    mean velocity x | 0.461       |
|    mean velocity y | 2.34        |
|    mean velocity z | 3.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.61e+04   |
| time/              |             |
|    total_timesteps | 1668000     |
------------------------------------
Eval num_timesteps=1668500, episode_reward=-91164.97 +/- 28898.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39089885 |
|    mean velocity x | -0.376      |
|    mean velocity y | 0.76        |
|    mean velocity z | 4.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.12e+04   |
| time/              |             |
|    total_timesteps | 1668500     |
------------------------------------
Eval num_timesteps=1669000, episode_reward=-92971.71 +/- 28544.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5269242 |
|    mean velocity x | -0.311     |
|    mean velocity y | 1.37       |
|    mean velocity z | 4.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.3e+04   |
| time/              |            |
|    total_timesteps | 1669000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 815     |
|    time_elapsed    | 67233   |
|    total_timesteps | 1669120 |
--------------------------------
Eval num_timesteps=1669500, episode_reward=-84902.89 +/- 15559.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.094290175  |
|    mean velocity x      | 0.122         |
|    mean velocity y      | 0.155         |
|    mean velocity z      | 0.352         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.49e+04     |
| time/                   |               |
|    total_timesteps      | 1669500       |
| train/                  |               |
|    approx_kl            | 3.7698745e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.288         |
|    learning_rate        | 0.001         |
|    loss                 | 6.84e+06      |
|    n_updates            | 8150          |
|    policy_gradient_loss | -0.000583     |
|    std                  | 1.55          |
|    value_loss           | 5.59e+07      |
-------------------------------------------
Eval num_timesteps=1670000, episode_reward=-58042.56 +/- 17971.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23714432 |
|    mean velocity x | -0.226      |
|    mean velocity y | -0.053      |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.8e+04    |
| time/              |             |
|    total_timesteps | 1670000     |
------------------------------------
Eval num_timesteps=1670500, episode_reward=-60066.55 +/- 51600.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42814538 |
|    mean velocity x | -0.222      |
|    mean velocity y | 0.647       |
|    mean velocity z | 0.853       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.01e+04   |
| time/              |             |
|    total_timesteps | 1670500     |
------------------------------------
Eval num_timesteps=1671000, episode_reward=-90332.96 +/- 30176.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3241651 |
|    mean velocity x | -0.801     |
|    mean velocity y | 0.255      |
|    mean velocity z | 2.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.03e+04  |
| time/              |            |
|    total_timesteps | 1671000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 816     |
|    time_elapsed    | 67314   |
|    total_timesteps | 1671168 |
--------------------------------
Eval num_timesteps=1671500, episode_reward=-72056.89 +/- 25248.64
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31207794   |
|    mean velocity x      | -0.215        |
|    mean velocity y      | 0.79          |
|    mean velocity z      | 1.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.21e+04     |
| time/                   |               |
|    total_timesteps      | 1671500       |
| train/                  |               |
|    approx_kl            | 1.5974103e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.001         |
|    loss                 | 6.85e+06      |
|    n_updates            | 8160          |
|    policy_gradient_loss | -0.000266     |
|    std                  | 1.55          |
|    value_loss           | 1.94e+07      |
-------------------------------------------
Eval num_timesteps=1672000, episode_reward=-96749.53 +/- 14312.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47932723 |
|    mean velocity x | -0.492      |
|    mean velocity y | 0.666       |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.67e+04   |
| time/              |             |
|    total_timesteps | 1672000     |
------------------------------------
Eval num_timesteps=1672500, episode_reward=-78497.59 +/- 22908.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4531316 |
|    mean velocity x | 0.459      |
|    mean velocity y | 1.07       |
|    mean velocity z | 2.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.85e+04  |
| time/              |            |
|    total_timesteps | 1672500    |
-----------------------------------
Eval num_timesteps=1673000, episode_reward=-88570.17 +/- 23282.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5299655 |
|    mean velocity x | -0.399     |
|    mean velocity y | 0.761      |
|    mean velocity z | 4.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.86e+04  |
| time/              |            |
|    total_timesteps | 1673000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 817     |
|    time_elapsed    | 67394   |
|    total_timesteps | 1673216 |
--------------------------------
Eval num_timesteps=1673500, episode_reward=-73360.07 +/- 36442.64
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2670136    |
|    mean velocity x      | -0.217        |
|    mean velocity y      | 0.531         |
|    mean velocity z      | 0.356         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.34e+04     |
| time/                   |               |
|    total_timesteps      | 1673500       |
| train/                  |               |
|    approx_kl            | 2.7173024e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.001         |
|    loss                 | 5.61e+07      |
|    n_updates            | 8170          |
|    policy_gradient_loss | -0.000424     |
|    std                  | 1.55          |
|    value_loss           | 5.55e+07      |
-------------------------------------------
Eval num_timesteps=1674000, episode_reward=-76429.96 +/- 22753.30
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.477664 |
|    mean velocity x | 0.185     |
|    mean velocity y | 1.74      |
|    mean velocity z | 3.67      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.64e+04 |
| time/              |           |
|    total_timesteps | 1674000   |
----------------------------------
Eval num_timesteps=1674500, episode_reward=-102766.29 +/- 46302.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2400657 |
|    mean velocity x | -0.173     |
|    mean velocity y | 0.443      |
|    mean velocity z | 0.345      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1674500    |
-----------------------------------
Eval num_timesteps=1675000, episode_reward=-46140.78 +/- 38981.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25737783 |
|    mean velocity x | -1.25       |
|    mean velocity y | -0.446      |
|    mean velocity z | 3.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.61e+04   |
| time/              |             |
|    total_timesteps | 1675000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 818     |
|    time_elapsed    | 67475   |
|    total_timesteps | 1675264 |
--------------------------------
Eval num_timesteps=1675500, episode_reward=-81286.20 +/- 41786.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.480669     |
|    mean velocity x      | -0.339        |
|    mean velocity y      | 1.12          |
|    mean velocity z      | 3.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.13e+04     |
| time/                   |               |
|    total_timesteps      | 1675500       |
| train/                  |               |
|    approx_kl            | 1.7909479e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.415         |
|    learning_rate        | 0.001         |
|    loss                 | 2.44e+07      |
|    n_updates            | 8180          |
|    policy_gradient_loss | -0.000307     |
|    std                  | 1.55          |
|    value_loss           | 3.06e+07      |
-------------------------------------------
Eval num_timesteps=1676000, episode_reward=-47026.77 +/- 23393.36
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4773033 |
|    mean velocity x | -0.282     |
|    mean velocity y | 1.3        |
|    mean velocity z | 4.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.7e+04   |
| time/              |            |
|    total_timesteps | 1676000    |
-----------------------------------
Eval num_timesteps=1676500, episode_reward=-70000.04 +/- 37999.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37674883 |
|    mean velocity x | -0.429      |
|    mean velocity y | 0.465       |
|    mean velocity z | 2.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7e+04      |
| time/              |             |
|    total_timesteps | 1676500     |
------------------------------------
Eval num_timesteps=1677000, episode_reward=-66770.88 +/- 26002.50
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5402499 |
|    mean velocity x | 0.281      |
|    mean velocity y | 1.38       |
|    mean velocity z | 3.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.68e+04  |
| time/              |            |
|    total_timesteps | 1677000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 819     |
|    time_elapsed    | 67555   |
|    total_timesteps | 1677312 |
--------------------------------
Eval num_timesteps=1677500, episode_reward=-82900.64 +/- 43695.81
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3995981  |
|    mean velocity x      | -1.15       |
|    mean velocity y      | 0.0192      |
|    mean velocity z      | 2.98        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -8.29e+04   |
| time/                   |             |
|    total_timesteps      | 1677500     |
| train/                  |             |
|    approx_kl            | 9.53424e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.001       |
|    loss                 | 4.83e+07    |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.000218   |
|    std                  | 1.55        |
|    value_loss           | 3.56e+07    |
-----------------------------------------
Eval num_timesteps=1678000, episode_reward=-63149.34 +/- 48238.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47430575 |
|    mean velocity x | -0.926      |
|    mean velocity y | 0.305       |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.31e+04   |
| time/              |             |
|    total_timesteps | 1678000     |
------------------------------------
Eval num_timesteps=1678500, episode_reward=-61547.79 +/- 28350.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4272009 |
|    mean velocity x | 0.359      |
|    mean velocity y | 1.18       |
|    mean velocity z | 3.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.15e+04  |
| time/              |            |
|    total_timesteps | 1678500    |
-----------------------------------
Eval num_timesteps=1679000, episode_reward=-72994.81 +/- 47041.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47136766 |
|    mean velocity x | -0.25       |
|    mean velocity y | 1.48        |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.3e+04    |
| time/              |             |
|    total_timesteps | 1679000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 820     |
|    time_elapsed    | 67635   |
|    total_timesteps | 1679360 |
--------------------------------
Eval num_timesteps=1679500, episode_reward=-101423.20 +/- 27737.16
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42465034  |
|    mean velocity x      | -0.0305      |
|    mean velocity y      | 1.13         |
|    mean velocity z      | 3.61         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.01e+05    |
| time/                   |              |
|    total_timesteps      | 1679500      |
| train/                  |              |
|    approx_kl            | 4.332571e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.001        |
|    loss                 | 1.79e+07     |
|    n_updates            | 8200         |
|    policy_gradient_loss | -0.000124    |
|    std                  | 1.55         |
|    value_loss           | 5.22e+07     |
------------------------------------------
Eval num_timesteps=1680000, episode_reward=-71568.35 +/- 42244.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44042885 |
|    mean velocity x | 0.441       |
|    mean velocity y | 1.39        |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.16e+04   |
| time/              |             |
|    total_timesteps | 1680000     |
------------------------------------
Eval num_timesteps=1680500, episode_reward=-97138.55 +/- 31247.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5832168 |
|    mean velocity x | 0.645      |
|    mean velocity y | 1.81       |
|    mean velocity z | 3.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.71e+04  |
| time/              |            |
|    total_timesteps | 1680500    |
-----------------------------------
Eval num_timesteps=1681000, episode_reward=-74833.21 +/- 32703.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4198683 |
|    mean velocity x | -0.42      |
|    mean velocity y | 0.716      |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.48e+04  |
| time/              |            |
|    total_timesteps | 1681000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 821     |
|    time_elapsed    | 67716   |
|    total_timesteps | 1681408 |
--------------------------------
Eval num_timesteps=1681500, episode_reward=-99616.18 +/- 29115.64
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43981218   |
|    mean velocity x      | -0.534        |
|    mean velocity y      | 0.567         |
|    mean velocity z      | 3.69          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.96e+04     |
| time/                   |               |
|    total_timesteps      | 1681500       |
| train/                  |               |
|    approx_kl            | 7.2988914e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.377         |
|    learning_rate        | 0.001         |
|    loss                 | 2.51e+07      |
|    n_updates            | 8210          |
|    policy_gradient_loss | -0.000253     |
|    std                  | 1.55          |
|    value_loss           | 4.38e+07      |
-------------------------------------------
Eval num_timesteps=1682000, episode_reward=-109719.11 +/- 21516.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3568432 |
|    mean velocity x | 0.571      |
|    mean velocity y | 0.883      |
|    mean velocity z | 2.51       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 1682000    |
-----------------------------------
Eval num_timesteps=1682500, episode_reward=-77858.83 +/- 44157.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4902794 |
|    mean velocity x | -0.288     |
|    mean velocity y | 1          |
|    mean velocity z | 4.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.79e+04  |
| time/              |            |
|    total_timesteps | 1682500    |
-----------------------------------
Eval num_timesteps=1683000, episode_reward=-107678.62 +/- 14295.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45616424 |
|    mean velocity x | -0.713      |
|    mean velocity y | 0.597       |
|    mean velocity z | 4.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1683000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 822     |
|    time_elapsed    | 67796   |
|    total_timesteps | 1683456 |
--------------------------------
Eval num_timesteps=1683500, episode_reward=-97663.83 +/- 34026.91
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47748384   |
|    mean velocity x      | -0.492        |
|    mean velocity y      | 0.395         |
|    mean velocity z      | 3.94          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.77e+04     |
| time/                   |               |
|    total_timesteps      | 1683500       |
| train/                  |               |
|    approx_kl            | 6.9040107e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.001         |
|    loss                 | 2.11e+07      |
|    n_updates            | 8220          |
|    policy_gradient_loss | -0.000148     |
|    std                  | 1.55          |
|    value_loss           | 7.45e+07      |
-------------------------------------------
Eval num_timesteps=1684000, episode_reward=-96406.70 +/- 22992.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26485926 |
|    mean velocity x | 0.0128      |
|    mean velocity y | 0.331       |
|    mean velocity z | 0.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.64e+04   |
| time/              |             |
|    total_timesteps | 1684000     |
------------------------------------
Eval num_timesteps=1684500, episode_reward=-85303.68 +/- 46252.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44512457 |
|    mean velocity x | -0.287      |
|    mean velocity y | 1.33        |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 1684500     |
------------------------------------
Eval num_timesteps=1685000, episode_reward=-50259.34 +/- 29770.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3687737 |
|    mean velocity x | 0.262      |
|    mean velocity y | 0.877      |
|    mean velocity z | 3.3        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.03e+04  |
| time/              |            |
|    total_timesteps | 1685000    |
-----------------------------------
Eval num_timesteps=1685500, episode_reward=-74139.84 +/- 35010.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46218902 |
|    mean velocity x | -0.975      |
|    mean velocity y | 0.265       |
|    mean velocity z | 3.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.41e+04   |
| time/              |             |
|    total_timesteps | 1685500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 823     |
|    time_elapsed    | 67895   |
|    total_timesteps | 1685504 |
--------------------------------
Eval num_timesteps=1686000, episode_reward=-77111.46 +/- 45332.57
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.18633774   |
|    mean velocity x      | -0.35         |
|    mean velocity y      | 0.551         |
|    mean velocity z      | 0.443         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.71e+04     |
| time/                   |               |
|    total_timesteps      | 1686000       |
| train/                  |               |
|    approx_kl            | 2.4082663e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.57         |
|    explained_variance   | 0.345         |
|    learning_rate        | 0.001         |
|    loss                 | 7.78e+06      |
|    n_updates            | 8230          |
|    policy_gradient_loss | -0.000355     |
|    std                  | 1.55          |
|    value_loss           | 4.28e+07      |
-------------------------------------------
Eval num_timesteps=1686500, episode_reward=-94341.92 +/- 36372.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41406047 |
|    mean velocity x | -0.365      |
|    mean velocity y | 0.711       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.43e+04   |
| time/              |             |
|    total_timesteps | 1686500     |
------------------------------------
Eval num_timesteps=1687000, episode_reward=-88468.75 +/- 41326.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4068156 |
|    mean velocity x | 0.193      |
|    mean velocity y | 1.09       |
|    mean velocity z | 3.29       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.85e+04  |
| time/              |            |
|    total_timesteps | 1687000    |
-----------------------------------
Eval num_timesteps=1687500, episode_reward=-123691.59 +/- 22245.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47557172 |
|    mean velocity x | -0.213      |
|    mean velocity y | 1.23        |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.24e+05   |
| time/              |             |
|    total_timesteps | 1687500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 824     |
|    time_elapsed    | 67976   |
|    total_timesteps | 1687552 |
--------------------------------
Eval num_timesteps=1688000, episode_reward=-74751.43 +/- 38858.78
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40436456  |
|    mean velocity x      | -0.37        |
|    mean velocity y      | 0.735        |
|    mean velocity z      | 4.61         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.48e+04    |
| time/                   |              |
|    total_timesteps      | 1688000      |
| train/                  |              |
|    approx_kl            | 9.269745e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.001        |
|    loss                 | 8.17e+07     |
|    n_updates            | 8240         |
|    policy_gradient_loss | -0.000362    |
|    std                  | 1.55         |
|    value_loss           | 7.8e+07      |
------------------------------------------
Eval num_timesteps=1688500, episode_reward=-57087.22 +/- 32161.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29871684 |
|    mean velocity x | -0.396      |
|    mean velocity y | 0.134       |
|    mean velocity z | 2.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.71e+04   |
| time/              |             |
|    total_timesteps | 1688500     |
------------------------------------
Eval num_timesteps=1689000, episode_reward=-62494.09 +/- 37365.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32230407 |
|    mean velocity x | -0.916      |
|    mean velocity y | -0.0762     |
|    mean velocity z | 2.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 1689000     |
------------------------------------
Eval num_timesteps=1689500, episode_reward=-91706.51 +/- 34735.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4564671 |
|    mean velocity x | 0.436      |
|    mean velocity y | 1.41       |
|    mean velocity z | 3.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.17e+04  |
| time/              |            |
|    total_timesteps | 1689500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 825     |
|    time_elapsed    | 68056   |
|    total_timesteps | 1689600 |
--------------------------------
Eval num_timesteps=1690000, episode_reward=-96683.69 +/- 39719.18
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48176947  |
|    mean velocity x      | -0.248       |
|    mean velocity y      | 1.03         |
|    mean velocity z      | 3.36         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.67e+04    |
| time/                   |              |
|    total_timesteps      | 1690000      |
| train/                  |              |
|    approx_kl            | 9.226031e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.402        |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+07     |
|    n_updates            | 8250         |
|    policy_gradient_loss | -0.000235    |
|    std                  | 1.55         |
|    value_loss           | 3.02e+07     |
------------------------------------------
Eval num_timesteps=1690500, episode_reward=-99699.09 +/- 41131.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49154124 |
|    mean velocity x | -0.0388     |
|    mean velocity y | 1.5         |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.97e+04   |
| time/              |             |
|    total_timesteps | 1690500     |
------------------------------------
Eval num_timesteps=1691000, episode_reward=-70774.24 +/- 47900.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30501327 |
|    mean velocity x | 0.231       |
|    mean velocity y | 0.774       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.08e+04   |
| time/              |             |
|    total_timesteps | 1691000     |
------------------------------------
Eval num_timesteps=1691500, episode_reward=-57652.33 +/- 36673.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33918396 |
|    mean velocity x | -1.54       |
|    mean velocity y | -0.221      |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.77e+04   |
| time/              |             |
|    total_timesteps | 1691500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 826     |
|    time_elapsed    | 68148   |
|    total_timesteps | 1691648 |
--------------------------------
Eval num_timesteps=1692000, episode_reward=-81767.73 +/- 47424.56
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.337662    |
|    mean velocity x      | -2.88        |
|    mean velocity y      | -2.68        |
|    mean velocity z      | 9.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.18e+04    |
| time/                   |              |
|    total_timesteps      | 1692000      |
| train/                  |              |
|    approx_kl            | 6.647606e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.436        |
|    learning_rate        | 0.001        |
|    loss                 | 3.52e+07     |
|    n_updates            | 8260         |
|    policy_gradient_loss | -0.000227    |
|    std                  | 1.55         |
|    value_loss           | 4.92e+07     |
------------------------------------------
Eval num_timesteps=1692500, episode_reward=-70641.85 +/- 33873.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5208175 |
|    mean velocity x | 0.306      |
|    mean velocity y | 1.9        |
|    mean velocity z | 3.67       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.06e+04  |
| time/              |            |
|    total_timesteps | 1692500    |
-----------------------------------
Eval num_timesteps=1693000, episode_reward=-74687.81 +/- 35452.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3845838 |
|    mean velocity x | -0.221     |
|    mean velocity y | 0.724      |
|    mean velocity z | 4.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.47e+04  |
| time/              |            |
|    total_timesteps | 1693000    |
-----------------------------------
Eval num_timesteps=1693500, episode_reward=-59191.26 +/- 34564.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35603124 |
|    mean velocity x | -0.185      |
|    mean velocity y | 0.618       |
|    mean velocity z | 4.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.92e+04   |
| time/              |             |
|    total_timesteps | 1693500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 827     |
|    time_elapsed    | 68228   |
|    total_timesteps | 1693696 |
--------------------------------
Eval num_timesteps=1694000, episode_reward=-96103.51 +/- 12059.45
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38800555   |
|    mean velocity x      | 0.224         |
|    mean velocity y      | 0.728         |
|    mean velocity z      | 2.61          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.61e+04     |
| time/                   |               |
|    total_timesteps      | 1694000       |
| train/                  |               |
|    approx_kl            | 4.4703513e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.287         |
|    learning_rate        | 0.001         |
|    loss                 | 9.78e+07      |
|    n_updates            | 8270          |
|    policy_gradient_loss | -0.000373     |
|    std                  | 1.55          |
|    value_loss           | 9.01e+07      |
-------------------------------------------
Eval num_timesteps=1694500, episode_reward=-94921.65 +/- 20163.16
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1764231 |
|    mean velocity x | -0.0656    |
|    mean velocity y | 0.309      |
|    mean velocity z | 0.394      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.49e+04  |
| time/              |            |
|    total_timesteps | 1694500    |
-----------------------------------
Eval num_timesteps=1695000, episode_reward=-96890.13 +/- 11705.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5576719 |
|    mean velocity x | -0.439     |
|    mean velocity y | 0.943      |
|    mean velocity z | 4.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.69e+04  |
| time/              |            |
|    total_timesteps | 1695000    |
-----------------------------------
Eval num_timesteps=1695500, episode_reward=-100321.11 +/- 25642.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42132917 |
|    mean velocity x | -0.416      |
|    mean velocity y | 0.836       |
|    mean velocity z | 3.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1695500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 828     |
|    time_elapsed    | 68308   |
|    total_timesteps | 1695744 |
--------------------------------
Eval num_timesteps=1696000, episode_reward=-53856.40 +/- 41757.71
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34617132   |
|    mean velocity x      | -0.911        |
|    mean velocity y      | 0.479         |
|    mean velocity z      | 2.43          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.39e+04     |
| time/                   |               |
|    total_timesteps      | 1696000       |
| train/                  |               |
|    approx_kl            | 2.5822577e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+07      |
|    n_updates            | 8280          |
|    policy_gradient_loss | -0.000296     |
|    std                  | 1.55          |
|    value_loss           | 4.08e+07      |
-------------------------------------------
Eval num_timesteps=1696500, episode_reward=-48456.73 +/- 52388.45
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3720772 |
|    mean velocity x | -0.452     |
|    mean velocity y | -0.0234    |
|    mean velocity z | 3.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.85e+04  |
| time/              |            |
|    total_timesteps | 1696500    |
-----------------------------------
Eval num_timesteps=1697000, episode_reward=-66781.59 +/- 38091.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44800317 |
|    mean velocity x | -0.454      |
|    mean velocity y | 0.342       |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 1697000     |
------------------------------------
Eval num_timesteps=1697500, episode_reward=-83196.53 +/- 31391.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.58297914 |
|    mean velocity x | 0.691       |
|    mean velocity y | 2.06        |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.32e+04   |
| time/              |             |
|    total_timesteps | 1697500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 829     |
|    time_elapsed    | 68389   |
|    total_timesteps | 1697792 |
--------------------------------
Eval num_timesteps=1698000, episode_reward=-72535.47 +/- 25699.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49635577   |
|    mean velocity x      | -0.808        |
|    mean velocity y      | 0.645         |
|    mean velocity z      | 4.14          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.25e+04     |
| time/                   |               |
|    total_timesteps      | 1698000       |
| train/                  |               |
|    approx_kl            | 1.8543506e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.333         |
|    learning_rate        | 0.001         |
|    loss                 | 2.04e+07      |
|    n_updates            | 8290          |
|    policy_gradient_loss | -8.21e-05     |
|    std                  | 1.55          |
|    value_loss           | 5.71e+07      |
-------------------------------------------
Eval num_timesteps=1698500, episode_reward=-76315.77 +/- 54609.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3816745 |
|    mean velocity x | -0.636     |
|    mean velocity y | 0.327      |
|    mean velocity z | 3.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 1698500    |
-----------------------------------
Eval num_timesteps=1699000, episode_reward=-109603.76 +/- 31039.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3639141 |
|    mean velocity x | -0.327     |
|    mean velocity y | 0.51       |
|    mean velocity z | 4.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 1699000    |
-----------------------------------
Eval num_timesteps=1699500, episode_reward=-82175.20 +/- 41008.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4179983 |
|    mean velocity x | -0.2       |
|    mean velocity y | 0.864      |
|    mean velocity z | 3.99       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.22e+04  |
| time/              |            |
|    total_timesteps | 1699500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 830     |
|    time_elapsed    | 68469   |
|    total_timesteps | 1699840 |
--------------------------------
Eval num_timesteps=1700000, episode_reward=-90438.32 +/- 47297.59
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4209281    |
|    mean velocity x      | 0.108         |
|    mean velocity y      | 0.951         |
|    mean velocity z      | 3.55          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.04e+04     |
| time/                   |               |
|    total_timesteps      | 1700000       |
| train/                  |               |
|    approx_kl            | 3.1140225e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+07      |
|    n_updates            | 8300          |
|    policy_gradient_loss | -0.000146     |
|    std                  | 1.55          |
|    value_loss           | 6.76e+07      |
-------------------------------------------
Eval num_timesteps=1700500, episode_reward=-95234.40 +/- 20709.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36872602 |
|    mean velocity x | -0.011      |
|    mean velocity y | 0.97        |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.52e+04   |
| time/              |             |
|    total_timesteps | 1700500     |
------------------------------------
Eval num_timesteps=1701000, episode_reward=-83579.92 +/- 25781.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5114658 |
|    mean velocity x | -0.834     |
|    mean velocity y | 0.247      |
|    mean velocity z | 4.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.36e+04  |
| time/              |            |
|    total_timesteps | 1701000    |
-----------------------------------
Eval num_timesteps=1701500, episode_reward=-89346.00 +/- 39488.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36915067 |
|    mean velocity x | 0.172       |
|    mean velocity y | 0.723       |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1701500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 831     |
|    time_elapsed    | 68549   |
|    total_timesteps | 1701888 |
--------------------------------
Eval num_timesteps=1702000, episode_reward=-54689.46 +/- 50602.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19880083   |
|    mean velocity x      | -0.432        |
|    mean velocity y      | -0.204        |
|    mean velocity z      | 2.92          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.47e+04     |
| time/                   |               |
|    total_timesteps      | 1702000       |
| train/                  |               |
|    approx_kl            | 3.7974096e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.001         |
|    loss                 | 1.51e+07      |
|    n_updates            | 8310          |
|    policy_gradient_loss | -0.000111     |
|    std                  | 1.55          |
|    value_loss           | 5.28e+07      |
-------------------------------------------
Eval num_timesteps=1702500, episode_reward=-96536.17 +/- 25419.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24606681 |
|    mean velocity x | -0.662      |
|    mean velocity y | -0.188      |
|    mean velocity z | 2.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.65e+04   |
| time/              |             |
|    total_timesteps | 1702500     |
------------------------------------
Eval num_timesteps=1703000, episode_reward=-96596.54 +/- 44116.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6038915 |
|    mean velocity x | 0.618      |
|    mean velocity y | 1.79       |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.66e+04  |
| time/              |            |
|    total_timesteps | 1703000    |
-----------------------------------
Eval num_timesteps=1703500, episode_reward=-86060.12 +/- 33405.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5314813 |
|    mean velocity x | -0.648     |
|    mean velocity y | 0.441      |
|    mean velocity z | 4.56       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.61e+04  |
| time/              |            |
|    total_timesteps | 1703500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 832     |
|    time_elapsed    | 68630   |
|    total_timesteps | 1703936 |
--------------------------------
Eval num_timesteps=1704000, episode_reward=-71578.67 +/- 42776.71
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5335283   |
|    mean velocity x      | -0.286       |
|    mean velocity y      | 1.32         |
|    mean velocity z      | 3.99         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.16e+04    |
| time/                   |              |
|    total_timesteps      | 1704000      |
| train/                  |              |
|    approx_kl            | 9.839714e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.35         |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+07     |
|    n_updates            | 8320         |
|    policy_gradient_loss | -0.000225    |
|    std                  | 1.55         |
|    value_loss           | 4.55e+07     |
------------------------------------------
Eval num_timesteps=1704500, episode_reward=-88446.76 +/- 32315.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38507086 |
|    mean velocity x | -0.025      |
|    mean velocity y | 1.05        |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 1704500     |
------------------------------------
Eval num_timesteps=1705000, episode_reward=-97495.50 +/- 61493.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34565625 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.761       |
|    mean velocity z | 2.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.75e+04   |
| time/              |             |
|    total_timesteps | 1705000     |
------------------------------------
Eval num_timesteps=1705500, episode_reward=-93788.72 +/- 45840.83
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4313509 |
|    mean velocity x | -0.444     |
|    mean velocity y | 0.842      |
|    mean velocity z | 4.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.38e+04  |
| time/              |            |
|    total_timesteps | 1705500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 833     |
|    time_elapsed    | 68710   |
|    total_timesteps | 1705984 |
--------------------------------
Eval num_timesteps=1706000, episode_reward=-74575.64 +/- 38921.65
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31247535  |
|    mean velocity x      | -0.163       |
|    mean velocity y      | 0.69         |
|    mean velocity z      | 0.414        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.46e+04    |
| time/                   |              |
|    total_timesteps      | 1706000      |
| train/                  |              |
|    approx_kl            | 7.546274e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+07      |
|    n_updates            | 8330         |
|    policy_gradient_loss | -0.000216    |
|    std                  | 1.55         |
|    value_loss           | 5.11e+07     |
------------------------------------------
Eval num_timesteps=1706500, episode_reward=-74117.66 +/- 32445.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32165918 |
|    mean velocity x | -0.982      |
|    mean velocity y | -0.177      |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.41e+04   |
| time/              |             |
|    total_timesteps | 1706500     |
------------------------------------
Eval num_timesteps=1707000, episode_reward=-97745.14 +/- 39110.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4181978 |
|    mean velocity x | -0.25      |
|    mean velocity y | 1.19       |
|    mean velocity z | 4.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.77e+04  |
| time/              |            |
|    total_timesteps | 1707000    |
-----------------------------------
Eval num_timesteps=1707500, episode_reward=-89368.85 +/- 36438.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47966272 |
|    mean velocity x | -0.612      |
|    mean velocity y | 0.702       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.94e+04   |
| time/              |             |
|    total_timesteps | 1707500     |
------------------------------------
Eval num_timesteps=1708000, episode_reward=-92724.09 +/- 18903.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40210998 |
|    mean velocity x | -0.141      |
|    mean velocity y | 1.28        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 1708000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 834     |
|    time_elapsed    | 68809   |
|    total_timesteps | 1708032 |
--------------------------------
Eval num_timesteps=1708500, episode_reward=-93339.37 +/- 49426.65
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5238673   |
|    mean velocity x      | -0.223       |
|    mean velocity y      | 1.17         |
|    mean velocity z      | 4.2          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.33e+04    |
| time/                   |              |
|    total_timesteps      | 1708500      |
| train/                  |              |
|    approx_kl            | 8.135481e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+07        |
|    n_updates            | 8340         |
|    policy_gradient_loss | -0.000289    |
|    std                  | 1.55         |
|    value_loss           | 1.1e+08      |
------------------------------------------
Eval num_timesteps=1709000, episode_reward=-79685.93 +/- 47835.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.62050104 |
|    mean velocity x | 0.97        |
|    mean velocity y | 2.03        |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.97e+04   |
| time/              |             |
|    total_timesteps | 1709000     |
------------------------------------
Eval num_timesteps=1709500, episode_reward=-109288.33 +/- 23906.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3576403 |
|    mean velocity x | -0.833     |
|    mean velocity y | 0.45       |
|    mean velocity z | 2.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 1709500    |
-----------------------------------
Eval num_timesteps=1710000, episode_reward=-95631.66 +/- 43001.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5263125 |
|    mean velocity x | 0.0106     |
|    mean velocity y | 1.17       |
|    mean velocity z | 2.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.56e+04  |
| time/              |            |
|    total_timesteps | 1710000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 835     |
|    time_elapsed    | 68890   |
|    total_timesteps | 1710080 |
--------------------------------
Eval num_timesteps=1710500, episode_reward=-89176.52 +/- 48928.11
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4131343    |
|    mean velocity x      | -0.324        |
|    mean velocity y      | 0.779         |
|    mean velocity z      | 4.39          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.92e+04     |
| time/                   |               |
|    total_timesteps      | 1710500       |
| train/                  |               |
|    approx_kl            | 4.3100736e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.347         |
|    learning_rate        | 0.001         |
|    loss                 | 8.87e+06      |
|    n_updates            | 8350          |
|    policy_gradient_loss | -0.000111     |
|    std                  | 1.55          |
|    value_loss           | 3.75e+07      |
-------------------------------------------
Eval num_timesteps=1711000, episode_reward=-85759.52 +/- 21151.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47290212 |
|    mean velocity x | -0.401      |
|    mean velocity y | 0.641       |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.58e+04   |
| time/              |             |
|    total_timesteps | 1711000     |
------------------------------------
Eval num_timesteps=1711500, episode_reward=-79369.44 +/- 27910.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.89316505 |
|    mean velocity x | 0.453       |
|    mean velocity y | 3.09        |
|    mean velocity z | 7.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.94e+04   |
| time/              |             |
|    total_timesteps | 1711500     |
------------------------------------
Eval num_timesteps=1712000, episode_reward=-61692.94 +/- 23027.41
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3725113 |
|    mean velocity x | -0.0658    |
|    mean velocity y | 1.23       |
|    mean velocity z | 4.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.17e+04  |
| time/              |            |
|    total_timesteps | 1712000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 836     |
|    time_elapsed    | 68970   |
|    total_timesteps | 1712128 |
--------------------------------
Eval num_timesteps=1712500, episode_reward=-30799.72 +/- 37861.15
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4518089    |
|    mean velocity x      | -1.2          |
|    mean velocity y      | 0.263         |
|    mean velocity z      | 4.14          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.08e+04     |
| time/                   |               |
|    total_timesteps      | 1712500       |
| train/                  |               |
|    approx_kl            | 1.9560597e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.366         |
|    learning_rate        | 0.001         |
|    loss                 | 2.9e+06       |
|    n_updates            | 8360          |
|    policy_gradient_loss | -0.000291     |
|    std                  | 1.55          |
|    value_loss           | 6.48e+07      |
-------------------------------------------
Eval num_timesteps=1713000, episode_reward=-96462.10 +/- 36702.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3993725 |
|    mean velocity x | -0.313     |
|    mean velocity y | 0.735      |
|    mean velocity z | 4.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.65e+04  |
| time/              |            |
|    total_timesteps | 1713000    |
-----------------------------------
Eval num_timesteps=1713500, episode_reward=-86216.27 +/- 35726.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51726806 |
|    mean velocity x | -0.484      |
|    mean velocity y | 0.652       |
|    mean velocity z | 4.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 1713500     |
------------------------------------
Eval num_timesteps=1714000, episode_reward=-57934.00 +/- 23974.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43189397 |
|    mean velocity x | -1.1        |
|    mean velocity y | -0.138      |
|    mean velocity z | 4.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.79e+04   |
| time/              |             |
|    total_timesteps | 1714000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 837     |
|    time_elapsed    | 69051   |
|    total_timesteps | 1714176 |
--------------------------------
Eval num_timesteps=1714500, episode_reward=-82208.40 +/- 45080.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45079744   |
|    mean velocity x      | -0.0904       |
|    mean velocity y      | 1.59          |
|    mean velocity z      | 4.16          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.22e+04     |
| time/                   |               |
|    total_timesteps      | 1714500       |
| train/                  |               |
|    approx_kl            | 1.4923746e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.265         |
|    learning_rate        | 0.001         |
|    loss                 | 5.65e+07      |
|    n_updates            | 8370          |
|    policy_gradient_loss | -0.000335     |
|    std                  | 1.55          |
|    value_loss           | 9.79e+07      |
-------------------------------------------
Eval num_timesteps=1715000, episode_reward=-68933.34 +/- 43190.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41049486 |
|    mean velocity x | 0.53        |
|    mean velocity y | 1.67        |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.89e+04   |
| time/              |             |
|    total_timesteps | 1715000     |
------------------------------------
Eval num_timesteps=1715500, episode_reward=-94124.81 +/- 25602.38
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4653731 |
|    mean velocity x | -1.18      |
|    mean velocity y | -0.528     |
|    mean velocity z | 7.66       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.41e+04  |
| time/              |            |
|    total_timesteps | 1715500    |
-----------------------------------
Eval num_timesteps=1716000, episode_reward=-85025.23 +/- 40825.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35649198 |
|    mean velocity x | 0.136       |
|    mean velocity y | 0.863       |
|    mean velocity z | 3.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 1716000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 838     |
|    time_elapsed    | 69131   |
|    total_timesteps | 1716224 |
--------------------------------
Eval num_timesteps=1716500, episode_reward=-92134.36 +/- 47819.05
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4244018    |
|    mean velocity x      | -1.27         |
|    mean velocity y      | -0.523        |
|    mean velocity z      | 4.04          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.21e+04     |
| time/                   |               |
|    total_timesteps      | 1716500       |
| train/                  |               |
|    approx_kl            | 7.1756367e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.422         |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+07      |
|    n_updates            | 8380          |
|    policy_gradient_loss | -0.000193     |
|    std                  | 1.55          |
|    value_loss           | 5.66e+07      |
-------------------------------------------
Eval num_timesteps=1717000, episode_reward=-67302.40 +/- 44023.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3066035 |
|    mean velocity x | -0.576     |
|    mean velocity y | 0.375      |
|    mean velocity z | 1.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.73e+04  |
| time/              |            |
|    total_timesteps | 1717000    |
-----------------------------------
Eval num_timesteps=1717500, episode_reward=-58700.45 +/- 28818.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33598277 |
|    mean velocity x | -0.099      |
|    mean velocity y | 0.547       |
|    mean velocity z | 0.797       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.87e+04   |
| time/              |             |
|    total_timesteps | 1717500     |
------------------------------------
Eval num_timesteps=1718000, episode_reward=-102825.73 +/- 30649.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45350972 |
|    mean velocity x | -0.71       |
|    mean velocity y | 0.267       |
|    mean velocity z | 3.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1718000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 839     |
|    time_elapsed    | 69211   |
|    total_timesteps | 1718272 |
--------------------------------
Eval num_timesteps=1718500, episode_reward=-74614.21 +/- 23100.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31843388   |
|    mean velocity x      | 0.202         |
|    mean velocity y      | 0.747         |
|    mean velocity z      | 2.24          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.46e+04     |
| time/                   |               |
|    total_timesteps      | 1718500       |
| train/                  |               |
|    approx_kl            | 2.1129468e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.001         |
|    loss                 | 4.2e+05       |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000237     |
|    std                  | 1.55          |
|    value_loss           | 1.97e+07      |
-------------------------------------------
Eval num_timesteps=1719000, episode_reward=-77088.02 +/- 38028.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24590363 |
|    mean velocity x | -0.821      |
|    mean velocity y | -0.445      |
|    mean velocity z | 3.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 1719000     |
------------------------------------
Eval num_timesteps=1719500, episode_reward=-78760.87 +/- 20271.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21157901 |
|    mean velocity x | -0.0426     |
|    mean velocity y | 0.346       |
|    mean velocity z | 0.506       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.88e+04   |
| time/              |             |
|    total_timesteps | 1719500     |
------------------------------------
Eval num_timesteps=1720000, episode_reward=-93060.99 +/- 26600.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40101466 |
|    mean velocity x | -0.168      |
|    mean velocity y | 0.664       |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.31e+04   |
| time/              |             |
|    total_timesteps | 1720000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 840     |
|    time_elapsed    | 69292   |
|    total_timesteps | 1720320 |
--------------------------------
Eval num_timesteps=1720500, episode_reward=-90477.60 +/- 39257.56
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35899103   |
|    mean velocity x      | -0.614        |
|    mean velocity y      | 0.921         |
|    mean velocity z      | 0.762         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.05e+04     |
| time/                   |               |
|    total_timesteps      | 1720500       |
| train/                  |               |
|    approx_kl            | 5.2788237e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.001         |
|    loss                 | 2.01e+07      |
|    n_updates            | 8400          |
|    policy_gradient_loss | -0.000184     |
|    std                  | 1.55          |
|    value_loss           | 3.18e+07      |
-------------------------------------------
Eval num_timesteps=1721000, episode_reward=-101959.02 +/- 20905.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43188757 |
|    mean velocity x | 0.08        |
|    mean velocity y | 1.26        |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1721000     |
------------------------------------
Eval num_timesteps=1721500, episode_reward=-84045.39 +/- 14594.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42381385 |
|    mean velocity x | -0.306      |
|    mean velocity y | 0.843       |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.4e+04    |
| time/              |             |
|    total_timesteps | 1721500     |
------------------------------------
Eval num_timesteps=1722000, episode_reward=-67457.14 +/- 9994.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38308987 |
|    mean velocity x | -0.237      |
|    mean velocity y | 0.686       |
|    mean velocity z | 4.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.75e+04   |
| time/              |             |
|    total_timesteps | 1722000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 841     |
|    time_elapsed    | 69372   |
|    total_timesteps | 1722368 |
--------------------------------
Eval num_timesteps=1722500, episode_reward=-75359.19 +/- 29889.55
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41090986  |
|    mean velocity x      | -0.482       |
|    mean velocity y      | 0.112        |
|    mean velocity z      | 4.04         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.54e+04    |
| time/                   |              |
|    total_timesteps      | 1722500      |
| train/                  |              |
|    approx_kl            | 6.310991e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 2.31e+07     |
|    n_updates            | 8410         |
|    policy_gradient_loss | -0.00027     |
|    std                  | 1.55         |
|    value_loss           | 8.57e+07     |
------------------------------------------
Eval num_timesteps=1723000, episode_reward=-86896.85 +/- 24901.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09363008 |
|    mean velocity x | -1.06       |
|    mean velocity y | -1.05       |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 1723000     |
------------------------------------
Eval num_timesteps=1723500, episode_reward=-99546.69 +/- 28188.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40765074 |
|    mean velocity x | -0.412      |
|    mean velocity y | 0.894       |
|    mean velocity z | 1.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 1723500     |
------------------------------------
Eval num_timesteps=1724000, episode_reward=-83961.05 +/- 23277.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35097107 |
|    mean velocity x | 0.0235      |
|    mean velocity y | 0.8         |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.4e+04    |
| time/              |             |
|    total_timesteps | 1724000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 842     |
|    time_elapsed    | 69453   |
|    total_timesteps | 1724416 |
--------------------------------
Eval num_timesteps=1724500, episode_reward=-31605.50 +/- 30660.98
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35935876   |
|    mean velocity x      | -1.23         |
|    mean velocity y      | -0.038        |
|    mean velocity z      | 3.16          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.16e+04     |
| time/                   |               |
|    total_timesteps      | 1724500       |
| train/                  |               |
|    approx_kl            | 2.4740875e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.4           |
|    learning_rate        | 0.001         |
|    loss                 | 2.2e+07       |
|    n_updates            | 8420          |
|    policy_gradient_loss | -0.000106     |
|    std                  | 1.55          |
|    value_loss           | 3.16e+07      |
-------------------------------------------
Eval num_timesteps=1725000, episode_reward=-77450.86 +/- 60387.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13744237 |
|    mean velocity x | -0.112      |
|    mean velocity y | 0.234       |
|    mean velocity z | 0.458       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.75e+04   |
| time/              |             |
|    total_timesteps | 1725000     |
------------------------------------
Eval num_timesteps=1725500, episode_reward=-60781.74 +/- 28503.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40271184 |
|    mean velocity x | -0.234      |
|    mean velocity y | 0.89        |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.08e+04   |
| time/              |             |
|    total_timesteps | 1725500     |
------------------------------------
Eval num_timesteps=1726000, episode_reward=-35254.38 +/- 36010.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38588354 |
|    mean velocity x | -2.01       |
|    mean velocity y | -1.71       |
|    mean velocity z | 6.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.53e+04   |
| time/              |             |
|    total_timesteps | 1726000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 843     |
|    time_elapsed    | 69533   |
|    total_timesteps | 1726464 |
--------------------------------
Eval num_timesteps=1726500, episode_reward=-69362.39 +/- 48397.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.371843     |
|    mean velocity x      | -0.821        |
|    mean velocity y      | -0.107        |
|    mean velocity z      | 3.37          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.94e+04     |
| time/                   |               |
|    total_timesteps      | 1726500       |
| train/                  |               |
|    approx_kl            | 6.4785127e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.379         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+06      |
|    n_updates            | 8430          |
|    policy_gradient_loss | -0.00018      |
|    std                  | 1.55          |
|    value_loss           | 5.21e+07      |
-------------------------------------------
Eval num_timesteps=1727000, episode_reward=-86923.79 +/- 38363.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44642377 |
|    mean velocity x | -0.148      |
|    mean velocity y | 0.917       |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 1727000     |
------------------------------------
Eval num_timesteps=1727500, episode_reward=-80181.23 +/- 41429.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2891565 |
|    mean velocity x | -0.207     |
|    mean velocity y | 0.731      |
|    mean velocity z | 0.496      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.02e+04  |
| time/              |            |
|    total_timesteps | 1727500    |
-----------------------------------
Eval num_timesteps=1728000, episode_reward=-101176.90 +/- 33321.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44641566 |
|    mean velocity x | -0.283      |
|    mean velocity y | 0.771       |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1728000     |
------------------------------------
Eval num_timesteps=1728500, episode_reward=-115541.28 +/- 53843.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34044617 |
|    mean velocity x | -0.103      |
|    mean velocity y | 0.704       |
|    mean velocity z | 0.954       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 1728500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 844     |
|    time_elapsed    | 69632   |
|    total_timesteps | 1728512 |
--------------------------------
Eval num_timesteps=1729000, episode_reward=-87329.83 +/- 41567.57
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.51905113  |
|    mean velocity x      | -0.345       |
|    mean velocity y      | 0.75         |
|    mean velocity z      | 3.57         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.73e+04    |
| time/                   |              |
|    total_timesteps      | 1729000      |
| train/                  |              |
|    approx_kl            | 5.492504e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+07     |
|    n_updates            | 8440         |
|    policy_gradient_loss | -0.000224    |
|    std                  | 1.55         |
|    value_loss           | 5.51e+07     |
------------------------------------------
Eval num_timesteps=1729500, episode_reward=-68405.00 +/- 46847.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43223932 |
|    mean velocity x | -0.199      |
|    mean velocity y | 1.18        |
|    mean velocity z | 4.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 1729500     |
------------------------------------
Eval num_timesteps=1730000, episode_reward=-73127.15 +/- 15263.37
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3489182 |
|    mean velocity x | -1.36      |
|    mean velocity y | -0.812     |
|    mean velocity z | 4.16       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 1730000    |
-----------------------------------
Eval num_timesteps=1730500, episode_reward=-84818.57 +/- 11578.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41358453 |
|    mean velocity x | -0.109      |
|    mean velocity y | 1.53        |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.48e+04   |
| time/              |             |
|    total_timesteps | 1730500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 845     |
|    time_elapsed    | 69713   |
|    total_timesteps | 1730560 |
--------------------------------
Eval num_timesteps=1731000, episode_reward=-43746.90 +/- 51010.44
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31183252   |
|    mean velocity x      | -0.187        |
|    mean velocity y      | 0.46          |
|    mean velocity z      | 2.22          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.37e+04     |
| time/                   |               |
|    total_timesteps      | 1731000       |
| train/                  |               |
|    approx_kl            | 3.5022676e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.326         |
|    learning_rate        | 0.001         |
|    loss                 | 2.23e+07      |
|    n_updates            | 8450          |
|    policy_gradient_loss | -0.000269     |
|    std                  | 1.55          |
|    value_loss           | 6.32e+07      |
-------------------------------------------
Eval num_timesteps=1731500, episode_reward=-102137.20 +/- 37457.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2807364 |
|    mean velocity x | -0.207     |
|    mean velocity y | 0.434      |
|    mean velocity z | 1.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 1731500    |
-----------------------------------
Eval num_timesteps=1732000, episode_reward=-42159.78 +/- 28473.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5332978 |
|    mean velocity x | -0.605     |
|    mean velocity y | 0.977      |
|    mean velocity z | 4.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.22e+04  |
| time/              |            |
|    total_timesteps | 1732000    |
-----------------------------------
Eval num_timesteps=1732500, episode_reward=-75683.02 +/- 38081.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41153437 |
|    mean velocity x | 0.00733     |
|    mean velocity y | 0.961       |
|    mean velocity z | 2.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 1732500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 846     |
|    time_elapsed    | 69793   |
|    total_timesteps | 1732608 |
--------------------------------
Eval num_timesteps=1733000, episode_reward=-50065.37 +/- 30740.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32694504   |
|    mean velocity x      | 0.0732        |
|    mean velocity y      | 0.568         |
|    mean velocity z      | 3.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.01e+04     |
| time/                   |               |
|    total_timesteps      | 1733000       |
| train/                  |               |
|    approx_kl            | 1.6707461e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.001         |
|    loss                 | 4.89e+07      |
|    n_updates            | 8460          |
|    policy_gradient_loss | -0.000367     |
|    std                  | 1.55          |
|    value_loss           | 4.3e+07       |
-------------------------------------------
Eval num_timesteps=1733500, episode_reward=-62055.73 +/- 44388.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4362929 |
|    mean velocity x | -0.304     |
|    mean velocity y | 0.96       |
|    mean velocity z | 4.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.21e+04  |
| time/              |            |
|    total_timesteps | 1733500    |
-----------------------------------
Eval num_timesteps=1734000, episode_reward=-69751.20 +/- 22666.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5362184 |
|    mean velocity x | -0.191     |
|    mean velocity y | 1.35       |
|    mean velocity z | 3.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.98e+04  |
| time/              |            |
|    total_timesteps | 1734000    |
-----------------------------------
Eval num_timesteps=1734500, episode_reward=-90131.45 +/- 45453.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49479252 |
|    mean velocity x | 0.525       |
|    mean velocity y | 1.67        |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.01e+04   |
| time/              |             |
|    total_timesteps | 1734500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 847     |
|    time_elapsed    | 69874   |
|    total_timesteps | 1734656 |
--------------------------------
Eval num_timesteps=1735000, episode_reward=-84801.68 +/- 13270.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47525623  |
|    mean velocity x      | -0.333       |
|    mean velocity y      | 0.881        |
|    mean velocity z      | 0.895        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.48e+04    |
| time/                   |              |
|    total_timesteps      | 1735000      |
| train/                  |              |
|    approx_kl            | 3.486115e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.001        |
|    loss                 | 8.57e+06     |
|    n_updates            | 8470         |
|    policy_gradient_loss | -0.000124    |
|    std                  | 1.55         |
|    value_loss           | 6.1e+07      |
------------------------------------------
Eval num_timesteps=1735500, episode_reward=-87413.27 +/- 49718.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44102523 |
|    mean velocity x | -0.394      |
|    mean velocity y | 0.464       |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.74e+04   |
| time/              |             |
|    total_timesteps | 1735500     |
------------------------------------
Eval num_timesteps=1736000, episode_reward=-118523.57 +/- 23254.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46996182 |
|    mean velocity x | -1.41       |
|    mean velocity y | 0.332       |
|    mean velocity z | 4.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 1736000     |
------------------------------------
Eval num_timesteps=1736500, episode_reward=-41598.10 +/- 39073.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38238055 |
|    mean velocity x | 0.289       |
|    mean velocity y | 0.985       |
|    mean velocity z | 3.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.16e+04   |
| time/              |             |
|    total_timesteps | 1736500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 848     |
|    time_elapsed    | 69954   |
|    total_timesteps | 1736704 |
--------------------------------
Eval num_timesteps=1737000, episode_reward=-90762.93 +/- 36116.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44054654   |
|    mean velocity x      | 0.303         |
|    mean velocity y      | 1.4           |
|    mean velocity z      | 3.87          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.08e+04     |
| time/                   |               |
|    total_timesteps      | 1737000       |
| train/                  |               |
|    approx_kl            | 2.9115414e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.369         |
|    learning_rate        | 0.001         |
|    loss                 | 3.44e+07      |
|    n_updates            | 8480          |
|    policy_gradient_loss | -0.000444     |
|    std                  | 1.55          |
|    value_loss           | 5.73e+07      |
-------------------------------------------
Eval num_timesteps=1737500, episode_reward=-82925.87 +/- 34222.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33020636 |
|    mean velocity x | -0.493      |
|    mean velocity y | -0.185      |
|    mean velocity z | 3.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 1737500     |
------------------------------------
Eval num_timesteps=1738000, episode_reward=-81834.23 +/- 46486.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55857116 |
|    mean velocity x | -0.501      |
|    mean velocity y | 0.911       |
|    mean velocity z | 4.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.18e+04   |
| time/              |             |
|    total_timesteps | 1738000     |
------------------------------------
Eval num_timesteps=1738500, episode_reward=-72816.25 +/- 21151.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2858434 |
|    mean velocity x | -0.33      |
|    mean velocity y | 0.519      |
|    mean velocity z | 0.297      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.28e+04  |
| time/              |            |
|    total_timesteps | 1738500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 849     |
|    time_elapsed    | 70034   |
|    total_timesteps | 1738752 |
--------------------------------
Eval num_timesteps=1739000, episode_reward=-93856.86 +/- 34885.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44937736   |
|    mean velocity x      | 0.554         |
|    mean velocity y      | 1.46          |
|    mean velocity z      | 3.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.39e+04     |
| time/                   |               |
|    total_timesteps      | 1739000       |
| train/                  |               |
|    approx_kl            | 6.4736523e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.305         |
|    learning_rate        | 0.001         |
|    loss                 | 6.97e+06      |
|    n_updates            | 8490          |
|    policy_gradient_loss | -0.000159     |
|    std                  | 1.55          |
|    value_loss           | 5e+07         |
-------------------------------------------
Eval num_timesteps=1739500, episode_reward=-101283.33 +/- 30781.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38145685 |
|    mean velocity x | 0.248       |
|    mean velocity y | 1.63        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1739500     |
------------------------------------
Eval num_timesteps=1740000, episode_reward=-82382.87 +/- 41453.90
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5097017 |
|    mean velocity x | 0.105      |
|    mean velocity y | 1.19       |
|    mean velocity z | 3.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 1740000    |
-----------------------------------
Eval num_timesteps=1740500, episode_reward=-97786.84 +/- 21604.67
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3503941 |
|    mean velocity x | -0.451     |
|    mean velocity y | 0.0836     |
|    mean velocity z | 3.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.78e+04  |
| time/              |            |
|    total_timesteps | 1740500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 850     |
|    time_elapsed    | 70115   |
|    total_timesteps | 1740800 |
--------------------------------
Eval num_timesteps=1741000, episode_reward=-89294.07 +/- 24675.14
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36240974  |
|    mean velocity x      | -0.328       |
|    mean velocity y      | 0.724        |
|    mean velocity z      | 2.98         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.93e+04    |
| time/                   |              |
|    total_timesteps      | 1741000      |
| train/                  |              |
|    approx_kl            | 1.302184e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.355        |
|    learning_rate        | 0.001        |
|    loss                 | 3.66e+07     |
|    n_updates            | 8500         |
|    policy_gradient_loss | -0.000394    |
|    std                  | 1.55         |
|    value_loss           | 4.25e+07     |
------------------------------------------
Eval num_timesteps=1741500, episode_reward=-80827.88 +/- 15534.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33331048 |
|    mean velocity x | -0.441      |
|    mean velocity y | 0.521       |
|    mean velocity z | 0.833       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.08e+04   |
| time/              |             |
|    total_timesteps | 1741500     |
------------------------------------
Eval num_timesteps=1742000, episode_reward=-14482.40 +/- 14470.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28750786 |
|    mean velocity x | -2.87       |
|    mean velocity y | -2.26       |
|    mean velocity z | 8.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.45e+04   |
| time/              |             |
|    total_timesteps | 1742000     |
------------------------------------
New best mean reward!
Eval num_timesteps=1742500, episode_reward=-75817.05 +/- 43208.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38215357 |
|    mean velocity x | -0.564      |
|    mean velocity y | 0.483       |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 1742500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 851     |
|    time_elapsed    | 70195   |
|    total_timesteps | 1742848 |
--------------------------------
Eval num_timesteps=1743000, episode_reward=-83514.84 +/- 40914.43
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5476803   |
|    mean velocity x      | -0.394       |
|    mean velocity y      | 0.866        |
|    mean velocity z      | 4.89         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.35e+04    |
| time/                   |              |
|    total_timesteps      | 1743000      |
| train/                  |              |
|    approx_kl            | 4.481763e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.001        |
|    loss                 | 3.02e+07     |
|    n_updates            | 8510         |
|    policy_gradient_loss | -0.000453    |
|    std                  | 1.55         |
|    value_loss           | 5.99e+07     |
------------------------------------------
Eval num_timesteps=1743500, episode_reward=-90607.51 +/- 40910.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26831406 |
|    mean velocity x | -0.447      |
|    mean velocity y | -0.152      |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.06e+04   |
| time/              |             |
|    total_timesteps | 1743500     |
------------------------------------
Eval num_timesteps=1744000, episode_reward=-64716.93 +/- 42988.23
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5042721 |
|    mean velocity x | -0.537     |
|    mean velocity y | 0.739      |
|    mean velocity z | 4.48       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.47e+04  |
| time/              |            |
|    total_timesteps | 1744000    |
-----------------------------------
Eval num_timesteps=1744500, episode_reward=-58629.75 +/- 38639.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47084492 |
|    mean velocity x | -0.16       |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.86e+04   |
| time/              |             |
|    total_timesteps | 1744500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 852     |
|    time_elapsed    | 70275   |
|    total_timesteps | 1744896 |
--------------------------------
Eval num_timesteps=1745000, episode_reward=-104177.70 +/- 65905.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44180852   |
|    mean velocity x      | -0.373        |
|    mean velocity y      | 0.95          |
|    mean velocity z      | 4.25          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.04e+05     |
| time/                   |               |
|    total_timesteps      | 1745000       |
| train/                  |               |
|    approx_kl            | 8.3525665e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 2.98e+07      |
|    n_updates            | 8520          |
|    policy_gradient_loss | -0.000154     |
|    std                  | 1.55          |
|    value_loss           | 8.36e+07      |
-------------------------------------------
Eval num_timesteps=1745500, episode_reward=-84216.12 +/- 27522.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46939182 |
|    mean velocity x | -1.35       |
|    mean velocity y | 0.177       |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 1745500     |
------------------------------------
Eval num_timesteps=1746000, episode_reward=-86528.24 +/- 27843.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48549038 |
|    mean velocity x | 0.526       |
|    mean velocity y | 1.71        |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.65e+04   |
| time/              |             |
|    total_timesteps | 1746000     |
------------------------------------
Eval num_timesteps=1746500, episode_reward=-96958.89 +/- 48414.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37913644 |
|    mean velocity x | -0.0476     |
|    mean velocity y | 1.24        |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.7e+04    |
| time/              |             |
|    total_timesteps | 1746500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 853     |
|    time_elapsed    | 70356   |
|    total_timesteps | 1746944 |
--------------------------------
Eval num_timesteps=1747000, episode_reward=-108312.59 +/- 15038.33
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3386939   |
|    mean velocity x      | -0.309       |
|    mean velocity y      | 0.734        |
|    mean velocity z      | 1.38         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.08e+05    |
| time/                   |              |
|    total_timesteps      | 1747000      |
| train/                  |              |
|    approx_kl            | 7.727096e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.001        |
|    loss                 | 5.71e+06     |
|    n_updates            | 8530         |
|    policy_gradient_loss | -0.000196    |
|    std                  | 1.55         |
|    value_loss           | 4.23e+07     |
------------------------------------------
Eval num_timesteps=1747500, episode_reward=-87704.70 +/- 19122.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39746484 |
|    mean velocity x | -0.371      |
|    mean velocity y | 0.548       |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 1747500     |
------------------------------------
Eval num_timesteps=1748000, episode_reward=-75710.35 +/- 36719.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25748625 |
|    mean velocity x | -1.28       |
|    mean velocity y | -0.526      |
|    mean velocity z | 3.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 1748000     |
------------------------------------
Eval num_timesteps=1748500, episode_reward=-109222.42 +/- 22496.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43140492 |
|    mean velocity x | -1.3        |
|    mean velocity y | 0.11        |
|    mean velocity z | 3.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1748500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 854     |
|    time_elapsed    | 70436   |
|    total_timesteps | 1748992 |
--------------------------------
Eval num_timesteps=1749000, episode_reward=-95266.65 +/- 38510.40
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.45752174  |
|    mean velocity x      | -0.135       |
|    mean velocity y      | 1.29         |
|    mean velocity z      | 4.32         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.53e+04    |
| time/                   |              |
|    total_timesteps      | 1749000      |
| train/                  |              |
|    approx_kl            | 6.210874e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.001        |
|    loss                 | 9.04e+06     |
|    n_updates            | 8540         |
|    policy_gradient_loss | -0.000234    |
|    std                  | 1.55         |
|    value_loss           | 5.76e+07     |
------------------------------------------
Eval num_timesteps=1749500, episode_reward=-55682.57 +/- 41141.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38450286 |
|    mean velocity x | -1.04       |
|    mean velocity y | 0.209       |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.57e+04   |
| time/              |             |
|    total_timesteps | 1749500     |
------------------------------------
Eval num_timesteps=1750000, episode_reward=-88108.73 +/- 47040.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34818402 |
|    mean velocity x | -0.178      |
|    mean velocity y | 0.551       |
|    mean velocity z | 4.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.81e+04   |
| time/              |             |
|    total_timesteps | 1750000     |
------------------------------------
Eval num_timesteps=1750500, episode_reward=-59829.82 +/- 42772.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42566234 |
|    mean velocity x | -0.88       |
|    mean velocity y | 0.362       |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.98e+04   |
| time/              |             |
|    total_timesteps | 1750500     |
------------------------------------
Eval num_timesteps=1751000, episode_reward=-110531.74 +/- 23314.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34038982 |
|    mean velocity x | -0.382      |
|    mean velocity y | 0.275       |
|    mean velocity z | 2.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1751000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 855     |
|    time_elapsed    | 70536   |
|    total_timesteps | 1751040 |
--------------------------------
Eval num_timesteps=1751500, episode_reward=-93272.58 +/- 38428.66
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48854893   |
|    mean velocity x      | -0.368        |
|    mean velocity y      | 0.531         |
|    mean velocity z      | 4.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.33e+04     |
| time/                   |               |
|    total_timesteps      | 1751500       |
| train/                  |               |
|    approx_kl            | 2.4989713e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.001         |
|    loss                 | 7.39e+07      |
|    n_updates            | 8550          |
|    policy_gradient_loss | -0.000169     |
|    std                  | 1.55          |
|    value_loss           | 9.46e+07      |
-------------------------------------------
Eval num_timesteps=1752000, episode_reward=-89915.72 +/- 43269.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29827708 |
|    mean velocity x | -0.227      |
|    mean velocity y | 0.615       |
|    mean velocity z | 0.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.99e+04   |
| time/              |             |
|    total_timesteps | 1752000     |
------------------------------------
Eval num_timesteps=1752500, episode_reward=-95189.41 +/- 22430.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32063606 |
|    mean velocity x | -1.26       |
|    mean velocity y | -0.0691     |
|    mean velocity z | 2.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.52e+04   |
| time/              |             |
|    total_timesteps | 1752500     |
------------------------------------
Eval num_timesteps=1753000, episode_reward=-76002.62 +/- 38672.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40641448 |
|    mean velocity x | -0.296      |
|    mean velocity y | 0.771       |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.6e+04    |
| time/              |             |
|    total_timesteps | 1753000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 856     |
|    time_elapsed    | 70616   |
|    total_timesteps | 1753088 |
--------------------------------
Eval num_timesteps=1753500, episode_reward=-73989.01 +/- 37871.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5418781    |
|    mean velocity x      | -0.665        |
|    mean velocity y      | 0.73          |
|    mean velocity z      | 4.07          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.4e+04      |
| time/                   |               |
|    total_timesteps      | 1753500       |
| train/                  |               |
|    approx_kl            | 2.7365459e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.001         |
|    loss                 | 5.88e+06      |
|    n_updates            | 8560          |
|    policy_gradient_loss | -0.000115     |
|    std                  | 1.55          |
|    value_loss           | 3.96e+07      |
-------------------------------------------
Eval num_timesteps=1754000, episode_reward=-113465.69 +/- 16289.06
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2354866 |
|    mean velocity x | -0.0827    |
|    mean velocity y | 0.314      |
|    mean velocity z | 0.832      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.13e+05  |
| time/              |            |
|    total_timesteps | 1754000    |
-----------------------------------
Eval num_timesteps=1754500, episode_reward=-98377.83 +/- 43052.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46748054 |
|    mean velocity x | -0.546      |
|    mean velocity y | 0.249       |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.84e+04   |
| time/              |             |
|    total_timesteps | 1754500     |
------------------------------------
Eval num_timesteps=1755000, episode_reward=-82039.32 +/- 46055.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46730506 |
|    mean velocity x | 0.096       |
|    mean velocity y | 1.52        |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 1755000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 857     |
|    time_elapsed    | 70697   |
|    total_timesteps | 1755136 |
--------------------------------
Eval num_timesteps=1755500, episode_reward=-39838.10 +/- 28977.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37519354   |
|    mean velocity x      | -0.679        |
|    mean velocity y      | 0.09          |
|    mean velocity z      | 3.52          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.98e+04     |
| time/                   |               |
|    total_timesteps      | 1755500       |
| train/                  |               |
|    approx_kl            | 1.4618126e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.334         |
|    learning_rate        | 0.001         |
|    loss                 | 4.03e+06      |
|    n_updates            | 8570          |
|    policy_gradient_loss | -0.000458     |
|    std                  | 1.55          |
|    value_loss           | 5.39e+07      |
-------------------------------------------
Eval num_timesteps=1756000, episode_reward=-85355.39 +/- 39758.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5283513 |
|    mean velocity x | -0.0957    |
|    mean velocity y | 1.45       |
|    mean velocity z | 2.7        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.54e+04  |
| time/              |            |
|    total_timesteps | 1756000    |
-----------------------------------
Eval num_timesteps=1756500, episode_reward=-59011.15 +/- 37851.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4822617 |
|    mean velocity x | -0.41      |
|    mean velocity y | 0.826      |
|    mean velocity z | 4.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.9e+04   |
| time/              |            |
|    total_timesteps | 1756500    |
-----------------------------------
Eval num_timesteps=1757000, episode_reward=-65429.05 +/- 39716.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40367487 |
|    mean velocity x | 0.368       |
|    mean velocity y | 0.88        |
|    mean velocity z | 3.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.54e+04   |
| time/              |             |
|    total_timesteps | 1757000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 858     |
|    time_elapsed    | 70777   |
|    total_timesteps | 1757184 |
--------------------------------
Eval num_timesteps=1757500, episode_reward=-64599.52 +/- 24007.09
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42634284  |
|    mean velocity x      | -0.511       |
|    mean velocity y      | 0.505        |
|    mean velocity z      | 3.99         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.46e+04    |
| time/                   |              |
|    total_timesteps      | 1757500      |
| train/                  |              |
|    approx_kl            | 7.365772e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.001        |
|    loss                 | 3.21e+07     |
|    n_updates            | 8580         |
|    policy_gradient_loss | -0.000293    |
|    std                  | 1.55         |
|    value_loss           | 6.42e+07     |
------------------------------------------
Eval num_timesteps=1758000, episode_reward=-60153.40 +/- 40214.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37124303 |
|    mean velocity x | -0.659      |
|    mean velocity y | 0.802       |
|    mean velocity z | 2.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.02e+04   |
| time/              |             |
|    total_timesteps | 1758000     |
------------------------------------
Eval num_timesteps=1758500, episode_reward=-78217.48 +/- 24254.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46780306 |
|    mean velocity x | -0.335      |
|    mean velocity y | 0.822       |
|    mean velocity z | 4.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 1758500     |
------------------------------------
Eval num_timesteps=1759000, episode_reward=-83178.65 +/- 44054.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25200465 |
|    mean velocity x | -0.336      |
|    mean velocity y | 0.48        |
|    mean velocity z | 0.815       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.32e+04   |
| time/              |             |
|    total_timesteps | 1759000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 859     |
|    time_elapsed    | 70857   |
|    total_timesteps | 1759232 |
--------------------------------
Eval num_timesteps=1759500, episode_reward=-83510.14 +/- 39559.25
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.51579374   |
|    mean velocity x      | -0.497        |
|    mean velocity y      | 0.506         |
|    mean velocity z      | 5.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.35e+04     |
| time/                   |               |
|    total_timesteps      | 1759500       |
| train/                  |               |
|    approx_kl            | 4.0216837e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.349         |
|    learning_rate        | 0.001         |
|    loss                 | 1.5e+07       |
|    n_updates            | 8590          |
|    policy_gradient_loss | -0.000156     |
|    std                  | 1.55          |
|    value_loss           | 4.19e+07      |
-------------------------------------------
Eval num_timesteps=1760000, episode_reward=-65311.25 +/- 41374.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32843572 |
|    mean velocity x | -0.414      |
|    mean velocity y | -0.0788     |
|    mean velocity z | 3.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.53e+04   |
| time/              |             |
|    total_timesteps | 1760000     |
------------------------------------
Eval num_timesteps=1760500, episode_reward=-82464.92 +/- 35128.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50645846 |
|    mean velocity x | -0.35       |
|    mean velocity y | 0.753       |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.25e+04   |
| time/              |             |
|    total_timesteps | 1760500     |
------------------------------------
Eval num_timesteps=1761000, episode_reward=-48391.39 +/- 30942.82
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.507916 |
|    mean velocity x | -1.12     |
|    mean velocity y | 0.425     |
|    mean velocity z | 3.3       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.84e+04 |
| time/              |           |
|    total_timesteps | 1761000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 860     |
|    time_elapsed    | 70938   |
|    total_timesteps | 1761280 |
--------------------------------
Eval num_timesteps=1761500, episode_reward=-80469.35 +/- 47435.00
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44428474   |
|    mean velocity x      | -0.394        |
|    mean velocity y      | 0.722         |
|    mean velocity z      | 4.79          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.05e+04     |
| time/                   |               |
|    total_timesteps      | 1761500       |
| train/                  |               |
|    approx_kl            | 5.8133737e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.001         |
|    loss                 | 5.59e+07      |
|    n_updates            | 8600          |
|    policy_gradient_loss | -0.000221     |
|    std                  | 1.55          |
|    value_loss           | 9.06e+07      |
-------------------------------------------
Eval num_timesteps=1762000, episode_reward=-59762.98 +/- 24679.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42688808 |
|    mean velocity x | -0.431      |
|    mean velocity y | 0.729       |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.98e+04   |
| time/              |             |
|    total_timesteps | 1762000     |
------------------------------------
Eval num_timesteps=1762500, episode_reward=-55972.67 +/- 36087.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37243828 |
|    mean velocity x | -0.958      |
|    mean velocity y | 0.394       |
|    mean velocity z | 2.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.6e+04    |
| time/              |             |
|    total_timesteps | 1762500     |
------------------------------------
Eval num_timesteps=1763000, episode_reward=-98472.90 +/- 16829.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34774378 |
|    mean velocity x | 0.271       |
|    mean velocity y | 0.611       |
|    mean velocity z | 2.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.85e+04   |
| time/              |             |
|    total_timesteps | 1763000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 861     |
|    time_elapsed    | 71018   |
|    total_timesteps | 1763328 |
--------------------------------
Eval num_timesteps=1763500, episode_reward=-94026.68 +/- 25876.04
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6574186    |
|    mean velocity x      | -0.336        |
|    mean velocity y      | 1.61          |
|    mean velocity z      | 6.87          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.4e+04      |
| time/                   |               |
|    total_timesteps      | 1763500       |
| train/                  |               |
|    approx_kl            | 6.1337487e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.427         |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+07      |
|    n_updates            | 8610          |
|    policy_gradient_loss | -0.00015      |
|    std                  | 1.55          |
|    value_loss           | 3.84e+07      |
-------------------------------------------
Eval num_timesteps=1764000, episode_reward=-71544.47 +/- 36354.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32564023 |
|    mean velocity x | -0.385      |
|    mean velocity y | 0.697       |
|    mean velocity z | 0.444       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.15e+04   |
| time/              |             |
|    total_timesteps | 1764000     |
------------------------------------
Eval num_timesteps=1764500, episode_reward=-92330.41 +/- 46032.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36817428 |
|    mean velocity x | -0.221      |
|    mean velocity y | 0.653       |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 1764500     |
------------------------------------
Eval num_timesteps=1765000, episode_reward=-81912.15 +/- 42065.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5688285 |
|    mean velocity x | -0.477     |
|    mean velocity y | 1.3        |
|    mean velocity z | 4.68       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.19e+04  |
| time/              |            |
|    total_timesteps | 1765000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 862     |
|    time_elapsed    | 71099   |
|    total_timesteps | 1765376 |
--------------------------------
Eval num_timesteps=1765500, episode_reward=-38872.90 +/- 34463.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44462493  |
|    mean velocity x      | -0.302       |
|    mean velocity y      | 0.925        |
|    mean velocity z      | 4.5          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.89e+04    |
| time/                   |              |
|    total_timesteps      | 1765500      |
| train/                  |              |
|    approx_kl            | 1.221642e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.001        |
|    loss                 | 5.2e+07      |
|    n_updates            | 8620         |
|    policy_gradient_loss | -0.000383    |
|    std                  | 1.55         |
|    value_loss           | 8.22e+07     |
------------------------------------------
Eval num_timesteps=1766000, episode_reward=-50535.15 +/- 42446.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4039995 |
|    mean velocity x | -0.0334    |
|    mean velocity y | 0.973      |
|    mean velocity z | 3.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.05e+04  |
| time/              |            |
|    total_timesteps | 1766000    |
-----------------------------------
Eval num_timesteps=1766500, episode_reward=-88005.33 +/- 45401.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3952165 |
|    mean velocity x | -0.508     |
|    mean velocity y | 0.573      |
|    mean velocity z | 3.71       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.8e+04   |
| time/              |            |
|    total_timesteps | 1766500    |
-----------------------------------
Eval num_timesteps=1767000, episode_reward=-51142.84 +/- 38141.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46431652 |
|    mean velocity x | -0.295      |
|    mean velocity y | 0.704       |
|    mean velocity z | 3.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.11e+04   |
| time/              |             |
|    total_timesteps | 1767000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 863     |
|    time_elapsed    | 71179   |
|    total_timesteps | 1767424 |
--------------------------------
Eval num_timesteps=1767500, episode_reward=-72330.71 +/- 43672.03
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5351722   |
|    mean velocity x      | -0.886       |
|    mean velocity y      | 0.585        |
|    mean velocity z      | 5.5          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.23e+04    |
| time/                   |              |
|    total_timesteps      | 1767500      |
| train/                  |              |
|    approx_kl            | 6.205257e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+07     |
|    n_updates            | 8630         |
|    policy_gradient_loss | -0.000192    |
|    std                  | 1.55         |
|    value_loss           | 5.55e+07     |
------------------------------------------
Eval num_timesteps=1768000, episode_reward=-84365.21 +/- 31321.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3622096 |
|    mean velocity x | 0.167      |
|    mean velocity y | 0.878      |
|    mean velocity z | 0.789      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.44e+04  |
| time/              |            |
|    total_timesteps | 1768000    |
-----------------------------------
Eval num_timesteps=1768500, episode_reward=-105396.35 +/- 35230.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43414062 |
|    mean velocity x | 0.221       |
|    mean velocity y | 1.49        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1768500     |
------------------------------------
Eval num_timesteps=1769000, episode_reward=-83985.86 +/- 16812.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31561506 |
|    mean velocity x | -0.22       |
|    mean velocity y | 0.575       |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.4e+04    |
| time/              |             |
|    total_timesteps | 1769000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 864     |
|    time_elapsed    | 71259   |
|    total_timesteps | 1769472 |
--------------------------------
Eval num_timesteps=1769500, episode_reward=-62222.91 +/- 42452.46
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5179688    |
|    mean velocity x      | 0.371         |
|    mean velocity y      | 1.64          |
|    mean velocity z      | 3.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.22e+04     |
| time/                   |               |
|    total_timesteps      | 1769500       |
| train/                  |               |
|    approx_kl            | 1.1437776e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.368         |
|    learning_rate        | 0.001         |
|    loss                 | 5.68e+06      |
|    n_updates            | 8640          |
|    policy_gradient_loss | -0.00034      |
|    std                  | 1.55          |
|    value_loss           | 3.21e+07      |
-------------------------------------------
Eval num_timesteps=1770000, episode_reward=-107799.85 +/- 15377.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43168676 |
|    mean velocity x | -0.352      |
|    mean velocity y | 0.732       |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1770000     |
------------------------------------
Eval num_timesteps=1770500, episode_reward=-83689.46 +/- 45956.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44101903 |
|    mean velocity x | -0.311      |
|    mean velocity y | 0.9         |
|    mean velocity z | 4.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 1770500     |
------------------------------------
Eval num_timesteps=1771000, episode_reward=-72282.38 +/- 11515.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4361631 |
|    mean velocity x | -1.49      |
|    mean velocity y | -0.279     |
|    mean velocity z | 3.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 1771000    |
-----------------------------------
Eval num_timesteps=1771500, episode_reward=-65969.93 +/- 44422.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5003139 |
|    mean velocity x | 0.326      |
|    mean velocity y | 1.48       |
|    mean velocity z | 3.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.6e+04   |
| time/              |            |
|    total_timesteps | 1771500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 865     |
|    time_elapsed    | 71359   |
|    total_timesteps | 1771520 |
--------------------------------
Eval num_timesteps=1772000, episode_reward=-76137.51 +/- 39930.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44311687   |
|    mean velocity x      | -1.17         |
|    mean velocity y      | 0.323         |
|    mean velocity z      | 3.06          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.61e+04     |
| time/                   |               |
|    total_timesteps      | 1772000       |
| train/                  |               |
|    approx_kl            | 2.7243921e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.001         |
|    loss                 | 4.84e+07      |
|    n_updates            | 8650          |
|    policy_gradient_loss | -0.000303     |
|    std                  | 1.55          |
|    value_loss           | 8.16e+07      |
-------------------------------------------
Eval num_timesteps=1772500, episode_reward=-107790.89 +/- 46416.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4223921 |
|    mean velocity x | -0.339     |
|    mean velocity y | 0.683      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 1772500    |
-----------------------------------
Eval num_timesteps=1773000, episode_reward=-62113.63 +/- 34038.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49484706 |
|    mean velocity x | -0.354      |
|    mean velocity y | 0.898       |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+04   |
| time/              |             |
|    total_timesteps | 1773000     |
------------------------------------
Eval num_timesteps=1773500, episode_reward=-96935.14 +/- 34781.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5094243 |
|    mean velocity x | 0.142      |
|    mean velocity y | 1.3        |
|    mean velocity z | 3.56       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.69e+04  |
| time/              |            |
|    total_timesteps | 1773500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 866     |
|    time_elapsed    | 71439   |
|    total_timesteps | 1773568 |
--------------------------------
Eval num_timesteps=1774000, episode_reward=-65941.06 +/- 53154.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41083714   |
|    mean velocity x      | 0.011         |
|    mean velocity y      | 1.31          |
|    mean velocity z      | 3.42          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.59e+04     |
| time/                   |               |
|    total_timesteps      | 1774000       |
| train/                  |               |
|    approx_kl            | 0.00022810165 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.259         |
|    learning_rate        | 0.001         |
|    loss                 | 5.83e+07      |
|    n_updates            | 8660          |
|    policy_gradient_loss | -0.00166      |
|    std                  | 1.55          |
|    value_loss           | 8.77e+07      |
-------------------------------------------
Eval num_timesteps=1774500, episode_reward=-75830.13 +/- 38302.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2282827 |
|    mean velocity x | -0.0798    |
|    mean velocity y | 0.222      |
|    mean velocity z | 0.472      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.58e+04  |
| time/              |            |
|    total_timesteps | 1774500    |
-----------------------------------
Eval num_timesteps=1775000, episode_reward=-55597.47 +/- 41254.15
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3831485 |
|    mean velocity x | -0.173     |
|    mean velocity y | 0.554      |
|    mean velocity z | 1.81       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.56e+04  |
| time/              |            |
|    total_timesteps | 1775000    |
-----------------------------------
Eval num_timesteps=1775500, episode_reward=-67557.84 +/- 40454.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29512036 |
|    mean velocity x | -0.64       |
|    mean velocity y | -0.0733     |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 1775500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 867     |
|    time_elapsed    | 71520   |
|    total_timesteps | 1775616 |
--------------------------------
Eval num_timesteps=1776000, episode_reward=-80443.13 +/- 30928.03
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39860445   |
|    mean velocity x      | -0.0762       |
|    mean velocity y      | 0.981         |
|    mean velocity z      | 4.19          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.04e+04     |
| time/                   |               |
|    total_timesteps      | 1776000       |
| train/                  |               |
|    approx_kl            | 1.9222643e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.254         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+07      |
|    n_updates            | 8670          |
|    policy_gradient_loss | -0.000389     |
|    std                  | 1.55          |
|    value_loss           | 4.12e+07      |
-------------------------------------------
Eval num_timesteps=1776500, episode_reward=-84200.57 +/- 14415.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46374065 |
|    mean velocity x | -1.15       |
|    mean velocity y | -0.29       |
|    mean velocity z | 5.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 1776500     |
------------------------------------
Eval num_timesteps=1777000, episode_reward=-63905.09 +/- 48839.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25435293 |
|    mean velocity x | -0.302      |
|    mean velocity y | 0.308       |
|    mean velocity z | 2.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.39e+04   |
| time/              |             |
|    total_timesteps | 1777000     |
------------------------------------
Eval num_timesteps=1777500, episode_reward=-79360.79 +/- 46086.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37478235 |
|    mean velocity x | -0.233      |
|    mean velocity y | 0.682       |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.94e+04   |
| time/              |             |
|    total_timesteps | 1777500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 868     |
|    time_elapsed    | 71600   |
|    total_timesteps | 1777664 |
--------------------------------
Eval num_timesteps=1778000, episode_reward=-80510.86 +/- 27596.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47867543  |
|    mean velocity x      | 0.0242       |
|    mean velocity y      | 1.41         |
|    mean velocity z      | 3.54         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.05e+04    |
| time/                   |              |
|    total_timesteps      | 1778000      |
| train/                  |              |
|    approx_kl            | 0.0001316957 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+07     |
|    n_updates            | 8680         |
|    policy_gradient_loss | -0.000961    |
|    std                  | 1.55         |
|    value_loss           | 4.11e+07     |
------------------------------------------
Eval num_timesteps=1778500, episode_reward=-78364.11 +/- 41589.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4135624 |
|    mean velocity x | -0.299     |
|    mean velocity y | 0.744      |
|    mean velocity z | 4.59       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.84e+04  |
| time/              |            |
|    total_timesteps | 1778500    |
-----------------------------------
Eval num_timesteps=1779000, episode_reward=-81176.55 +/- 42616.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4107213 |
|    mean velocity x | -0.328     |
|    mean velocity y | 0.585      |
|    mean velocity z | 4.43       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.12e+04  |
| time/              |            |
|    total_timesteps | 1779000    |
-----------------------------------
Eval num_timesteps=1779500, episode_reward=-99481.71 +/- 65048.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41032082 |
|    mean velocity x | 0.502       |
|    mean velocity y | 1.3         |
|    mean velocity z | 3.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 1779500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 869     |
|    time_elapsed    | 71680   |
|    total_timesteps | 1779712 |
--------------------------------
Eval num_timesteps=1780000, episode_reward=-84214.20 +/- 25414.65
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42100728  |
|    mean velocity x      | -0.0898      |
|    mean velocity y      | 0.87         |
|    mean velocity z      | 4.08         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.42e+04    |
| time/                   |              |
|    total_timesteps      | 1780000      |
| train/                  |              |
|    approx_kl            | 7.580657e-05 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+07     |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.000597    |
|    std                  | 1.55         |
|    value_loss           | 9.58e+07     |
------------------------------------------
Eval num_timesteps=1780500, episode_reward=-106448.16 +/- 21736.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4940688 |
|    mean velocity x | -0.659     |
|    mean velocity y | 0.638      |
|    mean velocity z | 4.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.06e+05  |
| time/              |            |
|    total_timesteps | 1780500    |
-----------------------------------
Eval num_timesteps=1781000, episode_reward=-63102.57 +/- 47289.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3782522 |
|    mean velocity x | -0.557     |
|    mean velocity y | 0.555      |
|    mean velocity z | 2.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.31e+04  |
| time/              |            |
|    total_timesteps | 1781000    |
-----------------------------------
Eval num_timesteps=1781500, episode_reward=-80357.87 +/- 18242.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46753016 |
|    mean velocity x | -0.44       |
|    mean velocity y | 0.796       |
|    mean velocity z | 4.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.04e+04   |
| time/              |             |
|    total_timesteps | 1781500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 870     |
|    time_elapsed    | 71761   |
|    total_timesteps | 1781760 |
--------------------------------
Eval num_timesteps=1782000, episode_reward=-8582.63 +/- 11004.98
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48201507   |
|    mean velocity x      | -0.311        |
|    mean velocity y      | 0.628         |
|    mean velocity z      | 4.08          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.58e+03     |
| time/                   |               |
|    total_timesteps      | 1782000       |
| train/                  |               |
|    approx_kl            | 6.9681264e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 3.46e+07      |
|    n_updates            | 8700          |
|    policy_gradient_loss | -0.000221     |
|    std                  | 1.55          |
|    value_loss           | 8.93e+07      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=1782500, episode_reward=-91311.19 +/- 21018.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1891837 |
|    mean velocity x | -0.962     |
|    mean velocity y | -0.509     |
|    mean velocity z | 2.97       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.13e+04  |
| time/              |            |
|    total_timesteps | 1782500    |
-----------------------------------
Eval num_timesteps=1783000, episode_reward=-109165.62 +/- 30352.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36144122 |
|    mean velocity x | -0.00932    |
|    mean velocity y | 0.908       |
|    mean velocity z | 2.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 1783000     |
------------------------------------
Eval num_timesteps=1783500, episode_reward=-72122.84 +/- 37584.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38999185 |
|    mean velocity x | -0.273      |
|    mean velocity y | 0.532       |
|    mean velocity z | 4.38        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.21e+04   |
| time/              |             |
|    total_timesteps | 1783500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 871     |
|    time_elapsed    | 71841   |
|    total_timesteps | 1783808 |
--------------------------------
Eval num_timesteps=1784000, episode_reward=-55648.72 +/- 8399.13
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26354834  |
|    mean velocity x      | -0.998       |
|    mean velocity y      | -0.251       |
|    mean velocity z      | 2.75         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.56e+04    |
| time/                   |              |
|    total_timesteps      | 1784000      |
| train/                  |              |
|    approx_kl            | 9.321113e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+07        |
|    n_updates            | 8710         |
|    policy_gradient_loss | -0.000191    |
|    std                  | 1.55         |
|    value_loss           | 3.85e+07     |
------------------------------------------
Eval num_timesteps=1784500, episode_reward=-94169.71 +/- 23493.60
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.553097 |
|    mean velocity x | -0.8      |
|    mean velocity y | 0.798     |
|    mean velocity z | 4.72      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.42e+04 |
| time/              |           |
|    total_timesteps | 1784500   |
----------------------------------
Eval num_timesteps=1785000, episode_reward=-87506.86 +/- 41486.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2633706 |
|    mean velocity x | -1.04      |
|    mean velocity y | -0.299     |
|    mean velocity z | 2.9        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.75e+04  |
| time/              |            |
|    total_timesteps | 1785000    |
-----------------------------------
Eval num_timesteps=1785500, episode_reward=-71869.12 +/- 23686.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44019616 |
|    mean velocity x | -0.261      |
|    mean velocity y | 1.13        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.19e+04   |
| time/              |             |
|    total_timesteps | 1785500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 872     |
|    time_elapsed    | 71921   |
|    total_timesteps | 1785856 |
--------------------------------
Eval num_timesteps=1786000, episode_reward=-83537.07 +/- 22254.20
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41814068  |
|    mean velocity x      | -0.307       |
|    mean velocity y      | 0.877        |
|    mean velocity z      | 3.98         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.35e+04    |
| time/                   |              |
|    total_timesteps      | 1786000      |
| train/                  |              |
|    approx_kl            | 2.296851e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+07     |
|    n_updates            | 8720         |
|    policy_gradient_loss | -0.000456    |
|    std                  | 1.55         |
|    value_loss           | 6.35e+07     |
------------------------------------------
Eval num_timesteps=1786500, episode_reward=-63335.02 +/- 26284.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36811617 |
|    mean velocity x | -0.483      |
|    mean velocity y | 0.242       |
|    mean velocity z | 2.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.33e+04   |
| time/              |             |
|    total_timesteps | 1786500     |
------------------------------------
Eval num_timesteps=1787000, episode_reward=-99957.55 +/- 21706.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10793582 |
|    mean velocity x | -1.24       |
|    mean velocity y | -0.387      |
|    mean velocity z | 2.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 1787000     |
------------------------------------
Eval num_timesteps=1787500, episode_reward=-97120.62 +/- 25836.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.340906 |
|    mean velocity x | -0.295    |
|    mean velocity y | 0.555     |
|    mean velocity z | 0.8       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.71e+04 |
| time/              |           |
|    total_timesteps | 1787500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 873     |
|    time_elapsed    | 72002   |
|    total_timesteps | 1787904 |
--------------------------------
Eval num_timesteps=1788000, episode_reward=-58445.22 +/- 31258.46
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4475276    |
|    mean velocity x      | 0.31          |
|    mean velocity y      | 1.44          |
|    mean velocity z      | 3.36          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.84e+04     |
| time/                   |               |
|    total_timesteps      | 1788000       |
| train/                  |               |
|    approx_kl            | 2.0903564e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.396         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+07      |
|    n_updates            | 8730          |
|    policy_gradient_loss | -0.000505     |
|    std                  | 1.55          |
|    value_loss           | 2.4e+07       |
-------------------------------------------
Eval num_timesteps=1788500, episode_reward=-89903.66 +/- 23964.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37646878 |
|    mean velocity x | -0.532      |
|    mean velocity y | 0.606       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.99e+04   |
| time/              |             |
|    total_timesteps | 1788500     |
------------------------------------
Eval num_timesteps=1789000, episode_reward=-58810.08 +/- 49294.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2527966 |
|    mean velocity x | -0.13      |
|    mean velocity y | 0.101      |
|    mean velocity z | 3.71       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.88e+04  |
| time/              |            |
|    total_timesteps | 1789000    |
-----------------------------------
Eval num_timesteps=1789500, episode_reward=-100997.41 +/- 25246.87
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.2963   |
|    mean velocity x | -0.614    |
|    mean velocity y | 0.0209    |
|    mean velocity z | 2.49      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 1789500   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 874     |
|    time_elapsed    | 72082   |
|    total_timesteps | 1789952 |
--------------------------------
Eval num_timesteps=1790000, episode_reward=-51740.47 +/- 32486.00
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46846062   |
|    mean velocity x      | 0.453         |
|    mean velocity y      | 1.31          |
|    mean velocity z      | 3.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.17e+04     |
| time/                   |               |
|    total_timesteps      | 1790000       |
| train/                  |               |
|    approx_kl            | 5.4716656e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+07      |
|    n_updates            | 8740          |
|    policy_gradient_loss | -0.00029      |
|    std                  | 1.55          |
|    value_loss           | 5.36e+07      |
-------------------------------------------
Eval num_timesteps=1790500, episode_reward=-79406.46 +/- 37467.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4290406 |
|    mean velocity x | -0.0868    |
|    mean velocity y | 1.09       |
|    mean velocity z | 3.82       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.94e+04  |
| time/              |            |
|    total_timesteps | 1790500    |
-----------------------------------
Eval num_timesteps=1791000, episode_reward=-87210.92 +/- 37882.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40537542 |
|    mean velocity x | 0.0359      |
|    mean velocity y | 0.687       |
|    mean velocity z | 1.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.72e+04   |
| time/              |             |
|    total_timesteps | 1791000     |
------------------------------------
Eval num_timesteps=1791500, episode_reward=-113277.09 +/- 20313.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33839154 |
|    mean velocity x | -0.112      |
|    mean velocity y | 0.666       |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 1791500     |
------------------------------------
Eval num_timesteps=1792000, episode_reward=-95616.71 +/- 19975.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44647765 |
|    mean velocity x | -0.166      |
|    mean velocity y | 1.16        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.56e+04   |
| time/              |             |
|    total_timesteps | 1792000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 875     |
|    time_elapsed    | 72181   |
|    total_timesteps | 1792000 |
--------------------------------
Eval num_timesteps=1792500, episode_reward=-76925.19 +/- 25506.54
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.29446393   |
|    mean velocity x      | -0.219        |
|    mean velocity y      | 0.493         |
|    mean velocity z      | 0.995         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.69e+04     |
| time/                   |               |
|    total_timesteps      | 1792500       |
| train/                  |               |
|    approx_kl            | 1.9151426e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.001         |
|    loss                 | 4.26e+07      |
|    n_updates            | 8750          |
|    policy_gradient_loss | -0.00032      |
|    std                  | 1.55          |
|    value_loss           | 7.21e+07      |
-------------------------------------------
Eval num_timesteps=1793000, episode_reward=-101482.19 +/- 11998.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6060683 |
|    mean velocity x | -0.837     |
|    mean velocity y | 0.304      |
|    mean velocity z | 7.39       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 1793000    |
-----------------------------------
Eval num_timesteps=1793500, episode_reward=-93603.49 +/- 48420.12
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.497716 |
|    mean velocity x | -1.3      |
|    mean velocity y | -0.0211   |
|    mean velocity z | 3.3       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.36e+04 |
| time/              |           |
|    total_timesteps | 1793500   |
----------------------------------
Eval num_timesteps=1794000, episode_reward=-73647.15 +/- 28795.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24325012 |
|    mean velocity x | -0.0458     |
|    mean velocity y | 0.482       |
|    mean velocity z | 0.314       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1794000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 876     |
|    time_elapsed    | 72262   |
|    total_timesteps | 1794048 |
--------------------------------
Eval num_timesteps=1794500, episode_reward=-73299.00 +/- 32518.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3783198    |
|    mean velocity x      | -0.121        |
|    mean velocity y      | 1.13          |
|    mean velocity z      | 4.33          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.33e+04     |
| time/                   |               |
|    total_timesteps      | 1794500       |
| train/                  |               |
|    approx_kl            | 4.5482593e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.348         |
|    learning_rate        | 0.001         |
|    loss                 | 5.28e+07      |
|    n_updates            | 8760          |
|    policy_gradient_loss | -0.000144     |
|    std                  | 1.55          |
|    value_loss           | 5.94e+07      |
-------------------------------------------
Eval num_timesteps=1795000, episode_reward=-61663.13 +/- 50368.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35520563 |
|    mean velocity x | 0.36        |
|    mean velocity y | 0.956       |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+04   |
| time/              |             |
|    total_timesteps | 1795000     |
------------------------------------
Eval num_timesteps=1795500, episode_reward=-64051.97 +/- 52099.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38418806 |
|    mean velocity x | -0.46       |
|    mean velocity y | 0.693       |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.41e+04   |
| time/              |             |
|    total_timesteps | 1795500     |
------------------------------------
Eval num_timesteps=1796000, episode_reward=-65501.15 +/- 34700.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38283643 |
|    mean velocity x | -0.351      |
|    mean velocity y | 0.453       |
|    mean velocity z | 2.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.55e+04   |
| time/              |             |
|    total_timesteps | 1796000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 877     |
|    time_elapsed    | 72342   |
|    total_timesteps | 1796096 |
--------------------------------
Eval num_timesteps=1796500, episode_reward=-93183.72 +/- 43091.06
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48657307  |
|    mean velocity x      | -0.537       |
|    mean velocity y      | 0.422        |
|    mean velocity z      | 4.1          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.32e+04    |
| time/                   |              |
|    total_timesteps      | 1796500      |
| train/                  |              |
|    approx_kl            | 4.647387e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.001        |
|    loss                 | 3.26e+07     |
|    n_updates            | 8770         |
|    policy_gradient_loss | -0.000139    |
|    std                  | 1.55         |
|    value_loss           | 5.13e+07     |
------------------------------------------
Eval num_timesteps=1797000, episode_reward=-103547.35 +/- 32553.94
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5185428 |
|    mean velocity x | -0.881     |
|    mean velocity y | 0.596      |
|    mean velocity z | 3.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1797000    |
-----------------------------------
Eval num_timesteps=1797500, episode_reward=-94720.40 +/- 27515.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4519389 |
|    mean velocity x | 0.00768    |
|    mean velocity y | 1.68       |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.47e+04  |
| time/              |            |
|    total_timesteps | 1797500    |
-----------------------------------
Eval num_timesteps=1798000, episode_reward=-90266.33 +/- 21778.07
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.506658 |
|    mean velocity x | -0.437    |
|    mean velocity y | 1.27      |
|    mean velocity z | 3.97      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -9.03e+04 |
| time/              |           |
|    total_timesteps | 1798000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 878     |
|    time_elapsed    | 72422   |
|    total_timesteps | 1798144 |
--------------------------------
Eval num_timesteps=1798500, episode_reward=-99146.03 +/- 5783.22
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37822196  |
|    mean velocity x      | -0.342       |
|    mean velocity y      | 0.693        |
|    mean velocity z      | 0.731        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.91e+04    |
| time/                   |              |
|    total_timesteps      | 1798500      |
| train/                  |              |
|    approx_kl            | 7.208466e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.392        |
|    learning_rate        | 0.001        |
|    loss                 | 2.21e+07     |
|    n_updates            | 8780         |
|    policy_gradient_loss | -0.00019     |
|    std                  | 1.55         |
|    value_loss           | 3.19e+07     |
------------------------------------------
Eval num_timesteps=1799000, episode_reward=-81783.61 +/- 52159.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40572828 |
|    mean velocity x | 0.393       |
|    mean velocity y | 1.15        |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.18e+04   |
| time/              |             |
|    total_timesteps | 1799000     |
------------------------------------
Eval num_timesteps=1799500, episode_reward=-75263.21 +/- 39517.96
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4226393 |
|    mean velocity x | 0.00926    |
|    mean velocity y | 1.7        |
|    mean velocity z | 3.71       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.53e+04  |
| time/              |            |
|    total_timesteps | 1799500    |
-----------------------------------
Eval num_timesteps=1800000, episode_reward=-55163.88 +/- 34411.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49520996 |
|    mean velocity x | -0.476      |
|    mean velocity y | 0.69        |
|    mean velocity z | 4.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.52e+04   |
| time/              |             |
|    total_timesteps | 1800000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 879     |
|    time_elapsed    | 72503   |
|    total_timesteps | 1800192 |
--------------------------------
Eval num_timesteps=1800500, episode_reward=-107486.87 +/- 16760.62
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31461665   |
|    mean velocity x      | -1.23         |
|    mean velocity y      | -0.916        |
|    mean velocity z      | 5.14          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 1800500       |
| train/                  |               |
|    approx_kl            | 1.4928024e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.388         |
|    learning_rate        | 0.001         |
|    loss                 | 2.66e+07      |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000335     |
|    std                  | 1.55          |
|    value_loss           | 5.25e+07      |
-------------------------------------------
Eval num_timesteps=1801000, episode_reward=-99001.78 +/- 19081.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4169629 |
|    mean velocity x | -0.0927    |
|    mean velocity y | 1.23       |
|    mean velocity z | 4.06       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.9e+04   |
| time/              |            |
|    total_timesteps | 1801000    |
-----------------------------------
Eval num_timesteps=1801500, episode_reward=-78302.10 +/- 38894.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51226485 |
|    mean velocity x | -0.626      |
|    mean velocity y | 0.529       |
|    mean velocity z | 4.85        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.83e+04   |
| time/              |             |
|    total_timesteps | 1801500     |
------------------------------------
Eval num_timesteps=1802000, episode_reward=-72809.30 +/- 45433.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52082175 |
|    mean velocity x | 0.0971      |
|    mean velocity y | 1.47        |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 1802000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 880     |
|    time_elapsed    | 72583   |
|    total_timesteps | 1802240 |
--------------------------------
Eval num_timesteps=1802500, episode_reward=-70620.41 +/- 41684.03
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.23759767   |
|    mean velocity x      | -2.59         |
|    mean velocity y      | -2.54         |
|    mean velocity z      | 8.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.06e+04     |
| time/                   |               |
|    total_timesteps      | 1802500       |
| train/                  |               |
|    approx_kl            | 6.6850916e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.432         |
|    learning_rate        | 0.001         |
|    loss                 | 7.63e+06      |
|    n_updates            | 8800          |
|    policy_gradient_loss | -0.000197     |
|    std                  | 1.55          |
|    value_loss           | 6.63e+07      |
-------------------------------------------
Eval num_timesteps=1803000, episode_reward=-67622.32 +/- 49318.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33168647 |
|    mean velocity x | 0.0875      |
|    mean velocity y | 0.446       |
|    mean velocity z | 1.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 1803000     |
------------------------------------
Eval num_timesteps=1803500, episode_reward=-47642.21 +/- 39070.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41037822 |
|    mean velocity x | -0.271      |
|    mean velocity y | 0.761       |
|    mean velocity z | 4.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.76e+04   |
| time/              |             |
|    total_timesteps | 1803500     |
------------------------------------
Eval num_timesteps=1804000, episode_reward=-52581.70 +/- 38182.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4551212 |
|    mean velocity x | -0.375     |
|    mean velocity y | 0.911      |
|    mean velocity z | 4.61       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.26e+04  |
| time/              |            |
|    total_timesteps | 1804000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 881     |
|    time_elapsed    | 72663   |
|    total_timesteps | 1804288 |
--------------------------------
Eval num_timesteps=1804500, episode_reward=-88859.99 +/- 18549.01
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47282824   |
|    mean velocity x      | 0.244         |
|    mean velocity y      | 0.978         |
|    mean velocity z      | 2.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.89e+04     |
| time/                   |               |
|    total_timesteps      | 1804500       |
| train/                  |               |
|    approx_kl            | 1.0114221e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.001         |
|    loss                 | 4.59e+07      |
|    n_updates            | 8810          |
|    policy_gradient_loss | -0.000298     |
|    std                  | 1.55          |
|    value_loss           | 7.85e+07      |
-------------------------------------------
Eval num_timesteps=1805000, episode_reward=-99526.61 +/- 20560.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51858276 |
|    mean velocity x | -0.388      |
|    mean velocity y | 0.858       |
|    mean velocity z | 3.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 1805000     |
------------------------------------
Eval num_timesteps=1805500, episode_reward=-63564.61 +/- 37428.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43319976 |
|    mean velocity x | -0.198      |
|    mean velocity y | 1           |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.36e+04   |
| time/              |             |
|    total_timesteps | 1805500     |
------------------------------------
Eval num_timesteps=1806000, episode_reward=-73098.49 +/- 53886.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46936822 |
|    mean velocity x | 0.224       |
|    mean velocity y | 1.11        |
|    mean velocity z | 3.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.31e+04   |
| time/              |             |
|    total_timesteps | 1806000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 882     |
|    time_elapsed    | 72744   |
|    total_timesteps | 1806336 |
--------------------------------
Eval num_timesteps=1806500, episode_reward=-91736.14 +/- 40880.32
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41238996  |
|    mean velocity x      | -0.549       |
|    mean velocity y      | 0.451        |
|    mean velocity z      | 2.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.17e+04    |
| time/                   |              |
|    total_timesteps      | 1806500      |
| train/                  |              |
|    approx_kl            | 8.858653e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.001        |
|    loss                 | 9.23e+06     |
|    n_updates            | 8820         |
|    policy_gradient_loss | -0.000298    |
|    std                  | 1.55         |
|    value_loss           | 3.71e+07     |
------------------------------------------
Eval num_timesteps=1807000, episode_reward=-63287.30 +/- 46408.60
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4608354 |
|    mean velocity x | -0.522     |
|    mean velocity y | 0.609      |
|    mean velocity z | 3.87       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.33e+04  |
| time/              |            |
|    total_timesteps | 1807000    |
-----------------------------------
Eval num_timesteps=1807500, episode_reward=-71338.91 +/- 42678.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26674768 |
|    mean velocity x | -0.206      |
|    mean velocity y | 0.246       |
|    mean velocity z | 2.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 1807500     |
------------------------------------
Eval num_timesteps=1808000, episode_reward=-99378.47 +/- 48617.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5118772 |
|    mean velocity x | -0.81      |
|    mean velocity y | 0.481      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.94e+04  |
| time/              |            |
|    total_timesteps | 1808000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 883     |
|    time_elapsed    | 72824   |
|    total_timesteps | 1808384 |
--------------------------------
Eval num_timesteps=1808500, episode_reward=-36986.53 +/- 28909.95
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36070642   |
|    mean velocity x      | 0.0739        |
|    mean velocity y      | 0.686         |
|    mean velocity z      | 3.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.7e+04      |
| time/                   |               |
|    total_timesteps      | 1808500       |
| train/                  |               |
|    approx_kl            | 1.4110847e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+07      |
|    n_updates            | 8830          |
|    policy_gradient_loss | -0.000299     |
|    std                  | 1.55          |
|    value_loss           | 5.25e+07      |
-------------------------------------------
Eval num_timesteps=1809000, episode_reward=-64912.20 +/- 32569.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41848096 |
|    mean velocity x | -0.0965     |
|    mean velocity y | 0.518       |
|    mean velocity z | 1.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.49e+04   |
| time/              |             |
|    total_timesteps | 1809000     |
------------------------------------
Eval num_timesteps=1809500, episode_reward=-97401.93 +/- 20796.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31762958 |
|    mean velocity x | -0.287      |
|    mean velocity y | 0.474       |
|    mean velocity z | 0.622       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.74e+04   |
| time/              |             |
|    total_timesteps | 1809500     |
------------------------------------
Eval num_timesteps=1810000, episode_reward=-96204.59 +/- 26308.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41459388 |
|    mean velocity x | -1.58       |
|    mean velocity y | -0.961      |
|    mean velocity z | 6.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.62e+04   |
| time/              |             |
|    total_timesteps | 1810000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 884     |
|    time_elapsed    | 72904   |
|    total_timesteps | 1810432 |
--------------------------------
Eval num_timesteps=1810500, episode_reward=-90841.06 +/- 18125.73
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3135716   |
|    mean velocity x      | -0.976       |
|    mean velocity y      | -0.651       |
|    mean velocity z      | 3.54         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.08e+04    |
| time/                   |              |
|    total_timesteps      | 1810500      |
| train/                  |              |
|    approx_kl            | 8.293777e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.495        |
|    learning_rate        | 0.001        |
|    loss                 | 4.13e+06     |
|    n_updates            | 8840         |
|    policy_gradient_loss | -0.000682    |
|    std                  | 1.55         |
|    value_loss           | 2.48e+07     |
------------------------------------------
Eval num_timesteps=1811000, episode_reward=-57876.37 +/- 45962.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50590837 |
|    mean velocity x | -0.493      |
|    mean velocity y | 0.591       |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.79e+04   |
| time/              |             |
|    total_timesteps | 1811000     |
------------------------------------
Eval num_timesteps=1811500, episode_reward=-78946.82 +/- 46845.12
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3627246 |
|    mean velocity x | -0.708     |
|    mean velocity y | -0.235     |
|    mean velocity z | 3.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 1811500    |
-----------------------------------
Eval num_timesteps=1812000, episode_reward=-59011.94 +/- 47391.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37178174 |
|    mean velocity x | -1.52       |
|    mean velocity y | -0.082      |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.9e+04    |
| time/              |             |
|    total_timesteps | 1812000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 885     |
|    time_elapsed    | 72985   |
|    total_timesteps | 1812480 |
--------------------------------
Eval num_timesteps=1812500, episode_reward=-82131.46 +/- 20830.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35708404   |
|    mean velocity x      | -0.33         |
|    mean velocity y      | 0.271         |
|    mean velocity z      | 3.98          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.21e+04     |
| time/                   |               |
|    total_timesteps      | 1812500       |
| train/                  |               |
|    approx_kl            | 1.8521474e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.322         |
|    learning_rate        | 0.001         |
|    loss                 | 5.71e+07      |
|    n_updates            | 8850          |
|    policy_gradient_loss | -0.000471     |
|    std                  | 1.54          |
|    value_loss           | 6.6e+07       |
-------------------------------------------
Eval num_timesteps=1813000, episode_reward=-96261.96 +/- 12536.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2949697 |
|    mean velocity x | -0.0443    |
|    mean velocity y | 0.32       |
|    mean velocity z | 3.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.63e+04  |
| time/              |            |
|    total_timesteps | 1813000    |
-----------------------------------
Eval num_timesteps=1813500, episode_reward=-86179.07 +/- 30415.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3329996 |
|    mean velocity x | 0.0144     |
|    mean velocity y | 0.406      |
|    mean velocity z | 1.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.62e+04  |
| time/              |            |
|    total_timesteps | 1813500    |
-----------------------------------
Eval num_timesteps=1814000, episode_reward=-91709.64 +/- 51474.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2549904 |
|    mean velocity x | 0.0705     |
|    mean velocity y | 0.284      |
|    mean velocity z | 1.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.17e+04  |
| time/              |            |
|    total_timesteps | 1814000    |
-----------------------------------
Eval num_timesteps=1814500, episode_reward=-83009.27 +/- 28172.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11528862 |
|    mean velocity x | -1.84       |
|    mean velocity y | -1.93       |
|    mean velocity z | 6.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.3e+04    |
| time/              |             |
|    total_timesteps | 1814500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 886     |
|    time_elapsed    | 73084   |
|    total_timesteps | 1814528 |
--------------------------------
Eval num_timesteps=1815000, episode_reward=-63165.86 +/- 32377.43
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.20769691 |
|    mean velocity x      | -0.251      |
|    mean velocity y      | 0.36        |
|    mean velocity z      | 0.655       |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.32e+04   |
| time/                   |             |
|    total_timesteps      | 1815000     |
| train/                  |             |
|    approx_kl            | 3.27022e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.001       |
|    loss                 | 1.38e+07    |
|    n_updates            | 8860        |
|    policy_gradient_loss | -0.000448   |
|    std                  | 1.54        |
|    value_loss           | 3.12e+07    |
-----------------------------------------
Eval num_timesteps=1815500, episode_reward=-59875.45 +/- 30019.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42436427 |
|    mean velocity x | -0.00845    |
|    mean velocity y | 1.51        |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.99e+04   |
| time/              |             |
|    total_timesteps | 1815500     |
------------------------------------
Eval num_timesteps=1816000, episode_reward=-89094.63 +/- 23475.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44771418 |
|    mean velocity x | -0.354      |
|    mean velocity y | 0.687       |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.91e+04   |
| time/              |             |
|    total_timesteps | 1816000     |
------------------------------------
Eval num_timesteps=1816500, episode_reward=-84158.89 +/- 19155.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36757296 |
|    mean velocity x | -0.211      |
|    mean velocity y | 0.518       |
|    mean velocity z | 2.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 1816500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 887     |
|    time_elapsed    | 73165   |
|    total_timesteps | 1816576 |
--------------------------------
Eval num_timesteps=1817000, episode_reward=-68538.89 +/- 32292.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.353133     |
|    mean velocity x      | -0.177        |
|    mean velocity y      | 0.71          |
|    mean velocity z      | 4.36          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.85e+04     |
| time/                   |               |
|    total_timesteps      | 1817000       |
| train/                  |               |
|    approx_kl            | 1.8745486e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.262         |
|    learning_rate        | 0.001         |
|    loss                 | 3.03e+07      |
|    n_updates            | 8870          |
|    policy_gradient_loss | -8.16e-05     |
|    std                  | 1.54          |
|    value_loss           | 8.53e+07      |
-------------------------------------------
Eval num_timesteps=1817500, episode_reward=-96700.90 +/- 21046.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36045644 |
|    mean velocity x | 0.766       |
|    mean velocity y | 1.54        |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.67e+04   |
| time/              |             |
|    total_timesteps | 1817500     |
------------------------------------
Eval num_timesteps=1818000, episode_reward=-61729.46 +/- 28269.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35084382 |
|    mean velocity x | -0.0397     |
|    mean velocity y | 0.737       |
|    mean velocity z | 3.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+04   |
| time/              |             |
|    total_timesteps | 1818000     |
------------------------------------
Eval num_timesteps=1818500, episode_reward=-92623.63 +/- 33082.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3640534 |
|    mean velocity x | 0.0895     |
|    mean velocity y | 0.687      |
|    mean velocity z | 3.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.26e+04  |
| time/              |            |
|    total_timesteps | 1818500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 888     |
|    time_elapsed    | 73245   |
|    total_timesteps | 1818624 |
--------------------------------
Eval num_timesteps=1819000, episode_reward=-69000.54 +/- 38155.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40193218  |
|    mean velocity x      | -0.0965      |
|    mean velocity y      | 0.902        |
|    mean velocity z      | 4.25         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.9e+04     |
| time/                   |              |
|    total_timesteps      | 1819000      |
| train/                  |              |
|    approx_kl            | 5.343056e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.001        |
|    loss                 | 7.64e+07     |
|    n_updates            | 8880         |
|    policy_gradient_loss | -9.56e-05    |
|    std                  | 1.54         |
|    value_loss           | 6.86e+07     |
------------------------------------------
Eval num_timesteps=1819500, episode_reward=-52211.50 +/- 39526.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37016377 |
|    mean velocity x | -0.658      |
|    mean velocity y | 0.702       |
|    mean velocity z | 2.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.22e+04   |
| time/              |             |
|    total_timesteps | 1819500     |
------------------------------------
Eval num_timesteps=1820000, episode_reward=-100708.52 +/- 53786.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4080274 |
|    mean velocity x | -0.36      |
|    mean velocity y | 0.725      |
|    mean velocity z | 4.44       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 1820000    |
-----------------------------------
Eval num_timesteps=1820500, episode_reward=-75138.66 +/- 14635.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33088335 |
|    mean velocity x | -0.0223     |
|    mean velocity y | 0.482       |
|    mean velocity z | 3.67        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1820500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 889     |
|    time_elapsed    | 73325   |
|    total_timesteps | 1820672 |
--------------------------------
Eval num_timesteps=1821000, episode_reward=-103425.68 +/- 14515.59
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48516262  |
|    mean velocity x      | -0.221       |
|    mean velocity y      | 1.62         |
|    mean velocity z      | 4.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.03e+05    |
| time/                   |              |
|    total_timesteps      | 1821000      |
| train/                  |              |
|    approx_kl            | 7.869559e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.001        |
|    loss                 | 2.01e+06     |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.0002      |
|    std                  | 1.54         |
|    value_loss           | 6.39e+07     |
------------------------------------------
Eval num_timesteps=1821500, episode_reward=-103922.50 +/- 31355.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55677384 |
|    mean velocity x | 0.791       |
|    mean velocity y | 2.22        |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1821500     |
------------------------------------
Eval num_timesteps=1822000, episode_reward=-41064.20 +/- 35681.04
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4654548 |
|    mean velocity x | -0.398     |
|    mean velocity y | 1.01       |
|    mean velocity z | 4.77       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.11e+04  |
| time/              |            |
|    total_timesteps | 1822000    |
-----------------------------------
Eval num_timesteps=1822500, episode_reward=-80736.35 +/- 34557.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49747685 |
|    mean velocity x | -0.517      |
|    mean velocity y | 0.932       |
|    mean velocity z | 4.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 1822500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 890     |
|    time_elapsed    | 73406   |
|    total_timesteps | 1822720 |
--------------------------------
Eval num_timesteps=1823000, episode_reward=-98220.18 +/- 21983.42
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.41699272    |
|    mean velocity x      | -0.445         |
|    mean velocity y      | 0.605          |
|    mean velocity z      | 4.07           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -9.82e+04      |
| time/                   |                |
|    total_timesteps      | 1823000        |
| train/                  |                |
|    approx_kl            | 1.18569005e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.55          |
|    explained_variance   | 0.321          |
|    learning_rate        | 0.001          |
|    loss                 | 3.21e+07       |
|    n_updates            | 8900           |
|    policy_gradient_loss | -0.000305      |
|    std                  | 1.54           |
|    value_loss           | 6.69e+07       |
--------------------------------------------
Eval num_timesteps=1823500, episode_reward=-84345.34 +/- 51392.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2182587 |
|    mean velocity x | -0.107     |
|    mean velocity y | 0.332      |
|    mean velocity z | 0.569      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 1823500    |
-----------------------------------
Eval num_timesteps=1824000, episode_reward=-92749.00 +/- 27936.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45385247 |
|    mean velocity x | -0.378      |
|    mean velocity y | 0.938       |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.27e+04   |
| time/              |             |
|    total_timesteps | 1824000     |
------------------------------------
Eval num_timesteps=1824500, episode_reward=-66226.00 +/- 36038.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33428642 |
|    mean velocity x | -0.546      |
|    mean velocity y | 0.962       |
|    mean velocity z | 0.944       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.62e+04   |
| time/              |             |
|    total_timesteps | 1824500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 891     |
|    time_elapsed    | 73486   |
|    total_timesteps | 1824768 |
--------------------------------
Eval num_timesteps=1825000, episode_reward=-83643.43 +/- 25926.40
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3782385    |
|    mean velocity x      | -0.254        |
|    mean velocity y      | 0.792         |
|    mean velocity z      | 2.3           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.36e+04     |
| time/                   |               |
|    total_timesteps      | 1825000       |
| train/                  |               |
|    approx_kl            | 3.4675177e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.001         |
|    loss                 | 2.52e+07      |
|    n_updates            | 8910          |
|    policy_gradient_loss | -7.89e-05     |
|    std                  | 1.54          |
|    value_loss           | 2.73e+07      |
-------------------------------------------
Eval num_timesteps=1825500, episode_reward=-96264.91 +/- 34427.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40964407 |
|    mean velocity x | -0.427      |
|    mean velocity y | 0.225       |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.63e+04   |
| time/              |             |
|    total_timesteps | 1825500     |
------------------------------------
Eval num_timesteps=1826000, episode_reward=-102865.89 +/- 16410.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3399565 |
|    mean velocity x | -0.216     |
|    mean velocity y | 0.249      |
|    mean velocity z | 3.84       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1826000    |
-----------------------------------
Eval num_timesteps=1826500, episode_reward=-76727.23 +/- 41790.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6011266 |
|    mean velocity x | -0.57      |
|    mean velocity y | 0.813      |
|    mean velocity z | 4.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.67e+04  |
| time/              |            |
|    total_timesteps | 1826500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 892     |
|    time_elapsed    | 73566   |
|    total_timesteps | 1826816 |
--------------------------------
Eval num_timesteps=1827000, episode_reward=-107358.26 +/- 42066.02
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37504736   |
|    mean velocity x      | 0.331         |
|    mean velocity y      | 1.57          |
|    mean velocity z      | 3.53          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.07e+05     |
| time/                   |               |
|    total_timesteps      | 1827000       |
| train/                  |               |
|    approx_kl            | 2.2286258e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 2.97e+07      |
|    n_updates            | 8920          |
|    policy_gradient_loss | -0.000132     |
|    std                  | 1.54          |
|    value_loss           | 7.05e+07      |
-------------------------------------------
Eval num_timesteps=1827500, episode_reward=-69681.82 +/- 36505.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48851633 |
|    mean velocity x | -0.503      |
|    mean velocity y | 0.704       |
|    mean velocity z | 5.07        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.97e+04   |
| time/              |             |
|    total_timesteps | 1827500     |
------------------------------------
Eval num_timesteps=1828000, episode_reward=-70181.44 +/- 51217.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46299854 |
|    mean velocity x | -0.87       |
|    mean velocity y | 0.6         |
|    mean velocity z | 4.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.02e+04   |
| time/              |             |
|    total_timesteps | 1828000     |
------------------------------------
Eval num_timesteps=1828500, episode_reward=-125244.23 +/- 8356.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43589026 |
|    mean velocity x | -0.258      |
|    mean velocity y | 0.899       |
|    mean velocity z | 4.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.25e+05   |
| time/              |             |
|    total_timesteps | 1828500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 893     |
|    time_elapsed    | 73647   |
|    total_timesteps | 1828864 |
--------------------------------
Eval num_timesteps=1829000, episode_reward=-85134.22 +/- 45422.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.48319218  |
|    mean velocity x      | -0.429       |
|    mean velocity y      | 0.705        |
|    mean velocity z      | 4.55         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.51e+04    |
| time/                   |              |
|    total_timesteps      | 1829000      |
| train/                  |              |
|    approx_kl            | 6.841219e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.001        |
|    loss                 | 2.16e+07     |
|    n_updates            | 8930         |
|    policy_gradient_loss | -0.000685    |
|    std                  | 1.54         |
|    value_loss           | 9.42e+07     |
------------------------------------------
Eval num_timesteps=1829500, episode_reward=-111022.72 +/- 21775.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40688556 |
|    mean velocity x | -0.987      |
|    mean velocity y | 0.144       |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1829500     |
------------------------------------
Eval num_timesteps=1830000, episode_reward=-88213.77 +/- 64283.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4091109 |
|    mean velocity x | -0.289     |
|    mean velocity y | 0.957      |
|    mean velocity z | 4.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.82e+04  |
| time/              |            |
|    total_timesteps | 1830000    |
-----------------------------------
Eval num_timesteps=1830500, episode_reward=-62260.56 +/- 47674.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39002398 |
|    mean velocity x | -0.521      |
|    mean velocity y | 0.107       |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.23e+04   |
| time/              |             |
|    total_timesteps | 1830500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 894     |
|    time_elapsed    | 73727   |
|    total_timesteps | 1830912 |
--------------------------------
Eval num_timesteps=1831000, episode_reward=-44398.78 +/- 48424.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50745547  |
|    mean velocity x      | 0.12         |
|    mean velocity y      | 1.81         |
|    mean velocity z      | 3.69         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.44e+04    |
| time/                   |              |
|    total_timesteps      | 1831000      |
| train/                  |              |
|    approx_kl            | 4.970847e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.001        |
|    loss                 | 4.07e+07     |
|    n_updates            | 8940         |
|    policy_gradient_loss | -0.000111    |
|    std                  | 1.54         |
|    value_loss           | 6.51e+07     |
------------------------------------------
Eval num_timesteps=1831500, episode_reward=-87360.88 +/- 39653.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3034898 |
|    mean velocity x | -0.751     |
|    mean velocity y | 0.669      |
|    mean velocity z | 1.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.74e+04  |
| time/              |            |
|    total_timesteps | 1831500    |
-----------------------------------
Eval num_timesteps=1832000, episode_reward=-107757.81 +/- 26001.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30099803 |
|    mean velocity x | -2.19       |
|    mean velocity y | -1.17       |
|    mean velocity z | 5.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1832000     |
------------------------------------
Eval num_timesteps=1832500, episode_reward=-86388.75 +/- 29519.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3147505 |
|    mean velocity x | -0.772     |
|    mean velocity y | 0.13       |
|    mean velocity z | 3.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.64e+04  |
| time/              |            |
|    total_timesteps | 1832500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 895     |
|    time_elapsed    | 73808   |
|    total_timesteps | 1832960 |
--------------------------------
Eval num_timesteps=1833000, episode_reward=-78333.60 +/- 12353.54
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3462538    |
|    mean velocity x      | -0.0668       |
|    mean velocity y      | 0.507         |
|    mean velocity z      | 2.55          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.83e+04     |
| time/                   |               |
|    total_timesteps      | 1833000       |
| train/                  |               |
|    approx_kl            | 5.6379067e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.485         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+07      |
|    n_updates            | 8950          |
|    policy_gradient_loss | -0.00012      |
|    std                  | 1.55          |
|    value_loss           | 2.39e+07      |
-------------------------------------------
Eval num_timesteps=1833500, episode_reward=-81723.51 +/- 37975.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3483665 |
|    mean velocity x | -0.884     |
|    mean velocity y | -0.359     |
|    mean velocity z | 3.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.17e+04  |
| time/              |            |
|    total_timesteps | 1833500    |
-----------------------------------
Eval num_timesteps=1834000, episode_reward=-89710.39 +/- 15309.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48859796 |
|    mean velocity x | -0.399      |
|    mean velocity y | 1.13        |
|    mean velocity z | 4.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 1834000     |
------------------------------------
Eval num_timesteps=1834500, episode_reward=-50615.71 +/- 34236.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5083389 |
|    mean velocity x | 0.0879     |
|    mean velocity y | 1.38       |
|    mean velocity z | 3.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.06e+04  |
| time/              |            |
|    total_timesteps | 1834500    |
-----------------------------------
Eval num_timesteps=1835000, episode_reward=-62833.38 +/- 49857.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43319273 |
|    mean velocity x | -0.463      |
|    mean velocity y | 0.588       |
|    mean velocity z | 3.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 1835000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 896     |
|    time_elapsed    | 73907   |
|    total_timesteps | 1835008 |
--------------------------------
Eval num_timesteps=1835500, episode_reward=-65297.53 +/- 43786.13
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2371297    |
|    mean velocity x      | -0.539        |
|    mean velocity y      | 0.619         |
|    mean velocity z      | 1.48          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.53e+04     |
| time/                   |               |
|    total_timesteps      | 1835500       |
| train/                  |               |
|    approx_kl            | 5.3107797e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.001         |
|    loss                 | 1.98e+07      |
|    n_updates            | 8960          |
|    policy_gradient_loss | -0.000161     |
|    std                  | 1.55          |
|    value_loss           | 5.79e+07      |
-------------------------------------------
Eval num_timesteps=1836000, episode_reward=-70440.15 +/- 4162.84
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.508845 |
|    mean velocity x | -0.375    |
|    mean velocity y | 1.11      |
|    mean velocity z | 4.44      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.04e+04 |
| time/              |           |
|    total_timesteps | 1836000   |
----------------------------------
Eval num_timesteps=1836500, episode_reward=-68847.22 +/- 51363.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32410255 |
|    mean velocity x | -0.209      |
|    mean velocity y | 0.537       |
|    mean velocity z | 4.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.88e+04   |
| time/              |             |
|    total_timesteps | 1836500     |
------------------------------------
Eval num_timesteps=1837000, episode_reward=-76910.07 +/- 39379.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46356243 |
|    mean velocity x | -0.286      |
|    mean velocity y | 0.858       |
|    mean velocity z | 4.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 1837000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 897     |
|    time_elapsed    | 73988   |
|    total_timesteps | 1837056 |
--------------------------------
Eval num_timesteps=1837500, episode_reward=-70889.16 +/- 27564.55
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.43319276   |
|    mean velocity x      | -0.698        |
|    mean velocity y      | 0.31          |
|    mean velocity z      | 3.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.09e+04     |
| time/                   |               |
|    total_timesteps      | 1837500       |
| train/                  |               |
|    approx_kl            | 1.3046316e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.001         |
|    loss                 | 2.96e+07      |
|    n_updates            | 8970          |
|    policy_gradient_loss | -0.000404     |
|    std                  | 1.55          |
|    value_loss           | 1.1e+08       |
-------------------------------------------
Eval num_timesteps=1838000, episode_reward=-64498.63 +/- 31464.99
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.435724 |
|    mean velocity x | -0.153    |
|    mean velocity y | 1.17      |
|    mean velocity z | 4.5       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.45e+04 |
| time/              |           |
|    total_timesteps | 1838000   |
----------------------------------
Eval num_timesteps=1838500, episode_reward=-65264.95 +/- 35295.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.60814756 |
|    mean velocity x | 0.24        |
|    mean velocity y | 2.27        |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.53e+04   |
| time/              |             |
|    total_timesteps | 1838500     |
------------------------------------
Eval num_timesteps=1839000, episode_reward=-144733.37 +/- 54491.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27884158 |
|    mean velocity x | -1.41       |
|    mean velocity y | -0.861      |
|    mean velocity z | 4.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.45e+05   |
| time/              |             |
|    total_timesteps | 1839000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 898     |
|    time_elapsed    | 74068   |
|    total_timesteps | 1839104 |
--------------------------------
Eval num_timesteps=1839500, episode_reward=-49829.73 +/- 40770.90
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48534903   |
|    mean velocity x      | -0.286        |
|    mean velocity y      | 1.34          |
|    mean velocity z      | 4.22          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.98e+04     |
| time/                   |               |
|    total_timesteps      | 1839500       |
| train/                  |               |
|    approx_kl            | 4.7479116e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.001         |
|    loss                 | 2.5e+07       |
|    n_updates            | 8980          |
|    policy_gradient_loss | -0.000131     |
|    std                  | 1.55          |
|    value_loss           | 7.49e+07      |
-------------------------------------------
Eval num_timesteps=1840000, episode_reward=-89386.38 +/- 22617.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25032204 |
|    mean velocity x | -0.335      |
|    mean velocity y | 0.637       |
|    mean velocity z | 0.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.94e+04   |
| time/              |             |
|    total_timesteps | 1840000     |
------------------------------------
Eval num_timesteps=1840500, episode_reward=-71687.89 +/- 16538.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27540088 |
|    mean velocity x | -0.0356     |
|    mean velocity y | 0.267       |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.17e+04   |
| time/              |             |
|    total_timesteps | 1840500     |
------------------------------------
Eval num_timesteps=1841000, episode_reward=-80713.28 +/- 32222.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2154215 |
|    mean velocity x | -0.198     |
|    mean velocity y | 0.371      |
|    mean velocity z | 0.123      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 1841000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 899     |
|    time_elapsed    | 74149   |
|    total_timesteps | 1841152 |
--------------------------------
Eval num_timesteps=1841500, episode_reward=-71269.14 +/- 39006.33
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42577085  |
|    mean velocity x      | -0.276       |
|    mean velocity y      | 0.711        |
|    mean velocity z      | 3.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.13e+04    |
| time/                   |              |
|    total_timesteps      | 1841500      |
| train/                  |              |
|    approx_kl            | 9.021169e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+06     |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.000182    |
|    std                  | 1.55         |
|    value_loss           | 4.18e+07     |
------------------------------------------
Eval num_timesteps=1842000, episode_reward=-62728.23 +/- 33273.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5129885 |
|    mean velocity x | -0.753     |
|    mean velocity y | 0.6        |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.27e+04  |
| time/              |            |
|    total_timesteps | 1842000    |
-----------------------------------
Eval num_timesteps=1842500, episode_reward=-67565.12 +/- 36281.61
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.7313411 |
|    mean velocity x | 0.436      |
|    mean velocity y | 2.74       |
|    mean velocity z | 6.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.76e+04  |
| time/              |            |
|    total_timesteps | 1842500    |
-----------------------------------
Eval num_timesteps=1843000, episode_reward=-92474.78 +/- 38486.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2991252 |
|    mean velocity x | -0.721     |
|    mean velocity y | -0.532     |
|    mean velocity z | 3.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.25e+04  |
| time/              |            |
|    total_timesteps | 1843000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 900     |
|    time_elapsed    | 74229   |
|    total_timesteps | 1843200 |
--------------------------------
Eval num_timesteps=1843500, episode_reward=-45594.91 +/- 38941.90
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.30805993  |
|    mean velocity x      | 0.0468       |
|    mean velocity y      | 0.642        |
|    mean velocity z      | 3.72         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.56e+04    |
| time/                   |              |
|    total_timesteps      | 1843500      |
| train/                  |              |
|    approx_kl            | 3.222056e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.383        |
|    learning_rate        | 0.001        |
|    loss                 | 1.69e+07     |
|    n_updates            | 9000         |
|    policy_gradient_loss | -9.18e-05    |
|    std                  | 1.55         |
|    value_loss           | 5.7e+07      |
------------------------------------------
Eval num_timesteps=1844000, episode_reward=-98856.87 +/- 15178.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36114752 |
|    mean velocity x | -0.33       |
|    mean velocity y | 0.612       |
|    mean velocity z | 2.73        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.89e+04   |
| time/              |             |
|    total_timesteps | 1844000     |
------------------------------------
Eval num_timesteps=1844500, episode_reward=-58714.56 +/- 33941.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20159152 |
|    mean velocity x | -0.0198     |
|    mean velocity y | 0.44        |
|    mean velocity z | 0.383       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.87e+04   |
| time/              |             |
|    total_timesteps | 1844500     |
------------------------------------
Eval num_timesteps=1845000, episode_reward=-107810.91 +/- 19019.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41703907 |
|    mean velocity x | -0.218      |
|    mean velocity y | 0.847       |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 1845000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 901     |
|    time_elapsed    | 74309   |
|    total_timesteps | 1845248 |
--------------------------------
Eval num_timesteps=1845500, episode_reward=-79532.38 +/- 10766.22
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2970178   |
|    mean velocity x      | -0.145       |
|    mean velocity y      | 0.181        |
|    mean velocity z      | 3.73         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.95e+04    |
| time/                   |              |
|    total_timesteps      | 1845500      |
| train/                  |              |
|    approx_kl            | 1.186374e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.001        |
|    loss                 | 6.49e+07     |
|    n_updates            | 9010         |
|    policy_gradient_loss | -0.000138    |
|    std                  | 1.55         |
|    value_loss           | 6.06e+07     |
------------------------------------------
Eval num_timesteps=1846000, episode_reward=-45453.84 +/- 25272.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49849585 |
|    mean velocity x | -0.331      |
|    mean velocity y | 1.02        |
|    mean velocity z | 4.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.55e+04   |
| time/              |             |
|    total_timesteps | 1846000     |
------------------------------------
Eval num_timesteps=1846500, episode_reward=-83994.14 +/- 37014.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3880102 |
|    mean velocity x | -0.185     |
|    mean velocity y | 1.01       |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.4e+04   |
| time/              |            |
|    total_timesteps | 1846500    |
-----------------------------------
Eval num_timesteps=1847000, episode_reward=-67778.78 +/- 31549.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29866028 |
|    mean velocity x | -0.306      |
|    mean velocity y | -0.0277     |
|    mean velocity z | 3.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.78e+04   |
| time/              |             |
|    total_timesteps | 1847000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 902     |
|    time_elapsed    | 74390   |
|    total_timesteps | 1847296 |
--------------------------------
Eval num_timesteps=1847500, episode_reward=-100227.06 +/- 31263.90
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.28278044    |
|    mean velocity x      | -0.169         |
|    mean velocity y      | 0.592          |
|    mean velocity z      | 1.58           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1e+05         |
| time/                   |                |
|    total_timesteps      | 1847500        |
| train/                  |                |
|    approx_kl            | 1.50918495e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.55          |
|    explained_variance   | 0.238          |
|    learning_rate        | 0.001          |
|    loss                 | 2.84e+07       |
|    n_updates            | 9020           |
|    policy_gradient_loss | -0.000268      |
|    std                  | 1.55           |
|    value_loss           | 6.55e+07       |
--------------------------------------------
Eval num_timesteps=1848000, episode_reward=-78751.93 +/- 23574.62
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4887262 |
|    mean velocity x | -0.531     |
|    mean velocity y | 0.724      |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.88e+04  |
| time/              |            |
|    total_timesteps | 1848000    |
-----------------------------------
Eval num_timesteps=1848500, episode_reward=-86427.27 +/- 31178.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32414645 |
|    mean velocity x | -0.737      |
|    mean velocity y | 0.0905      |
|    mean velocity z | 3.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 1848500     |
------------------------------------
Eval num_timesteps=1849000, episode_reward=-90387.83 +/- 46563.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45042986 |
|    mean velocity x | -0.14       |
|    mean velocity y | 1.41        |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.04e+04   |
| time/              |             |
|    total_timesteps | 1849000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 903     |
|    time_elapsed    | 74471   |
|    total_timesteps | 1849344 |
--------------------------------
Eval num_timesteps=1849500, episode_reward=-118063.25 +/- 36278.62
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35786322  |
|    mean velocity x      | -0.267       |
|    mean velocity y      | 0.482        |
|    mean velocity z      | 3.98         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.18e+05    |
| time/                   |              |
|    total_timesteps      | 1849500      |
| train/                  |              |
|    approx_kl            | 8.846255e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+07     |
|    n_updates            | 9030         |
|    policy_gradient_loss | -0.00034     |
|    std                  | 1.55         |
|    value_loss           | 6.45e+07     |
------------------------------------------
Eval num_timesteps=1850000, episode_reward=-104777.29 +/- 23996.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39646026 |
|    mean velocity x | -0.64       |
|    mean velocity y | -0.171      |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1850000     |
------------------------------------
Eval num_timesteps=1850500, episode_reward=-77713.31 +/- 45798.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29173595 |
|    mean velocity x | -0.167      |
|    mean velocity y | 0.381       |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 1850500     |
------------------------------------
Eval num_timesteps=1851000, episode_reward=-52441.92 +/- 44173.50
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35038766 |
|    mean velocity x | -0.489      |
|    mean velocity y | -0.117      |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.24e+04   |
| time/              |             |
|    total_timesteps | 1851000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 904     |
|    time_elapsed    | 74551   |
|    total_timesteps | 1851392 |
--------------------------------
Eval num_timesteps=1851500, episode_reward=-101398.22 +/- 17119.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44885546   |
|    mean velocity x      | -0.387        |
|    mean velocity y      | 0.699         |
|    mean velocity z      | 4.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+05     |
| time/                   |               |
|    total_timesteps      | 1851500       |
| train/                  |               |
|    approx_kl            | 7.6822296e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.241         |
|    learning_rate        | 0.001         |
|    loss                 | 7.42e+07      |
|    n_updates            | 9040          |
|    policy_gradient_loss | -0.000836     |
|    std                  | 1.55          |
|    value_loss           | 8.33e+07      |
-------------------------------------------
Eval num_timesteps=1852000, episode_reward=-94686.43 +/- 49384.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37124345 |
|    mean velocity x | 0.391       |
|    mean velocity y | 1.24        |
|    mean velocity z | 3.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.47e+04   |
| time/              |             |
|    total_timesteps | 1852000     |
------------------------------------
Eval num_timesteps=1852500, episode_reward=-60795.43 +/- 51553.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39359254 |
|    mean velocity x | 0.424       |
|    mean velocity y | 1.37        |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.08e+04   |
| time/              |             |
|    total_timesteps | 1852500     |
------------------------------------
Eval num_timesteps=1853000, episode_reward=-68070.31 +/- 44178.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4965968 |
|    mean velocity x | -0.494     |
|    mean velocity y | 0.821      |
|    mean velocity z | 4.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.81e+04  |
| time/              |            |
|    total_timesteps | 1853000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 905     |
|    time_elapsed    | 74632   |
|    total_timesteps | 1853440 |
--------------------------------
Eval num_timesteps=1853500, episode_reward=-98256.97 +/- 24254.39
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27815717   |
|    mean velocity x      | -0.18         |
|    mean velocity y      | 0.547         |
|    mean velocity z      | 1.13          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.83e+04     |
| time/                   |               |
|    total_timesteps      | 1853500       |
| train/                  |               |
|    approx_kl            | 1.4670397e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.001         |
|    loss                 | 9.59e+06      |
|    n_updates            | 9050          |
|    policy_gradient_loss | -0.000191     |
|    std                  | 1.55          |
|    value_loss           | 4.27e+07      |
-------------------------------------------
Eval num_timesteps=1854000, episode_reward=-69989.11 +/- 14850.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3810872 |
|    mean velocity x | 0.0274     |
|    mean velocity y | 0.528      |
|    mean velocity z | 1.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7e+04     |
| time/              |            |
|    total_timesteps | 1854000    |
-----------------------------------
Eval num_timesteps=1854500, episode_reward=-67972.57 +/- 37555.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39669377 |
|    mean velocity x | -0.18       |
|    mean velocity y | 0.678       |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.8e+04    |
| time/              |             |
|    total_timesteps | 1854500     |
------------------------------------
Eval num_timesteps=1855000, episode_reward=-76946.13 +/- 45516.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32813054 |
|    mean velocity x | -0.958      |
|    mean velocity y | -0.0697     |
|    mean velocity z | 3.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 1855000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 906     |
|    time_elapsed    | 74712   |
|    total_timesteps | 1855488 |
--------------------------------
Eval num_timesteps=1855500, episode_reward=-75119.03 +/- 36252.92
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45335546   |
|    mean velocity x      | 0.259         |
|    mean velocity y      | 1.34          |
|    mean velocity z      | 3.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.51e+04     |
| time/                   |               |
|    total_timesteps      | 1855500       |
| train/                  |               |
|    approx_kl            | 0.00021295567 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.001         |
|    loss                 | 7.11e+07      |
|    n_updates            | 9060          |
|    policy_gradient_loss | -0.0013       |
|    std                  | 1.55          |
|    value_loss           | 4.78e+07      |
-------------------------------------------
Eval num_timesteps=1856000, episode_reward=-107259.55 +/- 25679.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43850744 |
|    mean velocity x | -0.32       |
|    mean velocity y | 0.727       |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1856000     |
------------------------------------
Eval num_timesteps=1856500, episode_reward=-77819.38 +/- 36660.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35342744 |
|    mean velocity x | -1.1        |
|    mean velocity y | -0.0989     |
|    mean velocity z | 2.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 1856500     |
------------------------------------
Eval num_timesteps=1857000, episode_reward=-82367.42 +/- 36880.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38564524 |
|    mean velocity x | -0.751      |
|    mean velocity y | 0.245       |
|    mean velocity z | 3.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.24e+04   |
| time/              |             |
|    total_timesteps | 1857000     |
------------------------------------
Eval num_timesteps=1857500, episode_reward=-81194.41 +/- 66549.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45510948 |
|    mean velocity x | -0.266      |
|    mean velocity y | 1.1         |
|    mean velocity z | 4.55        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 1857500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 907     |
|    time_elapsed    | 74812   |
|    total_timesteps | 1857536 |
--------------------------------
Eval num_timesteps=1858000, episode_reward=-71000.71 +/- 54343.04
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46329793  |
|    mean velocity x      | -0.391       |
|    mean velocity y      | 1.05         |
|    mean velocity z      | 4.56         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.1e+04     |
| time/                   |              |
|    total_timesteps      | 1858000      |
| train/                  |              |
|    approx_kl            | 4.450849e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.001        |
|    loss                 | 5.64e+07     |
|    n_updates            | 9070         |
|    policy_gradient_loss | -0.00022     |
|    std                  | 1.55         |
|    value_loss           | 8.96e+07     |
------------------------------------------
Eval num_timesteps=1858500, episode_reward=-54578.28 +/- 24266.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19542646 |
|    mean velocity x | -0.165      |
|    mean velocity y | 0.361       |
|    mean velocity z | 0.278       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.46e+04   |
| time/              |             |
|    total_timesteps | 1858500     |
------------------------------------
Eval num_timesteps=1859000, episode_reward=-46827.83 +/- 38571.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32554412 |
|    mean velocity x | -0.342      |
|    mean velocity y | -0.0898     |
|    mean velocity z | 3.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.68e+04   |
| time/              |             |
|    total_timesteps | 1859000     |
------------------------------------
Eval num_timesteps=1859500, episode_reward=-64829.46 +/- 36285.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35471162 |
|    mean velocity x | -1.14       |
|    mean velocity y | -0.717      |
|    mean velocity z | 5.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 1859500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 908     |
|    time_elapsed    | 74892   |
|    total_timesteps | 1859584 |
--------------------------------
Eval num_timesteps=1860000, episode_reward=-88394.68 +/- 40853.87
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.40437192  |
|    mean velocity x      | -0.0131      |
|    mean velocity y      | 0.899        |
|    mean velocity z      | 4.18         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.84e+04    |
| time/                   |              |
|    total_timesteps      | 1860000      |
| train/                  |              |
|    approx_kl            | 9.312935e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.001        |
|    loss                 | 2.03e+06     |
|    n_updates            | 9080         |
|    policy_gradient_loss | -0.000236    |
|    std                  | 1.55         |
|    value_loss           | 5.71e+07     |
------------------------------------------
Eval num_timesteps=1860500, episode_reward=-106184.21 +/- 20361.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37120777 |
|    mean velocity x | -0.725      |
|    mean velocity y | 0.23        |
|    mean velocity z | 3.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1860500     |
------------------------------------
Eval num_timesteps=1861000, episode_reward=-75601.75 +/- 29684.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4642846 |
|    mean velocity x | -0.63      |
|    mean velocity y | 0.319      |
|    mean velocity z | 4.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.56e+04  |
| time/              |            |
|    total_timesteps | 1861000    |
-----------------------------------
Eval num_timesteps=1861500, episode_reward=-107884.41 +/- 2892.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3288544 |
|    mean velocity x | -0.42      |
|    mean velocity y | 0.0779     |
|    mean velocity z | 3.3        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 1861500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 909     |
|    time_elapsed    | 74972   |
|    total_timesteps | 1861632 |
--------------------------------
Eval num_timesteps=1862000, episode_reward=-60034.85 +/- 40357.39
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47435153   |
|    mean velocity x      | -0.489        |
|    mean velocity y      | 0.646         |
|    mean velocity z      | 4.73          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6e+04        |
| time/                   |               |
|    total_timesteps      | 1862000       |
| train/                  |               |
|    approx_kl            | 1.1532102e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.417         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+06      |
|    n_updates            | 9090          |
|    policy_gradient_loss | -0.000343     |
|    std                  | 1.55          |
|    value_loss           | 5.92e+07      |
-------------------------------------------
Eval num_timesteps=1862500, episode_reward=-64046.28 +/- 28438.01
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.267612 |
|    mean velocity x | -0.0774   |
|    mean velocity y | 0.461     |
|    mean velocity z | 1.72      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.4e+04  |
| time/              |           |
|    total_timesteps | 1862500   |
----------------------------------
Eval num_timesteps=1863000, episode_reward=-78188.96 +/- 44873.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48649985 |
|    mean velocity x | -0.201      |
|    mean velocity y | 0.847       |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 1863000     |
------------------------------------
Eval num_timesteps=1863500, episode_reward=-94026.12 +/- 40230.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42696285 |
|    mean velocity x | -0.495      |
|    mean velocity y | 0.886       |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.4e+04    |
| time/              |             |
|    total_timesteps | 1863500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 910     |
|    time_elapsed    | 75053   |
|    total_timesteps | 1863680 |
--------------------------------
Eval num_timesteps=1864000, episode_reward=-100054.24 +/- 10425.55
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.57287306  |
|    mean velocity x      | 0.655        |
|    mean velocity y      | 1.73         |
|    mean velocity z      | 3.65         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1e+05       |
| time/                   |              |
|    total_timesteps      | 1864000      |
| train/                  |              |
|    approx_kl            | 2.510802e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+07     |
|    n_updates            | 9100         |
|    policy_gradient_loss | -0.000387    |
|    std                  | 1.55         |
|    value_loss           | 3.85e+07     |
------------------------------------------
Eval num_timesteps=1864500, episode_reward=-70523.18 +/- 36334.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3340976 |
|    mean velocity x | -0.122     |
|    mean velocity y | 0.379      |
|    mean velocity z | 2.46       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.05e+04  |
| time/              |            |
|    total_timesteps | 1864500    |
-----------------------------------
Eval num_timesteps=1865000, episode_reward=-89043.03 +/- 15932.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38037914 |
|    mean velocity x | -1.21       |
|    mean velocity y | -0.373      |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.9e+04    |
| time/              |             |
|    total_timesteps | 1865000     |
------------------------------------
Eval num_timesteps=1865500, episode_reward=-45057.69 +/- 36970.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36783174 |
|    mean velocity x | -0.539      |
|    mean velocity y | 0.362       |
|    mean velocity z | 3.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.51e+04   |
| time/              |             |
|    total_timesteps | 1865500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 911     |
|    time_elapsed    | 75133   |
|    total_timesteps | 1865728 |
--------------------------------
Eval num_timesteps=1866000, episode_reward=-97796.92 +/- 7331.50
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37940395  |
|    mean velocity x      | 0.338        |
|    mean velocity y      | 1.32         |
|    mean velocity z      | 3.45         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.78e+04    |
| time/                   |              |
|    total_timesteps      | 1866000      |
| train/                  |              |
|    approx_kl            | 9.870593e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+07      |
|    n_updates            | 9110         |
|    policy_gradient_loss | -0.000196    |
|    std                  | 1.55         |
|    value_loss           | 3.49e+07     |
------------------------------------------
Eval num_timesteps=1866500, episode_reward=-90094.74 +/- 15649.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4600039 |
|    mean velocity x | -0.376     |
|    mean velocity y | 0.54       |
|    mean velocity z | 4.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.01e+04  |
| time/              |            |
|    total_timesteps | 1866500    |
-----------------------------------
Eval num_timesteps=1867000, episode_reward=-76221.89 +/- 39575.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53513294 |
|    mean velocity x | 0.419       |
|    mean velocity y | 1.78        |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.62e+04   |
| time/              |             |
|    total_timesteps | 1867000     |
------------------------------------
Eval num_timesteps=1867500, episode_reward=-72848.07 +/- 45683.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42309338 |
|    mean velocity x | 0.212       |
|    mean velocity y | 1.12        |
|    mean velocity z | 3.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 1867500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 912     |
|    time_elapsed    | 75214   |
|    total_timesteps | 1867776 |
--------------------------------
Eval num_timesteps=1868000, episode_reward=-87785.47 +/- 18897.26
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4631481   |
|    mean velocity x      | -0.387       |
|    mean velocity y      | 0.812        |
|    mean velocity z      | 3.48         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.78e+04    |
| time/                   |              |
|    total_timesteps      | 1868000      |
| train/                  |              |
|    approx_kl            | 1.032342e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.001        |
|    loss                 | 1.91e+07     |
|    n_updates            | 9120         |
|    policy_gradient_loss | -0.000227    |
|    std                  | 1.55         |
|    value_loss           | 6.16e+07     |
------------------------------------------
Eval num_timesteps=1868500, episode_reward=-61402.44 +/- 44826.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36108497 |
|    mean velocity x | -0.28       |
|    mean velocity y | 0.582       |
|    mean velocity z | 0.627       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.14e+04   |
| time/              |             |
|    total_timesteps | 1868500     |
------------------------------------
Eval num_timesteps=1869000, episode_reward=-83075.65 +/- 28425.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39617833 |
|    mean velocity x | -0.997      |
|    mean velocity y | -0.506      |
|    mean velocity z | 4.59        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 1869000     |
------------------------------------
Eval num_timesteps=1869500, episode_reward=-73641.49 +/- 37175.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35231477 |
|    mean velocity x | -0.149      |
|    mean velocity y | 0.69        |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1869500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 913     |
|    time_elapsed    | 75294   |
|    total_timesteps | 1869824 |
--------------------------------
Eval num_timesteps=1870000, episode_reward=-57676.21 +/- 42237.29
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.22751358   |
|    mean velocity x      | -0.917        |
|    mean velocity y      | 0.3           |
|    mean velocity z      | 1.88          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.77e+04     |
| time/                   |               |
|    total_timesteps      | 1870000       |
| train/                  |               |
|    approx_kl            | 2.2229302e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.45          |
|    learning_rate        | 0.001         |
|    loss                 | 3.49e+06      |
|    n_updates            | 9130          |
|    policy_gradient_loss | -0.000446     |
|    std                  | 1.55          |
|    value_loss           | 2.15e+07      |
-------------------------------------------
Eval num_timesteps=1870500, episode_reward=-80095.08 +/- 28961.23
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33103055 |
|    mean velocity x | -0.282      |
|    mean velocity y | 0.26        |
|    mean velocity z | 4.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 1870500     |
------------------------------------
Eval num_timesteps=1871000, episode_reward=-85637.19 +/- 25731.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48428082 |
|    mean velocity x | 0.544       |
|    mean velocity y | 1.54        |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 1871000     |
------------------------------------
Eval num_timesteps=1871500, episode_reward=-92313.07 +/- 28807.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49445835 |
|    mean velocity x | -0.358      |
|    mean velocity y | 0.759       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 1871500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 914     |
|    time_elapsed    | 75374   |
|    total_timesteps | 1871872 |
--------------------------------
Eval num_timesteps=1872000, episode_reward=-91361.19 +/- 32176.84
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.51509637  |
|    mean velocity x      | -0.468       |
|    mean velocity y      | 0.76         |
|    mean velocity z      | 4.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.14e+04    |
| time/                   |              |
|    total_timesteps      | 1872000      |
| train/                  |              |
|    approx_kl            | 5.017733e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.001        |
|    loss                 | 4.75e+07     |
|    n_updates            | 9140         |
|    policy_gradient_loss | -0.000209    |
|    std                  | 1.55         |
|    value_loss           | 6.38e+07     |
------------------------------------------
Eval num_timesteps=1872500, episode_reward=-80179.45 +/- 33906.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46479666 |
|    mean velocity x | 0.288       |
|    mean velocity y | 1.29        |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.02e+04   |
| time/              |             |
|    total_timesteps | 1872500     |
------------------------------------
Eval num_timesteps=1873000, episode_reward=-88318.49 +/- 46647.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3627645 |
|    mean velocity x | -0.688     |
|    mean velocity y | 0.58       |
|    mean velocity z | 2.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.83e+04  |
| time/              |            |
|    total_timesteps | 1873000    |
-----------------------------------
Eval num_timesteps=1873500, episode_reward=-40326.81 +/- 33640.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26837933 |
|    mean velocity x | -0.12       |
|    mean velocity y | 0.528       |
|    mean velocity z | 0.315       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.03e+04   |
| time/              |             |
|    total_timesteps | 1873500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 915     |
|    time_elapsed    | 75455   |
|    total_timesteps | 1873920 |
--------------------------------
Eval num_timesteps=1874000, episode_reward=-73591.55 +/- 38754.50
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34803447  |
|    mean velocity x      | 0.121        |
|    mean velocity y      | 0.964        |
|    mean velocity z      | 3.94         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.36e+04    |
| time/                   |              |
|    total_timesteps      | 1874000      |
| train/                  |              |
|    approx_kl            | 8.802046e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+07     |
|    n_updates            | 9150         |
|    policy_gradient_loss | -0.000299    |
|    std                  | 1.55         |
|    value_loss           | 3.37e+07     |
------------------------------------------
Eval num_timesteps=1874500, episode_reward=-50645.36 +/- 37261.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.73592776 |
|    mean velocity x | 1.07        |
|    mean velocity y | 2.52        |
|    mean velocity z | 4.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 1874500     |
------------------------------------
Eval num_timesteps=1875000, episode_reward=-104831.77 +/- 49088.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4547764 |
|    mean velocity x | -0.972     |
|    mean velocity y | 0.278      |
|    mean velocity z | 3.57       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.05e+05  |
| time/              |            |
|    total_timesteps | 1875000    |
-----------------------------------
Eval num_timesteps=1875500, episode_reward=-44067.95 +/- 26871.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37340063 |
|    mean velocity x | -0.349      |
|    mean velocity y | 0.512       |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.41e+04   |
| time/              |             |
|    total_timesteps | 1875500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 916     |
|    time_elapsed    | 75535   |
|    total_timesteps | 1875968 |
--------------------------------
Eval num_timesteps=1876000, episode_reward=-63477.46 +/- 38865.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19890627   |
|    mean velocity x      | -0.0592       |
|    mean velocity y      | 0.354         |
|    mean velocity z      | 0.665         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.35e+04     |
| time/                   |               |
|    total_timesteps      | 1876000       |
| train/                  |               |
|    approx_kl            | 3.7315884e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.423         |
|    learning_rate        | 0.001         |
|    loss                 | 5.71e+06      |
|    n_updates            | 9160          |
|    policy_gradient_loss | -0.000471     |
|    std                  | 1.55          |
|    value_loss           | 3.97e+07      |
-------------------------------------------
Eval num_timesteps=1876500, episode_reward=-97010.14 +/- 20651.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44987723 |
|    mean velocity x | -0.235      |
|    mean velocity y | 0.865       |
|    mean velocity z | 4.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.7e+04    |
| time/              |             |
|    total_timesteps | 1876500     |
------------------------------------
Eval num_timesteps=1877000, episode_reward=-49022.34 +/- 39943.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3421919 |
|    mean velocity x | -2.46      |
|    mean velocity y | -1.34      |
|    mean velocity z | 7.17       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.9e+04   |
| time/              |            |
|    total_timesteps | 1877000    |
-----------------------------------
Eval num_timesteps=1877500, episode_reward=-89954.51 +/- 23293.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4568449 |
|    mean velocity x | -0.00896   |
|    mean velocity y | 1.15       |
|    mean velocity z | 4.08       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9e+04     |
| time/              |            |
|    total_timesteps | 1877500    |
-----------------------------------
Eval num_timesteps=1878000, episode_reward=-95201.13 +/- 27556.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33546707 |
|    mean velocity x | -0.391      |
|    mean velocity y | 0.301       |
|    mean velocity z | 3.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.52e+04   |
| time/              |             |
|    total_timesteps | 1878000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 917     |
|    time_elapsed    | 75634   |
|    total_timesteps | 1878016 |
--------------------------------
Eval num_timesteps=1878500, episode_reward=-105781.15 +/- 27686.81
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45418712   |
|    mean velocity x      | 0.363         |
|    mean velocity y      | 1.44          |
|    mean velocity z      | 3.43          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.06e+05     |
| time/                   |               |
|    total_timesteps      | 1878500       |
| train/                  |               |
|    approx_kl            | 2.9966526e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.35          |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+07      |
|    n_updates            | 9170          |
|    policy_gradient_loss | -0.000284     |
|    std                  | 1.55          |
|    value_loss           | 8.34e+07      |
-------------------------------------------
Eval num_timesteps=1879000, episode_reward=-95078.97 +/- 37601.54
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4876713 |
|    mean velocity x | -0.504     |
|    mean velocity y | 0.826      |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 1879000    |
-----------------------------------
Eval num_timesteps=1879500, episode_reward=-67421.80 +/- 52267.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6572538 |
|    mean velocity x | -0.0719    |
|    mean velocity y | 1.45       |
|    mean velocity z | 6.34       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.74e+04  |
| time/              |            |
|    total_timesteps | 1879500    |
-----------------------------------
Eval num_timesteps=1880000, episode_reward=-56282.98 +/- 22257.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31476566 |
|    mean velocity x | -0.195      |
|    mean velocity y | 0.189       |
|    mean velocity z | 2.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.63e+04   |
| time/              |             |
|    total_timesteps | 1880000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 918     |
|    time_elapsed    | 75715   |
|    total_timesteps | 1880064 |
--------------------------------
Eval num_timesteps=1880500, episode_reward=-89108.94 +/- 11939.77
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.28335986  |
|    mean velocity x      | -0.536       |
|    mean velocity y      | -0.277       |
|    mean velocity z      | 3.43         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.91e+04    |
| time/                   |              |
|    total_timesteps      | 1880500      |
| train/                  |              |
|    approx_kl            | 5.900569e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.423        |
|    learning_rate        | 0.001        |
|    loss                 | 5.13e+07     |
|    n_updates            | 9180         |
|    policy_gradient_loss | -0.000164    |
|    std                  | 1.55         |
|    value_loss           | 5.21e+07     |
------------------------------------------
Eval num_timesteps=1881000, episode_reward=-118015.19 +/- 17897.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21239652 |
|    mean velocity x | -0.206      |
|    mean velocity y | 0.468       |
|    mean velocity z | 0.317       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 1881000     |
------------------------------------
Eval num_timesteps=1881500, episode_reward=-118189.83 +/- 15480.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44148475 |
|    mean velocity x | -0.259      |
|    mean velocity y | 0.932       |
|    mean velocity z | 4.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 1881500     |
------------------------------------
Eval num_timesteps=1882000, episode_reward=-95914.07 +/- 23042.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48141524 |
|    mean velocity x | -0.484      |
|    mean velocity y | 0.973       |
|    mean velocity z | 4.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.59e+04   |
| time/              |             |
|    total_timesteps | 1882000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 919     |
|    time_elapsed    | 75795   |
|    total_timesteps | 1882112 |
--------------------------------
Eval num_timesteps=1882500, episode_reward=-106057.72 +/- 13213.93
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34878814   |
|    mean velocity x      | 0.146         |
|    mean velocity y      | 0.357         |
|    mean velocity z      | 1.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.06e+05     |
| time/                   |               |
|    total_timesteps      | 1882500       |
| train/                  |               |
|    approx_kl            | 1.0778749e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 6.34e+07      |
|    n_updates            | 9190          |
|    policy_gradient_loss | -0.000281     |
|    std                  | 1.55          |
|    value_loss           | 6.03e+07      |
-------------------------------------------
Eval num_timesteps=1883000, episode_reward=-76277.66 +/- 61047.97
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4368145 |
|    mean velocity x | -0.302     |
|    mean velocity y | 0.666      |
|    mean velocity z | 3.3        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 1883000    |
-----------------------------------
Eval num_timesteps=1883500, episode_reward=-61790.05 +/- 35211.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41963062 |
|    mean velocity x | -0.224      |
|    mean velocity y | 1.2         |
|    mean velocity z | 4.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 1883500     |
------------------------------------
Eval num_timesteps=1884000, episode_reward=-54474.75 +/- 47991.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48102996 |
|    mean velocity x | -0.113      |
|    mean velocity y | 1.21        |
|    mean velocity z | 1.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.45e+04   |
| time/              |             |
|    total_timesteps | 1884000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 920     |
|    time_elapsed    | 75876   |
|    total_timesteps | 1884160 |
--------------------------------
Eval num_timesteps=1884500, episode_reward=-91320.17 +/- 45412.99
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44722992   |
|    mean velocity x      | -0.152        |
|    mean velocity y      | 0.758         |
|    mean velocity z      | 3.39          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.13e+04     |
| time/                   |               |
|    total_timesteps      | 1884500       |
| train/                  |               |
|    approx_kl            | 7.2327966e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+07      |
|    n_updates            | 9200          |
|    policy_gradient_loss | -0.00033      |
|    std                  | 1.55          |
|    value_loss           | 5.46e+07      |
-------------------------------------------
Eval num_timesteps=1885000, episode_reward=-62527.53 +/- 28657.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3895307 |
|    mean velocity x | -0.0509    |
|    mean velocity y | 1.16       |
|    mean velocity z | 4.15       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.25e+04  |
| time/              |            |
|    total_timesteps | 1885000    |
-----------------------------------
Eval num_timesteps=1885500, episode_reward=-74574.25 +/- 11528.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35615894 |
|    mean velocity x | -0.913      |
|    mean velocity y | -0.299      |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 1885500     |
------------------------------------
Eval num_timesteps=1886000, episode_reward=-84784.20 +/- 21675.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26271847 |
|    mean velocity x | -1.15       |
|    mean velocity y | -0.135      |
|    mean velocity z | 2.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.48e+04   |
| time/              |             |
|    total_timesteps | 1886000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 921     |
|    time_elapsed    | 75956   |
|    total_timesteps | 1886208 |
--------------------------------
Eval num_timesteps=1886500, episode_reward=-90421.20 +/- 46928.79
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5350244    |
|    mean velocity x      | -0.0836       |
|    mean velocity y      | 1.69          |
|    mean velocity z      | 3.98          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.04e+04     |
| time/                   |               |
|    total_timesteps      | 1886500       |
| train/                  |               |
|    approx_kl            | 3.8048893e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.421         |
|    learning_rate        | 0.001         |
|    loss                 | 2.23e+07      |
|    n_updates            | 9210          |
|    policy_gradient_loss | -0.000107     |
|    std                  | 1.55          |
|    value_loss           | 5.02e+07      |
-------------------------------------------
Eval num_timesteps=1887000, episode_reward=-67354.63 +/- 35985.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37917846 |
|    mean velocity x | 0.0654      |
|    mean velocity y | 0.815       |
|    mean velocity z | 2.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 1887000     |
------------------------------------
Eval num_timesteps=1887500, episode_reward=-68127.05 +/- 32487.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40058398 |
|    mean velocity x | 0.143       |
|    mean velocity y | 0.817       |
|    mean velocity z | 3.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1887500     |
------------------------------------
Eval num_timesteps=1888000, episode_reward=-58938.38 +/- 39567.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17152189 |
|    mean velocity x | -0.0843     |
|    mean velocity y | 0.421       |
|    mean velocity z | 0.197       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.89e+04   |
| time/              |             |
|    total_timesteps | 1888000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 922     |
|    time_elapsed    | 76036   |
|    total_timesteps | 1888256 |
--------------------------------
Eval num_timesteps=1888500, episode_reward=-51625.67 +/- 47627.18
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.33309442 |
|    mean velocity x      | -0.44       |
|    mean velocity y      | 0.686       |
|    mean velocity z      | 1.88        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.16e+04   |
| time/                   |             |
|    total_timesteps      | 1888500     |
| train/                  |             |
|    approx_kl            | 5.82161e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.001       |
|    loss                 | 7.16e+06    |
|    n_updates            | 9220        |
|    policy_gradient_loss | -8.7e-05    |
|    std                  | 1.55        |
|    value_loss           | 1.22e+07    |
-----------------------------------------
Eval num_timesteps=1889000, episode_reward=-98674.49 +/- 34989.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40175283 |
|    mean velocity x | -0.225      |
|    mean velocity y | 0.593       |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.87e+04   |
| time/              |             |
|    total_timesteps | 1889000     |
------------------------------------
Eval num_timesteps=1889500, episode_reward=-92328.87 +/- 20927.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20712534 |
|    mean velocity x | 0.0218      |
|    mean velocity y | 0.456       |
|    mean velocity z | 0.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 1889500     |
------------------------------------
Eval num_timesteps=1890000, episode_reward=-89386.99 +/- 24915.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28023925 |
|    mean velocity x | -1.66       |
|    mean velocity y | -1.47       |
|    mean velocity z | 6.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.94e+04   |
| time/              |             |
|    total_timesteps | 1890000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 923     |
|    time_elapsed    | 76117   |
|    total_timesteps | 1890304 |
--------------------------------
Eval num_timesteps=1890500, episode_reward=-98172.49 +/- 25363.60
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40598217   |
|    mean velocity x      | -0.317        |
|    mean velocity y      | 0.726         |
|    mean velocity z      | 4.86          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.82e+04     |
| time/                   |               |
|    total_timesteps      | 1890500       |
| train/                  |               |
|    approx_kl            | 1.0188436e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.368         |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+07      |
|    n_updates            | 9230          |
|    policy_gradient_loss | -0.000254     |
|    std                  | 1.55          |
|    value_loss           | 6.37e+07      |
-------------------------------------------
Eval num_timesteps=1891000, episode_reward=-93594.99 +/- 22376.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42137638 |
|    mean velocity x | -0.0961     |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.08        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 1891000     |
------------------------------------
Eval num_timesteps=1891500, episode_reward=-96153.15 +/- 21205.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35468563 |
|    mean velocity x | -0.957      |
|    mean velocity y | 0.193       |
|    mean velocity z | 2.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.62e+04   |
| time/              |             |
|    total_timesteps | 1891500     |
------------------------------------
Eval num_timesteps=1892000, episode_reward=-43452.31 +/- 29248.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28534666 |
|    mean velocity x | -0.454      |
|    mean velocity y | 0.543       |
|    mean velocity z | 2.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.35e+04   |
| time/              |             |
|    total_timesteps | 1892000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 924     |
|    time_elapsed    | 76197   |
|    total_timesteps | 1892352 |
--------------------------------
Eval num_timesteps=1892500, episode_reward=-94703.69 +/- 26806.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3449318    |
|    mean velocity x      | 0.0366        |
|    mean velocity y      | 0.88          |
|    mean velocity z      | 3.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.47e+04     |
| time/                   |               |
|    total_timesteps      | 1892500       |
| train/                  |               |
|    approx_kl            | 1.8684892e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.31          |
|    learning_rate        | 0.001         |
|    loss                 | 2.23e+07      |
|    n_updates            | 9240          |
|    policy_gradient_loss | -0.000196     |
|    std                  | 1.55          |
|    value_loss           | 4.41e+07      |
-------------------------------------------
Eval num_timesteps=1893000, episode_reward=-54921.35 +/- 27960.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46237445 |
|    mean velocity x | -0.884      |
|    mean velocity y | 0.454       |
|    mean velocity z | 3.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.49e+04   |
| time/              |             |
|    total_timesteps | 1893000     |
------------------------------------
Eval num_timesteps=1893500, episode_reward=-62556.60 +/- 22912.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36739254 |
|    mean velocity x | -1.02       |
|    mean velocity y | -0.296      |
|    mean velocity z | 3.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.26e+04   |
| time/              |             |
|    total_timesteps | 1893500     |
------------------------------------
Eval num_timesteps=1894000, episode_reward=-102823.61 +/- 17656.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3142398 |
|    mean velocity x | -0.00588   |
|    mean velocity y | 0.763      |
|    mean velocity z | 3.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1894000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 925     |
|    time_elapsed    | 76278   |
|    total_timesteps | 1894400 |
--------------------------------
Eval num_timesteps=1894500, episode_reward=-64503.71 +/- 45033.36
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35978684   |
|    mean velocity x      | -0.387        |
|    mean velocity y      | 0.589         |
|    mean velocity z      | 3.81          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.45e+04     |
| time/                   |               |
|    total_timesteps      | 1894500       |
| train/                  |               |
|    approx_kl            | 6.0248713e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.347         |
|    learning_rate        | 0.001         |
|    loss                 | 3.12e+07      |
|    n_updates            | 9250          |
|    policy_gradient_loss | -0.000146     |
|    std                  | 1.55          |
|    value_loss           | 6.05e+07      |
-------------------------------------------
Eval num_timesteps=1895000, episode_reward=-103410.41 +/- 38566.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3579864 |
|    mean velocity x | -0.408     |
|    mean velocity y | 0.461      |
|    mean velocity z | 3.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 1895000    |
-----------------------------------
Eval num_timesteps=1895500, episode_reward=-111948.10 +/- 18825.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45945555 |
|    mean velocity x | -0.312      |
|    mean velocity y | 0.891       |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 1895500     |
------------------------------------
Eval num_timesteps=1896000, episode_reward=-82884.06 +/- 40479.29
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.53556  |
|    mean velocity x | 0.333     |
|    mean velocity y | 1.89      |
|    mean velocity z | 3.66      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.29e+04 |
| time/              |           |
|    total_timesteps | 1896000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 926     |
|    time_elapsed    | 76358   |
|    total_timesteps | 1896448 |
--------------------------------
Eval num_timesteps=1896500, episode_reward=-76831.80 +/- 46654.94
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47175938   |
|    mean velocity x      | -0.294        |
|    mean velocity y      | 0.851         |
|    mean velocity z      | 4.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.68e+04     |
| time/                   |               |
|    total_timesteps      | 1896500       |
| train/                  |               |
|    approx_kl            | 5.3247262e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.001         |
|    loss                 | 6.35e+07      |
|    n_updates            | 9260          |
|    policy_gradient_loss | -0.000709     |
|    std                  | 1.55          |
|    value_loss           | 8.72e+07      |
-------------------------------------------
Eval num_timesteps=1897000, episode_reward=-86424.86 +/- 35653.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4272366 |
|    mean velocity x | -0.204     |
|    mean velocity y | 1.17       |
|    mean velocity z | 4.03       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.64e+04  |
| time/              |            |
|    total_timesteps | 1897000    |
-----------------------------------
Eval num_timesteps=1897500, episode_reward=-75707.74 +/- 37885.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42128283 |
|    mean velocity x | -0.316      |
|    mean velocity y | 0.851       |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 1897500     |
------------------------------------
Eval num_timesteps=1898000, episode_reward=-106827.87 +/- 29362.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39523897 |
|    mean velocity x | -0.349      |
|    mean velocity y | 0.332       |
|    mean velocity z | 4.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1898000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 927     |
|    time_elapsed    | 76438   |
|    total_timesteps | 1898496 |
--------------------------------
Eval num_timesteps=1898500, episode_reward=-70378.27 +/- 39210.06
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.17113201   |
|    mean velocity x      | -0.0868       |
|    mean velocity y      | 0.325         |
|    mean velocity z      | 0.268         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.04e+04     |
| time/                   |               |
|    total_timesteps      | 1898500       |
| train/                  |               |
|    approx_kl            | 3.9195438e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.001         |
|    loss                 | 4.43e+07      |
|    n_updates            | 9270          |
|    policy_gradient_loss | -0.000495     |
|    std                  | 1.55          |
|    value_loss           | 6.84e+07      |
-------------------------------------------
Eval num_timesteps=1899000, episode_reward=-106747.53 +/- 13396.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4674391 |
|    mean velocity x | -0.288     |
|    mean velocity y | 1.42       |
|    mean velocity z | 3.92       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 1899000    |
-----------------------------------
Eval num_timesteps=1899500, episode_reward=-52187.88 +/- 45697.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37699693 |
|    mean velocity x | -0.415      |
|    mean velocity y | 0.379       |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.22e+04   |
| time/              |             |
|    total_timesteps | 1899500     |
------------------------------------
Eval num_timesteps=1900000, episode_reward=-97338.12 +/- 16514.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33083805 |
|    mean velocity x | -0.0278     |
|    mean velocity y | 0.531       |
|    mean velocity z | 2           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.73e+04   |
| time/              |             |
|    total_timesteps | 1900000     |
------------------------------------
Eval num_timesteps=1900500, episode_reward=-54441.91 +/- 45854.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47170275 |
|    mean velocity x | 0.251       |
|    mean velocity y | 1.4         |
|    mean velocity z | 3.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.44e+04   |
| time/              |             |
|    total_timesteps | 1900500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 928     |
|    time_elapsed    | 76538   |
|    total_timesteps | 1900544 |
--------------------------------
Eval num_timesteps=1901000, episode_reward=-81569.36 +/- 22501.76
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.50487053   |
|    mean velocity x      | -0.422        |
|    mean velocity y      | 1.18          |
|    mean velocity z      | 4.54          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.16e+04     |
| time/                   |               |
|    total_timesteps      | 1901000       |
| train/                  |               |
|    approx_kl            | 1.0272401e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.001         |
|    loss                 | 8.78e+07      |
|    n_updates            | 9280          |
|    policy_gradient_loss | -0.000213     |
|    std                  | 1.55          |
|    value_loss           | 6.83e+07      |
-------------------------------------------
Eval num_timesteps=1901500, episode_reward=-88627.04 +/- 19860.80
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2398497 |
|    mean velocity x | 0.23       |
|    mean velocity y | 0.538      |
|    mean velocity z | 3.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.86e+04  |
| time/              |            |
|    total_timesteps | 1901500    |
-----------------------------------
Eval num_timesteps=1902000, episode_reward=-91599.49 +/- 23041.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4179302 |
|    mean velocity x | 0.126      |
|    mean velocity y | 0.561      |
|    mean velocity z | 2.37       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.16e+04  |
| time/              |            |
|    total_timesteps | 1902000    |
-----------------------------------
Eval num_timesteps=1902500, episode_reward=-43446.31 +/- 49821.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19187899 |
|    mean velocity x | -1.26       |
|    mean velocity y | -0.478      |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.34e+04   |
| time/              |             |
|    total_timesteps | 1902500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 929     |
|    time_elapsed    | 76618   |
|    total_timesteps | 1902592 |
--------------------------------
Eval num_timesteps=1903000, episode_reward=-91584.91 +/- 24646.19
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46111476   |
|    mean velocity x      | -0.42         |
|    mean velocity y      | 0.719         |
|    mean velocity z      | 3.02          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.16e+04     |
| time/                   |               |
|    total_timesteps      | 1903000       |
| train/                  |               |
|    approx_kl            | 7.3473784e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.398         |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+07      |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.000274     |
|    std                  | 1.55          |
|    value_loss           | 3.04e+07      |
-------------------------------------------
Eval num_timesteps=1903500, episode_reward=-91112.37 +/- 33052.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36422378 |
|    mean velocity x | -1.21       |
|    mean velocity y | -0.288      |
|    mean velocity z | 3.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.11e+04   |
| time/              |             |
|    total_timesteps | 1903500     |
------------------------------------
Eval num_timesteps=1904000, episode_reward=-103646.39 +/- 18146.48
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3002575 |
|    mean velocity x | -0.946     |
|    mean velocity y | -0.161     |
|    mean velocity z | 4.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1904000    |
-----------------------------------
Eval num_timesteps=1904500, episode_reward=-67712.35 +/- 33757.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45855024 |
|    mean velocity x | -0.416      |
|    mean velocity y | 0.771       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.77e+04   |
| time/              |             |
|    total_timesteps | 1904500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 930     |
|    time_elapsed    | 76699   |
|    total_timesteps | 1904640 |
--------------------------------
Eval num_timesteps=1905000, episode_reward=-73120.40 +/- 32130.43
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4043739   |
|    mean velocity x      | -0.128       |
|    mean velocity y      | 0.9          |
|    mean velocity z      | 0.831        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.31e+04    |
| time/                   |              |
|    total_timesteps      | 1905000      |
| train/                  |              |
|    approx_kl            | 8.206931e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.001        |
|    loss                 | 4.11e+06     |
|    n_updates            | 9300         |
|    policy_gradient_loss | -0.000268    |
|    std                  | 1.55         |
|    value_loss           | 5.36e+07     |
------------------------------------------
Eval num_timesteps=1905500, episode_reward=-80491.29 +/- 30540.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43807268 |
|    mean velocity x | 0.231       |
|    mean velocity y | 1.54        |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 1905500     |
------------------------------------
Eval num_timesteps=1906000, episode_reward=-60455.18 +/- 32604.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45148307 |
|    mean velocity x | -0.699      |
|    mean velocity y | 0.481       |
|    mean velocity z | 3.36        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.05e+04   |
| time/              |             |
|    total_timesteps | 1906000     |
------------------------------------
Eval num_timesteps=1906500, episode_reward=-74658.59 +/- 23245.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46736243 |
|    mean velocity x | -0.407      |
|    mean velocity y | 0.843       |
|    mean velocity z | 4.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.47e+04   |
| time/              |             |
|    total_timesteps | 1906500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 931     |
|    time_elapsed    | 76779   |
|    total_timesteps | 1906688 |
--------------------------------
Eval num_timesteps=1907000, episode_reward=-90246.26 +/- 22719.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4360233   |
|    mean velocity x      | 0.0604       |
|    mean velocity y      | 1.31         |
|    mean velocity z      | 2.81         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.02e+04    |
| time/                   |              |
|    total_timesteps      | 1907000      |
| train/                  |              |
|    approx_kl            | 7.384544e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.001        |
|    loss                 | 1.94e+07     |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.000218    |
|    std                  | 1.55         |
|    value_loss           | 5.87e+07     |
------------------------------------------
Eval num_timesteps=1907500, episode_reward=-92590.52 +/- 26682.16
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38278544 |
|    mean velocity x | 0.0425      |
|    mean velocity y | 0.835       |
|    mean velocity z | 3.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.26e+04   |
| time/              |             |
|    total_timesteps | 1907500     |
------------------------------------
Eval num_timesteps=1908000, episode_reward=-82315.86 +/- 13030.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23476543 |
|    mean velocity x | -0.93       |
|    mean velocity y | -0.122      |
|    mean velocity z | 2.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1908000     |
------------------------------------
Eval num_timesteps=1908500, episode_reward=-74899.03 +/- 33010.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48143533 |
|    mean velocity x | 0.455       |
|    mean velocity y | 1.41        |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.49e+04   |
| time/              |             |
|    total_timesteps | 1908500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 932     |
|    time_elapsed    | 76859   |
|    total_timesteps | 1908736 |
--------------------------------
Eval num_timesteps=1909000, episode_reward=-98502.47 +/- 24678.88
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27960345   |
|    mean velocity x      | -0.0694       |
|    mean velocity y      | 0.389         |
|    mean velocity z      | 0.675         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.85e+04     |
| time/                   |               |
|    total_timesteps      | 1909000       |
| train/                  |               |
|    approx_kl            | 3.8041617e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.414         |
|    learning_rate        | 0.001         |
|    loss                 | 8.08e+06      |
|    n_updates            | 9320          |
|    policy_gradient_loss | -0.000114     |
|    std                  | 1.55          |
|    value_loss           | 2.66e+07      |
-------------------------------------------
Eval num_timesteps=1909500, episode_reward=-80804.95 +/- 36391.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39319992 |
|    mean velocity x | -0.242      |
|    mean velocity y | 1.2         |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.08e+04   |
| time/              |             |
|    total_timesteps | 1909500     |
------------------------------------
Eval num_timesteps=1910000, episode_reward=-45270.46 +/- 40552.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4520233 |
|    mean velocity x | -0.293     |
|    mean velocity y | 0.612      |
|    mean velocity z | 2.04       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -4.53e+04  |
| time/              |            |
|    total_timesteps | 1910000    |
-----------------------------------
Eval num_timesteps=1910500, episode_reward=-65947.08 +/- 40582.55
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3645846 |
|    mean velocity x | -0.276     |
|    mean velocity y | 0.286      |
|    mean velocity z | 3.36       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.59e+04  |
| time/              |            |
|    total_timesteps | 1910500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 933     |
|    time_elapsed    | 76940   |
|    total_timesteps | 1910784 |
--------------------------------
Eval num_timesteps=1911000, episode_reward=-38298.33 +/- 43945.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41690186   |
|    mean velocity x      | -0.0646       |
|    mean velocity y      | 0.593         |
|    mean velocity z      | 2.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.83e+04     |
| time/                   |               |
|    total_timesteps      | 1911000       |
| train/                  |               |
|    approx_kl            | 4.6002388e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.338         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+07      |
|    n_updates            | 9330          |
|    policy_gradient_loss | -0.00019      |
|    std                  | 1.55          |
|    value_loss           | 3.23e+07      |
-------------------------------------------
Eval num_timesteps=1911500, episode_reward=-101100.63 +/- 32055.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36188197 |
|    mean velocity x | -0.47       |
|    mean velocity y | 0.297       |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1911500     |
------------------------------------
Eval num_timesteps=1912000, episode_reward=-94421.87 +/- 40301.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50095403 |
|    mean velocity x | -0.558      |
|    mean velocity y | 0.865       |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.44e+04   |
| time/              |             |
|    total_timesteps | 1912000     |
------------------------------------
Eval num_timesteps=1912500, episode_reward=-87297.50 +/- 42084.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50249904 |
|    mean velocity x | -0.475      |
|    mean velocity y | 0.678       |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.73e+04   |
| time/              |             |
|    total_timesteps | 1912500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 934     |
|    time_elapsed    | 77020   |
|    total_timesteps | 1912832 |
--------------------------------
Eval num_timesteps=1913000, episode_reward=-71221.86 +/- 40430.79
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.24692309    |
|    mean velocity x      | -0.175         |
|    mean velocity y      | 0.392          |
|    mean velocity z      | 0.779          |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -7.12e+04      |
| time/                   |                |
|    total_timesteps      | 1913000        |
| train/                  |                |
|    approx_kl            | 0.000105126936 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.261          |
|    learning_rate        | 0.001          |
|    loss                 | 5.71e+06       |
|    n_updates            | 9340           |
|    policy_gradient_loss | -0.000859      |
|    std                  | 1.55           |
|    value_loss           | 7.91e+07       |
--------------------------------------------
Eval num_timesteps=1913500, episode_reward=-73562.36 +/- 24155.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41334948 |
|    mean velocity x | 0.24        |
|    mean velocity y | 0.682       |
|    mean velocity z | 2.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1913500     |
------------------------------------
Eval num_timesteps=1914000, episode_reward=-79430.40 +/- 30557.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53147537 |
|    mean velocity x | -0.566      |
|    mean velocity y | 0.757       |
|    mean velocity z | 4.74        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.94e+04   |
| time/              |             |
|    total_timesteps | 1914000     |
------------------------------------
Eval num_timesteps=1914500, episode_reward=-100842.29 +/- 28107.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46321046 |
|    mean velocity x | -0.439      |
|    mean velocity y | 0.75        |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1914500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 935     |
|    time_elapsed    | 77100   |
|    total_timesteps | 1914880 |
--------------------------------
Eval num_timesteps=1915000, episode_reward=-88447.20 +/- 14106.46
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47997278  |
|    mean velocity x      | -0.356       |
|    mean velocity y      | 1.15         |
|    mean velocity z      | 3.85         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.84e+04    |
| time/                   |              |
|    total_timesteps      | 1915000      |
| train/                  |              |
|    approx_kl            | 9.352225e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.331        |
|    learning_rate        | 0.001        |
|    loss                 | 2.3e+07      |
|    n_updates            | 9350         |
|    policy_gradient_loss | -0.000199    |
|    std                  | 1.55         |
|    value_loss           | 6.01e+07     |
------------------------------------------
Eval num_timesteps=1915500, episode_reward=-67097.24 +/- 45440.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49162203 |
|    mean velocity x | 0.362       |
|    mean velocity y | 1.46        |
|    mean velocity z | 3.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 1915500     |
------------------------------------
Eval num_timesteps=1916000, episode_reward=-107455.71 +/- 16961.07
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30690917 |
|    mean velocity x | 0.469       |
|    mean velocity y | 0.698       |
|    mean velocity z | 2.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1916000     |
------------------------------------
Eval num_timesteps=1916500, episode_reward=-75885.63 +/- 51428.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49099725 |
|    mean velocity x | -0.372      |
|    mean velocity y | 0.666       |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 1916500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 936     |
|    time_elapsed    | 77181   |
|    total_timesteps | 1916928 |
--------------------------------
Eval num_timesteps=1917000, episode_reward=-75796.34 +/- 41741.10
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38435686   |
|    mean velocity x      | -0.0315       |
|    mean velocity y      | 1.17          |
|    mean velocity z      | 3.6           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.58e+04     |
| time/                   |               |
|    total_timesteps      | 1917000       |
| train/                  |               |
|    approx_kl            | 3.1932897e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.35          |
|    learning_rate        | 0.001         |
|    loss                 | 4.35e+07      |
|    n_updates            | 9360          |
|    policy_gradient_loss | -0.000672     |
|    std                  | 1.55          |
|    value_loss           | 4.7e+07       |
-------------------------------------------
Eval num_timesteps=1917500, episode_reward=-66934.40 +/- 46011.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.55860376 |
|    mean velocity x | -0.647      |
|    mean velocity y | 0.915       |
|    mean velocity z | 4.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.69e+04   |
| time/              |             |
|    total_timesteps | 1917500     |
------------------------------------
Eval num_timesteps=1918000, episode_reward=-83002.06 +/- 38588.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21418269 |
|    mean velocity x | 0.0337      |
|    mean velocity y | 0.0873      |
|    mean velocity z | 3.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.3e+04    |
| time/              |             |
|    total_timesteps | 1918000     |
------------------------------------
Eval num_timesteps=1918500, episode_reward=-112286.45 +/- 19063.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2769917 |
|    mean velocity x | -1.94      |
|    mean velocity y | -0.898     |
|    mean velocity z | 4.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 1918500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 937     |
|    time_elapsed    | 77261   |
|    total_timesteps | 1918976 |
--------------------------------
Eval num_timesteps=1919000, episode_reward=-106709.33 +/- 50036.93
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26829356  |
|    mean velocity x      | -0.421       |
|    mean velocity y      | -0.243       |
|    mean velocity z      | 3.16         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.07e+05    |
| time/                   |              |
|    total_timesteps      | 1919000      |
| train/                  |              |
|    approx_kl            | 8.549396e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.402        |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+07     |
|    n_updates            | 9370         |
|    policy_gradient_loss | -0.000245    |
|    std                  | 1.55         |
|    value_loss           | 4.61e+07     |
------------------------------------------
Eval num_timesteps=1919500, episode_reward=-90909.46 +/- 25603.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27604315 |
|    mean velocity x | -0.216      |
|    mean velocity y | 0.194       |
|    mean velocity z | 2.44        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 1919500     |
------------------------------------
Eval num_timesteps=1920000, episode_reward=-87689.81 +/- 34460.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46768993 |
|    mean velocity x | -0.432      |
|    mean velocity y | 0.786       |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 1920000     |
------------------------------------
Eval num_timesteps=1920500, episode_reward=-79927.97 +/- 10766.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4216518 |
|    mean velocity x | -0.214     |
|    mean velocity y | 0.75       |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.99e+04  |
| time/              |            |
|    total_timesteps | 1920500    |
-----------------------------------
Eval num_timesteps=1921000, episode_reward=-90768.80 +/- 19919.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3946195 |
|    mean velocity x | -0.546     |
|    mean velocity y | 0.519      |
|    mean velocity z | 3.96       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.08e+04  |
| time/              |            |
|    total_timesteps | 1921000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 938     |
|    time_elapsed    | 77360   |
|    total_timesteps | 1921024 |
--------------------------------
Eval num_timesteps=1921500, episode_reward=-106643.48 +/- 51720.00
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47173283  |
|    mean velocity x      | 0.0559       |
|    mean velocity y      | 1.07         |
|    mean velocity z      | 3.92         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.07e+05    |
| time/                   |              |
|    total_timesteps      | 1921500      |
| train/                  |              |
|    approx_kl            | 1.451018e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.289        |
|    learning_rate        | 0.001        |
|    loss                 | 3.13e+07     |
|    n_updates            | 9380         |
|    policy_gradient_loss | -0.000299    |
|    std                  | 1.55         |
|    value_loss           | 8.86e+07     |
------------------------------------------
Eval num_timesteps=1922000, episode_reward=-85981.37 +/- 49144.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4808023 |
|    mean velocity x | -0.441     |
|    mean velocity y | 0.685      |
|    mean velocity z | 4.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.6e+04   |
| time/              |            |
|    total_timesteps | 1922000    |
-----------------------------------
Eval num_timesteps=1922500, episode_reward=-75741.97 +/- 25768.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38369435 |
|    mean velocity x | -0.361      |
|    mean velocity y | 0.775       |
|    mean velocity z | 3.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 1922500     |
------------------------------------
Eval num_timesteps=1923000, episode_reward=-97204.18 +/- 36906.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35322258 |
|    mean velocity x | -0.511      |
|    mean velocity y | -0.0116     |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.72e+04   |
| time/              |             |
|    total_timesteps | 1923000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 939     |
|    time_elapsed    | 77441   |
|    total_timesteps | 1923072 |
--------------------------------
Eval num_timesteps=1923500, episode_reward=-62448.79 +/- 50028.80
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39347863   |
|    mean velocity x      | 0.134         |
|    mean velocity y      | 1.32          |
|    mean velocity z      | 2.62          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.24e+04     |
| time/                   |               |
|    total_timesteps      | 1923500       |
| train/                  |               |
|    approx_kl            | 1.8967316e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.26          |
|    learning_rate        | 0.001         |
|    loss                 | 2.06e+07      |
|    n_updates            | 9390          |
|    policy_gradient_loss | -0.000327     |
|    std                  | 1.55          |
|    value_loss           | 6.69e+07      |
-------------------------------------------
Eval num_timesteps=1924000, episode_reward=-89932.92 +/- 35302.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33912745 |
|    mean velocity x | 0.455       |
|    mean velocity y | 0.763       |
|    mean velocity z | 2.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.99e+04   |
| time/              |             |
|    total_timesteps | 1924000     |
------------------------------------
Eval num_timesteps=1924500, episode_reward=-88941.56 +/- 20182.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29897788 |
|    mean velocity x | -0.921      |
|    mean velocity y | 0.255       |
|    mean velocity z | 2.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.89e+04   |
| time/              |             |
|    total_timesteps | 1924500     |
------------------------------------
Eval num_timesteps=1925000, episode_reward=-79173.30 +/- 41643.91
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3452826 |
|    mean velocity x | -0.24      |
|    mean velocity y | 0.757      |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.92e+04  |
| time/              |            |
|    total_timesteps | 1925000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 940     |
|    time_elapsed    | 77521   |
|    total_timesteps | 1925120 |
--------------------------------
Eval num_timesteps=1925500, episode_reward=-64982.91 +/- 40487.00
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.1730557    |
|    mean velocity x      | -0.905        |
|    mean velocity y      | -0.0717       |
|    mean velocity z      | 2.12          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.5e+04      |
| time/                   |               |
|    total_timesteps      | 1925500       |
| train/                  |               |
|    approx_kl            | 1.0168413e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.284         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+07      |
|    n_updates            | 9400          |
|    policy_gradient_loss | -0.000248     |
|    std                  | 1.55          |
|    value_loss           | 4.27e+07      |
-------------------------------------------
Eval num_timesteps=1926000, episode_reward=-70524.58 +/- 56707.84
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.502649 |
|    mean velocity x | -0.462    |
|    mean velocity y | 0.783     |
|    mean velocity z | 4.8       |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.05e+04 |
| time/              |           |
|    total_timesteps | 1926000   |
----------------------------------
Eval num_timesteps=1926500, episode_reward=-87767.25 +/- 55544.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30625626 |
|    mean velocity x | -1.15       |
|    mean velocity y | -0.724      |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 1926500     |
------------------------------------
Eval num_timesteps=1927000, episode_reward=-86598.73 +/- 49341.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40888157 |
|    mean velocity x | -0.578      |
|    mean velocity y | 0.701       |
|    mean velocity z | 3.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 1927000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 941     |
|    time_elapsed    | 77602   |
|    total_timesteps | 1927168 |
--------------------------------
Eval num_timesteps=1927500, episode_reward=-76839.43 +/- 28668.49
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35111395  |
|    mean velocity x      | -0.163       |
|    mean velocity y      | 1.18         |
|    mean velocity z      | 3.91         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.68e+04    |
| time/                   |              |
|    total_timesteps      | 1927500      |
| train/                  |              |
|    approx_kl            | 2.117778e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.001        |
|    loss                 | 2.12e+07     |
|    n_updates            | 9410         |
|    policy_gradient_loss | -0.000531    |
|    std                  | 1.55         |
|    value_loss           | 6.34e+07     |
------------------------------------------
Eval num_timesteps=1928000, episode_reward=-89200.98 +/- 28544.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3458414 |
|    mean velocity x | 0.32       |
|    mean velocity y | 1.21       |
|    mean velocity z | 3.58       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.92e+04  |
| time/              |            |
|    total_timesteps | 1928000    |
-----------------------------------
Eval num_timesteps=1928500, episode_reward=-103770.55 +/- 17412.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52625257 |
|    mean velocity x | -0.426      |
|    mean velocity y | 1.23        |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 1928500     |
------------------------------------
Eval num_timesteps=1929000, episode_reward=-71977.15 +/- 17782.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40486467 |
|    mean velocity x | -0.989      |
|    mean velocity y | 0.244       |
|    mean velocity z | 4.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 1929000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 942     |
|    time_elapsed    | 77682   |
|    total_timesteps | 1929216 |
--------------------------------
Eval num_timesteps=1929500, episode_reward=-75460.94 +/- 42516.08
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44631743  |
|    mean velocity x      | -0.215       |
|    mean velocity y      | 1.27         |
|    mean velocity z      | 4.16         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.55e+04    |
| time/                   |              |
|    total_timesteps      | 1929500      |
| train/                  |              |
|    approx_kl            | 5.155074e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.362        |
|    learning_rate        | 0.001        |
|    loss                 | 2.94e+07     |
|    n_updates            | 9420         |
|    policy_gradient_loss | -0.00015     |
|    std                  | 1.55         |
|    value_loss           | 6.46e+07     |
------------------------------------------
Eval num_timesteps=1930000, episode_reward=-79523.75 +/- 41809.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21806285 |
|    mean velocity x | -0.255      |
|    mean velocity y | 0.534       |
|    mean velocity z | 0.194       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.95e+04   |
| time/              |             |
|    total_timesteps | 1930000     |
------------------------------------
Eval num_timesteps=1930500, episode_reward=-89291.36 +/- 44868.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42268977 |
|    mean velocity x | -0.148      |
|    mean velocity y | 1.12        |
|    mean velocity z | 4.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1930500     |
------------------------------------
Eval num_timesteps=1931000, episode_reward=-43309.88 +/- 41535.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43917376 |
|    mean velocity x | -2.23       |
|    mean velocity y | -1.05       |
|    mean velocity z | 9.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.33e+04   |
| time/              |             |
|    total_timesteps | 1931000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 943     |
|    time_elapsed    | 77762   |
|    total_timesteps | 1931264 |
--------------------------------
Eval num_timesteps=1931500, episode_reward=-91183.55 +/- 42358.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3897147    |
|    mean velocity x      | -0.784        |
|    mean velocity y      | 0.0382        |
|    mean velocity z      | 3.36          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.12e+04     |
| time/                   |               |
|    total_timesteps      | 1931500       |
| train/                  |               |
|    approx_kl            | 2.2200984e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.414         |
|    learning_rate        | 0.001         |
|    loss                 | 2.88e+07      |
|    n_updates            | 9430          |
|    policy_gradient_loss | -0.000447     |
|    std                  | 1.55          |
|    value_loss           | 6.54e+07      |
-------------------------------------------
Eval num_timesteps=1932000, episode_reward=-76287.36 +/- 35385.02
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.34392  |
|    mean velocity x | -0.216    |
|    mean velocity y | 0.69      |
|    mean velocity z | 1.94      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.63e+04 |
| time/              |           |
|    total_timesteps | 1932000   |
----------------------------------
Eval num_timesteps=1932500, episode_reward=-121196.71 +/- 73634.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39987645 |
|    mean velocity x | -0.0587     |
|    mean velocity y | 0.87        |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.21e+05   |
| time/              |             |
|    total_timesteps | 1932500     |
------------------------------------
Eval num_timesteps=1933000, episode_reward=-72109.42 +/- 40190.98
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5097678 |
|    mean velocity x | 0.26       |
|    mean velocity y | 1.39       |
|    mean velocity z | 3.72       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.21e+04  |
| time/              |            |
|    total_timesteps | 1933000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 944     |
|    time_elapsed    | 77843   |
|    total_timesteps | 1933312 |
--------------------------------
Eval num_timesteps=1933500, episode_reward=-65234.08 +/- 42405.47
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.36571327   |
|    mean velocity x      | -0.996        |
|    mean velocity y      | -0.0899       |
|    mean velocity z      | 3.52          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.52e+04     |
| time/                   |               |
|    total_timesteps      | 1933500       |
| train/                  |               |
|    approx_kl            | 1.8504797e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.396         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+07      |
|    n_updates            | 9440          |
|    policy_gradient_loss | -0.000382     |
|    std                  | 1.55          |
|    value_loss           | 4.1e+07       |
-------------------------------------------
Eval num_timesteps=1934000, episode_reward=-67788.23 +/- 47903.00
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3401831 |
|    mean velocity x | -0.235     |
|    mean velocity y | 0.493      |
|    mean velocity z | 4.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.78e+04  |
| time/              |            |
|    total_timesteps | 1934000    |
-----------------------------------
Eval num_timesteps=1934500, episode_reward=-113728.23 +/- 23411.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40940458 |
|    mean velocity x | 0.00213     |
|    mean velocity y | 1.34        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 1934500     |
------------------------------------
Eval num_timesteps=1935000, episode_reward=-91123.57 +/- 42721.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45040718 |
|    mean velocity x | 0.185       |
|    mean velocity y | 0.918       |
|    mean velocity z | 3.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.11e+04   |
| time/              |             |
|    total_timesteps | 1935000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 945     |
|    time_elapsed    | 77923   |
|    total_timesteps | 1935360 |
--------------------------------
Eval num_timesteps=1935500, episode_reward=-69665.43 +/- 44538.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5318916   |
|    mean velocity x      | 0.516        |
|    mean velocity y      | 2.07         |
|    mean velocity z      | 3.9          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.97e+04    |
| time/                   |              |
|    total_timesteps      | 1935500      |
| train/                  |              |
|    approx_kl            | 0.0006122515 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.001        |
|    loss                 | 1.96e+07     |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.55         |
|    value_loss           | 6.6e+07      |
------------------------------------------
Eval num_timesteps=1936000, episode_reward=-102625.66 +/- 15948.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.51603746 |
|    mean velocity x | 0.156       |
|    mean velocity y | 1.41        |
|    mean velocity z | 3.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1936000     |
------------------------------------
Eval num_timesteps=1936500, episode_reward=-60908.53 +/- 37270.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36136466 |
|    mean velocity x | -0.219      |
|    mean velocity y | 1.09        |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.09e+04   |
| time/              |             |
|    total_timesteps | 1936500     |
------------------------------------
Eval num_timesteps=1937000, episode_reward=-65994.46 +/- 34517.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.61549383 |
|    mean velocity x | 0.335       |
|    mean velocity y | 2.33        |
|    mean velocity z | 4.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.6e+04    |
| time/              |             |
|    total_timesteps | 1937000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 946     |
|    time_elapsed    | 78003   |
|    total_timesteps | 1937408 |
--------------------------------
Eval num_timesteps=1937500, episode_reward=-86808.37 +/- 33483.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45907032   |
|    mean velocity x      | -0.796        |
|    mean velocity y      | 0.432         |
|    mean velocity z      | 4.08          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.68e+04     |
| time/                   |               |
|    total_timesteps      | 1937500       |
| train/                  |               |
|    approx_kl            | 2.8071401e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.479         |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+07      |
|    n_updates            | 9460          |
|    policy_gradient_loss | -0.000511     |
|    std                  | 1.55          |
|    value_loss           | 3.85e+07      |
-------------------------------------------
Eval num_timesteps=1938000, episode_reward=-80104.78 +/- 38373.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39119905 |
|    mean velocity x | -0.404      |
|    mean velocity y | 0.334       |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 1938000     |
------------------------------------
Eval num_timesteps=1938500, episode_reward=-100719.76 +/- 29128.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38808525 |
|    mean velocity x | -1.01       |
|    mean velocity y | 0.424       |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 1938500     |
------------------------------------
Eval num_timesteps=1939000, episode_reward=-58517.82 +/- 42996.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32437187 |
|    mean velocity x | -0.08       |
|    mean velocity y | 0.583       |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.85e+04   |
| time/              |             |
|    total_timesteps | 1939000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 947     |
|    time_elapsed    | 78084   |
|    total_timesteps | 1939456 |
--------------------------------
Eval num_timesteps=1939500, episode_reward=-103692.65 +/- 16569.26
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48152953   |
|    mean velocity x      | 0.225         |
|    mean velocity y      | 1.38          |
|    mean velocity z      | 1.9           |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.04e+05     |
| time/                   |               |
|    total_timesteps      | 1939500       |
| train/                  |               |
|    approx_kl            | 2.4845562e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.368         |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+06      |
|    n_updates            | 9470          |
|    policy_gradient_loss | -0.000238     |
|    std                  | 1.55          |
|    value_loss           | 4.13e+07      |
-------------------------------------------
Eval num_timesteps=1940000, episode_reward=-66265.75 +/- 49587.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44342253 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.927       |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.63e+04   |
| time/              |             |
|    total_timesteps | 1940000     |
------------------------------------
Eval num_timesteps=1940500, episode_reward=-67407.59 +/- 38556.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34331733 |
|    mean velocity x | -0.617      |
|    mean velocity y | 0.819       |
|    mean velocity z | 1.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 1940500     |
------------------------------------
Eval num_timesteps=1941000, episode_reward=-48749.03 +/- 41485.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41625917 |
|    mean velocity x | -0.174      |
|    mean velocity y | 1           |
|    mean velocity z | 4.34        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.87e+04   |
| time/              |             |
|    total_timesteps | 1941000     |
------------------------------------
Eval num_timesteps=1941500, episode_reward=-92782.40 +/- 21449.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39467102 |
|    mean velocity x | -0.221      |
|    mean velocity y | 1.18        |
|    mean velocity z | 4.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.28e+04   |
| time/              |             |
|    total_timesteps | 1941500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 948     |
|    time_elapsed    | 78183   |
|    total_timesteps | 1941504 |
--------------------------------
Eval num_timesteps=1942000, episode_reward=-95850.34 +/- 30197.02
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.23373996   |
|    mean velocity x      | -1.55         |
|    mean velocity y      | -0.294        |
|    mean velocity z      | 3.31          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.59e+04     |
| time/                   |               |
|    total_timesteps      | 1942000       |
| train/                  |               |
|    approx_kl            | 1.0047283e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.285         |
|    learning_rate        | 0.001         |
|    loss                 | 2.51e+07      |
|    n_updates            | 9480          |
|    policy_gradient_loss | -0.000315     |
|    std                  | 1.55          |
|    value_loss           | 6.17e+07      |
-------------------------------------------
Eval num_timesteps=1942500, episode_reward=-76160.41 +/- 31359.70
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3990391 |
|    mean velocity x | -0.282     |
|    mean velocity y | 0.636      |
|    mean velocity z | 4.28       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.62e+04  |
| time/              |            |
|    total_timesteps | 1942500    |
-----------------------------------
Eval num_timesteps=1943000, episode_reward=-61488.24 +/- 31094.03
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.397075 |
|    mean velocity x | -0.494    |
|    mean velocity y | 0.436     |
|    mean velocity z | 4.12      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.15e+04 |
| time/              |           |
|    total_timesteps | 1943000   |
----------------------------------
Eval num_timesteps=1943500, episode_reward=-52994.99 +/- 23805.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48319587 |
|    mean velocity x | -0.351      |
|    mean velocity y | 0.942       |
|    mean velocity z | 4.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.3e+04    |
| time/              |             |
|    total_timesteps | 1943500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 949     |
|    time_elapsed    | 78264   |
|    total_timesteps | 1943552 |
--------------------------------
Eval num_timesteps=1944000, episode_reward=-94488.62 +/- 24153.98
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.269724     |
|    mean velocity x      | -0.399        |
|    mean velocity y      | 0.41          |
|    mean velocity z      | 1.31          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.45e+04     |
| time/                   |               |
|    total_timesteps      | 1944000       |
| train/                  |               |
|    approx_kl            | 2.0219974e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.001         |
|    loss                 | 2.52e+07      |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.000225     |
|    std                  | 1.55          |
|    value_loss           | 8.85e+07      |
-------------------------------------------
Eval num_timesteps=1944500, episode_reward=-80684.90 +/- 41670.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4140491 |
|    mean velocity x | -0.177     |
|    mean velocity y | 0.716      |
|    mean velocity z | 4.32       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 1944500    |
-----------------------------------
Eval num_timesteps=1945000, episode_reward=-84775.24 +/- 44518.02
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36688536 |
|    mean velocity x | -0.311      |
|    mean velocity y | 0.51        |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.48e+04   |
| time/              |             |
|    total_timesteps | 1945000     |
------------------------------------
Eval num_timesteps=1945500, episode_reward=-72615.77 +/- 50369.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19963045 |
|    mean velocity x | -0.0243     |
|    mean velocity y | 0.169       |
|    mean velocity z | 1.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.26e+04   |
| time/              |             |
|    total_timesteps | 1945500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 950     |
|    time_elapsed    | 78344   |
|    total_timesteps | 1945600 |
--------------------------------
Eval num_timesteps=1946000, episode_reward=-92888.59 +/- 25276.13
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34503216  |
|    mean velocity x      | -0.238       |
|    mean velocity y      | 0.37         |
|    mean velocity z      | 3.11         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.29e+04    |
| time/                   |              |
|    total_timesteps      | 1946000      |
| train/                  |              |
|    approx_kl            | 0.0005798711 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+07     |
|    n_updates            | 9500         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 1.55         |
|    value_loss           | 6.24e+07     |
------------------------------------------
Eval num_timesteps=1946500, episode_reward=-70983.51 +/- 19203.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35854816 |
|    mean velocity x | -0.239      |
|    mean velocity y | 0.744       |
|    mean velocity z | 4.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.1e+04    |
| time/              |             |
|    total_timesteps | 1946500     |
------------------------------------
Eval num_timesteps=1947000, episode_reward=-94496.26 +/- 23127.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21694349 |
|    mean velocity x | -0.0488     |
|    mean velocity y | 0.339       |
|    mean velocity z | 0.383       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.45e+04   |
| time/              |             |
|    total_timesteps | 1947000     |
------------------------------------
Eval num_timesteps=1947500, episode_reward=-70791.31 +/- 40223.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25043797 |
|    mean velocity x | -0.228      |
|    mean velocity y | 0.497       |
|    mean velocity z | 0.502       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.08e+04   |
| time/              |             |
|    total_timesteps | 1947500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 951     |
|    time_elapsed    | 78424   |
|    total_timesteps | 1947648 |
--------------------------------
Eval num_timesteps=1948000, episode_reward=-70298.96 +/- 31627.08
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41185877  |
|    mean velocity x      | -0.256       |
|    mean velocity y      | 0.725        |
|    mean velocity z      | 4.23         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.03e+04    |
| time/                   |              |
|    total_timesteps      | 1948000      |
| train/                  |              |
|    approx_kl            | 2.867871e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.218        |
|    learning_rate        | 0.001        |
|    loss                 | 6.27e+06     |
|    n_updates            | 9510         |
|    policy_gradient_loss | -0.000212    |
|    std                  | 1.55         |
|    value_loss           | 6.07e+07     |
------------------------------------------
Eval num_timesteps=1948500, episode_reward=-58933.71 +/- 38483.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36310968 |
|    mean velocity x | -0.284      |
|    mean velocity y | 0.254       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.89e+04   |
| time/              |             |
|    total_timesteps | 1948500     |
------------------------------------
Eval num_timesteps=1949000, episode_reward=-41414.93 +/- 34222.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49195662 |
|    mean velocity x | 0.77        |
|    mean velocity y | 1.59        |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.14e+04   |
| time/              |             |
|    total_timesteps | 1949000     |
------------------------------------
Eval num_timesteps=1949500, episode_reward=-63544.21 +/- 44719.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37756187 |
|    mean velocity x | 0.738       |
|    mean velocity y | 1.36        |
|    mean velocity z | 3.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+04   |
| time/              |             |
|    total_timesteps | 1949500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 952     |
|    time_elapsed    | 78505   |
|    total_timesteps | 1949696 |
--------------------------------
Eval num_timesteps=1950000, episode_reward=-50381.23 +/- 27490.48
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24508707   |
|    mean velocity x      | -0.976        |
|    mean velocity y      | -0.539        |
|    mean velocity z      | 3.13          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.04e+04     |
| time/                   |               |
|    total_timesteps      | 1950000       |
| train/                  |               |
|    approx_kl            | 1.0984193e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.418         |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+07      |
|    n_updates            | 9520          |
|    policy_gradient_loss | -0.000336     |
|    std                  | 1.55          |
|    value_loss           | 4.11e+07      |
-------------------------------------------
Eval num_timesteps=1950500, episode_reward=-124056.07 +/- 13312.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40915644 |
|    mean velocity x | -0.25       |
|    mean velocity y | 0.856       |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.24e+05   |
| time/              |             |
|    total_timesteps | 1950500     |
------------------------------------
Eval num_timesteps=1951000, episode_reward=-54090.73 +/- 42764.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37851635 |
|    mean velocity x | -0.402      |
|    mean velocity y | 0.614       |
|    mean velocity z | 3.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.41e+04   |
| time/              |             |
|    total_timesteps | 1951000     |
------------------------------------
Eval num_timesteps=1951500, episode_reward=-75364.98 +/- 47557.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46137348 |
|    mean velocity x | 0.534       |
|    mean velocity y | 1.64        |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.54e+04   |
| time/              |             |
|    total_timesteps | 1951500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 953     |
|    time_elapsed    | 78585   |
|    total_timesteps | 1951744 |
--------------------------------
Eval num_timesteps=1952000, episode_reward=-107780.30 +/- 17437.07
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.43553418    |
|    mean velocity x      | -0.337         |
|    mean velocity y      | 0.813          |
|    mean velocity z      | 4.39           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1.08e+05      |
| time/                   |                |
|    total_timesteps      | 1952000        |
| train/                  |                |
|    approx_kl            | 1.48631225e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.296          |
|    learning_rate        | 0.001          |
|    loss                 | 6.02e+07       |
|    n_updates            | 9530           |
|    policy_gradient_loss | -0.000266      |
|    std                  | 1.55           |
|    value_loss           | 8.6e+07        |
--------------------------------------------
Eval num_timesteps=1952500, episode_reward=-110614.45 +/- 18816.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42885834 |
|    mean velocity x | 0.0606      |
|    mean velocity y | 1.08        |
|    mean velocity z | 3.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 1952500     |
------------------------------------
Eval num_timesteps=1953000, episode_reward=-118225.43 +/- 33348.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39233696 |
|    mean velocity x | -0.661      |
|    mean velocity y | 0.403       |
|    mean velocity z | 2.81        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 1953000     |
------------------------------------
Eval num_timesteps=1953500, episode_reward=-86414.24 +/- 28104.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39719975 |
|    mean velocity x | -0.254      |
|    mean velocity y | 0.505       |
|    mean velocity z | 4.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 1953500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 954     |
|    time_elapsed    | 78665   |
|    total_timesteps | 1953792 |
--------------------------------
Eval num_timesteps=1954000, episode_reward=-56541.62 +/- 44300.00
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3746793    |
|    mean velocity x      | -0.368        |
|    mean velocity y      | 0.598         |
|    mean velocity z      | 3.24          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.65e+04     |
| time/                   |               |
|    total_timesteps      | 1954000       |
| train/                  |               |
|    approx_kl            | 1.5445112e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.313         |
|    learning_rate        | 0.001         |
|    loss                 | 3.25e+07      |
|    n_updates            | 9540          |
|    policy_gradient_loss | -0.000331     |
|    std                  | 1.55          |
|    value_loss           | 5.85e+07      |
-------------------------------------------
Eval num_timesteps=1954500, episode_reward=-85506.66 +/- 39817.98
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37388664 |
|    mean velocity x | -0.484      |
|    mean velocity y | 0.485       |
|    mean velocity z | 3.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.55e+04   |
| time/              |             |
|    total_timesteps | 1954500     |
------------------------------------
Eval num_timesteps=1955000, episode_reward=-44280.34 +/- 37684.16
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.525999 |
|    mean velocity x | 0.397     |
|    mean velocity y | 1.46      |
|    mean velocity z | 3.14      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -4.43e+04 |
| time/              |           |
|    total_timesteps | 1955000   |
----------------------------------
Eval num_timesteps=1955500, episode_reward=-105400.33 +/- 14159.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22492091 |
|    mean velocity x | -0.155      |
|    mean velocity y | 0.439       |
|    mean velocity z | 0.297       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1955500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 955     |
|    time_elapsed    | 78746   |
|    total_timesteps | 1955840 |
--------------------------------
Eval num_timesteps=1956000, episode_reward=-83932.54 +/- 19686.44
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40901035   |
|    mean velocity x      | -0.308        |
|    mean velocity y      | 0.782         |
|    mean velocity z      | 4.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.39e+04     |
| time/                   |               |
|    total_timesteps      | 1956000       |
| train/                  |               |
|    approx_kl            | 5.2462274e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+07      |
|    n_updates            | 9550          |
|    policy_gradient_loss | -0.000674     |
|    std                  | 1.55          |
|    value_loss           | 4.62e+07      |
-------------------------------------------
Eval num_timesteps=1956500, episode_reward=-80009.63 +/- 38867.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38434747 |
|    mean velocity x | -0.399      |
|    mean velocity y | 0.425       |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8e+04      |
| time/              |             |
|    total_timesteps | 1956500     |
------------------------------------
Eval num_timesteps=1957000, episode_reward=-100506.92 +/- 14643.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4460921 |
|    mean velocity x | -0.111     |
|    mean velocity y | 0.862      |
|    mean velocity z | 3.76       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 1957000    |
-----------------------------------
Eval num_timesteps=1957500, episode_reward=-99445.78 +/- 19544.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4216488 |
|    mean velocity x | -0.0998    |
|    mean velocity y | 0.854      |
|    mean velocity z | 3.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.94e+04  |
| time/              |            |
|    total_timesteps | 1957500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 956     |
|    time_elapsed    | 78826   |
|    total_timesteps | 1957888 |
--------------------------------
Eval num_timesteps=1958000, episode_reward=-62027.08 +/- 38911.54
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47257474   |
|    mean velocity x      | -0.96         |
|    mean velocity y      | 0.023         |
|    mean velocity z      | 3.77          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.2e+04      |
| time/                   |               |
|    total_timesteps      | 1958000       |
| train/                  |               |
|    approx_kl            | 1.3292651e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.327         |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+07      |
|    n_updates            | 9560          |
|    policy_gradient_loss | -0.00019      |
|    std                  | 1.55          |
|    value_loss           | 6.66e+07      |
-------------------------------------------
Eval num_timesteps=1958500, episode_reward=-75134.46 +/- 38971.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40790138 |
|    mean velocity x | -0.577      |
|    mean velocity y | 0.732       |
|    mean velocity z | 1.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1958500     |
------------------------------------
Eval num_timesteps=1959000, episode_reward=-89253.64 +/- 42598.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40327758 |
|    mean velocity x | 0.0167      |
|    mean velocity y | 1.37        |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1959000     |
------------------------------------
Eval num_timesteps=1959500, episode_reward=-102539.54 +/- 17204.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39134312 |
|    mean velocity x | -0.656      |
|    mean velocity y | 0.531       |
|    mean velocity z | 3.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 1959500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 957     |
|    time_elapsed    | 78906   |
|    total_timesteps | 1959936 |
--------------------------------
Eval num_timesteps=1960000, episode_reward=-86695.53 +/- 28397.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.38015404   |
|    mean velocity x      | -1.1          |
|    mean velocity y      | 0.164         |
|    mean velocity z      | 3.61          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.67e+04     |
| time/                   |               |
|    total_timesteps      | 1960000       |
| train/                  |               |
|    approx_kl            | 5.5078417e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.393         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+07      |
|    n_updates            | 9570          |
|    policy_gradient_loss | -0.000164     |
|    std                  | 1.55          |
|    value_loss           | 4.08e+07      |
-------------------------------------------
Eval num_timesteps=1960500, episode_reward=-77116.18 +/- 28159.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4052873 |
|    mean velocity x | 0.0853     |
|    mean velocity y | 0.782      |
|    mean velocity z | 3.3        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.71e+04  |
| time/              |            |
|    total_timesteps | 1960500    |
-----------------------------------
Eval num_timesteps=1961000, episode_reward=-89310.95 +/- 40031.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44478956 |
|    mean velocity x | -0.223      |
|    mean velocity y | 1.38        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 1961000     |
------------------------------------
Eval num_timesteps=1961500, episode_reward=-107328.69 +/- 6858.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43560293 |
|    mean velocity x | -0.38       |
|    mean velocity y | 0.723       |
|    mean velocity z | 4.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 1961500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 958     |
|    time_elapsed    | 78987   |
|    total_timesteps | 1961984 |
--------------------------------
Eval num_timesteps=1962000, episode_reward=-94345.85 +/- 24178.60
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3259226   |
|    mean velocity x      | -0.873       |
|    mean velocity y      | -0.165       |
|    mean velocity z      | 3.09         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.43e+04    |
| time/                   |              |
|    total_timesteps      | 1962000      |
| train/                  |              |
|    approx_kl            | 5.523616e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.001        |
|    loss                 | 7.45e+07     |
|    n_updates            | 9580         |
|    policy_gradient_loss | -0.000183    |
|    std                  | 1.55         |
|    value_loss           | 5.08e+07     |
------------------------------------------
Eval num_timesteps=1962500, episode_reward=-56829.94 +/- 50701.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42572725 |
|    mean velocity x | -0.111      |
|    mean velocity y | 0.864       |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.68e+04   |
| time/              |             |
|    total_timesteps | 1962500     |
------------------------------------
Eval num_timesteps=1963000, episode_reward=-67778.63 +/- 42095.06
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09655382 |
|    mean velocity x | -0.201      |
|    mean velocity y | -0.0969     |
|    mean velocity z | 0.649       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.78e+04   |
| time/              |             |
|    total_timesteps | 1963000     |
------------------------------------
Eval num_timesteps=1963500, episode_reward=-68136.12 +/- 42433.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37903726 |
|    mean velocity x | -0.608      |
|    mean velocity y | 0.828       |
|    mean velocity z | 0.796       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1963500     |
------------------------------------
Eval num_timesteps=1964000, episode_reward=-78274.55 +/- 43872.51
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4446154 |
|    mean velocity x | -0.825     |
|    mean velocity y | 0.38       |
|    mean velocity z | 3.8        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.83e+04  |
| time/              |            |
|    total_timesteps | 1964000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 959     |
|    time_elapsed    | 79086   |
|    total_timesteps | 1964032 |
--------------------------------
Eval num_timesteps=1964500, episode_reward=-73245.19 +/- 43827.79
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3858616    |
|    mean velocity x      | -2.33         |
|    mean velocity y      | -0.949        |
|    mean velocity z      | 5.83          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.32e+04     |
| time/                   |               |
|    total_timesteps      | 1964500       |
| train/                  |               |
|    approx_kl            | 5.7953148e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.419         |
|    learning_rate        | 0.001         |
|    loss                 | 5.54e+05      |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.000509     |
|    std                  | 1.55          |
|    value_loss           | 2.94e+07      |
-------------------------------------------
Eval num_timesteps=1965000, episode_reward=-90065.25 +/- 19191.66
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3912012 |
|    mean velocity x | -0.88      |
|    mean velocity y | -0.0293    |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.01e+04  |
| time/              |            |
|    total_timesteps | 1965000    |
-----------------------------------
Eval num_timesteps=1965500, episode_reward=-70817.71 +/- 36832.59
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.342484 |
|    mean velocity x | -0.293    |
|    mean velocity y | 0.514     |
|    mean velocity z | 0.501     |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -7.08e+04 |
| time/              |           |
|    total_timesteps | 1965500   |
----------------------------------
Eval num_timesteps=1966000, episode_reward=-63719.87 +/- 45569.95
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3943808 |
|    mean velocity x | 0.548      |
|    mean velocity y | 1.42       |
|    mean velocity z | 3.22       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.37e+04  |
| time/              |            |
|    total_timesteps | 1966000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 960     |
|    time_elapsed    | 79167   |
|    total_timesteps | 1966080 |
--------------------------------
Eval num_timesteps=1966500, episode_reward=-69478.06 +/- 47703.15
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39517123   |
|    mean velocity x      | -0.264        |
|    mean velocity y      | 0.601         |
|    mean velocity z      | 4.17          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.95e+04     |
| time/                   |               |
|    total_timesteps      | 1966500       |
| train/                  |               |
|    approx_kl            | 1.4293939e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.454         |
|    learning_rate        | 0.001         |
|    loss                 | 9.68e+06      |
|    n_updates            | 9600          |
|    policy_gradient_loss | -0.000257     |
|    std                  | 1.55          |
|    value_loss           | 4.92e+07      |
-------------------------------------------
Eval num_timesteps=1967000, episode_reward=-66085.21 +/- 55930.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18071467 |
|    mean velocity x | 0.14        |
|    mean velocity y | 0.335       |
|    mean velocity z | 0.306       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.61e+04   |
| time/              |             |
|    total_timesteps | 1967000     |
------------------------------------
Eval num_timesteps=1967500, episode_reward=-80959.16 +/- 29481.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46418095 |
|    mean velocity x | -0.376      |
|    mean velocity y | 0.804       |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 1967500     |
------------------------------------
Eval num_timesteps=1968000, episode_reward=-39874.44 +/- 36450.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52158207 |
|    mean velocity x | -0.497      |
|    mean velocity y | 1.04        |
|    mean velocity z | 4.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.99e+04   |
| time/              |             |
|    total_timesteps | 1968000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 961     |
|    time_elapsed    | 79247   |
|    total_timesteps | 1968128 |
--------------------------------
Eval num_timesteps=1968500, episode_reward=-71819.52 +/- 34732.33
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.18789287   |
|    mean velocity x      | 0.0515        |
|    mean velocity y      | -0.0224       |
|    mean velocity z      | 2.66          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.18e+04     |
| time/                   |               |
|    total_timesteps      | 1968500       |
| train/                  |               |
|    approx_kl            | 2.6750116e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.268         |
|    learning_rate        | 0.001         |
|    loss                 | 4.55e+07      |
|    n_updates            | 9610          |
|    policy_gradient_loss | -0.000283     |
|    std                  | 1.55          |
|    value_loss           | 7.24e+07      |
-------------------------------------------
Eval num_timesteps=1969000, episode_reward=-107597.19 +/- 10730.49
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5022652 |
|    mean velocity x | 0.504      |
|    mean velocity y | 1.5        |
|    mean velocity z | 3.35       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 1969000    |
-----------------------------------
Eval num_timesteps=1969500, episode_reward=-102032.94 +/- 11987.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43862405 |
|    mean velocity x | -0.605      |
|    mean velocity y | 0.21        |
|    mean velocity z | 4.3         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1969500     |
------------------------------------
Eval num_timesteps=1970000, episode_reward=-75431.78 +/- 45446.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40988445 |
|    mean velocity x | -0.266      |
|    mean velocity y | 1.18        |
|    mean velocity z | 4.17        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.54e+04   |
| time/              |             |
|    total_timesteps | 1970000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 962     |
|    time_elapsed    | 79327   |
|    total_timesteps | 1970176 |
--------------------------------
Eval num_timesteps=1970500, episode_reward=-89857.43 +/- 22521.36
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32933387  |
|    mean velocity x      | -0.615       |
|    mean velocity y      | 0.367        |
|    mean velocity z      | 2.38         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.99e+04    |
| time/                   |              |
|    total_timesteps      | 1970500      |
| train/                  |              |
|    approx_kl            | 4.336005e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+07     |
|    n_updates            | 9620         |
|    policy_gradient_loss | -9.18e-05    |
|    std                  | 1.55         |
|    value_loss           | 5.67e+07     |
------------------------------------------
Eval num_timesteps=1971000, episode_reward=-67886.96 +/- 38060.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45813134 |
|    mean velocity x | 0.111       |
|    mean velocity y | 1.66        |
|    mean velocity z | 3.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.79e+04   |
| time/              |             |
|    total_timesteps | 1971000     |
------------------------------------
Eval num_timesteps=1971500, episode_reward=-98349.30 +/- 17917.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39465207 |
|    mean velocity x | -0.301      |
|    mean velocity y | 0.479       |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.83e+04   |
| time/              |             |
|    total_timesteps | 1971500     |
------------------------------------
Eval num_timesteps=1972000, episode_reward=-72279.35 +/- 47556.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44425583 |
|    mean velocity x | 0.111       |
|    mean velocity y | 0.864       |
|    mean velocity z | 2.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 1972000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 963     |
|    time_elapsed    | 79408   |
|    total_timesteps | 1972224 |
--------------------------------
Eval num_timesteps=1972500, episode_reward=-94212.60 +/- 49117.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25809905   |
|    mean velocity x      | -0.231        |
|    mean velocity y      | 0.416         |
|    mean velocity z      | 0.763         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.42e+04     |
| time/                   |               |
|    total_timesteps      | 1972500       |
| train/                  |               |
|    approx_kl            | 7.8552985e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.338         |
|    learning_rate        | 0.001         |
|    loss                 | 4.3e+07       |
|    n_updates            | 9630          |
|    policy_gradient_loss | -0.000229     |
|    std                  | 1.55          |
|    value_loss           | 4e+07         |
-------------------------------------------
Eval num_timesteps=1973000, episode_reward=-82261.65 +/- 14624.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42594424 |
|    mean velocity x | -0.433      |
|    mean velocity y | 0.944       |
|    mean velocity z | 2.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1973000     |
------------------------------------
Eval num_timesteps=1973500, episode_reward=-93372.76 +/- 25998.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4620043 |
|    mean velocity x | -0.325     |
|    mean velocity y | 0.842      |
|    mean velocity z | 4.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.34e+04  |
| time/              |            |
|    total_timesteps | 1973500    |
-----------------------------------
Eval num_timesteps=1974000, episode_reward=-86520.06 +/- 26281.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40835443 |
|    mean velocity x | 0.232       |
|    mean velocity y | 1.42        |
|    mean velocity z | 3.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.65e+04   |
| time/              |             |
|    total_timesteps | 1974000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 964     |
|    time_elapsed    | 79488   |
|    total_timesteps | 1974272 |
--------------------------------
Eval num_timesteps=1974500, episode_reward=-81064.97 +/- 57003.72
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.06423131   |
|    mean velocity x      | -0.357        |
|    mean velocity y      | 0.159         |
|    mean velocity z      | 0.709         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.11e+04     |
| time/                   |               |
|    total_timesteps      | 1974500       |
| train/                  |               |
|    approx_kl            | 2.8653536e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+07      |
|    n_updates            | 9640          |
|    policy_gradient_loss | -0.000507     |
|    std                  | 1.55          |
|    value_loss           | 4.51e+07      |
-------------------------------------------
Eval num_timesteps=1975000, episode_reward=-72673.39 +/- 45764.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4086613 |
|    mean velocity x | -0.416     |
|    mean velocity y | 0.59       |
|    mean velocity z | 1.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.27e+04  |
| time/              |            |
|    total_timesteps | 1975000    |
-----------------------------------
Eval num_timesteps=1975500, episode_reward=-105334.65 +/- 25718.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35025367 |
|    mean velocity x | -0.249      |
|    mean velocity y | 0.568       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 1975500     |
------------------------------------
Eval num_timesteps=1976000, episode_reward=-90891.38 +/- 25466.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44940767 |
|    mean velocity x | -0.406      |
|    mean velocity y | 0.827       |
|    mean velocity z | 4.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.09e+04   |
| time/              |             |
|    total_timesteps | 1976000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 965     |
|    time_elapsed    | 79568   |
|    total_timesteps | 1976320 |
--------------------------------
Eval num_timesteps=1976500, episode_reward=-94579.25 +/- 22135.74
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33329627   |
|    mean velocity x      | -1.17         |
|    mean velocity y      | -0.0421       |
|    mean velocity z      | 2.89          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.46e+04     |
| time/                   |               |
|    total_timesteps      | 1976500       |
| train/                  |               |
|    approx_kl            | 3.4506753e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.001         |
|    loss                 | 5.48e+06      |
|    n_updates            | 9650          |
|    policy_gradient_loss | -0.000514     |
|    std                  | 1.55          |
|    value_loss           | 6.68e+07      |
-------------------------------------------
Eval num_timesteps=1977000, episode_reward=-86233.65 +/- 32984.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44848028 |
|    mean velocity x | 0.34        |
|    mean velocity y | 1.32        |
|    mean velocity z | 2.97        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 1977000     |
------------------------------------
Eval num_timesteps=1977500, episode_reward=-83461.66 +/- 46267.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32692468 |
|    mean velocity x | -0.66       |
|    mean velocity y | 0.233       |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.35e+04   |
| time/              |             |
|    total_timesteps | 1977500     |
------------------------------------
Eval num_timesteps=1978000, episode_reward=-66083.17 +/- 40306.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43035153 |
|    mean velocity x | -0.304      |
|    mean velocity y | 0.672       |
|    mean velocity z | 3.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.61e+04   |
| time/              |             |
|    total_timesteps | 1978000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 966     |
|    time_elapsed    | 79649   |
|    total_timesteps | 1978368 |
--------------------------------
Eval num_timesteps=1978500, episode_reward=-45530.66 +/- 40239.79
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4535705    |
|    mean velocity x      | 0.0252        |
|    mean velocity y      | 0.797         |
|    mean velocity z      | 2.85          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.55e+04     |
| time/                   |               |
|    total_timesteps      | 1978500       |
| train/                  |               |
|    approx_kl            | 2.3231638e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.426         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+07       |
|    n_updates            | 9660          |
|    policy_gradient_loss | -0.000359     |
|    std                  | 1.55          |
|    value_loss           | 3.2e+07       |
-------------------------------------------
Eval num_timesteps=1979000, episode_reward=-72271.48 +/- 39641.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49971172 |
|    mean velocity x | -0.276      |
|    mean velocity y | 1.42        |
|    mean velocity z | 3.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 1979000     |
------------------------------------
Eval num_timesteps=1979500, episode_reward=-55711.50 +/- 48083.70
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.382744 |
|    mean velocity x | -0.491    |
|    mean velocity y | 0.421     |
|    mean velocity z | 2.97      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -5.57e+04 |
| time/              |           |
|    total_timesteps | 1979500   |
----------------------------------
Eval num_timesteps=1980000, episode_reward=-56199.36 +/- 43658.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4000326 |
|    mean velocity x | -0.293     |
|    mean velocity y | 0.919      |
|    mean velocity z | 4.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.62e+04  |
| time/              |            |
|    total_timesteps | 1980000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 967     |
|    time_elapsed    | 79729   |
|    total_timesteps | 1980416 |
--------------------------------
Eval num_timesteps=1980500, episode_reward=-86523.77 +/- 26746.29
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29751477  |
|    mean velocity x      | 0.323        |
|    mean velocity y      | 0.776        |
|    mean velocity z      | 2.98         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.65e+04    |
| time/                   |              |
|    total_timesteps      | 1980500      |
| train/                  |              |
|    approx_kl            | 7.875089e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.001        |
|    loss                 | 2.53e+06     |
|    n_updates            | 9670         |
|    policy_gradient_loss | -0.000193    |
|    std                  | 1.55         |
|    value_loss           | 5.31e+07     |
------------------------------------------
Eval num_timesteps=1981000, episode_reward=-64329.16 +/- 63643.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30854335 |
|    mean velocity x | -0.196      |
|    mean velocity y | 0.43        |
|    mean velocity z | 1.24        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.43e+04   |
| time/              |             |
|    total_timesteps | 1981000     |
------------------------------------
Eval num_timesteps=1981500, episode_reward=-113500.89 +/- 18649.02
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3214202 |
|    mean velocity x | -0.263     |
|    mean velocity y | 0.711      |
|    mean velocity z | 1.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.14e+05  |
| time/              |            |
|    total_timesteps | 1981500    |
-----------------------------------
Eval num_timesteps=1982000, episode_reward=-77847.72 +/- 23912.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31974956 |
|    mean velocity x | -0.901      |
|    mean velocity y | 0.459       |
|    mean velocity z | 1.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 1982000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 968     |
|    time_elapsed    | 79810   |
|    total_timesteps | 1982464 |
--------------------------------
Eval num_timesteps=1982500, episode_reward=-43743.80 +/- 42074.23
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48986882   |
|    mean velocity x      | -0.436        |
|    mean velocity y      | 0.805         |
|    mean velocity z      | 4.62          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.37e+04     |
| time/                   |               |
|    total_timesteps      | 1982500       |
| train/                  |               |
|    approx_kl            | 1.0528602e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.228         |
|    learning_rate        | 0.001         |
|    loss                 | 8.28e+05      |
|    n_updates            | 9680          |
|    policy_gradient_loss | -0.000188     |
|    std                  | 1.55          |
|    value_loss           | 3.66e+07      |
-------------------------------------------
Eval num_timesteps=1983000, episode_reward=-99743.39 +/- 12607.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4262642 |
|    mean velocity x | 0.0107     |
|    mean velocity y | 0.773      |
|    mean velocity z | 3.52       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.97e+04  |
| time/              |            |
|    total_timesteps | 1983000    |
-----------------------------------
Eval num_timesteps=1983500, episode_reward=-109953.10 +/- 27292.76
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40453458 |
|    mean velocity x | -0.323      |
|    mean velocity y | 0.628       |
|    mean velocity z | 4.6         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 1983500     |
------------------------------------
Eval num_timesteps=1984000, episode_reward=-77802.64 +/- 49378.45
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21420874 |
|    mean velocity x | -0.362      |
|    mean velocity y | -0.282      |
|    mean velocity z | 3.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 1984000     |
------------------------------------
Eval num_timesteps=1984500, episode_reward=-74181.26 +/- 41613.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39531627 |
|    mean velocity x | -1.35       |
|    mean velocity y | -0.306      |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.42e+04   |
| time/              |             |
|    total_timesteps | 1984500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 969     |
|    time_elapsed    | 79909   |
|    total_timesteps | 1984512 |
--------------------------------
Eval num_timesteps=1985000, episode_reward=-90569.79 +/- 34603.93
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2056521    |
|    mean velocity x      | -1.24         |
|    mean velocity y      | -1.07         |
|    mean velocity z      | 4.73          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.06e+04     |
| time/                   |               |
|    total_timesteps      | 1985000       |
| train/                  |               |
|    approx_kl            | 1.6250327e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.344         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+07      |
|    n_updates            | 9690          |
|    policy_gradient_loss | -0.000184     |
|    std                  | 1.55          |
|    value_loss           | 6.66e+07      |
-------------------------------------------
Eval num_timesteps=1985500, episode_reward=-82368.87 +/- 29653.74
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37266934 |
|    mean velocity x | 0.388       |
|    mean velocity y | 1.12        |
|    mean velocity z | 3.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.24e+04   |
| time/              |             |
|    total_timesteps | 1985500     |
------------------------------------
Eval num_timesteps=1986000, episode_reward=-71039.79 +/- 38480.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37082037 |
|    mean velocity x | -0.49       |
|    mean velocity y | 0.385       |
|    mean velocity z | 3.77        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.1e+04    |
| time/              |             |
|    total_timesteps | 1986000     |
------------------------------------
Eval num_timesteps=1986500, episode_reward=-72922.67 +/- 30803.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4337578 |
|    mean velocity x | -0.349     |
|    mean velocity y | 0.796      |
|    mean velocity z | 4.63       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.29e+04  |
| time/              |            |
|    total_timesteps | 1986500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 970     |
|    time_elapsed    | 79989   |
|    total_timesteps | 1986560 |
--------------------------------
Eval num_timesteps=1987000, episode_reward=-76202.17 +/- 41286.00
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.22401552    |
|    mean velocity x      | -1.51          |
|    mean velocity y      | -1.6           |
|    mean velocity z      | 6.08           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -7.62e+04      |
| time/                   |                |
|    total_timesteps      | 1987000        |
| train/                  |                |
|    approx_kl            | 1.16926385e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.356          |
|    learning_rate        | 0.001          |
|    loss                 | 6.62e+07       |
|    n_updates            | 9700           |
|    policy_gradient_loss | -0.000311      |
|    std                  | 1.55           |
|    value_loss           | 7.61e+07       |
--------------------------------------------
Eval num_timesteps=1987500, episode_reward=-79919.16 +/- 42972.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45381105 |
|    mean velocity x | -0.93       |
|    mean velocity y | 0.298       |
|    mean velocity z | 4.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.99e+04   |
| time/              |             |
|    total_timesteps | 1987500     |
------------------------------------
Eval num_timesteps=1988000, episode_reward=-73316.52 +/- 59343.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31664377 |
|    mean velocity x | -1.37       |
|    mean velocity y | -0.0573     |
|    mean velocity z | 2.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.33e+04   |
| time/              |             |
|    total_timesteps | 1988000     |
------------------------------------
Eval num_timesteps=1988500, episode_reward=-75872.44 +/- 36989.53
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2925716 |
|    mean velocity x | -0.503     |
|    mean velocity y | -0.339     |
|    mean velocity z | 3.66       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.59e+04  |
| time/              |            |
|    total_timesteps | 1988500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 971     |
|    time_elapsed    | 80070   |
|    total_timesteps | 1988608 |
--------------------------------
Eval num_timesteps=1989000, episode_reward=-68545.32 +/- 48037.66
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24177018   |
|    mean velocity x      | -0.152        |
|    mean velocity y      | 0.686         |
|    mean velocity z      | 0.719         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.85e+04     |
| time/                   |               |
|    total_timesteps      | 1989000       |
| train/                  |               |
|    approx_kl            | 2.0771957e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.494         |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+07      |
|    n_updates            | 9710          |
|    policy_gradient_loss | -0.000383     |
|    std                  | 1.55          |
|    value_loss           | 3.7e+07       |
-------------------------------------------
Eval num_timesteps=1989500, episode_reward=-76175.01 +/- 34901.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4926816 |
|    mean velocity x | -0.444     |
|    mean velocity y | 0.608      |
|    mean velocity z | 3.91       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.62e+04  |
| time/              |            |
|    total_timesteps | 1989500    |
-----------------------------------
Eval num_timesteps=1990000, episode_reward=-101573.63 +/- 27101.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42194661 |
|    mean velocity x | -0.348      |
|    mean velocity y | 0.656       |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 1990000     |
------------------------------------
Eval num_timesteps=1990500, episode_reward=-88044.08 +/- 33825.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48956308 |
|    mean velocity x | -0.373      |
|    mean velocity y | 0.849       |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.8e+04    |
| time/              |             |
|    total_timesteps | 1990500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 972     |
|    time_elapsed    | 80150   |
|    total_timesteps | 1990656 |
--------------------------------
Eval num_timesteps=1991000, episode_reward=-54905.44 +/- 45621.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27130532   |
|    mean velocity x      | -0.218        |
|    mean velocity y      | 0.52          |
|    mean velocity z      | 0.489         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.49e+04     |
| time/                   |               |
|    total_timesteps      | 1991000       |
| train/                  |               |
|    approx_kl            | 2.8607465e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 5.84e+07      |
|    n_updates            | 9720          |
|    policy_gradient_loss | -0.000336     |
|    std                  | 1.55          |
|    value_loss           | 7.12e+07      |
-------------------------------------------
Eval num_timesteps=1991500, episode_reward=-50843.80 +/- 38003.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23004778 |
|    mean velocity x | -0.0214     |
|    mean velocity y | 0.412       |
|    mean velocity z | 0.455       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.08e+04   |
| time/              |             |
|    total_timesteps | 1991500     |
------------------------------------
Eval num_timesteps=1992000, episode_reward=-67288.11 +/- 32155.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35788393 |
|    mean velocity x | -0.697      |
|    mean velocity y | 0.308       |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.73e+04   |
| time/              |             |
|    total_timesteps | 1992000     |
------------------------------------
Eval num_timesteps=1992500, episode_reward=-61728.72 +/- 39692.27
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3553884 |
|    mean velocity x | -0.347     |
|    mean velocity y | 0.569      |
|    mean velocity z | 4.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.17e+04  |
| time/              |            |
|    total_timesteps | 1992500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 973     |
|    time_elapsed    | 80230   |
|    total_timesteps | 1992704 |
--------------------------------
Eval num_timesteps=1993000, episode_reward=-76688.76 +/- 37823.86
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40261152   |
|    mean velocity x      | -0.302        |
|    mean velocity y      | 0.714         |
|    mean velocity z      | 4.32          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.67e+04     |
| time/                   |               |
|    total_timesteps      | 1993000       |
| train/                  |               |
|    approx_kl            | 9.2713744e-05 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.001         |
|    loss                 | 6.07e+07      |
|    n_updates            | 9730          |
|    policy_gradient_loss | -0.00114      |
|    std                  | 1.55          |
|    value_loss           | 7.87e+07      |
-------------------------------------------
Eval num_timesteps=1993500, episode_reward=-51954.98 +/- 41953.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4167397 |
|    mean velocity x | -0.306     |
|    mean velocity y | 0.836      |
|    mean velocity z | 4.71       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.2e+04   |
| time/              |            |
|    total_timesteps | 1993500    |
-----------------------------------
Eval num_timesteps=1994000, episode_reward=-86103.44 +/- 33612.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43697736 |
|    mean velocity x | -0.291      |
|    mean velocity y | 0.786       |
|    mean velocity z | 4.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.61e+04   |
| time/              |             |
|    total_timesteps | 1994000     |
------------------------------------
Eval num_timesteps=1994500, episode_reward=-104475.00 +/- 17687.13
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3192473 |
|    mean velocity x | -0.0192    |
|    mean velocity y | 0.4        |
|    mean velocity z | 3.02       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1994500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 974     |
|    time_elapsed    | 80311   |
|    total_timesteps | 1994752 |
--------------------------------
Eval num_timesteps=1995000, episode_reward=-79028.52 +/- 47641.06
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29675624  |
|    mean velocity x      | -0.371       |
|    mean velocity y      | 0.455        |
|    mean velocity z      | 0.591        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.9e+04     |
| time/                   |              |
|    total_timesteps      | 1995000      |
| train/                  |              |
|    approx_kl            | 1.086478e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.001        |
|    loss                 | 4.85e+07     |
|    n_updates            | 9740         |
|    policy_gradient_loss | -0.000282    |
|    std                  | 1.55         |
|    value_loss           | 7.42e+07     |
------------------------------------------
Eval num_timesteps=1995500, episode_reward=-70271.48 +/- 6642.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42753375 |
|    mean velocity x | -0.64       |
|    mean velocity y | 0.433       |
|    mean velocity z | 3.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.03e+04   |
| time/              |             |
|    total_timesteps | 1995500     |
------------------------------------
Eval num_timesteps=1996000, episode_reward=-93366.10 +/- 33385.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37577948 |
|    mean velocity x | 0.338       |
|    mean velocity y | 0.686       |
|    mean velocity z | 1.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.34e+04   |
| time/              |             |
|    total_timesteps | 1996000     |
------------------------------------
Eval num_timesteps=1996500, episode_reward=-86154.94 +/- 45733.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34409943 |
|    mean velocity x | -0.428      |
|    mean velocity y | 0.386       |
|    mean velocity z | 3.71        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 1996500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 975     |
|    time_elapsed    | 80391   |
|    total_timesteps | 1996800 |
--------------------------------
Eval num_timesteps=1997000, episode_reward=-79765.33 +/- 37100.69
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35057428   |
|    mean velocity x      | -0.677        |
|    mean velocity y      | 0.729         |
|    mean velocity z      | 1.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.98e+04     |
| time/                   |               |
|    total_timesteps      | 1997000       |
| train/                  |               |
|    approx_kl            | 3.3290708e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.284         |
|    learning_rate        | 0.001         |
|    loss                 | 2.64e+07      |
|    n_updates            | 9750          |
|    policy_gradient_loss | -0.000117     |
|    std                  | 1.55          |
|    value_loss           | 4.01e+07      |
-------------------------------------------
Eval num_timesteps=1997500, episode_reward=-79250.53 +/- 47595.18
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35637462 |
|    mean velocity x | 0.191       |
|    mean velocity y | 0.926       |
|    mean velocity z | 3.22        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.93e+04   |
| time/              |             |
|    total_timesteps | 1997500     |
------------------------------------
Eval num_timesteps=1998000, episode_reward=-68148.52 +/- 40935.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36818147 |
|    mean velocity x | -0.164      |
|    mean velocity y | 1.12        |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1998000     |
------------------------------------
Eval num_timesteps=1998500, episode_reward=-55028.04 +/- 42046.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47193298 |
|    mean velocity x | 0.422       |
|    mean velocity y | 1.36        |
|    mean velocity z | 3.52        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.5e+04    |
| time/              |             |
|    total_timesteps | 1998500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 976     |
|    time_elapsed    | 80472   |
|    total_timesteps | 1998848 |
--------------------------------
Eval num_timesteps=1999000, episode_reward=-63563.11 +/- 24288.21
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5134529    |
|    mean velocity x      | -0.015        |
|    mean velocity y      | 1.79          |
|    mean velocity z      | 4.05          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.36e+04     |
| time/                   |               |
|    total_timesteps      | 1999000       |
| train/                  |               |
|    approx_kl            | 7.3037518e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.421         |
|    learning_rate        | 0.001         |
|    loss                 | 3.22e+07      |
|    n_updates            | 9760          |
|    policy_gradient_loss | -0.000156     |
|    std                  | 1.55          |
|    value_loss           | 4.65e+07      |
-------------------------------------------
Eval num_timesteps=1999500, episode_reward=-105586.79 +/- 18933.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44564208 |
|    mean velocity x | -0.198      |
|    mean velocity y | 0.834       |
|    mean velocity z | 4.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1999500     |
------------------------------------
Eval num_timesteps=2000000, episode_reward=-92784.27 +/- 39202.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2339889 |
|    mean velocity x | -0.078     |
|    mean velocity y | 0.326      |
|    mean velocity z | 0.487      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.28e+04  |
| time/              |            |
|    total_timesteps | 2000000    |
-----------------------------------
Eval num_timesteps=2000500, episode_reward=-60488.15 +/- 35191.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35606465 |
|    mean velocity x | -0.59       |
|    mean velocity y | 0.566       |
|    mean velocity z | 3.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.05e+04   |
| time/              |             |
|    total_timesteps | 2000500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 977     |
|    time_elapsed    | 80552   |
|    total_timesteps | 2000896 |
--------------------------------
Eval num_timesteps=2001000, episode_reward=-102418.86 +/- 29332.62
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.37107262    |
|    mean velocity x      | -0.778         |
|    mean velocity y      | -0.129         |
|    mean velocity z      | 3.47           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -1.02e+05      |
| time/                   |                |
|    total_timesteps      | 2001000        |
| train/                  |                |
|    approx_kl            | 1.19777105e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.287          |
|    learning_rate        | 0.001          |
|    loss                 | 3.65e+07       |
|    n_updates            | 9770           |
|    policy_gradient_loss | -0.000284      |
|    std                  | 1.55           |
|    value_loss           | 5.28e+07       |
--------------------------------------------
Eval num_timesteps=2001500, episode_reward=-90553.40 +/- 18293.25
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.106522754 |
|    mean velocity x | 0.0551       |
|    mean velocity y | 0.286        |
|    mean velocity z | 0.238        |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -9.06e+04    |
| time/              |              |
|    total_timesteps | 2001500      |
-------------------------------------
Eval num_timesteps=2002000, episode_reward=-75454.43 +/- 44025.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29711726 |
|    mean velocity x | -0.444      |
|    mean velocity y | 0.0619      |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 2002000     |
------------------------------------
Eval num_timesteps=2002500, episode_reward=-115905.83 +/- 6199.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33699536 |
|    mean velocity x | -0.211      |
|    mean velocity y | 1.01        |
|    mean velocity z | 4.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.16e+05   |
| time/              |             |
|    total_timesteps | 2002500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 978     |
|    time_elapsed    | 80632   |
|    total_timesteps | 2002944 |
--------------------------------
Eval num_timesteps=2003000, episode_reward=-88493.53 +/- 37344.26
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3597356    |
|    mean velocity x      | 0.0558        |
|    mean velocity y      | 0.612         |
|    mean velocity z      | 3.35          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.85e+04     |
| time/                   |               |
|    total_timesteps      | 2003000       |
| train/                  |               |
|    approx_kl            | 1.2373028e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.296         |
|    learning_rate        | 0.001         |
|    loss                 | 3.7e+07       |
|    n_updates            | 9780          |
|    policy_gradient_loss | -0.000379     |
|    std                  | 1.55          |
|    value_loss           | 5.93e+07      |
-------------------------------------------
Eval num_timesteps=2003500, episode_reward=-80333.46 +/- 44708.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4174308 |
|    mean velocity x | -0.0261    |
|    mean velocity y | 1.42       |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.03e+04  |
| time/              |            |
|    total_timesteps | 2003500    |
-----------------------------------
Eval num_timesteps=2004000, episode_reward=-82192.72 +/- 33465.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14013769 |
|    mean velocity x | -0.0487     |
|    mean velocity y | 0.61        |
|    mean velocity z | 0.122       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.22e+04   |
| time/              |             |
|    total_timesteps | 2004000     |
------------------------------------
Eval num_timesteps=2004500, episode_reward=-111321.00 +/- 13379.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5285864 |
|    mean velocity x | 0.689      |
|    mean velocity y | 2.05       |
|    mean velocity z | 3.93       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.11e+05  |
| time/              |            |
|    total_timesteps | 2004500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 979     |
|    time_elapsed    | 80713   |
|    total_timesteps | 2004992 |
--------------------------------
Eval num_timesteps=2005000, episode_reward=-66681.03 +/- 33886.60
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.44198745    |
|    mean velocity x      | -0.127         |
|    mean velocity y      | 0.875          |
|    mean velocity z      | 3.51           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -6.67e+04      |
| time/                   |                |
|    total_timesteps      | 2005000        |
| train/                  |                |
|    approx_kl            | 1.52311695e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.441          |
|    learning_rate        | 0.001          |
|    loss                 | 3.32e+06       |
|    n_updates            | 9790           |
|    policy_gradient_loss | -0.000321      |
|    std                  | 1.55           |
|    value_loss           | 2.99e+07       |
--------------------------------------------
Eval num_timesteps=2005500, episode_reward=-81507.06 +/- 40409.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28256917 |
|    mean velocity x | 0.34        |
|    mean velocity y | 1.25        |
|    mean velocity z | 3.56        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.15e+04   |
| time/              |             |
|    total_timesteps | 2005500     |
------------------------------------
Eval num_timesteps=2006000, episode_reward=-86018.87 +/- 47167.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37295958 |
|    mean velocity x | -0.926      |
|    mean velocity y | -0.105      |
|    mean velocity z | 3.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.6e+04    |
| time/              |             |
|    total_timesteps | 2006000     |
------------------------------------
Eval num_timesteps=2006500, episode_reward=-63090.46 +/- 25882.65
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5963957 |
|    mean velocity x | 0.663      |
|    mean velocity y | 2.35       |
|    mean velocity z | 4.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.31e+04  |
| time/              |            |
|    total_timesteps | 2006500    |
-----------------------------------
Eval num_timesteps=2007000, episode_reward=-65145.44 +/- 42462.39
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3538172 |
|    mean velocity x | -0.172     |
|    mean velocity y | 0.55       |
|    mean velocity z | 3.86       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.51e+04  |
| time/              |            |
|    total_timesteps | 2007000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 980     |
|    time_elapsed    | 80812   |
|    total_timesteps | 2007040 |
--------------------------------
Eval num_timesteps=2007500, episode_reward=-78317.41 +/- 41296.53
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.38856328    |
|    mean velocity x      | -0.286         |
|    mean velocity y      | 0.762          |
|    mean velocity z      | 4.01           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -7.83e+04      |
| time/                   |                |
|    total_timesteps      | 2007500        |
| train/                  |                |
|    approx_kl            | 1.33346475e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.374          |
|    learning_rate        | 0.001          |
|    loss                 | 2.01e+07       |
|    n_updates            | 9800           |
|    policy_gradient_loss | -0.000165      |
|    std                  | 1.55           |
|    value_loss           | 7.03e+07       |
--------------------------------------------
Eval num_timesteps=2008000, episode_reward=-63059.73 +/- 31329.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09890602 |
|    mean velocity x | -1.45       |
|    mean velocity y | -1.25       |
|    mean velocity z | 4           |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.31e+04   |
| time/              |             |
|    total_timesteps | 2008000     |
------------------------------------
Eval num_timesteps=2008500, episode_reward=-56763.77 +/- 41199.53
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38827237 |
|    mean velocity x | -0.923      |
|    mean velocity y | 0.145       |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.68e+04   |
| time/              |             |
|    total_timesteps | 2008500     |
------------------------------------
Eval num_timesteps=2009000, episode_reward=-99149.20 +/- 20307.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43807116 |
|    mean velocity x | -1.18       |
|    mean velocity y | -0.0796     |
|    mean velocity z | 3.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.91e+04   |
| time/              |             |
|    total_timesteps | 2009000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 981     |
|    time_elapsed    | 80892   |
|    total_timesteps | 2009088 |
--------------------------------
Eval num_timesteps=2009500, episode_reward=-87180.53 +/- 57480.60
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3273753   |
|    mean velocity x      | -0.0689      |
|    mean velocity y      | 0.61         |
|    mean velocity z      | 3.39         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.72e+04    |
| time/                   |              |
|    total_timesteps      | 2009500      |
| train/                  |              |
|    approx_kl            | 2.843258e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.498        |
|    learning_rate        | 0.001        |
|    loss                 | 9.15e+06     |
|    n_updates            | 9810         |
|    policy_gradient_loss | -0.000507    |
|    std                  | 1.55         |
|    value_loss           | 3.76e+07     |
------------------------------------------
Eval num_timesteps=2010000, episode_reward=-55209.95 +/- 46368.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31765392 |
|    mean velocity x | -0.355      |
|    mean velocity y | 0.719       |
|    mean velocity z | 0.709       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.52e+04   |
| time/              |             |
|    total_timesteps | 2010000     |
------------------------------------
Eval num_timesteps=2010500, episode_reward=-88985.51 +/- 40987.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31411391 |
|    mean velocity x | 0.186       |
|    mean velocity y | 1.12        |
|    mean velocity z | 3.19        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.9e+04    |
| time/              |             |
|    total_timesteps | 2010500     |
------------------------------------
Eval num_timesteps=2011000, episode_reward=-102032.46 +/- 26290.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42891952 |
|    mean velocity x | 0.229       |
|    mean velocity y | 1.46        |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 2011000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 982     |
|    time_elapsed    | 80973   |
|    total_timesteps | 2011136 |
--------------------------------
Eval num_timesteps=2011500, episode_reward=-101872.96 +/- 21620.49
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34559843   |
|    mean velocity x      | -0.06         |
|    mean velocity y      | 0.99          |
|    mean velocity z      | 4.25          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 2011500       |
| train/                  |               |
|    approx_kl            | 2.1283398e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.35          |
|    learning_rate        | 0.001         |
|    loss                 | 4.34e+07      |
|    n_updates            | 9820          |
|    policy_gradient_loss | -0.000339     |
|    std                  | 1.55          |
|    value_loss           | 4.96e+07      |
-------------------------------------------
Eval num_timesteps=2012000, episode_reward=-80567.86 +/- 19244.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39862955 |
|    mean velocity x | -0.105      |
|    mean velocity y | 1.31        |
|    mean velocity z | 4.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 2012000     |
------------------------------------
Eval num_timesteps=2012500, episode_reward=-63007.92 +/- 32946.74
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3855553 |
|    mean velocity x | 0.399      |
|    mean velocity y | 1.35       |
|    mean velocity z | 3.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.3e+04   |
| time/              |            |
|    total_timesteps | 2012500    |
-----------------------------------
Eval num_timesteps=2013000, episode_reward=-68382.67 +/- 19802.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39815095 |
|    mean velocity x | -0.595      |
|    mean velocity y | 0.841       |
|    mean velocity z | 1.87        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 2013000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 983     |
|    time_elapsed    | 81053   |
|    total_timesteps | 2013184 |
--------------------------------
Eval num_timesteps=2013500, episode_reward=-110323.49 +/- 16229.81
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27910557  |
|    mean velocity x      | -0.623       |
|    mean velocity y      | 0.615        |
|    mean velocity z      | 1.8          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.1e+05     |
| time/                   |              |
|    total_timesteps      | 2013500      |
| train/                  |              |
|    approx_kl            | 7.123186e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+07     |
|    n_updates            | 9830         |
|    policy_gradient_loss | -0.000772    |
|    std                  | 1.55         |
|    value_loss           | 4.34e+07     |
------------------------------------------
Eval num_timesteps=2014000, episode_reward=-68023.63 +/- 40018.40
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4137073 |
|    mean velocity x | -0.243     |
|    mean velocity y | 0.95       |
|    mean velocity z | 4.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.8e+04   |
| time/              |            |
|    total_timesteps | 2014000    |
-----------------------------------
Eval num_timesteps=2014500, episode_reward=-61465.01 +/- 44375.22
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5455342 |
|    mean velocity x | -0.712     |
|    mean velocity y | 0.78       |
|    mean velocity z | 4.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.15e+04  |
| time/              |            |
|    total_timesteps | 2014500    |
-----------------------------------
Eval num_timesteps=2015000, episode_reward=-112363.33 +/- 14151.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39886865 |
|    mean velocity x | -0.377      |
|    mean velocity y | 0.835       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 2015000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 984     |
|    time_elapsed    | 81133   |
|    total_timesteps | 2015232 |
--------------------------------
Eval num_timesteps=2015500, episode_reward=-66967.49 +/- 27607.87
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5123008    |
|    mean velocity x      | 0.553         |
|    mean velocity y      | 1.54          |
|    mean velocity z      | 3.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.7e+04      |
| time/                   |               |
|    total_timesteps      | 2015500       |
| train/                  |               |
|    approx_kl            | 5.7330763e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.001         |
|    loss                 | 2.38e+07      |
|    n_updates            | 9840          |
|    policy_gradient_loss | -9.79e-05     |
|    std                  | 1.55          |
|    value_loss           | 8.1e+07       |
-------------------------------------------
Eval num_timesteps=2016000, episode_reward=-83126.06 +/- 17359.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47873145 |
|    mean velocity x | -0.309      |
|    mean velocity y | 0.915       |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 2016000     |
------------------------------------
Eval num_timesteps=2016500, episode_reward=-41763.48 +/- 41136.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38147873 |
|    mean velocity x | -0.185      |
|    mean velocity y | 0.8         |
|    mean velocity z | 3.68        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.18e+04   |
| time/              |             |
|    total_timesteps | 2016500     |
------------------------------------
Eval num_timesteps=2017000, episode_reward=-95790.40 +/- 13386.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3400555 |
|    mean velocity x | -0.183     |
|    mean velocity y | 0.69       |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.58e+04  |
| time/              |            |
|    total_timesteps | 2017000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 985     |
|    time_elapsed    | 81214   |
|    total_timesteps | 2017280 |
--------------------------------
Eval num_timesteps=2017500, episode_reward=-97302.46 +/- 20051.01
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.16211918  |
|    mean velocity x      | 0.0422       |
|    mean velocity y      | 0.206        |
|    mean velocity z      | 0.429        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.73e+04    |
| time/                   |              |
|    total_timesteps      | 2017500      |
| train/                  |              |
|    approx_kl            | 8.795201e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+07      |
|    n_updates            | 9850         |
|    policy_gradient_loss | -0.000579    |
|    std                  | 1.55         |
|    value_loss           | 6.08e+07     |
------------------------------------------
Eval num_timesteps=2018000, episode_reward=-85705.05 +/- 21900.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31823117 |
|    mean velocity x | -1.24       |
|    mean velocity y | -0.241      |
|    mean velocity z | 3.39        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.57e+04   |
| time/              |             |
|    total_timesteps | 2018000     |
------------------------------------
Eval num_timesteps=2018500, episode_reward=-87734.51 +/- 28100.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36017632 |
|    mean velocity x | 0.354       |
|    mean velocity y | 1.03        |
|    mean velocity z | 3.43        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 2018500     |
------------------------------------
Eval num_timesteps=2019000, episode_reward=-87403.15 +/- 47552.21
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.321634 |
|    mean velocity x | -0.838    |
|    mean velocity y | 0.527     |
|    mean velocity z | 1.72      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -8.74e+04 |
| time/              |           |
|    total_timesteps | 2019000   |
----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 986     |
|    time_elapsed    | 81294   |
|    total_timesteps | 2019328 |
--------------------------------
Eval num_timesteps=2019500, episode_reward=-64531.15 +/- 41787.00
Episode length: 5000.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.264951      |
|    mean velocity x      | -0.358         |
|    mean velocity y      | 0.273          |
|    mean velocity z      | 2.61           |
|    mean_ep_length       | 5e+03          |
|    mean_reward          | -6.45e+04      |
| time/                   |                |
|    total_timesteps      | 2019500        |
| train/                  |                |
|    approx_kl            | 1.38711475e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.56          |
|    explained_variance   | 0.469          |
|    learning_rate        | 0.001          |
|    loss                 | 7.18e+06       |
|    n_updates            | 9860           |
|    policy_gradient_loss | -0.000223      |
|    std                  | 1.55           |
|    value_loss           | 2.4e+07        |
--------------------------------------------
Eval num_timesteps=2020000, episode_reward=-88648.59 +/- 44002.63
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4615171 |
|    mean velocity x | 0.581      |
|    mean velocity y | 1.46       |
|    mean velocity z | 3.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.86e+04  |
| time/              |            |
|    total_timesteps | 2020000    |
-----------------------------------
Eval num_timesteps=2020500, episode_reward=-94202.93 +/- 36434.57
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44216463 |
|    mean velocity x | 0.233       |
|    mean velocity y | 1.13        |
|    mean velocity z | 2.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.42e+04   |
| time/              |             |
|    total_timesteps | 2020500     |
------------------------------------
Eval num_timesteps=2021000, episode_reward=-61773.89 +/- 43846.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43823767 |
|    mean velocity x | -0.272      |
|    mean velocity y | 0.816       |
|    mean velocity z | 4.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 2021000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 987     |
|    time_elapsed    | 81375   |
|    total_timesteps | 2021376 |
--------------------------------
Eval num_timesteps=2021500, episode_reward=-89354.46 +/- 20266.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32516      |
|    mean velocity x      | 0.161         |
|    mean velocity y      | 0.687         |
|    mean velocity z      | 3.46          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.94e+04     |
| time/                   |               |
|    total_timesteps      | 2021500       |
| train/                  |               |
|    approx_kl            | 1.1939264e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.001         |
|    loss                 | 4.36e+07      |
|    n_updates            | 9870          |
|    policy_gradient_loss | -0.000338     |
|    std                  | 1.55          |
|    value_loss           | 5.65e+07      |
-------------------------------------------
Eval num_timesteps=2022000, episode_reward=-86247.50 +/- 31476.29
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4201478 |
|    mean velocity x | -0.437     |
|    mean velocity y | 0.68       |
|    mean velocity z | 3.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.62e+04  |
| time/              |            |
|    total_timesteps | 2022000    |
-----------------------------------
Eval num_timesteps=2022500, episode_reward=-65475.79 +/- 31343.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16192967 |
|    mean velocity x | -1.52       |
|    mean velocity y | -0.858      |
|    mean velocity z | 3.96        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.55e+04   |
| time/              |             |
|    total_timesteps | 2022500     |
------------------------------------
Eval num_timesteps=2023000, episode_reward=-109032.16 +/- 19207.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38315636 |
|    mean velocity x | -0.159      |
|    mean velocity y | 1.03        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 2023000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 988     |
|    time_elapsed    | 81455   |
|    total_timesteps | 2023424 |
--------------------------------
Eval num_timesteps=2023500, episode_reward=-74476.79 +/- 52096.77
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35170782   |
|    mean velocity x      | -1.46         |
|    mean velocity y      | -0.789        |
|    mean velocity z      | 5.44          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.45e+04     |
| time/                   |               |
|    total_timesteps      | 2023500       |
| train/                  |               |
|    approx_kl            | 1.9635132e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.401         |
|    learning_rate        | 0.001         |
|    loss                 | 6.91e+06      |
|    n_updates            | 9880          |
|    policy_gradient_loss | -0.000305     |
|    std                  | 1.55          |
|    value_loss           | 6.04e+07      |
-------------------------------------------
Eval num_timesteps=2024000, episode_reward=-93277.58 +/- 27740.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34751892 |
|    mean velocity x | 0.0532      |
|    mean velocity y | 0.744       |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.33e+04   |
| time/              |             |
|    total_timesteps | 2024000     |
------------------------------------
Eval num_timesteps=2024500, episode_reward=-27291.16 +/- 19995.47
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3696657 |
|    mean velocity x | -0.889     |
|    mean velocity y | -0.369     |
|    mean velocity z | 3.64       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -2.73e+04  |
| time/              |            |
|    total_timesteps | 2024500    |
-----------------------------------
Eval num_timesteps=2025000, episode_reward=-80908.50 +/- 36451.11
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3903302 |
|    mean velocity x | -0.924     |
|    mean velocity y | 0.241      |
|    mean velocity z | 3.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.09e+04  |
| time/              |            |
|    total_timesteps | 2025000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 989     |
|    time_elapsed    | 81535   |
|    total_timesteps | 2025472 |
--------------------------------
Eval num_timesteps=2025500, episode_reward=-44904.37 +/- 35099.38
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.27363405   |
|    mean velocity x      | -0.168        |
|    mean velocity y      | 0.388         |
|    mean velocity z      | 0.422         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -4.49e+04     |
| time/                   |               |
|    total_timesteps      | 2025500       |
| train/                  |               |
|    approx_kl            | 3.9969513e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.444         |
|    learning_rate        | 0.001         |
|    loss                 | 4.22e+06      |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.000516     |
|    std                  | 1.55          |
|    value_loss           | 3.64e+07      |
-------------------------------------------
Eval num_timesteps=2026000, episode_reward=-97353.33 +/- 22152.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38350457 |
|    mean velocity x | -0.84       |
|    mean velocity y | 0.147       |
|    mean velocity z | 2.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.74e+04   |
| time/              |             |
|    total_timesteps | 2026000     |
------------------------------------
Eval num_timesteps=2026500, episode_reward=-79727.36 +/- 21388.82
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3628974 |
|    mean velocity x | -0.158     |
|    mean velocity y | 1.19       |
|    mean velocity z | 4.11       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.97e+04  |
| time/              |            |
|    total_timesteps | 2026500    |
-----------------------------------
Eval num_timesteps=2027000, episode_reward=-42105.40 +/- 22666.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37985885 |
|    mean velocity x | -0.529      |
|    mean velocity y | 0.112       |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.21e+04   |
| time/              |             |
|    total_timesteps | 2027000     |
------------------------------------
Eval num_timesteps=2027500, episode_reward=-66522.29 +/- 20342.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37175807 |
|    mean velocity x | -1.27       |
|    mean velocity y | -0.0627     |
|    mean velocity z | 3.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.65e+04   |
| time/              |             |
|    total_timesteps | 2027500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 990     |
|    time_elapsed    | 81635   |
|    total_timesteps | 2027520 |
--------------------------------
Eval num_timesteps=2028000, episode_reward=-54622.56 +/- 44226.82
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2709269   |
|    mean velocity x      | -0.355       |
|    mean velocity y      | 0.536        |
|    mean velocity z      | 1.27         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.46e+04    |
| time/                   |              |
|    total_timesteps      | 2028000      |
| train/                  |              |
|    approx_kl            | 7.263644e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.385        |
|    learning_rate        | 0.001        |
|    loss                 | 3.08e+07     |
|    n_updates            | 9900         |
|    policy_gradient_loss | -0.000355    |
|    std                  | 1.55         |
|    value_loss           | 4.89e+07     |
------------------------------------------
Eval num_timesteps=2028500, episode_reward=-47599.64 +/- 46511.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45418346 |
|    mean velocity x | -0.159      |
|    mean velocity y | 1.05        |
|    mean velocity z | 4.2         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.76e+04   |
| time/              |             |
|    total_timesteps | 2028500     |
------------------------------------
Eval num_timesteps=2029000, episode_reward=-78224.64 +/- 39188.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44146305 |
|    mean velocity x | 0.207       |
|    mean velocity y | 1.63        |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 2029000     |
------------------------------------
Eval num_timesteps=2029500, episode_reward=-74310.44 +/- 26393.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39062712 |
|    mean velocity x | -0.0331     |
|    mean velocity y | 1.4         |
|    mean velocity z | 3.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 2029500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 991     |
|    time_elapsed    | 81715   |
|    total_timesteps | 2029568 |
--------------------------------
Eval num_timesteps=2030000, episode_reward=-71341.75 +/- 28522.51
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4457356   |
|    mean velocity x      | -0.152       |
|    mean velocity y      | 1.08         |
|    mean velocity z      | 3.42         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.13e+04    |
| time/                   |              |
|    total_timesteps      | 2030000      |
| train/                  |              |
|    approx_kl            | 1.593915e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+07     |
|    n_updates            | 9910         |
|    policy_gradient_loss | -0.000355    |
|    std                  | 1.55         |
|    value_loss           | 6.67e+07     |
------------------------------------------
Eval num_timesteps=2030500, episode_reward=-93009.41 +/- 23613.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30134177 |
|    mean velocity x | -0.251      |
|    mean velocity y | 0.579       |
|    mean velocity z | 3.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.3e+04    |
| time/              |             |
|    total_timesteps | 2030500     |
------------------------------------
Eval num_timesteps=2031000, episode_reward=-67016.91 +/- 43006.84
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31764743 |
|    mean velocity x | 0.404       |
|    mean velocity y | 0.69        |
|    mean velocity z | 1.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 2031000     |
------------------------------------
Eval num_timesteps=2031500, episode_reward=-58646.00 +/- 22944.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31091782 |
|    mean velocity x | -0.165      |
|    mean velocity y | 0.323       |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.86e+04   |
| time/              |             |
|    total_timesteps | 2031500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 992     |
|    time_elapsed    | 81795   |
|    total_timesteps | 2031616 |
--------------------------------
Eval num_timesteps=2032000, episode_reward=-74110.54 +/- 41685.27
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25510722   |
|    mean velocity x      | -1.08         |
|    mean velocity y      | -0.679        |
|    mean velocity z      | 4.47          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.41e+04     |
| time/                   |               |
|    total_timesteps      | 2032000       |
| train/                  |               |
|    approx_kl            | 1.4733378e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.36          |
|    learning_rate        | 0.001         |
|    loss                 | 4.93e+07      |
|    n_updates            | 9920          |
|    policy_gradient_loss | -0.000359     |
|    std                  | 1.55          |
|    value_loss           | 5.57e+07      |
-------------------------------------------
Eval num_timesteps=2032500, episode_reward=-53909.60 +/- 43933.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29962873 |
|    mean velocity x | -0.87       |
|    mean velocity y | 0.185       |
|    mean velocity z | 2.9         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.39e+04   |
| time/              |             |
|    total_timesteps | 2032500     |
------------------------------------
Eval num_timesteps=2033000, episode_reward=-90123.28 +/- 28071.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42270586 |
|    mean velocity x | -0.312      |
|    mean velocity y | 0.688       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.01e+04   |
| time/              |             |
|    total_timesteps | 2033000     |
------------------------------------
Eval num_timesteps=2033500, episode_reward=-90757.28 +/- 14774.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28731504 |
|    mean velocity x | -0.213      |
|    mean velocity y | 0.355       |
|    mean velocity z | 0.495       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.08e+04   |
| time/              |             |
|    total_timesteps | 2033500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 993     |
|    time_elapsed    | 81876   |
|    total_timesteps | 2033664 |
--------------------------------
Eval num_timesteps=2034000, episode_reward=-105070.68 +/- 25603.68
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32477093   |
|    mean velocity x      | -0.805        |
|    mean velocity y      | -0.000778     |
|    mean velocity z      | 3.21          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 2034000       |
| train/                  |               |
|    approx_kl            | 2.5278889e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+07      |
|    n_updates            | 9930          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 1.55          |
|    value_loss           | 4.75e+07      |
-------------------------------------------
Eval num_timesteps=2034500, episode_reward=-88346.21 +/- 21404.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36708215 |
|    mean velocity x | -0.938      |
|    mean velocity y | 0.375       |
|    mean velocity z | 3.12        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.83e+04   |
| time/              |             |
|    total_timesteps | 2034500     |
------------------------------------
Eval num_timesteps=2035000, episode_reward=-103562.18 +/- 19021.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35580793 |
|    mean velocity x | -0.353      |
|    mean velocity y | 0.0187      |
|    mean velocity z | 3.8         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 2035000     |
------------------------------------
Eval num_timesteps=2035500, episode_reward=-89496.81 +/- 9236.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48370892 |
|    mean velocity x | -0.508      |
|    mean velocity y | 0.203       |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.95e+04   |
| time/              |             |
|    total_timesteps | 2035500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 994     |
|    time_elapsed    | 81956   |
|    total_timesteps | 2035712 |
--------------------------------
Eval num_timesteps=2036000, episode_reward=-78225.66 +/- 38038.68
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4070448   |
|    mean velocity x      | -0.105       |
|    mean velocity y      | 0.806        |
|    mean velocity z      | 2.86         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.82e+04    |
| time/                   |              |
|    total_timesteps      | 2036000      |
| train/                  |              |
|    approx_kl            | 6.003509e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.348        |
|    learning_rate        | 0.001        |
|    loss                 | 1.97e+07     |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.000172    |
|    std                  | 1.55         |
|    value_loss           | 5.19e+07     |
------------------------------------------
Eval num_timesteps=2036500, episode_reward=-96139.03 +/- 35398.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34694263 |
|    mean velocity x | -0.314      |
|    mean velocity y | 0.417       |
|    mean velocity z | 3.79        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.61e+04   |
| time/              |             |
|    total_timesteps | 2036500     |
------------------------------------
Eval num_timesteps=2037000, episode_reward=-81150.81 +/- 23228.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25773823 |
|    mean velocity x | -0.306      |
|    mean velocity y | -0.0546     |
|    mean velocity z | 3.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 2037000     |
------------------------------------
Eval num_timesteps=2037500, episode_reward=-103359.20 +/- 19192.78
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5334961 |
|    mean velocity x | -0.42      |
|    mean velocity y | 0.869      |
|    mean velocity z | 4.41       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.03e+05  |
| time/              |            |
|    total_timesteps | 2037500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 995     |
|    time_elapsed    | 82037   |
|    total_timesteps | 2037760 |
--------------------------------
Eval num_timesteps=2038000, episode_reward=-92186.70 +/- 43964.72
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3863505   |
|    mean velocity x      | 0.0975       |
|    mean velocity y      | 1.11         |
|    mean velocity z      | 3.73         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.22e+04    |
| time/                   |              |
|    total_timesteps      | 2038000      |
| train/                  |              |
|    approx_kl            | 5.472655e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.001        |
|    loss                 | 3.78e+07     |
|    n_updates            | 9950         |
|    policy_gradient_loss | -0.000143    |
|    std                  | 1.55         |
|    value_loss           | 1.02e+08     |
------------------------------------------
Eval num_timesteps=2038500, episode_reward=-70226.33 +/- 54971.21
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39922822 |
|    mean velocity x | -0.106      |
|    mean velocity y | 0.627       |
|    mean velocity z | 4.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.02e+04   |
| time/              |             |
|    total_timesteps | 2038500     |
------------------------------------
Eval num_timesteps=2039000, episode_reward=-88325.69 +/- 16109.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17057723 |
|    mean velocity x | -0.663      |
|    mean velocity y | 0.0613      |
|    mean velocity z | 1.86        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.83e+04   |
| time/              |             |
|    total_timesteps | 2039000     |
------------------------------------
Eval num_timesteps=2039500, episode_reward=-90092.85 +/- 31030.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52847856 |
|    mean velocity x | 0.612       |
|    mean velocity y | 1.72        |
|    mean velocity z | 3.76        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.01e+04   |
| time/              |             |
|    total_timesteps | 2039500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 996     |
|    time_elapsed    | 82117   |
|    total_timesteps | 2039808 |
--------------------------------
Eval num_timesteps=2040000, episode_reward=-48869.72 +/- 46728.68
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31492507  |
|    mean velocity x      | -1.08        |
|    mean velocity y      | -0.624       |
|    mean velocity z      | 3.37         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.89e+04    |
| time/                   |              |
|    total_timesteps      | 2040000      |
| train/                  |              |
|    approx_kl            | 2.636746e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.427        |
|    learning_rate        | 0.001        |
|    loss                 | 4.41e+07     |
|    n_updates            | 9960         |
|    policy_gradient_loss | -0.000454    |
|    std                  | 1.55         |
|    value_loss           | 3.27e+07     |
------------------------------------------
Eval num_timesteps=2040500, episode_reward=-95144.31 +/- 33640.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42620197 |
|    mean velocity x | -0.325      |
|    mean velocity y | 0.754       |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.51e+04   |
| time/              |             |
|    total_timesteps | 2040500     |
------------------------------------
Eval num_timesteps=2041000, episode_reward=-83020.19 +/- 25960.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39281508 |
|    mean velocity x | -0.395      |
|    mean velocity y | 0.696       |
|    mean velocity z | 4.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.3e+04    |
| time/              |             |
|    total_timesteps | 2041000     |
------------------------------------
Eval num_timesteps=2041500, episode_reward=-102813.29 +/- 27424.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34385815 |
|    mean velocity x | 0.12        |
|    mean velocity y | 0.35        |
|    mean velocity z | 1.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 2041500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 997     |
|    time_elapsed    | 82197   |
|    total_timesteps | 2041856 |
--------------------------------
Eval num_timesteps=2042000, episode_reward=-98317.33 +/- 31888.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.23698969  |
|    mean velocity x      | -1.36        |
|    mean velocity y      | -0.636       |
|    mean velocity z      | 3.5          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.83e+04    |
| time/                   |              |
|    total_timesteps      | 2042000      |
| train/                  |              |
|    approx_kl            | 7.955561e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.001        |
|    loss                 | 3.34e+07     |
|    n_updates            | 9970         |
|    policy_gradient_loss | -0.000252    |
|    std                  | 1.55         |
|    value_loss           | 5.51e+07     |
------------------------------------------
Eval num_timesteps=2042500, episode_reward=-87420.78 +/- 33026.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22149903 |
|    mean velocity x | -0.103      |
|    mean velocity y | 0.519       |
|    mean velocity z | 0.704       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.74e+04   |
| time/              |             |
|    total_timesteps | 2042500     |
------------------------------------
Eval num_timesteps=2043000, episode_reward=-96272.94 +/- 11203.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32056698 |
|    mean velocity x | -0.352      |
|    mean velocity y | 0.335       |
|    mean velocity z | 4.27        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.63e+04   |
| time/              |             |
|    total_timesteps | 2043000     |
------------------------------------
Eval num_timesteps=2043500, episode_reward=-77541.76 +/- 32543.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26674697 |
|    mean velocity x | 0.0708      |
|    mean velocity y | 0.263       |
|    mean velocity z | 2.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.75e+04   |
| time/              |             |
|    total_timesteps | 2043500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 998     |
|    time_elapsed    | 82278   |
|    total_timesteps | 2043904 |
--------------------------------
Eval num_timesteps=2044000, episode_reward=-116400.88 +/- 20387.10
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41940856  |
|    mean velocity x      | -0.135       |
|    mean velocity y      | 0.84         |
|    mean velocity z      | 3.87         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.16e+05    |
| time/                   |              |
|    total_timesteps      | 2044000      |
| train/                  |              |
|    approx_kl            | 2.407236e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.001        |
|    loss                 | 4.99e+07     |
|    n_updates            | 9980         |
|    policy_gradient_loss | -0.000139    |
|    std                  | 1.55         |
|    value_loss           | 5.61e+07     |
------------------------------------------
Eval num_timesteps=2044500, episode_reward=-111470.32 +/- 24676.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2341261 |
|    mean velocity x | -0.0468    |
|    mean velocity y | 0.354      |
|    mean velocity z | 0.47       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.11e+05  |
| time/              |            |
|    total_timesteps | 2044500    |
-----------------------------------
Eval num_timesteps=2045000, episode_reward=-89917.53 +/- 40781.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47878978 |
|    mean velocity x | -0.34       |
|    mean velocity y | 1.1         |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.99e+04   |
| time/              |             |
|    total_timesteps | 2045000     |
------------------------------------
Eval num_timesteps=2045500, episode_reward=-84973.34 +/- 27036.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32799065 |
|    mean velocity x | -0.0993     |
|    mean velocity y | 0.392       |
|    mean velocity z | 2.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 2045500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 999     |
|    time_elapsed    | 82358   |
|    total_timesteps | 2045952 |
--------------------------------
Eval num_timesteps=2046000, episode_reward=-46004.06 +/- 47685.40
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43554458  |
|    mean velocity x      | -0.457       |
|    mean velocity y      | 0.573        |
|    mean velocity z      | 4.15         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -4.6e+04     |
| time/                   |              |
|    total_timesteps      | 2046000      |
| train/                  |              |
|    approx_kl            | 2.348781e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.001        |
|    loss                 | 3.3e+07      |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.000508    |
|    std                  | 1.55         |
|    value_loss           | 5.31e+07     |
------------------------------------------
Eval num_timesteps=2046500, episode_reward=-53641.61 +/- 54894.09
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3822854 |
|    mean velocity x | 0.283      |
|    mean velocity y | 1.2        |
|    mean velocity z | 3.42       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.36e+04  |
| time/              |            |
|    total_timesteps | 2046500    |
-----------------------------------
Eval num_timesteps=2047000, episode_reward=-90540.19 +/- 7188.73
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3138011 |
|    mean velocity x | -0.52      |
|    mean velocity y | -0.234     |
|    mean velocity z | 3.73       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.05e+04  |
| time/              |            |
|    total_timesteps | 2047000    |
-----------------------------------
Eval num_timesteps=2047500, episode_reward=-86462.52 +/- 10817.35
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4634239 |
|    mean velocity x | 0.0547     |
|    mean velocity y | 1.69       |
|    mean velocity z | 3.83       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.65e+04  |
| time/              |            |
|    total_timesteps | 2047500    |
-----------------------------------
Eval num_timesteps=2048000, episode_reward=-95233.23 +/- 15180.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38688958 |
|    mean velocity x | -0.135      |
|    mean velocity y | 0.711       |
|    mean velocity z | 2.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.52e+04   |
| time/              |             |
|    total_timesteps | 2048000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1000    |
|    time_elapsed    | 82457   |
|    total_timesteps | 2048000 |
--------------------------------
Eval num_timesteps=2048500, episode_reward=-55590.22 +/- 37645.88
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26031715  |
|    mean velocity x      | 0.0301       |
|    mean velocity y      | 0.327        |
|    mean velocity z      | 0.998        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.56e+04    |
| time/                   |              |
|    total_timesteps      | 2048500      |
| train/                  |              |
|    approx_kl            | 9.366602e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.001        |
|    loss                 | 8.5e+06      |
|    n_updates            | 10000        |
|    policy_gradient_loss | -0.000206    |
|    std                  | 1.55         |
|    value_loss           | 4.08e+07     |
------------------------------------------
Eval num_timesteps=2049000, episode_reward=-60517.91 +/- 35882.08
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32624114 |
|    mean velocity x | -0.367      |
|    mean velocity y | 0.675       |
|    mean velocity z | 2.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.05e+04   |
| time/              |             |
|    total_timesteps | 2049000     |
------------------------------------
Eval num_timesteps=2049500, episode_reward=-51950.47 +/- 57556.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3363731 |
|    mean velocity x | 0.492      |
|    mean velocity y | 1.27       |
|    mean velocity z | 3.65       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.2e+04   |
| time/              |            |
|    total_timesteps | 2049500    |
-----------------------------------
Eval num_timesteps=2050000, episode_reward=-62510.28 +/- 51317.24
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.5336719 |
|    mean velocity x | 0.73       |
|    mean velocity y | 1.78       |
|    mean velocity z | 3.6        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.25e+04  |
| time/              |            |
|    total_timesteps | 2050000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1001    |
|    time_elapsed    | 82538   |
|    total_timesteps | 2050048 |
--------------------------------
Eval num_timesteps=2050500, episode_reward=-70746.10 +/- 41970.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30629233   |
|    mean velocity x      | 0.163         |
|    mean velocity y      | 0.697         |
|    mean velocity z      | 2.28          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.07e+04     |
| time/                   |               |
|    total_timesteps      | 2050500       |
| train/                  |               |
|    approx_kl            | 4.0649582e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.53          |
|    learning_rate        | 0.001         |
|    loss                 | 1.54e+07      |
|    n_updates            | 10010         |
|    policy_gradient_loss | -0.000288     |
|    std                  | 1.55          |
|    value_loss           | 2.4e+07       |
-------------------------------------------
Eval num_timesteps=2051000, episode_reward=-101165.82 +/- 20523.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17481156 |
|    mean velocity x | -0.772      |
|    mean velocity y | -0.518      |
|    mean velocity z | 2.93        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 2051000     |
------------------------------------
Eval num_timesteps=2051500, episode_reward=-31080.04 +/- 33982.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26436582 |
|    mean velocity x | -0.589      |
|    mean velocity y | -0.427      |
|    mean velocity z | 3.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -3.11e+04   |
| time/              |             |
|    total_timesteps | 2051500     |
------------------------------------
Eval num_timesteps=2052000, episode_reward=-112842.04 +/- 16864.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53948927 |
|    mean velocity x | 0.991       |
|    mean velocity y | 1.84        |
|    mean velocity z | 3.88        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 2052000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1002    |
|    time_elapsed    | 82618   |
|    total_timesteps | 2052096 |
--------------------------------
Eval num_timesteps=2052500, episode_reward=-78393.69 +/- 17317.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4364065    |
|    mean velocity x      | -0.404        |
|    mean velocity y      | 0.828         |
|    mean velocity z      | 4.87          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.84e+04     |
| time/                   |               |
|    total_timesteps      | 2052500       |
| train/                  |               |
|    approx_kl            | 1.8741179e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.469         |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+07      |
|    n_updates            | 10020         |
|    policy_gradient_loss | -0.000258     |
|    std                  | 1.55          |
|    value_loss           | 4.23e+07      |
-------------------------------------------
Eval num_timesteps=2053000, episode_reward=-98965.58 +/- 19258.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3374655 |
|    mean velocity x | -0.985     |
|    mean velocity y | 0.256      |
|    mean velocity z | 2.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.9e+04   |
| time/              |            |
|    total_timesteps | 2053000    |
-----------------------------------
Eval num_timesteps=2053500, episode_reward=-76528.69 +/- 38923.49
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26497462 |
|    mean velocity x | -0.0863     |
|    mean velocity y | 0.201       |
|    mean velocity z | 4.06        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.65e+04   |
| time/              |             |
|    total_timesteps | 2053500     |
------------------------------------
Eval num_timesteps=2054000, episode_reward=-81777.69 +/- 36877.10
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17741773 |
|    mean velocity x | -0.0618     |
|    mean velocity y | 0.325       |
|    mean velocity z | 0.281       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.18e+04   |
| time/              |             |
|    total_timesteps | 2054000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1003    |
|    time_elapsed    | 82699   |
|    total_timesteps | 2054144 |
--------------------------------
Eval num_timesteps=2054500, episode_reward=-36142.49 +/- 33822.34
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.29930183   |
|    mean velocity x      | 0.00314       |
|    mean velocity y      | 0.663         |
|    mean velocity z      | 4.09          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.61e+04     |
| time/                   |               |
|    total_timesteps      | 2054500       |
| train/                  |               |
|    approx_kl            | 4.6059897e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+07      |
|    n_updates            | 10030         |
|    policy_gradient_loss | -0.000439     |
|    std                  | 1.55          |
|    value_loss           | 6.5e+07       |
-------------------------------------------
Eval num_timesteps=2055000, episode_reward=-106898.54 +/- 25757.14
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4085309 |
|    mean velocity x | -0.327     |
|    mean velocity y | 1.11       |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 2055000    |
-----------------------------------
Eval num_timesteps=2055500, episode_reward=-53509.16 +/- 44769.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20820732 |
|    mean velocity x | -0.118      |
|    mean velocity y | 0.196       |
|    mean velocity z | 1.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.35e+04   |
| time/              |             |
|    total_timesteps | 2055500     |
------------------------------------
Eval num_timesteps=2056000, episode_reward=-66841.90 +/- 43237.13
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21056189 |
|    mean velocity x | -0.0377     |
|    mean velocity y | 0.355       |
|    mean velocity z | 0.379       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 2056000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1004    |
|    time_elapsed    | 82779   |
|    total_timesteps | 2056192 |
--------------------------------
Eval num_timesteps=2056500, episode_reward=-100595.72 +/- 28956.55
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41862482   |
|    mean velocity x      | -0.122        |
|    mean velocity y      | 0.829         |
|    mean velocity z      | 3.39          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.01e+05     |
| time/                   |               |
|    total_timesteps      | 2056500       |
| train/                  |               |
|    approx_kl            | 3.0232652e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.327         |
|    learning_rate        | 0.001         |
|    loss                 | 6.09e+07      |
|    n_updates            | 10040         |
|    policy_gradient_loss | -0.000443     |
|    std                  | 1.55          |
|    value_loss           | 3.53e+07      |
-------------------------------------------
Eval num_timesteps=2057000, episode_reward=-57188.52 +/- 29920.34
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32370389 |
|    mean velocity x | -1.21       |
|    mean velocity y | -0.322      |
|    mean velocity z | 3.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.72e+04   |
| time/              |             |
|    total_timesteps | 2057000     |
------------------------------------
Eval num_timesteps=2057500, episode_reward=-82438.81 +/- 39410.89
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3497331 |
|    mean velocity x | -0.102     |
|    mean velocity y | 0.548      |
|    mean velocity z | 0.957      |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 2057500    |
-----------------------------------
Eval num_timesteps=2058000, episode_reward=-76944.86 +/- 29517.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37112486 |
|    mean velocity x | -0.939      |
|    mean velocity y | 0.158       |
|    mean velocity z | 2.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 2058000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1005    |
|    time_elapsed    | 82859   |
|    total_timesteps | 2058240 |
--------------------------------
Eval num_timesteps=2058500, episode_reward=-82770.60 +/- 49560.37
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.22288653   |
|    mean velocity x      | -0.0251       |
|    mean velocity y      | 0.275         |
|    mean velocity z      | 0.531         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.28e+04     |
| time/                   |               |
|    total_timesteps      | 2058500       |
| train/                  |               |
|    approx_kl            | 0.00025808436 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.5           |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+05      |
|    n_updates            | 10050         |
|    policy_gradient_loss | -0.000689     |
|    std                  | 1.55          |
|    value_loss           | 1.57e+07      |
-------------------------------------------
Eval num_timesteps=2059000, episode_reward=-89604.23 +/- 33556.59
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28716254 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.404       |
|    mean velocity z | 2.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 2059000     |
------------------------------------
Eval num_timesteps=2059500, episode_reward=-115452.83 +/- 44114.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41816893 |
|    mean velocity x | -0.414      |
|    mean velocity y | 0.553       |
|    mean velocity z | 4.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.15e+05   |
| time/              |             |
|    total_timesteps | 2059500     |
------------------------------------
Eval num_timesteps=2060000, episode_reward=-89712.14 +/- 11140.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3187172 |
|    mean velocity x | -0.111     |
|    mean velocity y | 0.52       |
|    mean velocity z | 4.05       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.97e+04  |
| time/              |            |
|    total_timesteps | 2060000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1006    |
|    time_elapsed    | 82940   |
|    total_timesteps | 2060288 |
--------------------------------
Eval num_timesteps=2060500, episode_reward=-97516.39 +/- 20054.06
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2701282    |
|    mean velocity x      | -0.249        |
|    mean velocity y      | -0.0417       |
|    mean velocity z      | 3.18          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.75e+04     |
| time/                   |               |
|    total_timesteps      | 2060500       |
| train/                  |               |
|    approx_kl            | 1.7911021e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.325         |
|    learning_rate        | 0.001         |
|    loss                 | 3.21e+07      |
|    n_updates            | 10060         |
|    policy_gradient_loss | -0.000336     |
|    std                  | 1.55          |
|    value_loss           | 6.62e+07      |
-------------------------------------------
Eval num_timesteps=2061000, episode_reward=-80502.89 +/- 23879.59
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2589876 |
|    mean velocity x | -0.168     |
|    mean velocity y | 0.133      |
|    mean velocity z | 3.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.05e+04  |
| time/              |            |
|    total_timesteps | 2061000    |
-----------------------------------
Eval num_timesteps=2061500, episode_reward=-68879.34 +/- 40256.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44747493 |
|    mean velocity x | -0.925      |
|    mean velocity y | 0.457       |
|    mean velocity z | 3.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.89e+04   |
| time/              |             |
|    total_timesteps | 2061500     |
------------------------------------
Eval num_timesteps=2062000, episode_reward=-70799.01 +/- 39820.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26334852 |
|    mean velocity x | -0.0339     |
|    mean velocity y | 0.355       |
|    mean velocity z | 0.559       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.08e+04   |
| time/              |             |
|    total_timesteps | 2062000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1007    |
|    time_elapsed    | 83020   |
|    total_timesteps | 2062336 |
--------------------------------
Eval num_timesteps=2062500, episode_reward=-70019.67 +/- 39089.53
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45453575   |
|    mean velocity x      | -0.283        |
|    mean velocity y      | 0.854         |
|    mean velocity z      | 4.54          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7e+04        |
| time/                   |               |
|    total_timesteps      | 2062500       |
| train/                  |               |
|    approx_kl            | 7.0300302e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.001         |
|    loss                 | 3.99e+07      |
|    n_updates            | 10070         |
|    policy_gradient_loss | -0.000254     |
|    std                  | 1.55          |
|    value_loss           | 6.24e+07      |
-------------------------------------------
Eval num_timesteps=2063000, episode_reward=-59007.46 +/- 28316.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29668525 |
|    mean velocity x | -0.179      |
|    mean velocity y | 0.245       |
|    mean velocity z | 3.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.9e+04    |
| time/              |             |
|    total_timesteps | 2063000     |
------------------------------------
Eval num_timesteps=2063500, episode_reward=-85011.43 +/- 22328.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46519545 |
|    mean velocity x | -0.13       |
|    mean velocity y | 1.3         |
|    mean velocity z | 3.66        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 2063500     |
------------------------------------
Eval num_timesteps=2064000, episode_reward=-44336.28 +/- 30033.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43544513 |
|    mean velocity x | 0.242       |
|    mean velocity y | 1.4         |
|    mean velocity z | 3.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.43e+04   |
| time/              |             |
|    total_timesteps | 2064000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1008    |
|    time_elapsed    | 83100   |
|    total_timesteps | 2064384 |
--------------------------------
Eval num_timesteps=2064500, episode_reward=-30291.22 +/- 36095.52
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37817618   |
|    mean velocity x      | -0.259        |
|    mean velocity y      | 0.635         |
|    mean velocity z      | 4.09          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -3.03e+04     |
| time/                   |               |
|    total_timesteps      | 2064500       |
| train/                  |               |
|    approx_kl            | 2.5691203e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 3.51e+07      |
|    n_updates            | 10080         |
|    policy_gradient_loss | -0.000254     |
|    std                  | 1.55          |
|    value_loss           | 6.36e+07      |
-------------------------------------------
Eval num_timesteps=2065000, episode_reward=-96832.98 +/- 12221.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47805125 |
|    mean velocity x | 0.194       |
|    mean velocity y | 1.8         |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.68e+04   |
| time/              |             |
|    total_timesteps | 2065000     |
------------------------------------
Eval num_timesteps=2065500, episode_reward=-93770.42 +/- 23391.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39708564 |
|    mean velocity x | -0.26       |
|    mean velocity y | 0.699       |
|    mean velocity z | 4.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.38e+04   |
| time/              |             |
|    total_timesteps | 2065500     |
------------------------------------
Eval num_timesteps=2066000, episode_reward=-85386.26 +/- 25729.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34302223 |
|    mean velocity x | -0.119      |
|    mean velocity y | 0.34        |
|    mean velocity z | 2.84        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.54e+04   |
| time/              |             |
|    total_timesteps | 2066000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1009    |
|    time_elapsed    | 83181   |
|    total_timesteps | 2066432 |
--------------------------------
Eval num_timesteps=2066500, episode_reward=-95593.20 +/- 34957.28
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41105175   |
|    mean velocity x      | -0.283        |
|    mean velocity y      | 0.9           |
|    mean velocity z      | 4.26          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.56e+04     |
| time/                   |               |
|    total_timesteps      | 2066500       |
| train/                  |               |
|    approx_kl            | 2.7504953e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.001         |
|    loss                 | 6.03e+07      |
|    n_updates            | 10090         |
|    policy_gradient_loss | -0.000494     |
|    std                  | 1.55          |
|    value_loss           | 8.18e+07      |
-------------------------------------------
Eval num_timesteps=2067000, episode_reward=-102413.18 +/- 22807.33
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43197992 |
|    mean velocity x | -0.176      |
|    mean velocity y | 0.75        |
|    mean velocity z | 0.793       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 2067000     |
------------------------------------
Eval num_timesteps=2067500, episode_reward=-66896.47 +/- 43041.81
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3001187 |
|    mean velocity x | -0.928     |
|    mean velocity y | -0.111     |
|    mean velocity z | 2.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.69e+04  |
| time/              |            |
|    total_timesteps | 2067500    |
-----------------------------------
Eval num_timesteps=2068000, episode_reward=-74790.95 +/- 34742.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40401483 |
|    mean velocity x | -0.396      |
|    mean velocity y | 0.825       |
|    mean velocity z | 4.16        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.48e+04   |
| time/              |             |
|    total_timesteps | 2068000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1010    |
|    time_elapsed    | 83261   |
|    total_timesteps | 2068480 |
--------------------------------
Eval num_timesteps=2068500, episode_reward=-75589.73 +/- 20494.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.31729326   |
|    mean velocity x      | 0.442         |
|    mean velocity y      | 0.699         |
|    mean velocity z      | 1.99          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.56e+04     |
| time/                   |               |
|    total_timesteps      | 2068500       |
| train/                  |               |
|    approx_kl            | 9.1057445e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.381         |
|    learning_rate        | 0.001         |
|    loss                 | 3.27e+06      |
|    n_updates            | 10100         |
|    policy_gradient_loss | -0.000199     |
|    std                  | 1.55          |
|    value_loss           | 2.61e+07      |
-------------------------------------------
Eval num_timesteps=2069000, episode_reward=-90098.65 +/- 24709.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2700913 |
|    mean velocity x | -0.227     |
|    mean velocity y | 0.058      |
|    mean velocity z | 3.27       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.01e+04  |
| time/              |            |
|    total_timesteps | 2069000    |
-----------------------------------
Eval num_timesteps=2069500, episode_reward=-53758.21 +/- 39980.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40498972 |
|    mean velocity x | -0.0177     |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.47        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.38e+04   |
| time/              |             |
|    total_timesteps | 2069500     |
------------------------------------
Eval num_timesteps=2070000, episode_reward=-75077.50 +/- 40917.47
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32501268 |
|    mean velocity x | -0.211      |
|    mean velocity y | 0.683       |
|    mean velocity z | 3.94        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 2070000     |
------------------------------------
Eval num_timesteps=2070500, episode_reward=-76102.33 +/- 65973.99
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34799066 |
|    mean velocity x | 0.309       |
|    mean velocity y | 0.725       |
|    mean velocity z | 2.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 2070500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1011    |
|    time_elapsed    | 83361   |
|    total_timesteps | 2070528 |
--------------------------------
Eval num_timesteps=2071000, episode_reward=-63761.04 +/- 36540.25
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29555655  |
|    mean velocity x      | 0.0176       |
|    mean velocity y      | 0.271        |
|    mean velocity z      | 1.88         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.38e+04    |
| time/                   |              |
|    total_timesteps      | 2071000      |
| train/                  |              |
|    approx_kl            | 3.137067e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.3          |
|    learning_rate        | 0.001        |
|    loss                 | 1.91e+07     |
|    n_updates            | 10110        |
|    policy_gradient_loss | -0.000314    |
|    std                  | 1.55         |
|    value_loss           | 8.23e+07     |
------------------------------------------
Eval num_timesteps=2071500, episode_reward=-78425.95 +/- 18083.05
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3358814 |
|    mean velocity x | 0.21       |
|    mean velocity y | 0.916      |
|    mean velocity z | 3.2        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.84e+04  |
| time/              |            |
|    total_timesteps | 2071500    |
-----------------------------------
Eval num_timesteps=2072000, episode_reward=-73718.59 +/- 12441.90
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21627103 |
|    mean velocity x | -0.765      |
|    mean velocity y | -0.0214     |
|    mean velocity z | 2.7         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 2072000     |
------------------------------------
Eval num_timesteps=2072500, episode_reward=-56269.96 +/- 35164.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17420593 |
|    mean velocity x | -0.18       |
|    mean velocity y | 0.29        |
|    mean velocity z | 0.743       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.63e+04   |
| time/              |             |
|    total_timesteps | 2072500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1012    |
|    time_elapsed    | 83441   |
|    total_timesteps | 2072576 |
--------------------------------
Eval num_timesteps=2073000, episode_reward=-51635.90 +/- 41057.75
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35014954  |
|    mean velocity x      | -0.84        |
|    mean velocity y      | 0.351        |
|    mean velocity z      | 3.63         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.16e+04    |
| time/                   |              |
|    total_timesteps      | 2073000      |
| train/                  |              |
|    approx_kl            | 1.177247e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.437        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+07      |
|    n_updates            | 10120        |
|    policy_gradient_loss | -0.000207    |
|    std                  | 1.55         |
|    value_loss           | 2.91e+07     |
------------------------------------------
Eval num_timesteps=2073500, episode_reward=-88267.12 +/- 48715.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52764577 |
|    mean velocity x | -0.543      |
|    mean velocity y | 0.901       |
|    mean velocity z | 4.61        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.83e+04   |
| time/              |             |
|    total_timesteps | 2073500     |
------------------------------------
Eval num_timesteps=2074000, episode_reward=-99662.95 +/- 25611.70
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48631924 |
|    mean velocity x | -0.194      |
|    mean velocity y | 1.38        |
|    mean velocity z | 4.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.97e+04   |
| time/              |             |
|    total_timesteps | 2074000     |
------------------------------------
Eval num_timesteps=2074500, episode_reward=-116562.60 +/- 19878.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23829977 |
|    mean velocity x | -2.42       |
|    mean velocity y | -1.46       |
|    mean velocity z | 5.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.17e+05   |
| time/              |             |
|    total_timesteps | 2074500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1013    |
|    time_elapsed    | 83521   |
|    total_timesteps | 2074624 |
--------------------------------
Eval num_timesteps=2075000, episode_reward=-60825.77 +/- 40659.14
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4116307    |
|    mean velocity x      | -1.28         |
|    mean velocity y      | 0.15          |
|    mean velocity z      | 2.68          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.08e+04     |
| time/                   |               |
|    total_timesteps      | 2075000       |
| train/                  |               |
|    approx_kl            | 3.7461868e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.458         |
|    learning_rate        | 0.001         |
|    loss                 | 2.67e+07      |
|    n_updates            | 10130         |
|    policy_gradient_loss | -0.000104     |
|    std                  | 1.55          |
|    value_loss           | 5.47e+07      |
-------------------------------------------
Eval num_timesteps=2075500, episode_reward=-106252.41 +/- 20744.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.52258337 |
|    mean velocity x | -0.787      |
|    mean velocity y | 0.782       |
|    mean velocity z | 4.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 2075500     |
------------------------------------
Eval num_timesteps=2076000, episode_reward=-81550.76 +/- 16972.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43828827 |
|    mean velocity x | -0.0562     |
|    mean velocity y | 0.691       |
|    mean velocity z | 3.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 2076000     |
------------------------------------
Eval num_timesteps=2076500, episode_reward=-93573.69 +/- 17907.68
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3479343 |
|    mean velocity x | -1.24      |
|    mean velocity y | -0.319     |
|    mean velocity z | 3.24       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.36e+04  |
| time/              |            |
|    total_timesteps | 2076500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1014    |
|    time_elapsed    | 83602   |
|    total_timesteps | 2076672 |
--------------------------------
Eval num_timesteps=2077000, episode_reward=-93216.57 +/- 27677.65
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35219678   |
|    mean velocity x      | -0.294        |
|    mean velocity y      | 0.234         |
|    mean velocity z      | 4.06          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.32e+04     |
| time/                   |               |
|    total_timesteps      | 2077000       |
| train/                  |               |
|    approx_kl            | 1.0200165e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.37          |
|    learning_rate        | 0.001         |
|    loss                 | 4.85e+07      |
|    n_updates            | 10140         |
|    policy_gradient_loss | -0.000299     |
|    std                  | 1.55          |
|    value_loss           | 5.79e+07      |
-------------------------------------------
Eval num_timesteps=2077500, episode_reward=-90957.99 +/- 18814.29
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40359652 |
|    mean velocity x | -0.275      |
|    mean velocity y | 0.652       |
|    mean velocity z | 4.33        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 2077500     |
------------------------------------
Eval num_timesteps=2078000, episode_reward=-61335.23 +/- 33426.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28313777 |
|    mean velocity x | -0.406      |
|    mean velocity y | 0.677       |
|    mean velocity z | 0.697       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.13e+04   |
| time/              |             |
|    total_timesteps | 2078000     |
------------------------------------
Eval num_timesteps=2078500, episode_reward=-66264.01 +/- 48711.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36177185 |
|    mean velocity x | -0.375      |
|    mean velocity y | 0.471       |
|    mean velocity z | 2.75        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.63e+04   |
| time/              |             |
|    total_timesteps | 2078500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1015    |
|    time_elapsed    | 83682   |
|    total_timesteps | 2078720 |
--------------------------------
Eval num_timesteps=2079000, episode_reward=-69248.62 +/- 39522.75
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.26047784   |
|    mean velocity x      | -2.14         |
|    mean velocity y      | -1.44         |
|    mean velocity z      | 5.74          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.92e+04     |
| time/                   |               |
|    total_timesteps      | 2079000       |
| train/                  |               |
|    approx_kl            | 1.4582329e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.384         |
|    learning_rate        | 0.001         |
|    loss                 | 2.42e+07      |
|    n_updates            | 10150         |
|    policy_gradient_loss | -0.000232     |
|    std                  | 1.55          |
|    value_loss           | 5.18e+07      |
-------------------------------------------
Eval num_timesteps=2079500, episode_reward=-54087.25 +/- 44255.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20486493 |
|    mean velocity x | -0.639      |
|    mean velocity y | -0.0846     |
|    mean velocity z | 2.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.41e+04   |
| time/              |             |
|    total_timesteps | 2079500     |
------------------------------------
Eval num_timesteps=2080000, episode_reward=-93525.48 +/- 47536.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29057214 |
|    mean velocity x | -0.139      |
|    mean velocity y | 0.279       |
|    mean velocity z | 3.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.35e+04   |
| time/              |             |
|    total_timesteps | 2080000     |
------------------------------------
Eval num_timesteps=2080500, episode_reward=-125386.97 +/- 34355.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44087505 |
|    mean velocity x | -0.795      |
|    mean velocity y | 0.443       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.25e+05   |
| time/              |             |
|    total_timesteps | 2080500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1016    |
|    time_elapsed    | 83762   |
|    total_timesteps | 2080768 |
--------------------------------
Eval num_timesteps=2081000, episode_reward=-105551.20 +/- 17723.11
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.28356063 |
|    mean velocity x      | -0.53       |
|    mean velocity y      | 0.222       |
|    mean velocity z      | 3.11        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -1.06e+05   |
| time/                   |             |
|    total_timesteps      | 2081000     |
| train/                  |             |
|    approx_kl            | 8.02381e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.001       |
|    loss                 | 1.72e+07    |
|    n_updates            | 10160       |
|    policy_gradient_loss | -0.000162   |
|    std                  | 1.55        |
|    value_loss           | 4.27e+07    |
-----------------------------------------
Eval num_timesteps=2081500, episode_reward=-114283.94 +/- 22728.77
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3887868 |
|    mean velocity x | 0.271      |
|    mean velocity y | 1.21       |
|    mean velocity z | 3.13       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.14e+05  |
| time/              |            |
|    total_timesteps | 2081500    |
-----------------------------------
Eval num_timesteps=2082000, episode_reward=-81503.99 +/- 15315.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18004145 |
|    mean velocity x | -0.32       |
|    mean velocity y | 0.576       |
|    mean velocity z | 0.317       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.15e+04   |
| time/              |             |
|    total_timesteps | 2082000     |
------------------------------------
Eval num_timesteps=2082500, episode_reward=-84662.77 +/- 36716.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30188718 |
|    mean velocity x | 0.386       |
|    mean velocity y | 0.916       |
|    mean velocity z | 3.45        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.47e+04   |
| time/              |             |
|    total_timesteps | 2082500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1017    |
|    time_elapsed    | 83843   |
|    total_timesteps | 2082816 |
--------------------------------
Eval num_timesteps=2083000, episode_reward=-98867.65 +/- 50886.83
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.434843    |
|    mean velocity x      | -0.382       |
|    mean velocity y      | 1.03         |
|    mean velocity z      | 4.75         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.89e+04    |
| time/                   |              |
|    total_timesteps      | 2083000      |
| train/                  |              |
|    approx_kl            | 6.163027e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.001        |
|    loss                 | 1.55e+07     |
|    n_updates            | 10170        |
|    policy_gradient_loss | -0.000157    |
|    std                  | 1.55         |
|    value_loss           | 5.51e+07     |
------------------------------------------
Eval num_timesteps=2083500, episode_reward=-86503.62 +/- 13789.56
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45935276 |
|    mean velocity x | 0.459       |
|    mean velocity y | 1.42        |
|    mean velocity z | 3.28        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.65e+04   |
| time/              |             |
|    total_timesteps | 2083500     |
------------------------------------
Eval num_timesteps=2084000, episode_reward=-75526.19 +/- 45761.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43099493 |
|    mean velocity x | 0.346       |
|    mean velocity y | 0.868       |
|    mean velocity z | 3.01        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 2084000     |
------------------------------------
Eval num_timesteps=2084500, episode_reward=-103886.51 +/- 17402.76
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4793753 |
|    mean velocity x | 0.0444     |
|    mean velocity y | 1.45       |
|    mean velocity z | 2.09       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 2084500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1018    |
|    time_elapsed    | 83923   |
|    total_timesteps | 2084864 |
--------------------------------
Eval num_timesteps=2085000, episode_reward=-63088.34 +/- 40273.41
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3498075   |
|    mean velocity x      | -0.0521      |
|    mean velocity y      | 0.777        |
|    mean velocity z      | 3.7          |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.31e+04    |
| time/                   |              |
|    total_timesteps      | 2085000      |
| train/                  |              |
|    approx_kl            | 4.614092e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+07     |
|    n_updates            | 10180        |
|    policy_gradient_loss | -9.41e-05    |
|    std                  | 1.55         |
|    value_loss           | 3.51e+07     |
------------------------------------------
Eval num_timesteps=2085500, episode_reward=-61627.06 +/- 31204.18
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3496648 |
|    mean velocity x | 0.494      |
|    mean velocity y | 1.13       |
|    mean velocity z | 3.25       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.16e+04  |
| time/              |            |
|    total_timesteps | 2085500    |
-----------------------------------
Eval num_timesteps=2086000, episode_reward=-84954.02 +/- 21417.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38377985 |
|    mean velocity x | -0.811      |
|    mean velocity y | 0.266       |
|    mean velocity z | 3.53        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 2086000     |
------------------------------------
Eval num_timesteps=2086500, episode_reward=-89967.99 +/- 35001.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4868797 |
|    mean velocity x | -0.457     |
|    mean velocity y | 0.785      |
|    mean velocity z | 4.37       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9e+04     |
| time/              |            |
|    total_timesteps | 2086500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1019    |
|    time_elapsed    | 84004   |
|    total_timesteps | 2086912 |
--------------------------------
Eval num_timesteps=2087000, episode_reward=-65350.56 +/- 36338.59
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44519585  |
|    mean velocity x      | 0.239        |
|    mean velocity y      | 1.41         |
|    mean velocity z      | 3.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.54e+04    |
| time/                   |              |
|    total_timesteps      | 2087000      |
| train/                  |              |
|    approx_kl            | 9.784184e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.38         |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+07     |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.000184    |
|    std                  | 1.55         |
|    value_loss           | 6.15e+07     |
------------------------------------------
Eval num_timesteps=2087500, episode_reward=-117870.60 +/- 12825.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.33269885 |
|    mean velocity x | 0.0906      |
|    mean velocity y | 1.35        |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.18e+05   |
| time/              |             |
|    total_timesteps | 2087500     |
------------------------------------
Eval num_timesteps=2088000, episode_reward=-60262.30 +/- 35814.92
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3636015 |
|    mean velocity x | -0.54      |
|    mean velocity y | 0.537      |
|    mean velocity z | 3.45       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.03e+04  |
| time/              |            |
|    total_timesteps | 2088000    |
-----------------------------------
Eval num_timesteps=2088500, episode_reward=-84637.32 +/- 43241.17
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3304288 |
|    mean velocity x | -0.0496    |
|    mean velocity y | 0.753      |
|    mean velocity z | 4.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.46e+04  |
| time/              |            |
|    total_timesteps | 2088500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1020    |
|    time_elapsed    | 84084   |
|    total_timesteps | 2088960 |
--------------------------------
Eval num_timesteps=2089000, episode_reward=-103923.30 +/- 21704.44
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.33349627  |
|    mean velocity x      | -0.962       |
|    mean velocity y      | -0.538       |
|    mean velocity z      | 3.46         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.04e+05    |
| time/                   |              |
|    total_timesteps      | 2089000      |
| train/                  |              |
|    approx_kl            | 5.137146e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.001        |
|    loss                 | 2.65e+07     |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.00014     |
|    std                  | 1.55         |
|    value_loss           | 6.19e+07     |
------------------------------------------
Eval num_timesteps=2089500, episode_reward=-93602.23 +/- 29614.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.43052402 |
|    mean velocity x | -0.487      |
|    mean velocity y | 0.525       |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 2089500     |
------------------------------------
Eval num_timesteps=2090000, episode_reward=-74140.13 +/- 41855.26
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27632436 |
|    mean velocity x | -0.481      |
|    mean velocity y | -0.0698     |
|    mean velocity z | 3.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.41e+04   |
| time/              |             |
|    total_timesteps | 2090000     |
------------------------------------
Eval num_timesteps=2090500, episode_reward=-79831.05 +/- 36250.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11223167 |
|    mean velocity x | -0.0433     |
|    mean velocity y | 0.0333      |
|    mean velocity z | 0.623       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.98e+04   |
| time/              |             |
|    total_timesteps | 2090500     |
------------------------------------
Eval num_timesteps=2091000, episode_reward=-62152.29 +/- 43314.17
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44278914 |
|    mean velocity x | -1.05       |
|    mean velocity y | 0.312       |
|    mean velocity z | 3.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.22e+04   |
| time/              |             |
|    total_timesteps | 2091000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1021    |
|    time_elapsed    | 84183   |
|    total_timesteps | 2091008 |
--------------------------------
Eval num_timesteps=2091500, episode_reward=-57237.45 +/- 30010.85
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40326276   |
|    mean velocity x      | 0.512         |
|    mean velocity y      | 1.16          |
|    mean velocity z      | 3.41          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.72e+04     |
| time/                   |               |
|    total_timesteps      | 2091500       |
| train/                  |               |
|    approx_kl            | 3.4857832e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+07      |
|    n_updates            | 10210         |
|    policy_gradient_loss | -0.000435     |
|    std                  | 1.55          |
|    value_loss           | 5.72e+07      |
-------------------------------------------
Eval num_timesteps=2092000, episode_reward=-90156.66 +/- 22200.08
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4506247 |
|    mean velocity x | -0.53      |
|    mean velocity y | 0.654      |
|    mean velocity z | 4.38       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.02e+04  |
| time/              |            |
|    total_timesteps | 2092000    |
-----------------------------------
Eval num_timesteps=2092500, episode_reward=-81579.63 +/- 41155.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.47343585 |
|    mean velocity x | 0.384       |
|    mean velocity y | 1.51        |
|    mean velocity z | 3.25        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 2092500     |
------------------------------------
Eval num_timesteps=2093000, episode_reward=-56928.76 +/- 41637.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45097995 |
|    mean velocity x | -0.212      |
|    mean velocity y | 0.726       |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 2093000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1022    |
|    time_elapsed    | 84264   |
|    total_timesteps | 2093056 |
--------------------------------
Eval num_timesteps=2093500, episode_reward=-66259.59 +/- 41323.54
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43469074  |
|    mean velocity x      | -0.202       |
|    mean velocity y      | 1.12         |
|    mean velocity z      | 4.25         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.63e+04    |
| time/                   |              |
|    total_timesteps      | 2093500      |
| train/                  |              |
|    approx_kl            | 1.646683e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.413        |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+07     |
|    n_updates            | 10220        |
|    policy_gradient_loss | -0.000142    |
|    std                  | 1.55         |
|    value_loss           | 6.07e+07     |
------------------------------------------
Eval num_timesteps=2094000, episode_reward=-64229.58 +/- 17706.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26986098 |
|    mean velocity x | -0.295      |
|    mean velocity y | 0.212       |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 2094000     |
------------------------------------
Eval num_timesteps=2094500, episode_reward=-42438.12 +/- 41874.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36945292 |
|    mean velocity x | -0.196      |
|    mean velocity y | 0.722       |
|    mean velocity z | 4.11        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.24e+04   |
| time/              |             |
|    total_timesteps | 2094500     |
------------------------------------
Eval num_timesteps=2095000, episode_reward=-66055.44 +/- 44577.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36288196 |
|    mean velocity x | -0.0955     |
|    mean velocity y | 1.16        |
|    mean velocity z | 4.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.61e+04   |
| time/              |             |
|    total_timesteps | 2095000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1023    |
|    time_elapsed    | 84344   |
|    total_timesteps | 2095104 |
--------------------------------
Eval num_timesteps=2095500, episode_reward=-76892.42 +/- 19098.34
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26864     |
|    mean velocity x      | 0.00382      |
|    mean velocity y      | 0.439        |
|    mean velocity z      | 1.55         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.69e+04    |
| time/                   |              |
|    total_timesteps      | 2095500      |
| train/                  |              |
|    approx_kl            | 1.625088e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.257        |
|    learning_rate        | 0.001        |
|    loss                 | 1.56e+07     |
|    n_updates            | 10230        |
|    policy_gradient_loss | -0.000371    |
|    std                  | 1.55         |
|    value_loss           | 8.59e+07     |
------------------------------------------
Eval num_timesteps=2096000, episode_reward=-59908.28 +/- 41395.37
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29747626 |
|    mean velocity x | -0.175      |
|    mean velocity y | 0.439       |
|    mean velocity z | 1.42        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.99e+04   |
| time/              |             |
|    total_timesteps | 2096000     |
------------------------------------
Eval num_timesteps=2096500, episode_reward=-107531.49 +/- 21774.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24557056 |
|    mean velocity x | -1.12       |
|    mean velocity y | -0.11       |
|    mean velocity z | 2.54        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 2096500     |
------------------------------------
Eval num_timesteps=2097000, episode_reward=-83543.69 +/- 45145.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30724216 |
|    mean velocity x | -0.432      |
|    mean velocity y | -0.265      |
|    mean velocity z | 3.57        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.35e+04   |
| time/              |             |
|    total_timesteps | 2097000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1024    |
|    time_elapsed    | 84424   |
|    total_timesteps | 2097152 |
--------------------------------
Eval num_timesteps=2097500, episode_reward=-104934.03 +/- 40702.70
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46598077  |
|    mean velocity x      | 0.255        |
|    mean velocity y      | 1.44         |
|    mean velocity z      | 3.62         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 2097500      |
| train/                  |              |
|    approx_kl            | 2.204228e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.001        |
|    loss                 | 3.04e+06     |
|    n_updates            | 10240        |
|    policy_gradient_loss | -0.000321    |
|    std                  | 1.55         |
|    value_loss           | 3.07e+07     |
------------------------------------------
Eval num_timesteps=2098000, episode_reward=-90001.95 +/- 45586.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40442634 |
|    mean velocity x | -0.286      |
|    mean velocity y | 0.814       |
|    mean velocity z | 4.05        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9e+04      |
| time/              |             |
|    total_timesteps | 2098000     |
------------------------------------
Eval num_timesteps=2098500, episode_reward=-99539.21 +/- 20959.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.46057186 |
|    mean velocity x | 0.716       |
|    mean velocity y | 2.1         |
|    mean velocity z | 3.83        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 2098500     |
------------------------------------
Eval num_timesteps=2099000, episode_reward=-52579.67 +/- 24667.31
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3448803 |
|    mean velocity x | -0.564     |
|    mean velocity y | 0.0921     |
|    mean velocity z | 3.53       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.26e+04  |
| time/              |            |
|    total_timesteps | 2099000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1025    |
|    time_elapsed    | 84505   |
|    total_timesteps | 2099200 |
--------------------------------
Eval num_timesteps=2099500, episode_reward=-31783.57 +/- 24009.02
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.68282753  |
|    mean velocity x      | -0.333       |
|    mean velocity y      | 1.63         |
|    mean velocity z      | 5.28         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -3.18e+04    |
| time/                   |              |
|    total_timesteps      | 2099500      |
| train/                  |              |
|    approx_kl            | 3.835972e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+07     |
|    n_updates            | 10250        |
|    policy_gradient_loss | -0.000175    |
|    std                  | 1.55         |
|    value_loss           | 5.62e+07     |
------------------------------------------
Eval num_timesteps=2100000, episode_reward=-83689.86 +/- 33343.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35937384 |
|    mean velocity x | -0.148      |
|    mean velocity y | 0.605       |
|    mean velocity z | 4.15        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 2100000     |
------------------------------------
Eval num_timesteps=2100500, episode_reward=-119094.15 +/- 41249.58
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31646758 |
|    mean velocity x | -1.14       |
|    mean velocity y | 0.0829      |
|    mean velocity z | 2.46        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 2100500     |
------------------------------------
Eval num_timesteps=2101000, episode_reward=-105950.96 +/- 12010.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.54989755 |
|    mean velocity x | 0.697       |
|    mean velocity y | 1.74        |
|    mean velocity z | 3.69        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 2101000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1026    |
|    time_elapsed    | 84585   |
|    total_timesteps | 2101248 |
--------------------------------
Eval num_timesteps=2101500, episode_reward=-58925.94 +/- 30587.01
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3986827    |
|    mean velocity x      | -0.755        |
|    mean velocity y      | 0.159         |
|    mean velocity z      | 4.03          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.89e+04     |
| time/                   |               |
|    total_timesteps      | 2101500       |
| train/                  |               |
|    approx_kl            | 1.6623118e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.397         |
|    learning_rate        | 0.001         |
|    loss                 | 2.2e+07       |
|    n_updates            | 10260         |
|    policy_gradient_loss | -0.000282     |
|    std                  | 1.55          |
|    value_loss           | 5.38e+07      |
-------------------------------------------
Eval num_timesteps=2102000, episode_reward=-74657.50 +/- 32464.77
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26253617 |
|    mean velocity x | 0.38        |
|    mean velocity y | 0.62        |
|    mean velocity z | 2.95        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.47e+04   |
| time/              |             |
|    total_timesteps | 2102000     |
------------------------------------
Eval num_timesteps=2102500, episode_reward=-75697.52 +/- 35356.72
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44534984 |
|    mean velocity x | -0.196      |
|    mean velocity y | 1.19        |
|    mean velocity z | 3.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 2102500     |
------------------------------------
Eval num_timesteps=2103000, episode_reward=-110196.26 +/- 39980.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.27139375 |
|    mean velocity x | -0.0144     |
|    mean velocity y | 0.273       |
|    mean velocity z | 3.37        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 2103000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1027    |
|    time_elapsed    | 84665   |
|    total_timesteps | 2103296 |
--------------------------------
Eval num_timesteps=2103500, episode_reward=-82176.44 +/- 21718.82
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25691444   |
|    mean velocity x      | -0.118        |
|    mean velocity y      | 0.514         |
|    mean velocity z      | 0.527         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.22e+04     |
| time/                   |               |
|    total_timesteps      | 2103500       |
| train/                  |               |
|    approx_kl            | 6.0464954e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+07      |
|    n_updates            | 10270         |
|    policy_gradient_loss | -0.000224     |
|    std                  | 1.55          |
|    value_loss           | 2.89e+07      |
-------------------------------------------
Eval num_timesteps=2104000, episode_reward=-61357.10 +/- 32491.44
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3647148 |
|    mean velocity x | -0.0529    |
|    mean velocity y | 1.37       |
|    mean velocity z | 4.19       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.14e+04  |
| time/              |            |
|    total_timesteps | 2104000    |
-----------------------------------
Eval num_timesteps=2104500, episode_reward=-102054.30 +/- 16781.12
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42468917 |
|    mean velocity x | 0.0795      |
|    mean velocity y | 0.967       |
|    mean velocity z | 3.72        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 2104500     |
------------------------------------
Eval num_timesteps=2105000, episode_reward=-73245.50 +/- 19209.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.26457843 |
|    mean velocity x | 0.178       |
|    mean velocity y | 0.959       |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.32e+04   |
| time/              |             |
|    total_timesteps | 2105000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1028    |
|    time_elapsed    | 84746   |
|    total_timesteps | 2105344 |
--------------------------------
Eval num_timesteps=2105500, episode_reward=-88408.07 +/- 25217.51
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31349733  |
|    mean velocity x      | -0.202       |
|    mean velocity y      | 0.174        |
|    mean velocity z      | 3.64         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.84e+04    |
| time/                   |              |
|    total_timesteps      | 2105500      |
| train/                  |              |
|    approx_kl            | 8.873205e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.001        |
|    loss                 | 4.87e+07     |
|    n_updates            | 10280        |
|    policy_gradient_loss | -0.000261    |
|    std                  | 1.55         |
|    value_loss           | 7.99e+07     |
------------------------------------------
Eval num_timesteps=2106000, episode_reward=-114121.73 +/- 20451.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22594984 |
|    mean velocity x | -0.0798     |
|    mean velocity y | -0.0645     |
|    mean velocity z | 2.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.14e+05   |
| time/              |             |
|    total_timesteps | 2106000     |
------------------------------------
Eval num_timesteps=2106500, episode_reward=-95066.71 +/- 20415.20
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3703902 |
|    mean velocity x | -0.193     |
|    mean velocity y | 0.837      |
|    mean velocity z | 3.33       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.51e+04  |
| time/              |            |
|    total_timesteps | 2106500    |
-----------------------------------
Eval num_timesteps=2107000, episode_reward=-62494.96 +/- 28879.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25781846 |
|    mean velocity x | -0.933      |
|    mean velocity y | -0.722      |
|    mean velocity z | 3.49        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 2107000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1029    |
|    time_elapsed    | 84826   |
|    total_timesteps | 2107392 |
--------------------------------
Eval num_timesteps=2107500, episode_reward=-55942.56 +/- 38213.95
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48273802   |
|    mean velocity x      | -0.43         |
|    mean velocity y      | 0.629         |
|    mean velocity z      | 5.01          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -5.59e+04     |
| time/                   |               |
|    total_timesteps      | 2107500       |
| train/                  |               |
|    approx_kl            | 6.3363084e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.36          |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+07      |
|    n_updates            | 10290         |
|    policy_gradient_loss | -0.000833     |
|    std                  | 1.55          |
|    value_loss           | 5.1e+07       |
-------------------------------------------
Eval num_timesteps=2108000, episode_reward=-98370.33 +/- 12947.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3117052 |
|    mean velocity x | -0.219     |
|    mean velocity y | 0.717      |
|    mean velocity z | 4.07       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.84e+04  |
| time/              |            |
|    total_timesteps | 2108000    |
-----------------------------------
Eval num_timesteps=2108500, episode_reward=-83503.67 +/- 47535.60
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34737274 |
|    mean velocity x | -1.61       |
|    mean velocity y | -1.27       |
|    mean velocity z | 6.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.35e+04   |
| time/              |             |
|    total_timesteps | 2108500     |
------------------------------------
Eval num_timesteps=2109000, episode_reward=-84728.00 +/- 42193.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.48605663 |
|    mean velocity x | -1.24       |
|    mean velocity y | 0.122       |
|    mean velocity z | 3.92        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.47e+04   |
| time/              |             |
|    total_timesteps | 2109000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1030    |
|    time_elapsed    | 84906   |
|    total_timesteps | 2109440 |
--------------------------------
Eval num_timesteps=2109500, episode_reward=-70395.49 +/- 43344.69
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34230077  |
|    mean velocity x      | -0.379       |
|    mean velocity y      | 0.131        |
|    mean velocity z      | 3.77         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.04e+04    |
| time/                   |              |
|    total_timesteps      | 2109500      |
| train/                  |              |
|    approx_kl            | 7.030234e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.387        |
|    learning_rate        | 0.001        |
|    loss                 | 9.22e+07     |
|    n_updates            | 10300        |
|    policy_gradient_loss | -0.000199    |
|    std                  | 1.55         |
|    value_loss           | 7.08e+07     |
------------------------------------------
Eval num_timesteps=2110000, episode_reward=-111224.76 +/- 15050.09
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.470043 |
|    mean velocity x | -0.357    |
|    mean velocity y | 1.08      |
|    mean velocity z | 4.62      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -1.11e+05 |
| time/              |           |
|    total_timesteps | 2110000   |
----------------------------------
Eval num_timesteps=2110500, episode_reward=-83395.03 +/- 44413.24
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30914527 |
|    mean velocity x | -0.266      |
|    mean velocity y | 0.182       |
|    mean velocity z | 3.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.34e+04   |
| time/              |             |
|    total_timesteps | 2110500     |
------------------------------------
Eval num_timesteps=2111000, episode_reward=-58186.32 +/- 35340.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35193637 |
|    mean velocity x | -0.149      |
|    mean velocity y | 0.674       |
|    mean velocity z | 0.544       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.82e+04   |
| time/              |             |
|    total_timesteps | 2111000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1031    |
|    time_elapsed    | 84987   |
|    total_timesteps | 2111488 |
--------------------------------
Eval num_timesteps=2111500, episode_reward=-78605.94 +/- 24719.90
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0676034    |
|    mean velocity x      | -0.13         |
|    mean velocity y      | -0.177        |
|    mean velocity z      | 0.246         |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.86e+04     |
| time/                   |               |
|    total_timesteps      | 2111500       |
| train/                  |               |
|    approx_kl            | 0.00011950813 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 3.55e+06      |
|    n_updates            | 10310         |
|    policy_gradient_loss | -0.000851     |
|    std                  | 1.55          |
|    value_loss           | 5.41e+07      |
-------------------------------------------
Eval num_timesteps=2112000, episode_reward=-69040.78 +/- 56809.81
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45587042 |
|    mean velocity x | 0.163       |
|    mean velocity y | 1           |
|    mean velocity z | 2.82        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.9e+04    |
| time/              |             |
|    total_timesteps | 2112000     |
------------------------------------
Eval num_timesteps=2112500, episode_reward=-92835.55 +/- 23084.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32987317 |
|    mean velocity x | -0.113      |
|    mean velocity y | 0.472       |
|    mean velocity z | 3.4         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.28e+04   |
| time/              |             |
|    total_timesteps | 2112500     |
------------------------------------
Eval num_timesteps=2113000, episode_reward=-74058.43 +/- 35849.96
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44099298 |
|    mean velocity x | -0.444      |
|    mean velocity y | 0.665       |
|    mean velocity z | 4.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.41e+04   |
| time/              |             |
|    total_timesteps | 2113000     |
------------------------------------
Eval num_timesteps=2113500, episode_reward=-66768.75 +/- 25718.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29650626 |
|    mean velocity x | 0.183       |
|    mean velocity y | 0.561       |
|    mean velocity z | 1.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 2113500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1032    |
|    time_elapsed    | 85100   |
|    total_timesteps | 2113536 |
--------------------------------
Eval num_timesteps=2114000, episode_reward=-85053.34 +/- 20844.07
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40056467   |
|    mean velocity x      | -0.347        |
|    mean velocity y      | 0.797         |
|    mean velocity z      | 4.31          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.51e+04     |
| time/                   |               |
|    total_timesteps      | 2114000       |
| train/                  |               |
|    approx_kl            | 2.9069517e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.313         |
|    learning_rate        | 0.001         |
|    loss                 | 8e+07         |
|    n_updates            | 10320         |
|    policy_gradient_loss | -0.000473     |
|    std                  | 1.55          |
|    value_loss           | 6.9e+07       |
-------------------------------------------
Eval num_timesteps=2114500, episode_reward=-64215.89 +/- 43919.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41370568 |
|    mean velocity x | -0.00882    |
|    mean velocity y | 0.869       |
|    mean velocity z | 3.51        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 2114500     |
------------------------------------
Eval num_timesteps=2115000, episode_reward=-98360.47 +/- 48604.71
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3252729 |
|    mean velocity x | 0.0187     |
|    mean velocity y | 0.599      |
|    mean velocity z | 3.74       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.84e+04  |
| time/              |            |
|    total_timesteps | 2115000    |
-----------------------------------
Eval num_timesteps=2115500, episode_reward=-59959.81 +/- 45329.42
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.35261434 |
|    mean velocity x | -0.778      |
|    mean velocity y | 0.195       |
|    mean velocity z | 3.35        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6e+04      |
| time/              |             |
|    total_timesteps | 2115500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1033    |
|    time_elapsed    | 85180   |
|    total_timesteps | 2115584 |
--------------------------------
Eval num_timesteps=2116000, episode_reward=-56328.76 +/- 28471.18
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.39464805  |
|    mean velocity x      | -0.0162      |
|    mean velocity y      | 0.897        |
|    mean velocity z      | 2.88         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.63e+04    |
| time/                   |              |
|    total_timesteps      | 2116000      |
| train/                  |              |
|    approx_kl            | 5.888549e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.56        |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.001        |
|    loss                 | 2.72e+07     |
|    n_updates            | 10330        |
|    policy_gradient_loss | -0.00015     |
|    std                  | 1.55         |
|    value_loss           | 4.95e+07     |
------------------------------------------
Eval num_timesteps=2116500, episode_reward=-93040.33 +/- 31015.97
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44701445 |
|    mean velocity x | -0.353      |
|    mean velocity y | 0.732       |
|    mean velocity z | 4.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -9.3e+04    |
| time/              |             |
|    total_timesteps | 2116500     |
------------------------------------
Eval num_timesteps=2117000, episode_reward=-129867.25 +/- 58731.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29608983 |
|    mean velocity x | -0.752      |
|    mean velocity y | 0.377       |
|    mean velocity z | 2.99        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.3e+05    |
| time/              |             |
|    total_timesteps | 2117000     |
------------------------------------
Eval num_timesteps=2117500, episode_reward=-72745.33 +/- 46499.88
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22006561 |
|    mean velocity x | -0.128      |
|    mean velocity y | 0.43        |
|    mean velocity z | 0.284       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.27e+04   |
| time/              |             |
|    total_timesteps | 2117500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1034    |
|    time_elapsed    | 85261   |
|    total_timesteps | 2117632 |
--------------------------------
Eval num_timesteps=2118000, episode_reward=-105340.66 +/- 25347.18
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.16679886   |
|    mean velocity x      | -0.521        |
|    mean velocity y      | -0.01         |
|    mean velocity z      | 2.34          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 2118000       |
| train/                  |               |
|    approx_kl            | 4.9415394e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.56         |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.001         |
|    loss                 | 2.8e+06       |
|    n_updates            | 10340         |
|    policy_gradient_loss | -0.000663     |
|    std                  | 1.55          |
|    value_loss           | 4.38e+07      |
-------------------------------------------
Eval num_timesteps=2118500, episode_reward=-73042.80 +/- 41814.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50565183 |
|    mean velocity x | -0.464      |
|    mean velocity y | 1.21        |
|    mean velocity z | 4.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.3e+04    |
| time/              |             |
|    total_timesteps | 2118500     |
------------------------------------
Eval num_timesteps=2119000, episode_reward=-47857.69 +/- 25229.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34453753 |
|    mean velocity x | -0.0613     |
|    mean velocity y | 0.614       |
|    mean velocity z | 3.65        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.79e+04   |
| time/              |             |
|    total_timesteps | 2119000     |
------------------------------------
Eval num_timesteps=2119500, episode_reward=-98388.79 +/- 16144.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3170075 |
|    mean velocity x | -0.0659    |
|    mean velocity y | 0.496      |
|    mean velocity z | 3.94       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -9.84e+04  |
| time/              |            |
|    total_timesteps | 2119500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1035    |
|    time_elapsed    | 85341   |
|    total_timesteps | 2119680 |
--------------------------------
Eval num_timesteps=2120000, episode_reward=-84065.16 +/- 60990.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45350105   |
|    mean velocity x      | 0.555         |
|    mean velocity y      | 1.64          |
|    mean velocity z      | 3.29          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.41e+04     |
| time/                   |               |
|    total_timesteps      | 2120000       |
| train/                  |               |
|    approx_kl            | 3.2614742e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.354         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+07      |
|    n_updates            | 10350         |
|    policy_gradient_loss | -0.000272     |
|    std                  | 1.55          |
|    value_loss           | 6.38e+07      |
-------------------------------------------
Eval num_timesteps=2120500, episode_reward=-62325.65 +/- 42006.34
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3943443 |
|    mean velocity x | -0.497     |
|    mean velocity y | 0.065      |
|    mean velocity z | 4          |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.23e+04  |
| time/              |            |
|    total_timesteps | 2120500    |
-----------------------------------
Eval num_timesteps=2121000, episode_reward=-103784.59 +/- 17706.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25404567 |
|    mean velocity x | -0.264      |
|    mean velocity y | 0.503       |
|    mean velocity z | 0.454       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 2121000     |
------------------------------------
Eval num_timesteps=2121500, episode_reward=-60062.01 +/- 34546.79
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2820205 |
|    mean velocity x | -0.274     |
|    mean velocity y | 0.346      |
|    mean velocity z | 1.1        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.01e+04  |
| time/              |            |
|    total_timesteps | 2121500    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1036    |
|    time_elapsed    | 85421   |
|    total_timesteps | 2121728 |
--------------------------------
Eval num_timesteps=2122000, episode_reward=-93005.71 +/- 22410.63
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45746723   |
|    mean velocity x      | -0.375        |
|    mean velocity y      | 0.713         |
|    mean velocity z      | 4.58          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -9.3e+04      |
| time/                   |               |
|    total_timesteps      | 2122000       |
| train/                  |               |
|    approx_kl            | 3.4672907e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.3           |
|    learning_rate        | 0.001         |
|    loss                 | 3.54e+07      |
|    n_updates            | 10360         |
|    policy_gradient_loss | -0.000439     |
|    std                  | 1.55          |
|    value_loss           | 4.65e+07      |
-------------------------------------------
Eval num_timesteps=2122500, episode_reward=-67240.77 +/- 32813.61
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41098037 |
|    mean velocity x | -0.706      |
|    mean velocity y | 0.639       |
|    mean velocity z | 3.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 2122500     |
------------------------------------
Eval num_timesteps=2123000, episode_reward=-83968.18 +/- 30255.44
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45760393 |
|    mean velocity x | -0.602      |
|    mean velocity y | 0.705       |
|    mean velocity z | 4.13        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.4e+04    |
| time/              |             |
|    total_timesteps | 2123000     |
------------------------------------
Eval num_timesteps=2123500, episode_reward=-56535.15 +/- 35924.80
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29709098 |
|    mean velocity x | 0.11        |
|    mean velocity y | 0.577       |
|    mean velocity z | 1.63        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.65e+04   |
| time/              |             |
|    total_timesteps | 2123500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1037    |
|    time_elapsed    | 85502   |
|    total_timesteps | 2123776 |
--------------------------------
Eval num_timesteps=2124000, episode_reward=-98046.23 +/- 22455.59
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47109818  |
|    mean velocity x      | 0.253        |
|    mean velocity y      | 1.47         |
|    mean velocity z      | 3.07         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -9.8e+04     |
| time/                   |              |
|    total_timesteps      | 2124000      |
| train/                  |              |
|    approx_kl            | 6.055081e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.001        |
|    loss                 | 9.19e+06     |
|    n_updates            | 10370        |
|    policy_gradient_loss | -0.000121    |
|    std                  | 1.55         |
|    value_loss           | 3.5e+07      |
------------------------------------------
Eval num_timesteps=2124500, episode_reward=-101863.97 +/- 31849.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.44859302 |
|    mean velocity x | -0.405      |
|    mean velocity y | 1.15        |
|    mean velocity z | 1.21        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 2124500     |
------------------------------------
Eval num_timesteps=2125000, episode_reward=-50120.87 +/- 43067.25
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.30950353 |
|    mean velocity x | 0.24        |
|    mean velocity y | 0.855       |
|    mean velocity z | 2.91        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.01e+04   |
| time/              |             |
|    total_timesteps | 2125000     |
------------------------------------
Eval num_timesteps=2125500, episode_reward=-81000.22 +/- 30240.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23334008 |
|    mean velocity x | -0.451      |
|    mean velocity y | -0.408      |
|    mean velocity z | 3.31        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 2125500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1038    |
|    time_elapsed    | 85582   |
|    total_timesteps | 2125824 |
--------------------------------
Eval num_timesteps=2126000, episode_reward=-74389.32 +/- 25226.48
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.55292326  |
|    mean velocity x      | 0.509        |
|    mean velocity y      | 1.81         |
|    mean velocity z      | 3.67         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -7.44e+04    |
| time/                   |              |
|    total_timesteps      | 2126000      |
| train/                  |              |
|    approx_kl            | 1.967841e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.001        |
|    loss                 | 1.8e+07      |
|    n_updates            | 10380        |
|    policy_gradient_loss | -0.000332    |
|    std                  | 1.55         |
|    value_loss           | 2.47e+07     |
------------------------------------------
Eval num_timesteps=2126500, episode_reward=-68177.66 +/- 35955.86
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3430698 |
|    mean velocity x | -0.251     |
|    mean velocity y | 0.627      |
|    mean velocity z | 4.26       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.82e+04  |
| time/              |            |
|    total_timesteps | 2126500    |
-----------------------------------
Eval num_timesteps=2127000, episode_reward=-76571.95 +/- 43904.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29177964 |
|    mean velocity x | -0.235      |
|    mean velocity y | 0.27        |
|    mean velocity z | 3.64        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.66e+04   |
| time/              |             |
|    total_timesteps | 2127000     |
------------------------------------
Eval num_timesteps=2127500, episode_reward=-107387.36 +/- 18673.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.20514399 |
|    mean velocity x | 0.0333      |
|    mean velocity y | 0.147       |
|    mean velocity z | 3.14        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 2127500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1039    |
|    time_elapsed    | 85662   |
|    total_timesteps | 2127872 |
--------------------------------
Eval num_timesteps=2128000, episode_reward=-64014.56 +/- 42653.56
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.41704527   |
|    mean velocity x      | 0.409         |
|    mean velocity y      | 1.7           |
|    mean velocity z      | 3.49          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.4e+04      |
| time/                   |               |
|    total_timesteps      | 2128000       |
| train/                  |               |
|    approx_kl            | 3.5176054e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.31          |
|    learning_rate        | 0.001         |
|    loss                 | 4.75e+07      |
|    n_updates            | 10390         |
|    policy_gradient_loss | -0.000155     |
|    std                  | 1.55          |
|    value_loss           | 7.87e+07      |
-------------------------------------------
Eval num_timesteps=2128500, episode_reward=-89094.84 +/- 43886.32
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3476192 |
|    mean velocity x | -0.966     |
|    mean velocity y | -0.377     |
|    mean velocity z | 3.3        |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.91e+04  |
| time/              |            |
|    total_timesteps | 2128500    |
-----------------------------------
Eval num_timesteps=2129000, episode_reward=-87921.39 +/- 13052.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42134655 |
|    mean velocity x | -0.308      |
|    mean velocity y | 0.771       |
|    mean velocity z | 4.48        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.79e+04   |
| time/              |             |
|    total_timesteps | 2129000     |
------------------------------------
Eval num_timesteps=2129500, episode_reward=-103654.40 +/- 27613.39
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17119764 |
|    mean velocity x | -0.036      |
|    mean velocity y | 0.353       |
|    mean velocity z | 0.328       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 2129500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1040    |
|    time_elapsed    | 85743   |
|    total_timesteps | 2129920 |
--------------------------------
Eval num_timesteps=2130000, episode_reward=-89066.41 +/- 46060.46
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3553234   |
|    mean velocity x      | 0.286        |
|    mean velocity y      | 0.846        |
|    mean velocity z      | 0.991        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.91e+04    |
| time/                   |              |
|    total_timesteps      | 2130000      |
| train/                  |              |
|    approx_kl            | 7.484766e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.325        |
|    learning_rate        | 0.001        |
|    loss                 | 8.42e+06     |
|    n_updates            | 10400        |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.55         |
|    value_loss           | 4.1e+07      |
------------------------------------------
Eval num_timesteps=2130500, episode_reward=-68529.68 +/- 41463.64
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.6206092 |
|    mean velocity x | 0.164      |
|    mean velocity y | 2.35       |
|    mean velocity z | 4.31       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.85e+04  |
| time/              |            |
|    total_timesteps | 2130500    |
-----------------------------------
Eval num_timesteps=2131000, episode_reward=-81281.63 +/- 42105.78
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.53003347 |
|    mean velocity x | 0.428       |
|    mean velocity y | 2.34        |
|    mean velocity z | 4.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 2131000     |
------------------------------------
Eval num_timesteps=2131500, episode_reward=-49057.07 +/- 45106.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.29819068 |
|    mean velocity x | 0.265       |
|    mean velocity y | 0.745       |
|    mean velocity z | 2.18        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -4.91e+04   |
| time/              |             |
|    total_timesteps | 2131500     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1041    |
|    time_elapsed    | 85823   |
|    total_timesteps | 2131968 |
--------------------------------
Eval num_timesteps=2132000, episode_reward=-80820.31 +/- 29024.01
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3989682   |
|    mean velocity x      | -0.231       |
|    mean velocity y      | 0.587        |
|    mean velocity z      | 4.09         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.08e+04    |
| time/                   |              |
|    total_timesteps      | 2132000      |
| train/                  |              |
|    approx_kl            | 8.821284e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+07     |
|    n_updates            | 10410        |
|    policy_gradient_loss | -0.00015     |
|    std                  | 1.55         |
|    value_loss           | 4.44e+07     |
------------------------------------------
Eval num_timesteps=2132500, episode_reward=-66765.80 +/- 34013.41
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.40815175 |
|    mean velocity x | -0.458      |
|    mean velocity y | 0.437       |
|    mean velocity z | 4.23        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 2132500     |
------------------------------------
Eval num_timesteps=2133000, episode_reward=-77809.83 +/- 22019.65
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42159098 |
|    mean velocity x | -0.214      |
|    mean velocity y | 1.14        |
|    mean velocity z | 3.5         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 2133000     |
------------------------------------
Eval num_timesteps=2133500, episode_reward=-84870.99 +/- 47202.21
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3354845 |
|    mean velocity x | -0.524     |
|    mean velocity y | 0.565      |
|    mean velocity z | 3.14       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.49e+04  |
| time/              |            |
|    total_timesteps | 2133500    |
-----------------------------------
Eval num_timesteps=2134000, episode_reward=-55594.40 +/- 49792.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41874483 |
|    mean velocity x | 0.61        |
|    mean velocity y | 1.42        |
|    mean velocity z | 3.29        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.56e+04   |
| time/              |             |
|    total_timesteps | 2134000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1042    |
|    time_elapsed    | 85922   |
|    total_timesteps | 2134016 |
--------------------------------
Eval num_timesteps=2134500, episode_reward=-77824.59 +/- 40483.50
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35255018   |
|    mean velocity x      | 0.0132        |
|    mean velocity y      | 0.987         |
|    mean velocity z      | 4.02          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.78e+04     |
| time/                   |               |
|    total_timesteps      | 2134500       |
| train/                  |               |
|    approx_kl            | 1.9049912e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.334         |
|    learning_rate        | 0.001         |
|    loss                 | 4.46e+07      |
|    n_updates            | 10420         |
|    policy_gradient_loss | -0.000118     |
|    std                  | 1.55          |
|    value_loss           | 7.14e+07      |
-------------------------------------------
Eval num_timesteps=2135000, episode_reward=-82852.39 +/- 22966.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.42211416 |
|    mean velocity x | 0.217       |
|    mean velocity y | 1.05        |
|    mean velocity z | 2.89        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 2135000     |
------------------------------------
Eval num_timesteps=2135500, episode_reward=-79386.26 +/- 31828.28
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3456048 |
|    mean velocity x | -0.143     |
|    mean velocity y | 1.08       |
|    mean velocity z | 4.23       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -7.94e+04  |
| time/              |            |
|    total_timesteps | 2135500    |
-----------------------------------
Eval num_timesteps=2136000, episode_reward=-80560.02 +/- 23871.69
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4430641 |
|    mean velocity x | -0.364     |
|    mean velocity y | 1.03       |
|    mean velocity z | 4.54       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.06e+04  |
| time/              |            |
|    total_timesteps | 2136000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1043    |
|    time_elapsed    | 86003   |
|    total_timesteps | 2136064 |
--------------------------------
Eval num_timesteps=2136500, episode_reward=-104724.24 +/- 29118.19
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1926393   |
|    mean velocity x      | -0.172       |
|    mean velocity y      | 0.478        |
|    mean velocity z      | 0.214        |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 2136500      |
| train/                  |              |
|    approx_kl            | 8.338073e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.001        |
|    loss                 | 2.86e+07     |
|    n_updates            | 10430        |
|    policy_gradient_loss | -0.000226    |
|    std                  | 1.55         |
|    value_loss           | 5.54e+07     |
------------------------------------------
Eval num_timesteps=2137000, episode_reward=-62475.53 +/- 26653.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.34539625 |
|    mean velocity x | -0.61       |
|    mean velocity y | 0.779       |
|    mean velocity z | 1.03        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 2137000     |
------------------------------------
Eval num_timesteps=2137500, episode_reward=-69376.42 +/- 41430.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.36088586 |
|    mean velocity x | -0.391      |
|    mean velocity y | 0.343       |
|    mean velocity z | 3.62        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.94e+04   |
| time/              |             |
|    total_timesteps | 2137500     |
------------------------------------
Eval num_timesteps=2138000, episode_reward=-86595.48 +/- 47571.52
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.3391185 |
|    mean velocity x | -0.458     |
|    mean velocity y | 0.289      |
|    mean velocity z | 3.75       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.66e+04  |
| time/              |            |
|    total_timesteps | 2138000    |
-----------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1044    |
|    time_elapsed    | 86083   |
|    total_timesteps | 2138112 |
--------------------------------
Eval num_timesteps=2138500, episode_reward=-85891.81 +/- 18352.57
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26723236  |
|    mean velocity x      | 0.189        |
|    mean velocity y      | 0.66         |
|    mean velocity z      | 3.36         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -8.59e+04    |
| time/                   |              |
|    total_timesteps      | 2138500      |
| train/                  |              |
|    approx_kl            | 2.653239e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.266        |
|    learning_rate        | 0.001        |
|    loss                 | 5.77e+07     |
|    n_updates            | 10440        |
|    policy_gradient_loss | -0.000342    |
|    std                  | 1.55         |
|    value_loss           | 5.83e+07     |
------------------------------------------
Eval num_timesteps=2139000, episode_reward=-68570.88 +/- 9019.03
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4610507 |
|    mean velocity x | -0.159     |
|    mean velocity y | 1.04       |
|    mean velocity z | 3.12       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.86e+04  |
| time/              |            |
|    total_timesteps | 2139000    |
-----------------------------------
Eval num_timesteps=2139500, episode_reward=-66548.15 +/- 28967.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.37016264 |
|    mean velocity x | -0.374      |
|    mean velocity y | 0.431       |
|    mean velocity z | 4.26        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.65e+04   |
| time/              |             |
|    total_timesteps | 2139500     |
------------------------------------
Eval num_timesteps=2140000, episode_reward=-61103.58 +/- 37811.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.50783134 |
|    mean velocity x | -0.879      |
|    mean velocity y | 0.49        |
|    mean velocity z | 4.41        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.11e+04   |
| time/              |             |
|    total_timesteps | 2140000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1045    |
|    time_elapsed    | 86164   |
|    total_timesteps | 2140160 |
--------------------------------
Eval num_timesteps=2140500, episode_reward=-64516.66 +/- 37279.42
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.28991324   |
|    mean velocity x      | -0.645        |
|    mean velocity y      | 0.181         |
|    mean velocity z      | 2.99          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -6.45e+04     |
| time/                   |               |
|    total_timesteps      | 2140500       |
| train/                  |               |
|    approx_kl            | 2.3212197e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.297         |
|    learning_rate        | 0.001         |
|    loss                 | 4.42e+07      |
|    n_updates            | 10450         |
|    policy_gradient_loss | -0.000417     |
|    std                  | 1.55          |
|    value_loss           | 5.92e+07      |
-------------------------------------------
Eval num_timesteps=2141000, episode_reward=-85130.59 +/- 28786.62
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23786907 |
|    mean velocity x | -0.753      |
|    mean velocity y | -0.324      |
|    mean velocity z | 2.98        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.51e+04   |
| time/              |             |
|    total_timesteps | 2141000     |
------------------------------------
Eval num_timesteps=2141500, episode_reward=-51499.29 +/- 23198.32
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.31289893 |
|    mean velocity x | -0.864      |
|    mean velocity y | -0.271      |
|    mean velocity z | 3.09        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.15e+04   |
| time/              |             |
|    total_timesteps | 2141500     |
------------------------------------
Eval num_timesteps=2142000, episode_reward=-71660.95 +/- 35933.95
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.49156958 |
|    mean velocity x | 0.0398      |
|    mean velocity y | 1.5         |
|    mean velocity z | 3.58        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -7.17e+04   |
| time/              |             |
|    total_timesteps | 2142000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1046    |
|    time_elapsed    | 86244   |
|    total_timesteps | 2142208 |
--------------------------------
Eval num_timesteps=2142500, episode_reward=-84031.27 +/- 36234.03
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.35339984   |
|    mean velocity x      | -0.377        |
|    mean velocity y      | 0.594         |
|    mean velocity z      | 3.67          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -8.4e+04      |
| time/                   |               |
|    total_timesteps      | 2142500       |
| train/                  |               |
|    approx_kl            | 4.4637505e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.378         |
|    learning_rate        | 0.001         |
|    loss                 | 3.07e+07      |
|    n_updates            | 10460         |
|    policy_gradient_loss | -0.000604     |
|    std                  | 1.55          |
|    value_loss           | 4.44e+07      |
-------------------------------------------
Eval num_timesteps=2143000, episode_reward=-66993.63 +/- 36497.83
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41308767 |
|    mean velocity x | -0.243      |
|    mean velocity y | 0.62        |
|    mean velocity z | 4.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 2143000     |
------------------------------------
Eval num_timesteps=2143500, episode_reward=-86394.20 +/- 33673.42
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.4498497 |
|    mean velocity x | -0.932     |
|    mean velocity y | -0.00641   |
|    mean velocity z | 3.69       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -8.64e+04  |
| time/              |            |
|    total_timesteps | 2143500    |
-----------------------------------
Eval num_timesteps=2144000, episode_reward=-69828.10 +/- 33663.66
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39068192 |
|    mean velocity x | -0.988      |
|    mean velocity y | -0.214      |
|    mean velocity z | 3.04        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.98e+04   |
| time/              |             |
|    total_timesteps | 2144000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1047    |
|    time_elapsed    | 86324   |
|    total_timesteps | 2144256 |
--------------------------------
Eval num_timesteps=2144500, episode_reward=-79710.50 +/- 31813.89
Episode length: 5000.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40627062   |
|    mean velocity x      | -0.174        |
|    mean velocity y      | 0.867         |
|    mean velocity z      | 4.13          |
|    mean_ep_length       | 5e+03         |
|    mean_reward          | -7.97e+04     |
| time/                   |               |
|    total_timesteps      | 2144500       |
| train/                  |               |
|    approx_kl            | 9.8930905e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.55         |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.001         |
|    loss                 | 3.28e+07      |
|    n_updates            | 10470         |
|    policy_gradient_loss | -0.000232     |
|    std                  | 1.55          |
|    value_loss           | 8.03e+07      |
-------------------------------------------
Eval num_timesteps=2145000, episode_reward=-67565.36 +/- 45743.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19375052 |
|    mean velocity x | 0.0511      |
|    mean velocity y | 0.433       |
|    mean velocity z | 0.242       |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 2145000     |
------------------------------------
Eval num_timesteps=2145500, episode_reward=-62594.31 +/- 34777.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.39143524 |
|    mean velocity x | -0.272      |
|    mean velocity y | 1.22        |
|    mean velocity z | 4.1         |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.26e+04   |
| time/              |             |
|    total_timesteps | 2145500     |
------------------------------------
Eval num_timesteps=2146000, episode_reward=-81400.09 +/- 52760.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.45366472 |
|    mean velocity x | -0.294      |
|    mean velocity y | 1.08        |
|    mean velocity z | 4.32        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -8.14e+04   |
| time/              |             |
|    total_timesteps | 2146000     |
------------------------------------
--------------------------------
| time/              |         |
|    fps             | 24      |
|    iterations      | 1048    |
|    time_elapsed    | 86404   |
|    total_timesteps | 2146304 |
--------------------------------
