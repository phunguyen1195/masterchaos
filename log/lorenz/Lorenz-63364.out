Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./Lorenz_tensorboard/PPO_7
Eval num_timesteps=500, episode_reward=-6356872.33 +/- 32888.40
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.079482675 |
|    mean velocity x | 2.51         |
|    mean velocity y | 2.53         |
|    mean velocity z | 26.3         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.36e+06    |
| time/              |              |
|    total_timesteps | 500          |
-------------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-6359061.02 +/- 29916.46
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14055584 |
|    mean velocity x | 2.5         |
|    mean velocity y | 3.17        |
|    mean velocity z | 25.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.36e+06   |
| time/              |             |
|    total_timesteps | 1000        |
------------------------------------
Eval num_timesteps=1500, episode_reward=-6350183.46 +/- 28646.01
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.02248177 |
|    mean velocity x | 1.62        |
|    mean velocity y | 1.29        |
|    mean velocity z | 25.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 1500        |
------------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-6324428.03 +/- 25188.91
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.020533219 |
|    mean velocity x | -1.48        |
|    mean velocity y | -1.48        |
|    mean velocity z | 25.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.32e+06    |
| time/              |              |
|    total_timesteps | 2000         |
-------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 1    |
|    time_elapsed    | 83   |
|    total_timesteps | 2048 |
-----------------------------
Eval num_timesteps=2500, episode_reward=-6361383.47 +/- 27878.27
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.10049203 |
|    mean velocity x      | -5.39       |
|    mean velocity y      | -5.69       |
|    mean velocity z      | 26.7        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.36e+06   |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.004178981 |
|    clip_fraction        | 0.00801     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | -1.45e-05   |
|    learning_rate        | 0.001       |
|    loss                 | 2.15e+08    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00314    |
|    std                  | 0.998       |
|    value_loss           | 4.53e+08    |
-----------------------------------------
Eval num_timesteps=3000, episode_reward=-6392342.89 +/- 28377.56
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.013668763 |
|    mean velocity x | -4.41        |
|    mean velocity y | -4.84        |
|    mean velocity z | 25.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.39e+06    |
| time/              |              |
|    total_timesteps | 3000         |
-------------------------------------
Eval num_timesteps=3500, episode_reward=-6367594.03 +/- 34782.52
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.038228173 |
|    mean velocity x | -0.461       |
|    mean velocity y | -0.55        |
|    mean velocity z | 25.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.37e+06    |
| time/              |              |
|    total_timesteps | 3500         |
-------------------------------------
Eval num_timesteps=4000, episode_reward=-6402433.61 +/- 41778.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06229404 |
|    mean velocity x | -1.46       |
|    mean velocity y | -1.92       |
|    mean velocity z | 25          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.4e+06    |
| time/              |             |
|    total_timesteps | 4000        |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 2    |
|    time_elapsed    | 166  |
|    total_timesteps | 4096 |
-----------------------------
Eval num_timesteps=4500, episode_reward=-6312424.55 +/- 30888.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.023847882 |
|    mean velocity x      | -4.35        |
|    mean velocity y      | -4.56        |
|    mean velocity z      | 25.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.31e+06    |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0013181926 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 3.58e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 2.44e+08     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.998        |
|    value_loss           | 4.66e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=5000, episode_reward=-6328591.91 +/- 23452.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.02205717 |
|    mean velocity x | 1.38        |
|    mean velocity y | 1.55        |
|    mean velocity z | 24.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.33e+06   |
| time/              |             |
|    total_timesteps | 5000        |
------------------------------------
Eval num_timesteps=5500, episode_reward=-6336163.93 +/- 19851.35
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.059861634 |
|    mean velocity x | 5.2          |
|    mean velocity y | 4.91         |
|    mean velocity z | 24.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.34e+06    |
| time/              |              |
|    total_timesteps | 5500         |
-------------------------------------
Eval num_timesteps=6000, episode_reward=-6282310.05 +/- 47009.15
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.013123711 |
|    mean velocity x | 3.47         |
|    mean velocity y | 3.6          |
|    mean velocity z | 26.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.28e+06    |
| time/              |              |
|    total_timesteps | 6000         |
-------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 3    |
|    time_elapsed    | 250  |
|    total_timesteps | 6144 |
-----------------------------
Eval num_timesteps=6500, episode_reward=-6292359.61 +/- 23875.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.051866114 |
|    mean velocity x      | -2.66        |
|    mean velocity y      | -1.93        |
|    mean velocity z      | 25.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.29e+06    |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.003083319  |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | -9.54e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+08     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.998        |
|    value_loss           | 4.06e+08     |
------------------------------------------
Eval num_timesteps=7000, episode_reward=-6353335.88 +/- 42954.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09377217 |
|    mean velocity x | 2.08        |
|    mean velocity y | 2.31        |
|    mean velocity z | 26          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.35e+06   |
| time/              |             |
|    total_timesteps | 7000        |
------------------------------------
Eval num_timesteps=7500, episode_reward=-6320782.25 +/- 20395.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07933009 |
|    mean velocity x | 1.92        |
|    mean velocity y | 2.21        |
|    mean velocity z | 25.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.32e+06   |
| time/              |             |
|    total_timesteps | 7500        |
------------------------------------
Eval num_timesteps=8000, episode_reward=-6357651.13 +/- 18091.36
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.029416047 |
|    mean velocity x | -2.58       |
|    mean velocity y | -2.54       |
|    mean velocity z | 24.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.36e+06   |
| time/              |             |
|    total_timesteps | 8000        |
------------------------------------
-----------------------------
| time/              |      |
|    fps             | 24   |
|    iterations      | 4    |
|    time_elapsed    | 333  |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=8500, episode_reward=-6329213.99 +/- 31988.97
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.02319009  |
|    mean velocity x      | 2.82         |
|    mean velocity y      | 3.35         |
|    mean velocity z      | 24.6         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.33e+06    |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0026825215 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 5.72e-06     |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+08     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00194     |
|    std                  | 0.997        |
|    value_loss           | 4.66e+08     |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-6345504.43 +/- 9714.32
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.081621565 |
|    mean velocity x | 2.72         |
|    mean velocity y | 3.26         |
|    mean velocity z | 25.9         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.35e+06    |
| time/              |              |
|    total_timesteps | 9000         |
-------------------------------------
Eval num_timesteps=9500, episode_reward=-6333750.42 +/- 18223.64
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0037576389 |
|    mean velocity x | -1.23         |
|    mean velocity y | -1.1          |
|    mean velocity z | 25.2          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -6.33e+06     |
| time/              |               |
|    total_timesteps | 9500          |
--------------------------------------
Eval num_timesteps=10000, episode_reward=-6323335.09 +/- 42878.82
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.082041614 |
|    mean velocity x | 2.51         |
|    mean velocity y | 2.94         |
|    mean velocity z | 26.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.32e+06    |
| time/              |              |
|    total_timesteps | 10000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 5     |
|    time_elapsed    | 416   |
|    total_timesteps | 10240 |
------------------------------
Eval num_timesteps=10500, episode_reward=-6279039.96 +/- 27748.79
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.05256319   |
|    mean velocity x      | -0.594       |
|    mean velocity y      | -1.2         |
|    mean velocity z      | 24.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.28e+06    |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0033082156 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | -0.000253    |
|    learning_rate        | 0.001        |
|    loss                 | 2.51e+08     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.995        |
|    value_loss           | 4.37e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=11000, episode_reward=-6278548.36 +/- 35361.65
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.065256104 |
|    mean velocity x | -2.6         |
|    mean velocity y | -2.26        |
|    mean velocity z | 26.6         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.28e+06    |
| time/              |              |
|    total_timesteps | 11000        |
-------------------------------------
New best mean reward!
Eval num_timesteps=11500, episode_reward=-6280303.38 +/- 29658.93
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.03079632 |
|    mean velocity x | -1.89      |
|    mean velocity y | -1.51      |
|    mean velocity z | 25.8       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.28e+06  |
| time/              |            |
|    total_timesteps | 11500      |
-----------------------------------
Eval num_timesteps=12000, episode_reward=-6273341.32 +/- 28045.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.029364266 |
|    mean velocity x | -0.126      |
|    mean velocity y | -0.246      |
|    mean velocity z | 25.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.27e+06   |
| time/              |             |
|    total_timesteps | 12000       |
------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 6     |
|    time_elapsed    | 500   |
|    total_timesteps | 12288 |
------------------------------
Eval num_timesteps=12500, episode_reward=-6276895.58 +/- 19480.21
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08159845  |
|    mean velocity x      | 0.609        |
|    mean velocity y      | 0.688        |
|    mean velocity z      | 24.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.28e+06    |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0030021654 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.54e+08     |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00305     |
|    std                  | 0.994        |
|    value_loss           | 4.99e+08     |
------------------------------------------
Eval num_timesteps=13000, episode_reward=-6229460.62 +/- 23809.81
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.022455903 |
|    mean velocity x | 4.65         |
|    mean velocity y | 4.17         |
|    mean velocity z | 26.1         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.23e+06    |
| time/              |              |
|    total_timesteps | 13000        |
-------------------------------------
New best mean reward!
Eval num_timesteps=13500, episode_reward=-6265717.33 +/- 19010.35
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.082983464 |
|    mean velocity x | 2.35         |
|    mean velocity y | 2.74         |
|    mean velocity z | 25           |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.27e+06    |
| time/              |              |
|    total_timesteps | 13500        |
-------------------------------------
Eval num_timesteps=14000, episode_reward=-6271262.22 +/- 32387.09
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.006042448 |
|    mean velocity x | 7.57        |
|    mean velocity y | 7.89        |
|    mean velocity z | 27.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.27e+06   |
| time/              |             |
|    total_timesteps | 14000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 7     |
|    time_elapsed    | 583   |
|    total_timesteps | 14336 |
------------------------------
Eval num_timesteps=14500, episode_reward=-6200119.79 +/- 9431.35
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.029113697 |
|    mean velocity x      | -1.68        |
|    mean velocity y      | -1.74        |
|    mean velocity z      | 21.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.2e+06     |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.002412013  |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 1.07e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 2.57e+08     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0014      |
|    std                  | 0.994        |
|    value_loss           | 4.56e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-6225981.28 +/- 13775.14
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.082642056 |
|    mean velocity x | -4.32        |
|    mean velocity y | -4.41        |
|    mean velocity z | 24.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.23e+06    |
| time/              |              |
|    total_timesteps | 15000        |
-------------------------------------
Eval num_timesteps=15500, episode_reward=-6250955.18 +/- 27064.32
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.105689585 |
|    mean velocity x | 5.01         |
|    mean velocity y | 4.84         |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.25e+06    |
| time/              |              |
|    total_timesteps | 15500        |
-------------------------------------
Eval num_timesteps=16000, episode_reward=-6216895.50 +/- 23894.01
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.088478684 |
|    mean velocity x | -0.438       |
|    mean velocity y | 0.000842     |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.22e+06    |
| time/              |              |
|    total_timesteps | 16000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 8     |
|    time_elapsed    | 666   |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=16500, episode_reward=-6222079.77 +/- 40920.84
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.091195606 |
|    mean velocity x      | -0.156       |
|    mean velocity y      | 0.236        |
|    mean velocity z      | 25           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.22e+06    |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0013120917 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+08     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0021      |
|    std                  | 0.992        |
|    value_loss           | 3.92e+08     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=-6228998.47 +/- 22556.96
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.102267295 |
|    mean velocity x | 5.29         |
|    mean velocity y | 5.52         |
|    mean velocity z | 27           |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.23e+06    |
| time/              |              |
|    total_timesteps | 17000        |
-------------------------------------
Eval num_timesteps=17500, episode_reward=-6207491.16 +/- 18101.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10280126 |
|    mean velocity x | -2.12       |
|    mean velocity y | -2.09       |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+06   |
| time/              |             |
|    total_timesteps | 17500       |
------------------------------------
Eval num_timesteps=18000, episode_reward=-6190200.21 +/- 25480.26
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.046060983 |
|    mean velocity x | -0.681       |
|    mean velocity y | -1.09        |
|    mean velocity z | 25.5         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.19e+06    |
| time/              |              |
|    total_timesteps | 18000        |
-------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 9     |
|    time_elapsed    | 749   |
|    total_timesteps | 18432 |
------------------------------
Eval num_timesteps=18500, episode_reward=-6187518.64 +/- 19668.37
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07402321  |
|    mean velocity x      | 1.89         |
|    mean velocity y      | 1.82         |
|    mean velocity z      | 25.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.19e+06    |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0014783678 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.94e+08     |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0016      |
|    std                  | 0.99         |
|    value_loss           | 4.46e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=19000, episode_reward=-6168733.45 +/- 50278.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07814099 |
|    mean velocity x | 2.72        |
|    mean velocity y | 2.99        |
|    mean velocity z | 25.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.17e+06   |
| time/              |             |
|    total_timesteps | 19000       |
------------------------------------
New best mean reward!
Eval num_timesteps=19500, episode_reward=-6192961.91 +/- 14145.87
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.016572995 |
|    mean velocity x | -2.97        |
|    mean velocity y | -3.44        |
|    mean velocity z | 25.6         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.19e+06    |
| time/              |              |
|    total_timesteps | 19500        |
-------------------------------------
Eval num_timesteps=20000, episode_reward=-6191720.11 +/- 19012.04
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.038841654 |
|    mean velocity x | 0.504        |
|    mean velocity y | 0.827        |
|    mean velocity z | 25.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.19e+06    |
| time/              |              |
|    total_timesteps | 20000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 10    |
|    time_elapsed    | 832   |
|    total_timesteps | 20480 |
------------------------------
Eval num_timesteps=20500, episode_reward=-6231799.56 +/- 27402.19
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08460884  |
|    mean velocity x      | -0.758       |
|    mean velocity y      | -0.604       |
|    mean velocity z      | 25.3         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.23e+06    |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0019308835 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.59e+08     |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00156     |
|    std                  | 0.987        |
|    value_loss           | 4.76e+08     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-6219581.45 +/- 21045.91
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.101535685 |
|    mean velocity x | 2.36         |
|    mean velocity y | 2.36         |
|    mean velocity z | 25.3         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.22e+06    |
| time/              |              |
|    total_timesteps | 21000        |
-------------------------------------
Eval num_timesteps=21500, episode_reward=-6201770.23 +/- 20248.69
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.11857914 |
|    mean velocity x | -1.29       |
|    mean velocity y | -1          |
|    mean velocity z | 24          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.2e+06    |
| time/              |             |
|    total_timesteps | 21500       |
------------------------------------
Eval num_timesteps=22000, episode_reward=-4950727.34 +/- 2475694.66
Episode length: 4001.60 +/- 1996.80
------------------------------------
| eval/              |             |
|    mean action     | -0.13678332 |
|    mean velocity x | -1.44       |
|    mean velocity y | -1.08       |
|    mean velocity z | 25.5        |
|    mean_ep_length  | 4e+03       |
|    mean_reward     | -4.95e+06   |
| time/              |             |
|    total_timesteps | 22000       |
------------------------------------
New best mean reward!
Eval num_timesteps=22500, episode_reward=-6206102.42 +/- 15376.30
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18974628 |
|    mean velocity x | 4.98        |
|    mean velocity y | 5.95        |
|    mean velocity z | 26.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+06   |
| time/              |             |
|    total_timesteps | 22500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 11    |
|    time_elapsed    | 931   |
|    total_timesteps | 22528 |
------------------------------
Eval num_timesteps=23000, episode_reward=-6189213.67 +/- 38426.59
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21422988  |
|    mean velocity x      | -2.19        |
|    mean velocity y      | -1.18        |
|    mean velocity z      | 24.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.19e+06    |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 0.0006361442 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.32e+08     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000393    |
|    std                  | 0.987        |
|    value_loss           | 4.87e+08     |
------------------------------------------
Eval num_timesteps=23500, episode_reward=-6213246.68 +/- 34383.75
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14852932 |
|    mean velocity x | -2.73       |
|    mean velocity y | -2.43       |
|    mean velocity z | 25.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.21e+06   |
| time/              |             |
|    total_timesteps | 23500       |
------------------------------------
Eval num_timesteps=24000, episode_reward=-6211255.71 +/- 41911.99
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0731451 |
|    mean velocity x | -3.87      |
|    mean velocity y | -3.63      |
|    mean velocity z | 25.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.21e+06  |
| time/              |            |
|    total_timesteps | 24000      |
-----------------------------------
Eval num_timesteps=24500, episode_reward=-6186822.62 +/- 23053.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.15015595 |
|    mean velocity x | 0.101       |
|    mean velocity y | 0.963       |
|    mean velocity z | 25.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+06   |
| time/              |             |
|    total_timesteps | 24500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 12    |
|    time_elapsed    | 1013  |
|    total_timesteps | 24576 |
------------------------------
Eval num_timesteps=25000, episode_reward=-6228867.79 +/- 7860.90
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.103299834 |
|    mean velocity x      | -0.373       |
|    mean velocity y      | -0.407       |
|    mean velocity z      | 24.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.23e+06    |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0019680797 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+08     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 0.986        |
|    value_loss           | 4.46e+08     |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-6191411.17 +/- 33783.31
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23879631 |
|    mean velocity x | 0.841       |
|    mean velocity y | 1.89        |
|    mean velocity z | 25.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+06   |
| time/              |             |
|    total_timesteps | 25500       |
------------------------------------
Eval num_timesteps=26000, episode_reward=-6177955.12 +/- 32470.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18951346 |
|    mean velocity x | -0.142      |
|    mean velocity y | 0.338       |
|    mean velocity z | 24.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+06   |
| time/              |             |
|    total_timesteps | 26000       |
------------------------------------
Eval num_timesteps=26500, episode_reward=-6180045.92 +/- 61817.27
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16296579 |
|    mean velocity x | 4.18        |
|    mean velocity y | 4.83        |
|    mean velocity z | 25.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.18e+06   |
| time/              |             |
|    total_timesteps | 26500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 13    |
|    time_elapsed    | 1096  |
|    total_timesteps | 26624 |
------------------------------
Eval num_timesteps=27000, episode_reward=-6158167.79 +/- 29183.16
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.17350475 |
|    mean velocity x      | 3           |
|    mean velocity y      | 3.9         |
|    mean velocity z      | 25.8        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -6.16e+06   |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.002791834 |
|    clip_fraction        | 0.002       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 1.91e+08    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00241    |
|    std                  | 0.984       |
|    value_loss           | 4.57e+08    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-6166216.04 +/- 41281.04
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.101603284 |
|    mean velocity x | -1.42        |
|    mean velocity y | -0.979       |
|    mean velocity z | 23.5         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.17e+06    |
| time/              |              |
|    total_timesteps | 27500        |
-------------------------------------
Eval num_timesteps=28000, episode_reward=-6185954.89 +/- 32228.22
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13091254 |
|    mean velocity x | -1.28       |
|    mean velocity y | -1.39       |
|    mean velocity z | 23.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.19e+06   |
| time/              |             |
|    total_timesteps | 28000       |
------------------------------------
Eval num_timesteps=28500, episode_reward=-6200682.86 +/- 41666.87
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09925418 |
|    mean velocity x | 3.03        |
|    mean velocity y | 3.45        |
|    mean velocity z | 24.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.2e+06    |
| time/              |             |
|    total_timesteps | 28500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 14    |
|    time_elapsed    | 1179  |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=29000, episode_reward=-6120329.75 +/- 35577.28
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.077644184 |
|    mean velocity x      | 3.46         |
|    mean velocity y      | 3.57         |
|    mean velocity z      | 24.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.12e+06    |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.002731892  |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | -8.67e-05    |
|    learning_rate        | 0.001        |
|    loss                 | 2.3e+08      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00216     |
|    std                  | 0.986        |
|    value_loss           | 4.08e+08     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=-6106792.74 +/- 46671.96
Episode length: 5000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean action     | -0.126972 |
|    mean velocity x | -1.35     |
|    mean velocity y | -0.67     |
|    mean velocity z | 22.9      |
|    mean_ep_length  | 5e+03     |
|    mean_reward     | -6.11e+06 |
| time/              |           |
|    total_timesteps | 29500     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-6076414.31 +/- 61377.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17211063 |
|    mean velocity x | -3.65       |
|    mean velocity y | -3.56       |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.08e+06   |
| time/              |             |
|    total_timesteps | 30000       |
------------------------------------
Eval num_timesteps=30500, episode_reward=-6134715.58 +/- 26478.00
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.10016773 |
|    mean velocity x | 2.75        |
|    mean velocity y | 3.13        |
|    mean velocity z | 26.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.13e+06   |
| time/              |             |
|    total_timesteps | 30500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 15    |
|    time_elapsed    | 1262  |
|    total_timesteps | 30720 |
------------------------------
Eval num_timesteps=31000, episode_reward=-6128137.35 +/- 53245.55
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13596274  |
|    mean velocity x      | 3.06         |
|    mean velocity y      | 3.57         |
|    mean velocity z      | 26.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.13e+06    |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0034122923 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.56e+08     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.987        |
|    value_loss           | 4.4e+08      |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-6108364.14 +/- 25911.73
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08218802 |
|    mean velocity x | -1.6        |
|    mean velocity y | -1.41       |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.11e+06   |
| time/              |             |
|    total_timesteps | 31500       |
------------------------------------
Eval num_timesteps=32000, episode_reward=-6121508.25 +/- 47984.59
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.074105024 |
|    mean velocity x | -6.24        |
|    mean velocity y | -5.42        |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.12e+06    |
| time/              |              |
|    total_timesteps | 32000        |
-------------------------------------
Eval num_timesteps=32500, episode_reward=-6101672.28 +/- 75795.94
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13492432 |
|    mean velocity x | -2.34       |
|    mean velocity y | -2          |
|    mean velocity z | 24.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.1e+06    |
| time/              |             |
|    total_timesteps | 32500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 16    |
|    time_elapsed    | 1344  |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=33000, episode_reward=-6056172.22 +/- 60452.26
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.110615045 |
|    mean velocity x      | -2.59        |
|    mean velocity y      | -2.68        |
|    mean velocity z      | 24.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.06e+06    |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0031098914 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | -1.97e-05    |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+08     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.987        |
|    value_loss           | 4.17e+08     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=-6055176.83 +/- 34937.18
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.086759076 |
|    mean velocity x | 0.0549       |
|    mean velocity y | 0.457        |
|    mean velocity z | 24.6         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.06e+06    |
| time/              |              |
|    total_timesteps | 33500        |
-------------------------------------
Eval num_timesteps=34000, episode_reward=-6091793.68 +/- 46864.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14227352 |
|    mean velocity x | -0.54       |
|    mean velocity y | -0.191      |
|    mean velocity z | 24.8        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.09e+06   |
| time/              |             |
|    total_timesteps | 34000       |
------------------------------------
Eval num_timesteps=34500, episode_reward=-6117398.15 +/- 35658.15
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.16749342 |
|    mean velocity x | -5.24       |
|    mean velocity y | -5.07       |
|    mean velocity z | 25.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.12e+06   |
| time/              |             |
|    total_timesteps | 34500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 17    |
|    time_elapsed    | 1427  |
|    total_timesteps | 34816 |
------------------------------
Eval num_timesteps=35000, episode_reward=-6118720.04 +/- 73316.27
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15380338  |
|    mean velocity x      | -3.89        |
|    mean velocity y      | -3.56        |
|    mean velocity z      | 24.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.12e+06    |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0038453222 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+08     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00341     |
|    std                  | 0.984        |
|    value_loss           | 4.34e+08     |
------------------------------------------
Eval num_timesteps=35500, episode_reward=-6109347.13 +/- 55612.87
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1985054 |
|    mean velocity x | -0.849     |
|    mean velocity y | -0.433     |
|    mean velocity z | 25.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.11e+06  |
| time/              |            |
|    total_timesteps | 35500      |
-----------------------------------
Eval num_timesteps=36000, episode_reward=-6090793.17 +/- 39605.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17435986 |
|    mean velocity x | -3.75       |
|    mean velocity y | -3.39       |
|    mean velocity z | 23.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.09e+06   |
| time/              |             |
|    total_timesteps | 36000       |
------------------------------------
Eval num_timesteps=36500, episode_reward=-6066992.24 +/- 32931.38
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25137126 |
|    mean velocity x | 4.79        |
|    mean velocity y | 5.63        |
|    mean velocity z | 24.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.07e+06   |
| time/              |             |
|    total_timesteps | 36500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 18    |
|    time_elapsed    | 1509  |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=37000, episode_reward=-6108636.17 +/- 74838.39
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2103873   |
|    mean velocity x      | -0.461       |
|    mean velocity y      | -0.109       |
|    mean velocity z      | 25.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.11e+06    |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0042483746 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+08     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00352     |
|    std                  | 0.985        |
|    value_loss           | 4.33e+08     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=-6153944.72 +/- 93416.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32201526 |
|    mean velocity x | 4.23        |
|    mean velocity y | 5.51        |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.15e+06   |
| time/              |             |
|    total_timesteps | 37500       |
------------------------------------
Eval num_timesteps=38000, episode_reward=-6137579.82 +/- 44937.25
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2630133 |
|    mean velocity x | 0.923      |
|    mean velocity y | 1.51       |
|    mean velocity z | 25.1       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.14e+06  |
| time/              |            |
|    total_timesteps | 38000      |
-----------------------------------
Eval num_timesteps=38500, episode_reward=-6149769.33 +/- 58455.40
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22416276 |
|    mean velocity x | -4.86       |
|    mean velocity y | -3.87       |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.15e+06   |
| time/              |             |
|    total_timesteps | 38500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 19    |
|    time_elapsed    | 1592  |
|    total_timesteps | 38912 |
------------------------------
Eval num_timesteps=39000, episode_reward=-6065530.30 +/- 35915.84
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32820672  |
|    mean velocity x      | 3.54         |
|    mean velocity y      | 4.51         |
|    mean velocity z      | 25.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.07e+06    |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0024565984 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.09e+08     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.984        |
|    value_loss           | 3.98e+08     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-6077402.44 +/- 106095.63
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.19612075 |
|    mean velocity x | 1.04        |
|    mean velocity y | 1.4         |
|    mean velocity z | 25.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.08e+06   |
| time/              |             |
|    total_timesteps | 39500       |
------------------------------------
Eval num_timesteps=40000, episode_reward=-6036012.65 +/- 54178.07
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2765769 |
|    mean velocity x | -0.232     |
|    mean velocity y | 0.47       |
|    mean velocity z | 24.5       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -6.04e+06  |
| time/              |            |
|    total_timesteps | 40000      |
-----------------------------------
Eval num_timesteps=40500, episode_reward=-6056746.35 +/- 30557.86
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.22130285 |
|    mean velocity x | -3.02       |
|    mean velocity y | -2.74       |
|    mean velocity z | 24.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.06e+06   |
| time/              |             |
|    total_timesteps | 40500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 20    |
|    time_elapsed    | 1674  |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=41000, episode_reward=-6005183.80 +/- 62064.90
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3578115   |
|    mean velocity x      | 4.9          |
|    mean velocity y      | 5.88         |
|    mean velocity z      | 25.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.01e+06    |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 0.0033367914 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+08     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00338     |
|    std                  | 0.984        |
|    value_loss           | 4.27e+08     |
------------------------------------------
Eval num_timesteps=41500, episode_reward=-6073595.41 +/- 56652.14
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24747646 |
|    mean velocity x | -2.37       |
|    mean velocity y | -1.35       |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.07e+06   |
| time/              |             |
|    total_timesteps | 41500       |
------------------------------------
Eval num_timesteps=42000, episode_reward=-5654008.30 +/- 693966.10
Episode length: 4713.60 +/- 572.80
-----------------------------------
| eval/              |            |
|    mean action     | -0.2557344 |
|    mean velocity x | -2.78      |
|    mean velocity y | -2.32      |
|    mean velocity z | 24.4       |
|    mean_ep_length  | 4.71e+03   |
|    mean_reward     | -5.65e+06  |
| time/              |            |
|    total_timesteps | 42000      |
-----------------------------------
Eval num_timesteps=42500, episode_reward=-6006475.06 +/- 68132.79
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.23135954 |
|    mean velocity x | 0.722       |
|    mean velocity y | 0.7         |
|    mean velocity z | 23.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.01e+06   |
| time/              |             |
|    total_timesteps | 42500       |
------------------------------------
Eval num_timesteps=43000, episode_reward=-6046515.54 +/- 60034.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.21650265 |
|    mean velocity x | -0.564      |
|    mean velocity y | -0.00841    |
|    mean velocity z | 24.7        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.05e+06   |
| time/              |             |
|    total_timesteps | 43000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 21    |
|    time_elapsed    | 1775  |
|    total_timesteps | 43008 |
------------------------------
Eval num_timesteps=43500, episode_reward=-6092121.00 +/- 55882.27
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24500112  |
|    mean velocity x      | 4.12         |
|    mean velocity y      | 4.97         |
|    mean velocity z      | 25.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -6.09e+06    |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0028159267 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 4.17e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.75e+08     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.984        |
|    value_loss           | 3.96e+08     |
------------------------------------------
Eval num_timesteps=44000, episode_reward=-5994557.16 +/- 49616.89
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14367655 |
|    mean velocity x | -0.477      |
|    mean velocity y | -0.315      |
|    mean velocity z | 24.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.99e+06   |
| time/              |             |
|    total_timesteps | 44000       |
------------------------------------
Eval num_timesteps=44500, episode_reward=-6092904.07 +/- 52812.82
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.12120238 |
|    mean velocity x | 0.707       |
|    mean velocity y | 0.945       |
|    mean velocity z | 25.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.09e+06   |
| time/              |             |
|    total_timesteps | 44500       |
------------------------------------
Eval num_timesteps=45000, episode_reward=-6050188.54 +/- 82465.93
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.28931174 |
|    mean velocity x | 7.05        |
|    mean velocity y | 7.93        |
|    mean velocity z | 27.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.05e+06   |
| time/              |             |
|    total_timesteps | 45000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 22    |
|    time_elapsed    | 1858  |
|    total_timesteps | 45056 |
------------------------------
Eval num_timesteps=45500, episode_reward=-5981646.14 +/- 65470.51
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.11053237 |
|    mean velocity x      | 0.161       |
|    mean velocity y      | 0.651       |
|    mean velocity z      | 23.2        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.98e+06   |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.002151829 |
|    clip_fraction        | 0.00229     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.001       |
|    loss                 | 2.31e+08    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.984       |
|    value_loss           | 4.37e+08    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=-5996539.24 +/- 69574.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17262417 |
|    mean velocity x | 5.78        |
|    mean velocity y | 6.45        |
|    mean velocity z | 24.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6e+06      |
| time/              |             |
|    total_timesteps | 46000       |
------------------------------------
Eval num_timesteps=46500, episode_reward=-5973932.93 +/- 77453.51
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17136694 |
|    mean velocity x | -1.57       |
|    mean velocity y | -1.43       |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.97e+06   |
| time/              |             |
|    total_timesteps | 46500       |
------------------------------------
Eval num_timesteps=47000, episode_reward=-6039516.51 +/- 70816.67
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17642891 |
|    mean velocity x | 6.7         |
|    mean velocity y | 7.31        |
|    mean velocity z | 26.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6.04e+06   |
| time/              |             |
|    total_timesteps | 47000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 23    |
|    time_elapsed    | 1940  |
|    total_timesteps | 47104 |
------------------------------
Eval num_timesteps=47500, episode_reward=-5963763.78 +/- 81778.47
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08300231  |
|    mean velocity x      | 4            |
|    mean velocity y      | 4.2          |
|    mean velocity z      | 26.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.96e+06    |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 0.0014092589 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.83e+08     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00151     |
|    std                  | 0.983        |
|    value_loss           | 3.74e+08     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-5071324.45 +/- 1870865.55
Episode length: 4227.00 +/- 1546.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1666631 |
|    mean velocity x | 2.87       |
|    mean velocity y | 3.37       |
|    mean velocity z | 26.1       |
|    mean_ep_length  | 4.23e+03   |
|    mean_reward     | -5.07e+06  |
| time/              |            |
|    total_timesteps | 48000      |
-----------------------------------
Eval num_timesteps=48500, episode_reward=-5997743.91 +/- 91526.91
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07874132 |
|    mean velocity x | 1.54        |
|    mean velocity y | 1.86        |
|    mean velocity z | 24.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -6e+06      |
| time/              |             |
|    total_timesteps | 48500       |
------------------------------------
Eval num_timesteps=49000, episode_reward=-5952982.10 +/- 85862.47
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.042296913 |
|    mean velocity x | -1.74        |
|    mean velocity y | -1.81        |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.95e+06    |
| time/              |              |
|    total_timesteps | 49000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 24    |
|    time_elapsed    | 2020  |
|    total_timesteps | 49152 |
------------------------------
Eval num_timesteps=49500, episode_reward=-5970582.27 +/- 57015.11
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13303974  |
|    mean velocity x      | 3.01         |
|    mean velocity y      | 2.89         |
|    mean velocity z      | 24.5         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.97e+06    |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0049725827 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.15e+08     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00383     |
|    std                  | 0.981        |
|    value_loss           | 4.36e+08     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-5901575.87 +/- 90118.28
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08652066 |
|    mean velocity x | 0.0721      |
|    mean velocity y | 0.395       |
|    mean velocity z | 24.3        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.9e+06    |
| time/              |             |
|    total_timesteps | 50000       |
------------------------------------
Eval num_timesteps=50500, episode_reward=-5984987.69 +/- 50106.20
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14260241 |
|    mean velocity x | 6.48        |
|    mean velocity y | 7.03        |
|    mean velocity z | 26.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.98e+06   |
| time/              |             |
|    total_timesteps | 50500       |
------------------------------------
Eval num_timesteps=51000, episode_reward=-5916236.29 +/- 104533.29
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.062797904 |
|    mean velocity x | 3.33         |
|    mean velocity y | 3.85         |
|    mean velocity z | 25           |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.92e+06    |
| time/              |              |
|    total_timesteps | 51000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 25    |
|    time_elapsed    | 2102  |
|    total_timesteps | 51200 |
------------------------------
Eval num_timesteps=51500, episode_reward=-5822993.22 +/- 129424.14
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.06692726 |
|    mean velocity x      | 3.91        |
|    mean velocity y      | 4.5         |
|    mean velocity z      | 24.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.82e+06   |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.002007001 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 1.31e-06    |
|    learning_rate        | 0.001       |
|    loss                 | 1.91e+08    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00232    |
|    std                  | 0.978       |
|    value_loss           | 3.92e+08    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-5901342.96 +/- 35291.23
Episode length: 5000.00 +/- 0.00
--------------------------------------
| eval/              |               |
|    mean action     | -0.0059741465 |
|    mean velocity x | 4.5           |
|    mean velocity y | 4.68          |
|    mean velocity z | 25.3          |
|    mean_ep_length  | 5e+03         |
|    mean_reward     | -5.9e+06      |
| time/              |               |
|    total_timesteps | 52000         |
--------------------------------------
Eval num_timesteps=52500, episode_reward=-5871488.10 +/- 92906.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08504579 |
|    mean velocity x | 1.07        |
|    mean velocity y | 1.4         |
|    mean velocity z | 24.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.87e+06   |
| time/              |             |
|    total_timesteps | 52500       |
------------------------------------
Eval num_timesteps=53000, episode_reward=-5925788.81 +/- 39499.33
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.0561262 |
|    mean velocity x | 4.74       |
|    mean velocity y | 4.69       |
|    mean velocity z | 26         |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.93e+06  |
| time/              |            |
|    total_timesteps | 53000      |
-----------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 26    |
|    time_elapsed    | 2185  |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=53500, episode_reward=-5100276.49 +/- 1653000.92
Episode length: 4318.00 +/- 1364.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.042939514 |
|    mean velocity x      | 5.75         |
|    mean velocity y      | 5.53         |
|    mean velocity z      | 24.5         |
|    mean_ep_length       | 4.32e+03     |
|    mean_reward          | -5.1e+06     |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0022365744 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+08     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00171     |
|    std                  | 0.975        |
|    value_loss           | 4.2e+08      |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-5844520.96 +/- 85461.32
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0010858192 |
|    mean velocity x | 7.15         |
|    mean velocity y | 6.71         |
|    mean velocity z | 26.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.84e+06    |
| time/              |              |
|    total_timesteps | 54000        |
-------------------------------------
Eval num_timesteps=54500, episode_reward=-5918706.51 +/- 114566.11
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.042539384 |
|    mean velocity x | 2.14         |
|    mean velocity y | 2.5          |
|    mean velocity z | 24.2         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.92e+06    |
| time/              |              |
|    total_timesteps | 54500        |
-------------------------------------
Eval num_timesteps=55000, episode_reward=-5906495.14 +/- 105956.85
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.055600867 |
|    mean velocity x | -0.436      |
|    mean velocity y | -0.349      |
|    mean velocity z | 25          |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.91e+06   |
| time/              |             |
|    total_timesteps | 55000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 27    |
|    time_elapsed    | 2265  |
|    total_timesteps | 55296 |
------------------------------
Eval num_timesteps=55500, episode_reward=-5927408.38 +/- 64564.11
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.12206932  |
|    mean velocity x      | 3.4          |
|    mean velocity y      | 4.03         |
|    mean velocity z      | 23.9         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.93e+06    |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0029436674 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 2.14e+08     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00356     |
|    std                  | 0.972        |
|    value_loss           | 4.32e+08     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-6027782.79 +/- 131508.53
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | 0.0021108016 |
|    mean velocity x | 1.87         |
|    mean velocity y | 1.57         |
|    mean velocity z | 24.1         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.03e+06    |
| time/              |              |
|    total_timesteps | 56000        |
-------------------------------------
Eval num_timesteps=56500, episode_reward=-6017696.65 +/- 87286.83
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.067682855 |
|    mean velocity x | 2.82         |
|    mean velocity y | 3.09         |
|    mean velocity z | 24.4         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -6.02e+06    |
| time/              |              |
|    total_timesteps | 56500        |
-------------------------------------
Eval num_timesteps=57000, episode_reward=-5960462.73 +/- 80926.10
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.034502033 |
|    mean velocity x | 8.04         |
|    mean velocity y | 8.06         |
|    mean velocity z | 25.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.96e+06    |
| time/              |              |
|    total_timesteps | 57000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 28    |
|    time_elapsed    | 2347  |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=57500, episode_reward=-5829731.28 +/- 70150.31
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | 0.011983153  |
|    mean velocity x      | 5.1          |
|    mean velocity y      | 5.17         |
|    mean velocity z      | 25           |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.83e+06    |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0004886089 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+08     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000999    |
|    std                  | 0.971        |
|    value_loss           | 3.7e+08      |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-5922903.31 +/- 74337.41
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.043501675 |
|    mean velocity x | 1.62         |
|    mean velocity y | 1.42         |
|    mean velocity z | 23.1         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.92e+06    |
| time/              |              |
|    total_timesteps | 58000        |
-------------------------------------
Eval num_timesteps=58500, episode_reward=-5925384.46 +/- 120759.66
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.032989852 |
|    mean velocity x | 0.925        |
|    mean velocity y | 0.303        |
|    mean velocity z | 23.4         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.93e+06    |
| time/              |              |
|    total_timesteps | 58500        |
-------------------------------------
Eval num_timesteps=59000, episode_reward=-5878543.22 +/- 32208.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.03011216 |
|    mean velocity x | 0.626       |
|    mean velocity y | 0.887       |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.88e+06   |
| time/              |             |
|    total_timesteps | 59000       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 29    |
|    time_elapsed    | 2430  |
|    total_timesteps | 59392 |
------------------------------
Eval num_timesteps=59500, episode_reward=-5766773.37 +/- 89094.42
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.018757256 |
|    mean velocity x      | 3.47         |
|    mean velocity y      | 4.15         |
|    mean velocity z      | 23.1         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.77e+06    |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.004041151  |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+08      |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00314     |
|    std                  | 0.97         |
|    value_loss           | 3.69e+08     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-5783762.02 +/- 121732.68
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.034246594 |
|    mean velocity x | 1.86        |
|    mean velocity y | 1.28        |
|    mean velocity z | 24.4        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.78e+06   |
| time/              |             |
|    total_timesteps | 60000       |
------------------------------------
Eval num_timesteps=60500, episode_reward=-5796971.81 +/- 37844.42
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.026163997 |
|    mean velocity x | -2.07        |
|    mean velocity y | -2.28        |
|    mean velocity z | 22.7         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.8e+06     |
| time/              |              |
|    total_timesteps | 60500        |
-------------------------------------
Eval num_timesteps=61000, episode_reward=-5874510.75 +/- 97553.94
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.043927137 |
|    mean velocity x | -5.22        |
|    mean velocity y | -4.9         |
|    mean velocity z | 24.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.87e+06    |
| time/              |              |
|    total_timesteps | 61000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 30    |
|    time_elapsed    | 2512  |
|    total_timesteps | 61440 |
------------------------------
Eval num_timesteps=61500, episode_reward=-5841988.75 +/- 69202.77
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.050224688 |
|    mean velocity x      | 1.6          |
|    mean velocity y      | 1.64         |
|    mean velocity z      | 23.2         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.84e+06    |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0022195764 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.88e+08     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00177     |
|    std                  | 0.971        |
|    value_loss           | 3.9e+08      |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-5776364.46 +/- 35484.52
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06424646 |
|    mean velocity x | 1.19        |
|    mean velocity y | 1.64        |
|    mean velocity z | 23.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.78e+06   |
| time/              |             |
|    total_timesteps | 62000       |
------------------------------------
Eval num_timesteps=62500, episode_reward=-5745781.26 +/- 55350.92
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | 0.026147354 |
|    mean velocity x | 0.863       |
|    mean velocity y | 0.646       |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.75e+06   |
| time/              |             |
|    total_timesteps | 62500       |
------------------------------------
Eval num_timesteps=63000, episode_reward=-5795487.25 +/- 37748.02
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.007716867 |
|    mean velocity x | 0.246        |
|    mean velocity y | 0.827        |
|    mean velocity z | 23.6         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.8e+06     |
| time/              |              |
|    total_timesteps | 63000        |
-------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 31    |
|    time_elapsed    | 2595  |
|    total_timesteps | 63488 |
------------------------------
Eval num_timesteps=63500, episode_reward=-5780774.49 +/- 108810.74
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08857193  |
|    mean velocity x      | 0.206        |
|    mean velocity y      | 0.486        |
|    mean velocity z      | 23.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.78e+06    |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0029489375 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+08     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00268     |
|    std                  | 0.971        |
|    value_loss           | 3.53e+08     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-5779391.96 +/- 100959.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.06031601 |
|    mean velocity x | -0.185      |
|    mean velocity y | 0.0681      |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.78e+06   |
| time/              |             |
|    total_timesteps | 64000       |
------------------------------------
Eval num_timesteps=64500, episode_reward=-5791796.93 +/- 86386.03
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.07383806 |
|    mean velocity x | 0.341       |
|    mean velocity y | 0.0372      |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.79e+06   |
| time/              |             |
|    total_timesteps | 64500       |
------------------------------------
Eval num_timesteps=65000, episode_reward=-5718736.08 +/- 106802.48
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.028240997 |
|    mean velocity x | -1.93        |
|    mean velocity y | -2.04        |
|    mean velocity z | 21.8         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.72e+06    |
| time/              |              |
|    total_timesteps | 65000        |
-------------------------------------
Eval num_timesteps=65500, episode_reward=-5759068.89 +/- 94503.19
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.08566136 |
|    mean velocity x | 3.94        |
|    mean velocity y | 3.96        |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.76e+06   |
| time/              |             |
|    total_timesteps | 65500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 32    |
|    time_elapsed    | 2697  |
|    total_timesteps | 65536 |
------------------------------
Eval num_timesteps=66000, episode_reward=-5636327.52 +/- 56817.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.09570891  |
|    mean velocity x      | 1.74         |
|    mean velocity y      | 1.71         |
|    mean velocity z      | 21.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.64e+06    |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0038588112 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+08     |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00298     |
|    std                  | 0.969        |
|    value_loss           | 3.27e+08     |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-5721597.02 +/- 98978.54
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13050345 |
|    mean velocity x | 5.11        |
|    mean velocity y | 5.8         |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.72e+06   |
| time/              |             |
|    total_timesteps | 66500       |
------------------------------------
Eval num_timesteps=67000, episode_reward=-5757781.91 +/- 102117.64
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09967929 |
|    mean velocity x | 4.08        |
|    mean velocity y | 4.46        |
|    mean velocity z | 24.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.76e+06   |
| time/              |             |
|    total_timesteps | 67000       |
------------------------------------
Eval num_timesteps=67500, episode_reward=-5719981.51 +/- 150059.71
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.18673585 |
|    mean velocity x | 4.01        |
|    mean velocity y | 4.74        |
|    mean velocity z | 23.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.72e+06   |
| time/              |             |
|    total_timesteps | 67500       |
------------------------------------
------------------------------
| time/              |       |
|    fps             | 24    |
|    iterations      | 33    |
|    time_elapsed    | 2779  |
|    total_timesteps | 67584 |
------------------------------
Eval num_timesteps=68000, episode_reward=-5578077.14 +/- 49017.10
Episode length: 5000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.14647089 |
|    mean velocity x      | 1.98        |
|    mean velocity y      | 2.41        |
|    mean velocity z      | 23.5        |
|    mean_ep_length       | 5e+03       |
|    mean_reward          | -5.58e+06   |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.00460736  |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.001       |
|    loss                 | 2.03e+08    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00344    |
|    std                  | 0.969       |
|    value_loss           | 3.53e+08    |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=-5558742.43 +/- 104226.48
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.14534423 |
|    mean velocity x | 2.76        |
|    mean velocity y | 3.02        |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.56e+06   |
| time/              |             |
|    total_timesteps | 68500       |
------------------------------------
Eval num_timesteps=69000, episode_reward=-5614156.47 +/- 120818.70
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.042097427 |
|    mean velocity x | -3.49        |
|    mean velocity y | -3.46        |
|    mean velocity z | 22.5         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.61e+06    |
| time/              |              |
|    total_timesteps | 69000        |
-------------------------------------
Eval num_timesteps=69500, episode_reward=-4449088.55 +/- 1402112.19
Episode length: 4033.00 +/- 1254.38
------------------------------------
| eval/              |             |
|    mean action     | -0.06167029 |
|    mean velocity x | -4.15       |
|    mean velocity y | -3.76       |
|    mean velocity z | 24.1        |
|    mean_ep_length  | 4.03e+03    |
|    mean_reward     | -4.45e+06   |
| time/              |             |
|    total_timesteps | 69500       |
------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.43e+04  |
|    ep_rew_mean     | -4.46e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 34        |
|    time_elapsed    | 2859      |
|    total_timesteps | 69632     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-4642420.69 +/- 1999399.44
Episode length: 4117.80 +/- 1764.40
------------------------------------------
| eval/                   |              |
|    mean action          | -0.071442425 |
|    mean velocity x      | -0.441       |
|    mean velocity y      | -0.179       |
|    mean velocity z      | 22.7         |
|    mean_ep_length       | 4.12e+03     |
|    mean_reward          | -4.64e+06    |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0051097022 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+08     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00534     |
|    std                  | 0.967        |
|    value_loss           | 3.65e+08     |
------------------------------------------
Eval num_timesteps=70500, episode_reward=-5703295.71 +/- 44877.05
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.25349063 |
|    mean velocity x | 4.68        |
|    mean velocity y | 5.39        |
|    mean velocity z | 25.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.7e+06    |
| time/              |             |
|    total_timesteps | 70500       |
------------------------------------
Eval num_timesteps=71000, episode_reward=-4707886.09 +/- 1869648.84
Episode length: 4173.00 +/- 1654.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.006601846 |
|    mean velocity x | -5.08        |
|    mean velocity y | -5.8         |
|    mean velocity z | 22.7         |
|    mean_ep_length  | 4.17e+03     |
|    mean_reward     | -4.71e+06    |
| time/              |              |
|    total_timesteps | 71000        |
-------------------------------------
Eval num_timesteps=71500, episode_reward=-5635656.50 +/- 130305.11
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.17724091 |
|    mean velocity x | 5.34        |
|    mean velocity y | 5.51        |
|    mean velocity z | 23.5        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.64e+06   |
| time/              |             |
|    total_timesteps | 71500       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.43e+04  |
|    ep_rew_mean     | -4.46e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 35        |
|    time_elapsed    | 2934      |
|    total_timesteps | 71680     |
----------------------------------
Eval num_timesteps=72000, episode_reward=-4077430.71 +/- 1963022.13
Episode length: 3636.20 +/- 1686.49
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29906154  |
|    mean velocity x      | 6.07         |
|    mean velocity y      | 6.75         |
|    mean velocity z      | 24.7         |
|    mean_ep_length       | 3.64e+03     |
|    mean_reward          | -4.08e+06    |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0035215064 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.69e+08     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.967        |
|    value_loss           | 3.65e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=72500, episode_reward=-4721806.37 +/- 1725118.52
Episode length: 4214.40 +/- 1547.31
------------------------------------
| eval/              |             |
|    mean action     | -0.22948182 |
|    mean velocity x | 2.4         |
|    mean velocity y | 3.66        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 4.21e+03    |
|    mean_reward     | -4.72e+06   |
| time/              |             |
|    total_timesteps | 72500       |
------------------------------------
Eval num_timesteps=73000, episode_reward=-5614420.75 +/- 68331.04
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24989982 |
|    mean velocity x | 4.73        |
|    mean velocity y | 5.83        |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.61e+06   |
| time/              |             |
|    total_timesteps | 73000       |
------------------------------------
Eval num_timesteps=73500, episode_reward=-4592531.27 +/- 2128290.63
Episode length: 4063.60 +/- 1872.80
-----------------------------------
| eval/              |            |
|    mean action     | -0.2557958 |
|    mean velocity x | 1.77       |
|    mean velocity y | 2.01       |
|    mean velocity z | 23.6       |
|    mean_ep_length  | 4.06e+03   |
|    mean_reward     | -4.59e+06  |
| time/              |            |
|    total_timesteps | 73500      |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.43e+04  |
|    ep_rew_mean     | -4.46e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 36        |
|    time_elapsed    | 3005      |
|    total_timesteps | 73728     |
----------------------------------
Eval num_timesteps=74000, episode_reward=-5163261.80 +/- 847580.43
Episode length: 4613.80 +/- 772.40
------------------------------------------
| eval/                   |              |
|    mean action          | -0.19132006  |
|    mean velocity x      | 1.09         |
|    mean velocity y      | 1.62         |
|    mean velocity z      | 23.1         |
|    mean_ep_length       | 4.61e+03     |
|    mean_reward          | -5.16e+06    |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0038509448 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.75e+08     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00378     |
|    std                  | 0.967        |
|    value_loss           | 3.48e+08     |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-3324343.81 +/- 2270624.35
Episode length: 2975.40 +/- 2025.33
------------------------------------
| eval/              |             |
|    mean action     | -0.21804968 |
|    mean velocity x | -1.22       |
|    mean velocity y | -0.892      |
|    mean velocity z | 22.7        |
|    mean_ep_length  | 2.98e+03    |
|    mean_reward     | -3.32e+06   |
| time/              |             |
|    total_timesteps | 74500       |
------------------------------------
New best mean reward!
Eval num_timesteps=75000, episode_reward=-4714463.73 +/- 1710101.89
Episode length: 4241.40 +/- 1517.20
------------------------------------
| eval/              |             |
|    mean action     | -0.31243175 |
|    mean velocity x | -0.107      |
|    mean velocity y | 0.281       |
|    mean velocity z | 23.3        |
|    mean_ep_length  | 4.24e+03    |
|    mean_reward     | -4.71e+06   |
| time/              |             |
|    total_timesteps | 75000       |
------------------------------------
Eval num_timesteps=75500, episode_reward=-5157759.21 +/- 974607.35
Episode length: 4572.60 +/- 854.80
------------------------------------
| eval/              |             |
|    mean action     | -0.23984596 |
|    mean velocity x | -0.95       |
|    mean velocity y | 0.299       |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 4.57e+03    |
|    mean_reward     | -5.16e+06   |
| time/              |             |
|    total_timesteps | 75500       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.43e+04  |
|    ep_rew_mean     | -4.46e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 37        |
|    time_elapsed    | 3074      |
|    total_timesteps | 75776     |
----------------------------------
Eval num_timesteps=76000, episode_reward=-4545305.67 +/- 1881822.40
Episode length: 4139.60 +/- 1720.80
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.26065785 |
|    mean velocity x      | 1.21        |
|    mean velocity y      | 1.45        |
|    mean velocity z      | 22.4        |
|    mean_ep_length       | 4.14e+03    |
|    mean_reward          | -4.55e+06   |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.003871259 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.77e+08    |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00352    |
|    std                  | 0.969       |
|    value_loss           | 3.39e+08    |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=-4059822.39 +/- 1763982.35
Episode length: 3690.00 +/- 1625.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.2361542 |
|    mean velocity x | 4.75       |
|    mean velocity y | 5.66       |
|    mean velocity z | 24.8       |
|    mean_ep_length  | 3.69e+03   |
|    mean_reward     | -4.06e+06  |
| time/              |            |
|    total_timesteps | 76500      |
-----------------------------------
Eval num_timesteps=77000, episode_reward=-5556666.02 +/- 77487.55
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.32511157 |
|    mean velocity x | 3.33        |
|    mean velocity y | 4.25        |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.56e+06   |
| time/              |             |
|    total_timesteps | 77000       |
------------------------------------
Eval num_timesteps=77500, episode_reward=-4597460.48 +/- 1069766.97
Episode length: 4203.60 +/- 975.46
------------------------------------
| eval/              |             |
|    mean action     | -0.28358766 |
|    mean velocity x | 4.9         |
|    mean velocity y | 5.99        |
|    mean velocity z | 22.4        |
|    mean_ep_length  | 4.2e+03     |
|    mean_reward     | -4.6e+06    |
| time/              |             |
|    total_timesteps | 77500       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.43e+04  |
|    ep_rew_mean     | -4.46e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 38        |
|    time_elapsed    | 3145      |
|    total_timesteps | 77824     |
----------------------------------
Eval num_timesteps=78000, episode_reward=-5469958.16 +/- 24919.33
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.11977129  |
|    mean velocity x      | 0.0437       |
|    mean velocity y      | 0.181        |
|    mean velocity z      | 22.8         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.47e+06    |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0028764708 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.9e+08      |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.97         |
|    value_loss           | 3.63e+08     |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-5018532.03 +/- 946665.00
Episode length: 4576.80 +/- 846.40
-----------------------------------
| eval/              |            |
|    mean action     | -0.2547045 |
|    mean velocity x | 2.49       |
|    mean velocity y | 3.39       |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 4.58e+03   |
|    mean_reward     | -5.02e+06  |
| time/              |            |
|    total_timesteps | 78500      |
-----------------------------------
Eval num_timesteps=79000, episode_reward=-5508233.52 +/- 17499.35
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.24161896 |
|    mean velocity x | -1.77       |
|    mean velocity y | -1.38       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.51e+06   |
| time/              |             |
|    total_timesteps | 79000       |
------------------------------------
Eval num_timesteps=79500, episode_reward=-5474040.47 +/- 58528.43
Episode length: 5000.00 +/- 0.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13901247 |
|    mean velocity x | -6.06       |
|    mean velocity y | -5.85       |
|    mean velocity z | 23.1        |
|    mean_ep_length  | 5e+03       |
|    mean_reward     | -5.47e+06   |
| time/              |             |
|    total_timesteps | 79500       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.43e+04  |
|    ep_rew_mean     | -4.46e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 39        |
|    time_elapsed    | 3225      |
|    total_timesteps | 79872     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-5605050.97 +/- 44484.98
Episode length: 5000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean action          | -0.3663689 |
|    mean velocity x      | 5.41       |
|    mean velocity y      | 6.53       |
|    mean velocity z      | 24.4       |
|    mean_ep_length       | 5e+03      |
|    mean_reward          | -5.61e+06  |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.00316923 |
|    clip_fraction        | 0.00327    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0          |
|    learning_rate        | 0.001      |
|    loss                 | 1.1e+08    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.00237   |
|    std                  | 0.971      |
|    value_loss           | 3.24e+08   |
----------------------------------------
Eval num_timesteps=80500, episode_reward=-5261760.97 +/- 746508.95
Episode length: 4665.00 +/- 670.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13353032 |
|    mean velocity x | -0.983      |
|    mean velocity y | -0.454      |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 4.66e+03    |
|    mean_reward     | -5.26e+06   |
| time/              |             |
|    total_timesteps | 80500       |
------------------------------------
Eval num_timesteps=81000, episode_reward=-4451952.21 +/- 2151074.87
Episode length: 3994.40 +/- 1905.52
------------------------------------
| eval/              |             |
|    mean action     | -0.35301575 |
|    mean velocity x | 4.36        |
|    mean velocity y | 5.17        |
|    mean velocity z | 24.5        |
|    mean_ep_length  | 3.99e+03    |
|    mean_reward     | -4.45e+06   |
| time/              |             |
|    total_timesteps | 81000       |
------------------------------------
Eval num_timesteps=81500, episode_reward=-5402647.26 +/- 374109.23
Episode length: 4846.60 +/- 306.80
------------------------------------
| eval/              |             |
|    mean action     | -0.16033702 |
|    mean velocity x | -2.06       |
|    mean velocity y | -1.69       |
|    mean velocity z | 23          |
|    mean_ep_length  | 4.85e+03    |
|    mean_reward     | -5.4e+06    |
| time/              |             |
|    total_timesteps | 81500       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.68e+04  |
|    ep_rew_mean     | -3.43e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 40        |
|    time_elapsed    | 3302      |
|    total_timesteps | 81920     |
----------------------------------
Eval num_timesteps=82000, episode_reward=-5560481.19 +/- 101913.85
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.36182362  |
|    mean velocity x      | 7.11         |
|    mean velocity y      | 8.39         |
|    mean velocity z      | 25.7         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.56e+06    |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0024149946 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+08      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.971        |
|    value_loss           | 3.63e+08     |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-4347862.44 +/- 2181160.22
Episode length: 3939.60 +/- 1966.14
------------------------------------
| eval/              |             |
|    mean action     | -0.29529738 |
|    mean velocity x | 5.68        |
|    mean velocity y | 6.82        |
|    mean velocity z | 23.5        |
|    mean_ep_length  | 3.94e+03    |
|    mean_reward     | -4.35e+06   |
| time/              |             |
|    total_timesteps | 82500       |
------------------------------------
Eval num_timesteps=83000, episode_reward=-5452146.91 +/- 331947.36
Episode length: 4865.20 +/- 269.60
------------------------------------
| eval/              |             |
|    mean action     | -0.16349557 |
|    mean velocity x | -2.1        |
|    mean velocity y | -2.29       |
|    mean velocity z | 21.9        |
|    mean_ep_length  | 4.87e+03    |
|    mean_reward     | -5.45e+06   |
| time/              |             |
|    total_timesteps | 83000       |
------------------------------------
Eval num_timesteps=83500, episode_reward=-4791670.98 +/- 1542422.58
Episode length: 4306.20 +/- 1387.60
------------------------------------
| eval/              |             |
|    mean action     | -0.16499892 |
|    mean velocity x | 1.5         |
|    mean velocity y | 1.65        |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 4.31e+03    |
|    mean_reward     | -4.79e+06   |
| time/              |             |
|    total_timesteps | 83500       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.68e+04  |
|    ep_rew_mean     | -3.43e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 41        |
|    time_elapsed    | 3377      |
|    total_timesteps | 83968     |
----------------------------------
Eval num_timesteps=84000, episode_reward=-4763681.43 +/- 1404554.69
Episode length: 4355.60 +/- 1274.35
------------------------------------------
| eval/                   |              |
|    mean action          | -0.060602464 |
|    mean velocity x      | -3.44        |
|    mean velocity y      | -3.53        |
|    mean velocity z      | 23.5         |
|    mean_ep_length       | 4.36e+03     |
|    mean_reward          | -4.76e+06    |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0022006603 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 2.19e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 1.51e+08     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00151     |
|    std                  | 0.973        |
|    value_loss           | 3.38e+08     |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-5366394.45 +/- 200776.89
Episode length: 4926.20 +/- 147.60
------------------------------------
| eval/              |             |
|    mean action     | -0.21471052 |
|    mean velocity x | 0.36        |
|    mean velocity y | 0.72        |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 4.93e+03    |
|    mean_reward     | -5.37e+06   |
| time/              |             |
|    total_timesteps | 84500       |
------------------------------------
Eval num_timesteps=85000, episode_reward=-3518065.93 +/- 1795972.89
Episode length: 3211.40 +/- 1590.87
------------------------------------
| eval/              |             |
|    mean action     | -0.06831779 |
|    mean velocity x | -0.156      |
|    mean velocity y | 0.577       |
|    mean velocity z | 23.6        |
|    mean_ep_length  | 3.21e+03    |
|    mean_reward     | -3.52e+06   |
| time/              |             |
|    total_timesteps | 85000       |
------------------------------------
Eval num_timesteps=85500, episode_reward=-4787657.30 +/- 1659203.99
Episode length: 4281.00 +/- 1438.00
------------------------------------
| eval/              |             |
|    mean action     | -0.13425153 |
|    mean velocity x | 1.54        |
|    mean velocity y | 1.75        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 4.28e+03    |
|    mean_reward     | -4.79e+06   |
| time/              |             |
|    total_timesteps | 85500       |
------------------------------------
Eval num_timesteps=86000, episode_reward=-4172135.54 +/- 2031663.69
Episode length: 3781.00 +/- 1829.35
-----------------------------------
| eval/              |            |
|    mean action     | -0.0981455 |
|    mean velocity x | -2.04      |
|    mean velocity y | -1.55      |
|    mean velocity z | 23.4       |
|    mean_ep_length  | 3.78e+03   |
|    mean_reward     | -4.17e+06  |
| time/              |            |
|    total_timesteps | 86000      |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.68e+04  |
|    ep_rew_mean     | -3.43e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 42        |
|    time_elapsed    | 3462      |
|    total_timesteps | 86016     |
----------------------------------
Eval num_timesteps=86500, episode_reward=-4525272.68 +/- 921231.46
Episode length: 4147.00 +/- 837.17
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14458619  |
|    mean velocity x      | 0.0965       |
|    mean velocity y      | 0.418        |
|    mean velocity z      | 22.8         |
|    mean_ep_length       | 4.15e+03     |
|    mean_reward          | -4.53e+06    |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0029634866 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 2.7e+08      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.0021      |
|    std                  | 0.97         |
|    value_loss           | 3.79e+08     |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-5304455.95 +/- 50961.30
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1878291 |
|    mean velocity x | 0.692      |
|    mean velocity y | 1.92       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.3e+06   |
| time/              |            |
|    total_timesteps | 87000      |
-----------------------------------
Eval num_timesteps=87500, episode_reward=-4438726.95 +/- 2000011.39
Episode length: 4098.20 +/- 1803.60
------------------------------------
| eval/              |             |
|    mean action     | -0.36045292 |
|    mean velocity x | 4.73        |
|    mean velocity y | 6.19        |
|    mean velocity z | 23.7        |
|    mean_ep_length  | 4.1e+03     |
|    mean_reward     | -4.44e+06   |
| time/              |             |
|    total_timesteps | 87500       |
------------------------------------
Eval num_timesteps=88000, episode_reward=-5504617.49 +/- 77095.68
Episode length: 5000.00 +/- 0.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.121464364 |
|    mean velocity x | 0.0132       |
|    mean velocity y | -0.0524      |
|    mean velocity z | 22.1         |
|    mean_ep_length  | 5e+03        |
|    mean_reward     | -5.5e+06     |
| time/              |              |
|    total_timesteps | 88000        |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.68e+04  |
|    ep_rew_mean     | -3.43e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 43        |
|    time_elapsed    | 3539      |
|    total_timesteps | 88064     |
----------------------------------
Eval num_timesteps=88500, episode_reward=-5419390.31 +/- 46952.53
Episode length: 5000.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32790482  |
|    mean velocity x      | 3.6          |
|    mean velocity y      | 4.66         |
|    mean velocity z      | 23.4         |
|    mean_ep_length       | 5e+03        |
|    mean_reward          | -5.42e+06    |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0019202153 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+08      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.972        |
|    value_loss           | 3.48e+08     |
------------------------------------------
Eval num_timesteps=89000, episode_reward=-3726514.15 +/- 2253673.01
Episode length: 3385.60 +/- 2038.45
----------------------------------
| eval/              |           |
|    mean action     | -0.136378 |
|    mean velocity x | -2.08     |
|    mean velocity y | -2.12     |
|    mean velocity z | 22.9      |
|    mean_ep_length  | 3.39e+03  |
|    mean_reward     | -3.73e+06 |
| time/              |           |
|    total_timesteps | 89000     |
----------------------------------
Eval num_timesteps=89500, episode_reward=-4715478.40 +/- 1477151.70
Episode length: 4323.00 +/- 1354.00
------------------------------------
| eval/              |             |
|    mean action     | -0.38037288 |
|    mean velocity x | 7.26        |
|    mean velocity y | 8.69        |
|    mean velocity z | 26          |
|    mean_ep_length  | 4.32e+03    |
|    mean_reward     | -4.72e+06   |
| time/              |             |
|    total_timesteps | 89500       |
------------------------------------
Eval num_timesteps=90000, episode_reward=-5488874.43 +/- 57251.19
Episode length: 5000.00 +/- 0.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.2385288 |
|    mean velocity x | 2.05       |
|    mean velocity y | 2.95       |
|    mean velocity z | 22.4       |
|    mean_ep_length  | 5e+03      |
|    mean_reward     | -5.49e+06  |
| time/              |            |
|    total_timesteps | 90000      |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.68e+04  |
|    ep_rew_mean     | -3.43e+07 |
| time/              |           |
|    fps             | 24        |
|    iterations      | 44        |
|    time_elapsed    | 3612      |
|    total_timesteps | 90112     |
----------------------------------
Eval num_timesteps=90500, episode_reward=-3820935.72 +/- 1830007.13
Episode length: 3621.20 +/- 1747.36
------------------------------------------
| eval/                   |              |
|    mean action          | -0.051871248 |
|    mean velocity x      | -7.07        |
|    mean velocity y      | -6.79        |
|    mean velocity z      | 24           |
|    mean_ep_length       | 3.62e+03     |
|    mean_reward          | -3.82e+06    |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.003902807  |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.54e+08     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.973        |
|    value_loss           | 3.43e+08     |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-5092639.76 +/- 555120.51
Episode length: 4760.20 +/- 479.60
------------------------------------
| eval/              |             |
|    mean action     | -0.40112582 |
|    mean velocity x | 7.06        |
|    mean velocity y | 8.53        |
|    mean velocity z | 25.4        |
|    mean_ep_length  | 4.76e+03    |
|    mean_reward     | -5.09e+06   |
| time/              |             |
|    total_timesteps | 91000       |
------------------------------------
Eval num_timesteps=91500, episode_reward=-3268623.22 +/- 2178955.74
Episode length: 3035.40 +/- 1996.06
------------------------------------
| eval/              |             |
|    mean action     | -0.23919547 |
|    mean velocity x | 1.53        |
|    mean velocity y | 2.31        |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 3.04e+03    |
|    mean_reward     | -3.27e+06   |
| time/              |             |
|    total_timesteps | 91500       |
------------------------------------
New best mean reward!
Eval num_timesteps=92000, episode_reward=-5119392.93 +/- 344686.17
Episode length: 4847.00 +/- 306.00
-----------------------------------
| eval/              |            |
|    mean action     | -0.1448446 |
|    mean velocity x | -3.82      |
|    mean velocity y | -3.78      |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 4.85e+03   |
|    mean_reward     | -5.12e+06  |
| time/              |            |
|    total_timesteps | 92000      |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.68e+04  |
|    ep_rew_mean     | -3.43e+07 |
| time/              |           |
|    fps             | 25        |
|    iterations      | 45        |
|    time_elapsed    | 3680      |
|    total_timesteps | 92160     |
----------------------------------
Eval num_timesteps=92500, episode_reward=-4335164.43 +/- 1807721.81
Episode length: 4143.80 +/- 1712.40
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2734808   |
|    mean velocity x      | 2.71         |
|    mean velocity y      | 3.52         |
|    mean velocity z      | 21.6         |
|    mean_ep_length       | 4.14e+03     |
|    mean_reward          | -4.34e+06    |
| time/                   |              |
|    total_timesteps      | 92500        |
| train/                  |              |
|    approx_kl            | 0.0020828699 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+08     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.973        |
|    value_loss           | 3.43e+08     |
------------------------------------------
Eval num_timesteps=93000, episode_reward=-3131026.32 +/- 1941470.43
Episode length: 3007.40 +/- 1870.36
------------------------------------
| eval/              |             |
|    mean action     | -0.22998217 |
|    mean velocity x | 3.49        |
|    mean velocity y | 4.6         |
|    mean velocity z | 22.4        |
|    mean_ep_length  | 3.01e+03    |
|    mean_reward     | -3.13e+06   |
| time/              |             |
|    total_timesteps | 93000       |
------------------------------------
New best mean reward!
Eval num_timesteps=93500, episode_reward=-3256753.88 +/- 1607223.55
Episode length: 3187.40 +/- 1562.72
------------------------------------
| eval/              |             |
|    mean action     | -0.27356744 |
|    mean velocity x | 4.02        |
|    mean velocity y | 4.32        |
|    mean velocity z | 23.9        |
|    mean_ep_length  | 3.19e+03    |
|    mean_reward     | -3.26e+06   |
| time/              |             |
|    total_timesteps | 93500       |
------------------------------------
Eval num_timesteps=94000, episode_reward=-4808746.83 +/- 476765.86
Episode length: 4692.40 +/- 451.34
------------------------------------
| eval/              |             |
|    mean action     | -0.21203409 |
|    mean velocity x | -2.73       |
|    mean velocity y | -1.94       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 4.69e+03    |
|    mean_reward     | -4.81e+06   |
| time/              |             |
|    total_timesteps | 94000       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.31e+04  |
|    ep_rew_mean     | -2.93e+07 |
| time/              |           |
|    fps             | 25        |
|    iterations      | 46        |
|    time_elapsed    | 3744      |
|    total_timesteps | 94208     |
----------------------------------
Eval num_timesteps=94500, episode_reward=-3151675.68 +/- 1730348.95
Episode length: 3138.00 +/- 1721.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3862285   |
|    mean velocity x      | 3.71         |
|    mean velocity y      | 4.93         |
|    mean velocity z      | 23           |
|    mean_ep_length       | 3.14e+03     |
|    mean_reward          | -3.15e+06    |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0057576587 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 1.07e-06     |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+08     |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00441     |
|    std                  | 0.973        |
|    value_loss           | 3.17e+08     |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-3265097.52 +/- 1725533.98
Episode length: 3262.40 +/- 1716.25
------------------------------------
| eval/              |             |
|    mean action     | -0.37725514 |
|    mean velocity x | 3.32        |
|    mean velocity y | 4.46        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 3.26e+03    |
|    mean_reward     | -3.27e+06   |
| time/              |             |
|    total_timesteps | 95000       |
------------------------------------
Eval num_timesteps=95500, episode_reward=-3459736.35 +/- 1030636.48
Episode length: 3419.60 +/- 1026.80
------------------------------------
| eval/              |             |
|    mean action     | -0.24952452 |
|    mean velocity x | 2.33        |
|    mean velocity y | 2.42        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 3.42e+03    |
|    mean_reward     | -3.46e+06   |
| time/              |             |
|    total_timesteps | 95500       |
------------------------------------
Eval num_timesteps=96000, episode_reward=-4168929.67 +/- 1758687.00
Episode length: 4137.80 +/- 1724.40
------------------------------------
| eval/              |             |
|    mean action     | -0.26048052 |
|    mean velocity x | -0.798      |
|    mean velocity y | 0.188       |
|    mean velocity z | 21          |
|    mean_ep_length  | 4.14e+03    |
|    mean_reward     | -4.17e+06   |
| time/              |             |
|    total_timesteps | 96000       |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.9e+04  |
|    ep_rew_mean     | -2.4e+07 |
| time/              |          |
|    fps             | 25       |
|    iterations      | 47       |
|    time_elapsed    | 3803     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-2747790.13 +/- 1944744.52
Episode length: 2754.60 +/- 1951.06
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.2264739  |
|    mean velocity x      | 0.418       |
|    mean velocity y      | 0.844       |
|    mean velocity z      | 20.4        |
|    mean_ep_length       | 2.75e+03    |
|    mean_reward          | -2.75e+06   |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.002394733 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0           |
|    learning_rate        | 0.001       |
|    loss                 | 1.98e+08    |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00161    |
|    std                  | 0.972       |
|    value_loss           | 3.01e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=97000, episode_reward=-4617806.94 +/- 785076.88
Episode length: 4616.60 +/- 766.80
------------------------------------
| eval/              |             |
|    mean action     | -0.38057888 |
|    mean velocity x | 2.27        |
|    mean velocity y | 2.98        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 4.62e+03    |
|    mean_reward     | -4.62e+06   |
| time/              |             |
|    total_timesteps | 97000       |
------------------------------------
Eval num_timesteps=97500, episode_reward=-2589198.24 +/- 1566134.51
Episode length: 2569.60 +/- 1579.55
-----------------------------------
| eval/              |            |
|    mean action     | -0.3141824 |
|    mean velocity x | 1.89       |
|    mean velocity y | 2.51       |
|    mean velocity z | 22.9       |
|    mean_ep_length  | 2.57e+03   |
|    mean_reward     | -2.59e+06  |
| time/              |            |
|    total_timesteps | 97500      |
-----------------------------------
New best mean reward!
Eval num_timesteps=98000, episode_reward=-3957612.53 +/- 1434336.03
Episode length: 3963.20 +/- 1470.62
------------------------------------
| eval/              |             |
|    mean action     | -0.32231405 |
|    mean velocity x | 2.12        |
|    mean velocity y | 2.83        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 3.96e+03    |
|    mean_reward     | -3.96e+06   |
| time/              |             |
|    total_timesteps | 98000       |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.64e+04  |
|    ep_rew_mean     | -2.06e+07 |
| time/              |           |
|    fps             | 25        |
|    iterations      | 48        |
|    time_elapsed    | 3862      |
|    total_timesteps | 98304     |
----------------------------------
Eval num_timesteps=98500, episode_reward=-2360947.63 +/- 1519543.38
Episode length: 2384.40 +/- 1570.21
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43314937  |
|    mean velocity x      | 1.12         |
|    mean velocity y      | 2.01         |
|    mean velocity z      | 20.9         |
|    mean_ep_length       | 2.38e+03     |
|    mean_reward          | -2.36e+06    |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0048011187 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.31e+08     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00439     |
|    std                  | 0.976        |
|    value_loss           | 3.04e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=99000, episode_reward=-3660623.47 +/- 1411242.05
Episode length: 3778.80 +/- 1470.84
----------------------------------
| eval/              |           |
|    mean action     | -0.257288 |
|    mean velocity x | -1.03     |
|    mean velocity y | 0.182     |
|    mean velocity z | 20.7      |
|    mean_ep_length  | 3.78e+03  |
|    mean_reward     | -3.66e+06 |
| time/              |           |
|    total_timesteps | 99000     |
----------------------------------
Eval num_timesteps=99500, episode_reward=-2333480.56 +/- 1626331.48
Episode length: 2405.20 +/- 1633.89
-----------------------------------
| eval/              |            |
|    mean action     | -0.3039186 |
|    mean velocity x | -0.852     |
|    mean velocity y | 0.705      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 2.41e+03   |
|    mean_reward     | -2.33e+06  |
| time/              |            |
|    total_timesteps | 99500      |
-----------------------------------
New best mean reward!
Eval num_timesteps=100000, episode_reward=-3128606.89 +/- 1529514.38
Episode length: 3247.00 +/- 1544.27
------------------------------------
| eval/              |             |
|    mean action     | -0.24867493 |
|    mean velocity x | -1.28       |
|    mean velocity y | -0.956      |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 3.25e+03    |
|    mean_reward     | -3.13e+06   |
| time/              |             |
|    total_timesteps | 100000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.25e+04  |
|    ep_rew_mean     | -1.57e+07 |
| time/              |           |
|    fps             | 25        |
|    iterations      | 49        |
|    time_elapsed    | 3913      |
|    total_timesteps | 100352    |
----------------------------------
Eval num_timesteps=100500, episode_reward=-2880148.47 +/- 2167006.60
Episode length: 2993.20 +/- 2208.64
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07853312  |
|    mean velocity x      | -5.23        |
|    mean velocity y      | -5.79        |
|    mean velocity z      | 23.2         |
|    mean_ep_length       | 2.99e+03     |
|    mean_reward          | -2.88e+06    |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0015730788 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 9.54e-07     |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+08     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.0015      |
|    std                  | 0.977        |
|    value_loss           | 3.1e+08      |
------------------------------------------
Eval num_timesteps=101000, episode_reward=-2959369.77 +/- 1124462.18
Episode length: 3114.00 +/- 1110.72
------------------------------------
| eval/              |             |
|    mean action     | -0.23387346 |
|    mean velocity x | -1.71       |
|    mean velocity y | -1.21       |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 3.11e+03    |
|    mean_reward     | -2.96e+06   |
| time/              |             |
|    total_timesteps | 101000      |
------------------------------------
Eval num_timesteps=101500, episode_reward=-2667870.86 +/- 1763825.88
Episode length: 2774.20 +/- 1816.71
------------------------------------
| eval/              |             |
|    mean action     | -0.57919484 |
|    mean velocity x | 4.47        |
|    mean velocity y | 5.69        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 2.77e+03    |
|    mean_reward     | -2.67e+06   |
| time/              |             |
|    total_timesteps | 101500      |
------------------------------------
Eval num_timesteps=102000, episode_reward=-3230875.27 +/- 1405370.31
Episode length: 3388.20 +/- 1459.93
------------------------------------
| eval/              |             |
|    mean action     | -0.45722878 |
|    mean velocity x | 1.23        |
|    mean velocity y | 3.1         |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 3.39e+03    |
|    mean_reward     | -3.23e+06   |
| time/              |             |
|    total_timesteps | 102000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.02e+04  |
|    ep_rew_mean     | -1.27e+07 |
| time/              |           |
|    fps             | 25        |
|    iterations      | 50        |
|    time_elapsed    | 3966      |
|    total_timesteps | 102400    |
----------------------------------
Eval num_timesteps=102500, episode_reward=-1915844.24 +/- 1140095.95
Episode length: 2020.80 +/- 1213.46
------------------------------------------
| eval/                   |              |
|    mean action          | -0.28808534  |
|    mean velocity x      | -0.756       |
|    mean velocity y      | -0.0434      |
|    mean velocity z      | 21.9         |
|    mean_ep_length       | 2.02e+03     |
|    mean_reward          | -1.92e+06    |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0035306585 |
|    clip_fraction        | 0.00825      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+08     |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00366     |
|    std                  | 0.975        |
|    value_loss           | 3.27e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=103000, episode_reward=-2540641.46 +/- 1695111.37
Episode length: 2741.40 +/- 1744.52
-----------------------------------
| eval/              |            |
|    mean action     | -0.3296823 |
|    mean velocity x | -0.0141    |
|    mean velocity y | 0.542      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 2.74e+03   |
|    mean_reward     | -2.54e+06  |
| time/              |            |
|    total_timesteps | 103000     |
-----------------------------------
Eval num_timesteps=103500, episode_reward=-1457727.65 +/- 1649990.99
Episode length: 1505.40 +/- 1750.10
------------------------------------
| eval/              |             |
|    mean action     | -0.25011393 |
|    mean velocity x | -2.47       |
|    mean velocity y | -1.23       |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 1.51e+03    |
|    mean_reward     | -1.46e+06   |
| time/              |             |
|    total_timesteps | 103500      |
------------------------------------
New best mean reward!
Eval num_timesteps=104000, episode_reward=-2604199.72 +/- 1888427.93
Episode length: 2797.00 +/- 1913.21
------------------------------------
| eval/              |             |
|    mean action     | -0.42706996 |
|    mean velocity x | 2.7         |
|    mean velocity y | 3.49        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 2.8e+03     |
|    mean_reward     | -2.6e+06    |
| time/              |             |
|    total_timesteps | 104000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 9.48e+03  |
|    ep_rew_mean     | -1.18e+07 |
| time/              |           |
|    fps             | 26        |
|    iterations      | 51        |
|    time_elapsed    | 4006      |
|    total_timesteps | 104448    |
----------------------------------
Eval num_timesteps=104500, episode_reward=-1804518.02 +/- 1782512.66
Episode length: 1911.60 +/- 1860.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18098678  |
|    mean velocity x      | -5.01        |
|    mean velocity y      | -4.79        |
|    mean velocity z      | 20.9         |
|    mean_ep_length       | 1.91e+03     |
|    mean_reward          | -1.8e+06     |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0039853924 |
|    clip_fraction        | 0.00518      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.001        |
|    loss                 | 1.57e+08     |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.978        |
|    value_loss           | 2.76e+08     |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-1736389.87 +/- 971599.39
Episode length: 1821.80 +/- 1054.82
------------------------------------
| eval/              |             |
|    mean action     | -0.31911504 |
|    mean velocity x | 1.08        |
|    mean velocity y | 1.64        |
|    mean velocity z | 20          |
|    mean_ep_length  | 1.82e+03    |
|    mean_reward     | -1.74e+06   |
| time/              |             |
|    total_timesteps | 105000      |
------------------------------------
Eval num_timesteps=105500, episode_reward=-1487629.58 +/- 1758164.77
Episode length: 1540.00 +/- 1833.47
------------------------------------
| eval/              |             |
|    mean action     | -0.39834595 |
|    mean velocity x | 2.95        |
|    mean velocity y | 4.48        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 1.54e+03    |
|    mean_reward     | -1.49e+06   |
| time/              |             |
|    total_timesteps | 105500      |
------------------------------------
Eval num_timesteps=106000, episode_reward=-1269178.96 +/- 685717.66
Episode length: 1341.60 +/- 739.51
------------------------------------
| eval/              |             |
|    mean action     | -0.28876442 |
|    mean velocity x | -0.0578     |
|    mean velocity y | 0.617       |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 1.34e+03    |
|    mean_reward     | -1.27e+06   |
| time/              |             |
|    total_timesteps | 106000      |
------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 8.11e+03  |
|    ep_rew_mean     | -1.01e+07 |
| time/              |           |
|    fps             | 26        |
|    iterations      | 52        |
|    time_elapsed    | 4037      |
|    total_timesteps | 106496    |
----------------------------------
Eval num_timesteps=106500, episode_reward=-738495.96 +/- 479714.76
Episode length: 841.40 +/- 445.67
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.23380776 |
|    mean velocity x      | -0.232      |
|    mean velocity y      | 0.577       |
|    mean velocity z      | 18.9        |
|    mean_ep_length       | 841         |
|    mean_reward          | -7.38e+05   |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.004486085 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.000584    |
|    learning_rate        | 0.001       |
|    loss                 | 1.61e+08    |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00454    |
|    std                  | 0.978       |
|    value_loss           | 2.93e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=107000, episode_reward=-1555178.28 +/- 1099161.10
Episode length: 1627.40 +/- 1189.22
------------------------------------
| eval/              |             |
|    mean action     | -0.38806817 |
|    mean velocity x | 0.7         |
|    mean velocity y | 1.26        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 1.63e+03    |
|    mean_reward     | -1.56e+06   |
| time/              |             |
|    total_timesteps | 107000      |
------------------------------------
Eval num_timesteps=107500, episode_reward=-1241944.77 +/- 951948.77
Episode length: 1354.40 +/- 1041.73
------------------------------------
| eval/              |             |
|    mean action     | -0.30637586 |
|    mean velocity x | 1.63        |
|    mean velocity y | 2.58        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 1.35e+03    |
|    mean_reward     | -1.24e+06   |
| time/              |             |
|    total_timesteps | 107500      |
------------------------------------
Eval num_timesteps=108000, episode_reward=-1166693.77 +/- 587286.86
Episode length: 1253.00 +/- 654.97
------------------------------------
| eval/              |             |
|    mean action     | -0.28211945 |
|    mean velocity x | -0.679      |
|    mean velocity y | 0.246       |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 1.25e+03    |
|    mean_reward     | -1.17e+06   |
| time/              |             |
|    total_timesteps | 108000      |
------------------------------------
Eval num_timesteps=108500, episode_reward=-833507.22 +/- 384423.34
Episode length: 887.40 +/- 421.86
------------------------------------
| eval/              |             |
|    mean action     | -0.23171057 |
|    mean velocity x | -2.02       |
|    mean velocity y | -1.82       |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 887         |
|    mean_reward     | -8.34e+05   |
| time/              |             |
|    total_timesteps | 108500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.66e+03 |
|    ep_rew_mean     | -9.5e+06 |
| time/              |          |
|    fps             | 26       |
|    iterations      | 53       |
|    time_elapsed    | 4065     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-672632.23 +/- 334528.83
Episode length: 708.60 +/- 393.55
------------------------------------------
| eval/                   |              |
|    mean action          | -0.043807305 |
|    mean velocity x      | -3.12        |
|    mean velocity y      | -3.38        |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 709          |
|    mean_reward          | -6.73e+05    |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0027058744 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.000985     |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+08     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.0026      |
|    std                  | 0.978        |
|    value_loss           | 2.73e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=109500, episode_reward=-676056.43 +/- 702009.68
Episode length: 790.00 +/- 817.53
-----------------------------------
| eval/              |            |
|    mean action     | -0.1864383 |
|    mean velocity x | 1.52       |
|    mean velocity y | 1.74       |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 790        |
|    mean_reward     | -6.76e+05  |
| time/              |            |
|    total_timesteps | 109500     |
-----------------------------------
Eval num_timesteps=110000, episode_reward=-1156145.45 +/- 1168752.91
Episode length: 1305.00 +/- 1334.02
-----------------------------------
| eval/              |            |
|    mean action     | 0.12716348 |
|    mean velocity x | -7.3       |
|    mean velocity y | -8.17      |
|    mean velocity z | 23.9       |
|    mean_ep_length  | 1.3e+03    |
|    mean_reward     | -1.16e+06  |
| time/              |            |
|    total_timesteps | 110000     |
-----------------------------------
Eval num_timesteps=110500, episode_reward=-835413.77 +/- 406093.33
Episode length: 899.80 +/- 429.43
------------------------------------
| eval/              |             |
|    mean action     | -0.36723736 |
|    mean velocity x | 2.3         |
|    mean velocity y | 3.72        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 900         |
|    mean_reward     | -8.35e+05   |
| time/              |             |
|    total_timesteps | 110500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 7.34e+03  |
|    ep_rew_mean     | -9.06e+06 |
| time/              |           |
|    fps             | 27        |
|    iterations      | 54        |
|    time_elapsed    | 4086      |
|    total_timesteps | 110592    |
----------------------------------
Eval num_timesteps=111000, episode_reward=-415085.92 +/- 167747.05
Episode length: 477.60 +/- 214.96
------------------------------------------
| eval/                   |              |
|    mean action          | -0.040447038 |
|    mean velocity x      | -3.71        |
|    mean velocity y      | -3.54        |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 478          |
|    mean_reward          | -4.15e+05    |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.004512394  |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 3.99e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+08      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.977        |
|    value_loss           | 2.69e+08     |
------------------------------------------
New best mean reward!
Eval num_timesteps=111500, episode_reward=-535953.69 +/- 237057.90
Episode length: 615.20 +/- 273.14
------------------------------------
| eval/              |             |
|    mean action     | -0.32759953 |
|    mean velocity x | 1.33        |
|    mean velocity y | 2.44        |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 615         |
|    mean_reward     | -5.36e+05   |
| time/              |             |
|    total_timesteps | 111500      |
------------------------------------
Eval num_timesteps=112000, episode_reward=-877817.23 +/- 659931.93
Episode length: 1017.40 +/- 759.73
------------------------------------
| eval/              |             |
|    mean action     | -0.35764948 |
|    mean velocity x | 3.34        |
|    mean velocity y | 3.93        |
|    mean velocity z | 21          |
|    mean_ep_length  | 1.02e+03    |
|    mean_reward     | -8.78e+05   |
| time/              |             |
|    total_timesteps | 112000      |
------------------------------------
Eval num_timesteps=112500, episode_reward=-698433.59 +/- 573548.86
Episode length: 825.40 +/- 726.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.17101872 |
|    mean velocity x | -6.4       |
|    mean velocity y | -7.39      |
|    mean velocity z | 21.7       |
|    mean_ep_length  | 825        |
|    mean_reward     | -6.98e+05  |
| time/              |            |
|    total_timesteps | 112500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 7.03e+03  |
|    ep_rew_mean     | -8.65e+06 |
| time/              |           |
|    fps             | 27        |
|    iterations      | 55        |
|    time_elapsed    | 4103      |
|    total_timesteps | 112640    |
----------------------------------
Eval num_timesteps=113000, episode_reward=-563721.48 +/- 480180.71
Episode length: 655.20 +/- 518.85
------------------------------------------
| eval/                   |              |
|    mean action          | -0.09976953  |
|    mean velocity x      | -0.155       |
|    mean velocity y      | -0.727       |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 655          |
|    mean_reward          | -5.64e+05    |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0018629171 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.00071      |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+08     |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00148     |
|    std                  | 0.978        |
|    value_loss           | 3.02e+08     |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-838724.93 +/- 732089.71
Episode length: 990.00 +/- 873.87
------------------------------------
| eval/              |             |
|    mean action     | -0.25607306 |
|    mean velocity x | 0.623       |
|    mean velocity y | 1.4         |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 990         |
|    mean_reward     | -8.39e+05   |
| time/              |             |
|    total_timesteps | 113500      |
------------------------------------
Eval num_timesteps=114000, episode_reward=-483165.17 +/- 161639.48
Episode length: 518.40 +/- 179.65
-----------------------------------
| eval/              |            |
|    mean action     | -0.0846634 |
|    mean velocity x | -0.84      |
|    mean velocity y | -0.281     |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 518        |
|    mean_reward     | -4.83e+05  |
| time/              |            |
|    total_timesteps | 114000     |
-----------------------------------
Eval num_timesteps=114500, episode_reward=-761319.90 +/- 321226.51
Episode length: 884.00 +/- 352.60
------------------------------------
| eval/              |             |
|    mean action     | -0.20129082 |
|    mean velocity x | -1.21       |
|    mean velocity y | -0.385      |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 884         |
|    mean_reward     | -7.61e+05   |
| time/              |             |
|    total_timesteps | 114500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 6.64e+03  |
|    ep_rew_mean     | -8.16e+06 |
| time/              |           |
|    fps             | 27        |
|    iterations      | 56        |
|    time_elapsed    | 4120      |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=115000, episode_reward=-1145624.68 +/- 994749.86
Episode length: 1188.00 +/- 1001.25
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.21123783 |
|    mean velocity x      | 0.154       |
|    mean velocity y      | 0.909       |
|    mean velocity z      | 18.6        |
|    mean_ep_length       | 1.19e+03    |
|    mean_reward          | -1.15e+06   |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.003164215 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.0043      |
|    learning_rate        | 0.001       |
|    loss                 | 1.45e+08    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.98        |
|    value_loss           | 2.74e+08    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=-808258.13 +/- 647361.45
Episode length: 871.60 +/- 679.58
-----------------------------------
| eval/              |            |
|    mean action     | -0.1298693 |
|    mean velocity x | -2.54      |
|    mean velocity y | -2.26      |
|    mean velocity z | 22.8       |
|    mean_ep_length  | 872        |
|    mean_reward     | -8.08e+05  |
| time/              |            |
|    total_timesteps | 115500     |
-----------------------------------
Eval num_timesteps=116000, episode_reward=-536074.08 +/- 406299.50
Episode length: 555.60 +/- 426.05
------------------------------------
| eval/              |             |
|    mean action     | -0.35937756 |
|    mean velocity x | 1.07        |
|    mean velocity y | 1.9         |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 556         |
|    mean_reward     | -5.36e+05   |
| time/              |             |
|    total_timesteps | 116000      |
------------------------------------
Eval num_timesteps=116500, episode_reward=-798993.41 +/- 413673.41
Episode length: 908.20 +/- 494.66
------------------------------------
| eval/              |             |
|    mean action     | -0.25734144 |
|    mean velocity x | -1.22       |
|    mean velocity y | 0.101       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 908         |
|    mean_reward     | -7.99e+05   |
| time/              |             |
|    total_timesteps | 116500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 6.64e+03  |
|    ep_rew_mean     | -8.16e+06 |
| time/              |           |
|    fps             | 28        |
|    iterations      | 57        |
|    time_elapsed    | 4138      |
|    total_timesteps | 116736    |
----------------------------------
Eval num_timesteps=117000, episode_reward=-502480.20 +/- 304329.69
Episode length: 585.40 +/- 326.92
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27588618  |
|    mean velocity x      | -0.758       |
|    mean velocity y      | 0.0319       |
|    mean velocity z      | 18           |
|    mean_ep_length       | 585          |
|    mean_reward          | -5.02e+05    |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0037980238 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0            |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+08      |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00342     |
|    std                  | 0.982        |
|    value_loss           | 3.32e+08     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-969632.37 +/- 605361.51
Episode length: 1011.00 +/- 657.95
------------------------------------
| eval/              |             |
|    mean action     | -0.22154425 |
|    mean velocity x | 0.378       |
|    mean velocity y | 0.551       |
|    mean velocity z | 14.9        |
|    mean_ep_length  | 1.01e+03    |
|    mean_reward     | -9.7e+05    |
| time/              |             |
|    total_timesteps | 117500      |
------------------------------------
Eval num_timesteps=118000, episode_reward=-685903.89 +/- 580307.03
Episode length: 750.20 +/- 607.86
------------------------------------
| eval/              |             |
|    mean action     | -0.19983722 |
|    mean velocity x | -1.6        |
|    mean velocity y | -1.77       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 750         |
|    mean_reward     | -6.86e+05   |
| time/              |             |
|    total_timesteps | 118000      |
------------------------------------
Eval num_timesteps=118500, episode_reward=-312497.34 +/- 163400.69
Episode length: 366.40 +/- 180.69
-----------------------------------
| eval/              |            |
|    mean action     | 0.08462256 |
|    mean velocity x | -7.69      |
|    mean velocity y | -8.4       |
|    mean velocity z | 22.9       |
|    mean_ep_length  | 366        |
|    mean_reward     | -3.12e+05  |
| time/              |            |
|    total_timesteps | 118500     |
-----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 6.25e+03  |
|    ep_rew_mean     | -7.61e+06 |
| time/              |           |
|    fps             | 28        |
|    iterations      | 58        |
|    time_elapsed    | 4154      |
|    total_timesteps | 118784    |
----------------------------------
Eval num_timesteps=119000, episode_reward=-231120.99 +/- 177357.94
Episode length: 269.00 +/- 180.89
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.026008941 |
|    mean velocity x      | -2.5        |
|    mean velocity y      | -2.97       |
|    mean velocity z      | 16.1        |
|    mean_ep_length       | 269         |
|    mean_reward          | -2.31e+05   |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.004813465 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.000135    |
|    learning_rate        | 0.001       |
|    loss                 | 9.02e+07    |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00353    |
|    std                  | 0.979       |
|    value_loss           | 2.37e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=119500, episode_reward=-598816.61 +/- 496788.58
Episode length: 689.20 +/- 603.92
------------------------------------
| eval/              |             |
|    mean action     | -0.06362526 |
|    mean velocity x | -6.4        |
|    mean velocity y | -5.93       |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 689         |
|    mean_reward     | -5.99e+05   |
| time/              |             |
|    total_timesteps | 119500      |
------------------------------------
Eval num_timesteps=120000, episode_reward=-354899.07 +/- 264690.99
Episode length: 398.40 +/- 245.09
------------------------------------
| eval/              |             |
|    mean action     | -0.48418146 |
|    mean velocity x | 2.1         |
|    mean velocity y | 3.35        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 398         |
|    mean_reward     | -3.55e+05   |
| time/              |             |
|    total_timesteps | 120000      |
------------------------------------
Eval num_timesteps=120500, episode_reward=-349821.95 +/- 151336.35
Episode length: 400.20 +/- 180.72
------------------------------------
| eval/              |             |
|    mean action     | -0.12430681 |
|    mean velocity x | -3.49       |
|    mean velocity y | -3.86       |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 400         |
|    mean_reward     | -3.5e+05    |
| time/              |             |
|    total_timesteps | 120500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.94e+03  |
|    ep_rew_mean     | -7.23e+06 |
| time/              |           |
|    fps             | 28        |
|    iterations      | 59        |
|    time_elapsed    | 4166      |
|    total_timesteps | 120832    |
----------------------------------
Eval num_timesteps=121000, episode_reward=-565110.77 +/- 382366.22
Episode length: 596.20 +/- 381.99
------------------------------------------
| eval/                   |              |
|    mean action          | 0.10222891   |
|    mean velocity x      | -7.24        |
|    mean velocity y      | -7.64        |
|    mean velocity z      | 23.8         |
|    mean_ep_length       | 596          |
|    mean_reward          | -5.65e+05    |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0029660035 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.000109     |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+08     |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00232     |
|    std                  | 0.98         |
|    value_loss           | 2.72e+08     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-700801.29 +/- 636892.31
Episode length: 747.60 +/- 652.11
------------------------------------
| eval/              |             |
|    mean action     | -0.17895834 |
|    mean velocity x | -2.06       |
|    mean velocity y | -2.26       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 748         |
|    mean_reward     | -7.01e+05   |
| time/              |             |
|    total_timesteps | 121500      |
------------------------------------
Eval num_timesteps=122000, episode_reward=-601350.06 +/- 593745.43
Episode length: 633.40 +/- 587.01
------------------------------------
| eval/              |             |
|    mean action     | -0.26750737 |
|    mean velocity x | -0.0566     |
|    mean velocity y | 0.483       |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 633         |
|    mean_reward     | -6.01e+05   |
| time/              |             |
|    total_timesteps | 122000      |
------------------------------------
Eval num_timesteps=122500, episode_reward=-936328.22 +/- 683598.44
Episode length: 1021.60 +/- 677.97
-----------------------------------
| eval/              |            |
|    mean action     | -0.1690467 |
|    mean velocity x | -1.34      |
|    mean velocity y | -1.11      |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 1.02e+03   |
|    mean_reward     | -9.36e+05  |
| time/              |            |
|    total_timesteps | 122500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.11e+03  |
|    ep_rew_mean     | -6.19e+06 |
| time/              |           |
|    fps             | 29        |
|    iterations      | 60        |
|    time_elapsed    | 4183      |
|    total_timesteps | 122880    |
----------------------------------
Eval num_timesteps=123000, episode_reward=-503259.63 +/- 368305.50
Episode length: 585.20 +/- 423.55
------------------------------------------
| eval/                   |              |
|    mean action          | -0.09846554  |
|    mean velocity x      | -1.77        |
|    mean velocity y      | -1.33        |
|    mean velocity z      | 16.5         |
|    mean_ep_length       | 585          |
|    mean_reward          | -5.03e+05    |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0042410484 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.00126      |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+08     |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00351     |
|    std                  | 0.98         |
|    value_loss           | 2.66e+08     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=-272660.75 +/- 126348.44
Episode length: 371.00 +/- 180.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.13925977 |
|    mean velocity x | -6.75      |
|    mean velocity y | -7.06      |
|    mean velocity z | 23         |
|    mean_ep_length  | 371        |
|    mean_reward     | -2.73e+05  |
| time/              |            |
|    total_timesteps | 123500     |
-----------------------------------
Eval num_timesteps=124000, episode_reward=-442908.64 +/- 363658.58
Episode length: 513.80 +/- 459.81
------------------------------------
| eval/              |             |
|    mean action     | 0.014126335 |
|    mean velocity x | -3.67       |
|    mean velocity y | -3.94       |
|    mean velocity z | 20          |
|    mean_ep_length  | 514         |
|    mean_reward     | -4.43e+05   |
| time/              |             |
|    total_timesteps | 124000      |
------------------------------------
Eval num_timesteps=124500, episode_reward=-278780.23 +/- 163743.60
Episode length: 272.80 +/- 195.79
------------------------------------
| eval/              |             |
|    mean action     | -0.34541848 |
|    mean velocity x | -2.22       |
|    mean velocity y | -1.27       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 273         |
|    mean_reward     | -2.79e+05   |
| time/              |             |
|    total_timesteps | 124500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 4.3e+03   |
|    ep_rew_mean     | -5.19e+06 |
| time/              |           |
|    fps             | 29        |
|    iterations      | 61        |
|    time_elapsed    | 4196      |
|    total_timesteps | 124928    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-148869.94 +/- 85183.51
Episode length: 166.80 +/- 76.74
----------------------------------------
| eval/                   |            |
|    mean action          | 0.00627145 |
|    mean velocity x      | -3.68      |
|    mean velocity y      | -3.85      |
|    mean velocity z      | 17.6       |
|    mean_ep_length       | 167        |
|    mean_reward          | -1.49e+05  |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.00430373 |
|    clip_fraction        | 0.0104     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.2       |
|    explained_variance   | 0.000811   |
|    learning_rate        | 0.001      |
|    loss                 | 1.41e+08   |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0039    |
|    std                  | 0.984      |
|    value_loss           | 2.66e+08   |
----------------------------------------
New best mean reward!
Eval num_timesteps=125500, episode_reward=-313668.77 +/- 214965.52
Episode length: 356.60 +/- 221.90
-----------------------------------
| eval/              |            |
|    mean action     | 0.11276309 |
|    mean velocity x | -4.62      |
|    mean velocity y | -4.87      |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 357        |
|    mean_reward     | -3.14e+05  |
| time/              |            |
|    total_timesteps | 125500     |
-----------------------------------
Eval num_timesteps=126000, episode_reward=-157636.82 +/- 40708.68
Episode length: 135.80 +/- 47.15
-------------------------------------
| eval/              |              |
|    mean action     | -0.071702905 |
|    mean velocity x | -2.16        |
|    mean velocity y | -1.95        |
|    mean velocity z | 21.2         |
|    mean_ep_length  | 136          |
|    mean_reward     | -1.58e+05    |
| time/              |              |
|    total_timesteps | 126000       |
-------------------------------------
Eval num_timesteps=126500, episode_reward=-171075.89 +/- 93356.81
Episode length: 185.00 +/- 125.59
------------------------------------
| eval/              |             |
|    mean action     | -0.74754363 |
|    mean velocity x | 6.58        |
|    mean velocity y | 9.1         |
|    mean velocity z | 23.5        |
|    mean_ep_length  | 185         |
|    mean_reward     | -1.71e+05   |
| time/              |             |
|    total_timesteps | 126500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 4.04e+03  |
|    ep_rew_mean     | -4.88e+06 |
| time/              |           |
|    fps             | 30        |
|    iterations      | 62        |
|    time_elapsed    | 4204      |
|    total_timesteps | 126976    |
----------------------------------
Eval num_timesteps=127000, episode_reward=-232910.57 +/- 171947.77
Episode length: 264.20 +/- 191.87
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20610891  |
|    mean velocity x      | -0.649       |
|    mean velocity y      | 0.0431       |
|    mean velocity z      | 18.8         |
|    mean_ep_length       | 264          |
|    mean_reward          | -2.33e+05    |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0022794493 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.00166      |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+08     |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00216     |
|    std                  | 0.983        |
|    value_loss           | 2.71e+08     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-337550.30 +/- 206643.83
Episode length: 351.60 +/- 213.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.15125076 |
|    mean velocity x | -2.06      |
|    mean velocity y | -2.8       |
|    mean velocity z | 14.7       |
|    mean_ep_length  | 352        |
|    mean_reward     | -3.38e+05  |
| time/              |            |
|    total_timesteps | 127500     |
-----------------------------------
Eval num_timesteps=128000, episode_reward=-272285.41 +/- 199459.09
Episode length: 296.00 +/- 206.77
-------------------------------------
| eval/              |              |
|    mean action     | -0.044587877 |
|    mean velocity x | -2.94        |
|    mean velocity y | -3.61        |
|    mean velocity z | 18.7         |
|    mean_ep_length  | 296          |
|    mean_reward     | -2.72e+05    |
| time/              |              |
|    total_timesteps | 128000       |
-------------------------------------
Eval num_timesteps=128500, episode_reward=-311518.32 +/- 119925.00
Episode length: 350.20 +/- 155.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.40749344 |
|    mean velocity x | -7.08      |
|    mean velocity y | -8.91      |
|    mean velocity z | 22.7       |
|    mean_ep_length  | 350        |
|    mean_reward     | -3.12e+05  |
| time/              |            |
|    total_timesteps | 128500     |
-----------------------------------
Eval num_timesteps=129000, episode_reward=-393034.10 +/- 126482.03
Episode length: 441.40 +/- 121.71
----------------------------------
| eval/              |           |
|    mean action     | -0.188302 |
|    mean velocity x | 0.481     |
|    mean velocity y | 0.525     |
|    mean velocity z | 16.9      |
|    mean_ep_length  | 441       |
|    mean_reward     | -3.93e+05 |
| time/              |           |
|    total_timesteps | 129000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.65e+03  |
|    ep_rew_mean     | -4.39e+06 |
| time/              |           |
|    fps             | 30        |
|    iterations      | 63        |
|    time_elapsed    | 4216      |
|    total_timesteps | 129024    |
----------------------------------
Eval num_timesteps=129500, episode_reward=-371283.11 +/- 204605.84
Episode length: 446.80 +/- 252.49
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.12389759 |
|    mean velocity x      | -1.22       |
|    mean velocity y      | -0.775      |
|    mean velocity z      | 18.4        |
|    mean_ep_length       | 447         |
|    mean_reward          | -3.71e+05   |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.001317933 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.0021      |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+08    |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.986       |
|    value_loss           | 2.81e+08    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=-140162.20 +/- 102273.03
Episode length: 182.60 +/- 101.58
------------------------------------
| eval/              |             |
|    mean action     | -0.40416434 |
|    mean velocity x | 1.45        |
|    mean velocity y | 2           |
|    mean velocity z | 17          |
|    mean_ep_length  | 183         |
|    mean_reward     | -1.4e+05    |
| time/              |             |
|    total_timesteps | 130000      |
------------------------------------
New best mean reward!
Eval num_timesteps=130500, episode_reward=-169158.47 +/- 76205.36
Episode length: 206.00 +/- 107.62
------------------------------------
| eval/              |             |
|    mean action     | -0.23605952 |
|    mean velocity x | -0.36       |
|    mean velocity y | 0.0436      |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 206         |
|    mean_reward     | -1.69e+05   |
| time/              |             |
|    total_timesteps | 130500      |
------------------------------------
Eval num_timesteps=131000, episode_reward=-307069.70 +/- 171025.56
Episode length: 364.00 +/- 238.06
-----------------------------------
| eval/              |            |
|    mean action     | 0.08217491 |
|    mean velocity x | -3.83      |
|    mean velocity y | -4.14      |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 364        |
|    mean_reward     | -3.07e+05  |
| time/              |            |
|    total_timesteps | 131000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.52e+03  |
|    ep_rew_mean     | -4.21e+06 |
| time/              |           |
|    fps             | 31        |
|    iterations      | 64        |
|    time_elapsed    | 4226      |
|    total_timesteps | 131072    |
----------------------------------
Eval num_timesteps=131500, episode_reward=-153646.37 +/- 100117.43
Episode length: 176.60 +/- 89.66
------------------------------------------
| eval/                   |              |
|    mean action          | 0.10580963   |
|    mean velocity x      | -3.57        |
|    mean velocity y      | -4.01        |
|    mean velocity z      | 20.9         |
|    mean_ep_length       | 177          |
|    mean_reward          | -1.54e+05    |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0034151261 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.00282      |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+08     |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.983        |
|    value_loss           | 2.58e+08     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-259336.64 +/- 105579.29
Episode length: 285.40 +/- 132.65
------------------------------------
| eval/              |             |
|    mean action     | -0.61638206 |
|    mean velocity x | 4.56        |
|    mean velocity y | 7.19        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 285         |
|    mean_reward     | -2.59e+05   |
| time/              |             |
|    total_timesteps | 132000      |
------------------------------------
Eval num_timesteps=132500, episode_reward=-732785.81 +/- 707346.85
Episode length: 768.20 +/- 735.05
------------------------------------
| eval/              |             |
|    mean action     | -0.06816487 |
|    mean velocity x | -1.2        |
|    mean velocity y | -1.71       |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 768         |
|    mean_reward     | -7.33e+05   |
| time/              |             |
|    total_timesteps | 132500      |
------------------------------------
Eval num_timesteps=133000, episode_reward=-395611.83 +/- 541472.70
Episode length: 444.00 +/- 560.90
------------------------------------
| eval/              |             |
|    mean action     | -0.42117706 |
|    mean velocity x | 2.13        |
|    mean velocity y | 2.77        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 444         |
|    mean_reward     | -3.96e+05   |
| time/              |             |
|    total_timesteps | 133000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.39e+03  |
|    ep_rew_mean     | -4.05e+06 |
| time/              |           |
|    fps             | 31        |
|    iterations      | 65        |
|    time_elapsed    | 4238      |
|    total_timesteps | 133120    |
----------------------------------
Eval num_timesteps=133500, episode_reward=-579931.73 +/- 310507.61
Episode length: 627.60 +/- 330.58
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.00792685  |
|    mean velocity x      | -2.31       |
|    mean velocity y      | -2.42       |
|    mean velocity z      | 18          |
|    mean_ep_length       | 628         |
|    mean_reward          | -5.8e+05    |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.005136968 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 1.73e-06    |
|    learning_rate        | 0.001       |
|    loss                 | 1.42e+08    |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00516    |
|    std                  | 0.983       |
|    value_loss           | 2.58e+08    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=-269210.90 +/- 273172.73
Episode length: 291.80 +/- 299.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.21347329 |
|    mean velocity x | -4.68      |
|    mean velocity y | -5.41      |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 292        |
|    mean_reward     | -2.69e+05  |
| time/              |            |
|    total_timesteps | 134000     |
-----------------------------------
Eval num_timesteps=134500, episode_reward=-164125.09 +/- 69193.57
Episode length: 159.80 +/- 66.51
-----------------------------------
| eval/              |            |
|    mean action     | 0.21408576 |
|    mean velocity x | -2.37      |
|    mean velocity y | -2.53      |
|    mean velocity z | 13.7       |
|    mean_ep_length  | 160        |
|    mean_reward     | -1.64e+05  |
| time/              |            |
|    total_timesteps | 134500     |
-----------------------------------
Eval num_timesteps=135000, episode_reward=-256851.11 +/- 79954.25
Episode length: 274.80 +/- 75.36
-------------------------------------
| eval/              |              |
|    mean action     | 0.0051165754 |
|    mean velocity x | -0.844       |
|    mean velocity y | -1.07        |
|    mean velocity z | 16.1         |
|    mean_ep_length  | 275          |
|    mean_reward     | -2.57e+05    |
| time/              |              |
|    total_timesteps | 135000       |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 3.28e+03 |
|    ep_rew_mean     | -3.9e+06 |
| time/              |          |
|    fps             | 31       |
|    iterations      | 66       |
|    time_elapsed    | 4249     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-248872.09 +/- 270904.40
Episode length: 287.20 +/- 307.51
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.16557422 |
|    mean velocity x      | 0.248       |
|    mean velocity y      | 0.424       |
|    mean velocity z      | 20.5        |
|    mean_ep_length       | 287         |
|    mean_reward          | -2.49e+05   |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.002603321 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.000293    |
|    learning_rate        | 0.001       |
|    loss                 | 1.54e+08    |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00191    |
|    std                  | 0.984       |
|    value_loss           | 2.37e+08    |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=-239026.20 +/- 150000.49
Episode length: 289.00 +/- 165.87
-------------------------------------
| eval/              |              |
|    mean action     | -0.025592716 |
|    mean velocity x | -0.76        |
|    mean velocity y | 0.142        |
|    mean velocity z | 17.9         |
|    mean_ep_length  | 289          |
|    mean_reward     | -2.39e+05    |
| time/              |              |
|    total_timesteps | 136000       |
-------------------------------------
Eval num_timesteps=136500, episode_reward=-258171.52 +/- 244808.29
Episode length: 279.00 +/- 266.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.40466127 |
|    mean velocity x | -5.75      |
|    mean velocity y | -7.16      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 279        |
|    mean_reward     | -2.58e+05  |
| time/              |            |
|    total_timesteps | 136500     |
-----------------------------------
Eval num_timesteps=137000, episode_reward=-209937.55 +/- 146698.58
Episode length: 242.40 +/- 164.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.32546705 |
|    mean velocity x | -4.1       |
|    mean velocity y | -5.04      |
|    mean velocity z | 20         |
|    mean_ep_length  | 242        |
|    mean_reward     | -2.1e+05   |
| time/              |            |
|    total_timesteps | 137000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 3.11e+03  |
|    ep_rew_mean     | -3.69e+06 |
| time/              |           |
|    fps             | 32        |
|    iterations      | 67        |
|    time_elapsed    | 4259      |
|    total_timesteps | 137216    |
----------------------------------
Eval num_timesteps=137500, episode_reward=-281079.09 +/- 111509.42
Episode length: 329.20 +/- 120.62
------------------------------------------
| eval/                   |              |
|    mean action          | -0.035185445 |
|    mean velocity x      | 1.02         |
|    mean velocity y      | 1.41         |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 329          |
|    mean_reward          | -2.81e+05    |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0050121024 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | -0.000456    |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+08     |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00434     |
|    std                  | 0.982        |
|    value_loss           | 3.13e+08     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-68103.21 +/- 63803.52
Episode length: 105.60 +/- 74.69
----------------------------------
| eval/              |           |
|    mean action     | 0.0328564 |
|    mean velocity x | -0.138    |
|    mean velocity y | 0.391     |
|    mean velocity z | 18.6      |
|    mean_ep_length  | 106       |
|    mean_reward     | -6.81e+04 |
| time/              |           |
|    total_timesteps | 138000    |
----------------------------------
New best mean reward!
Eval num_timesteps=138500, episode_reward=-203198.49 +/- 97567.60
Episode length: 262.40 +/- 115.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.17969103 |
|    mean velocity x | -3.48      |
|    mean velocity y | -3.23      |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 262        |
|    mean_reward     | -2.03e+05  |
| time/              |            |
|    total_timesteps | 138500     |
-----------------------------------
Eval num_timesteps=139000, episode_reward=-187151.65 +/- 94435.12
Episode length: 198.00 +/- 106.44
------------------------------------
| eval/              |             |
|    mean action     | -0.28592896 |
|    mean velocity x | 1.76        |
|    mean velocity y | 2.61        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 198         |
|    mean_reward     | -1.87e+05   |
| time/              |             |
|    total_timesteps | 139000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.73e+03  |
|    ep_rew_mean     | -3.22e+06 |
| time/              |           |
|    fps             | 32        |
|    iterations      | 68        |
|    time_elapsed    | 4268      |
|    total_timesteps | 139264    |
----------------------------------
Eval num_timesteps=139500, episode_reward=-220284.53 +/- 149024.35
Episode length: 250.80 +/- 143.96
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.45516527  |
|    mean velocity x      | -4.19       |
|    mean velocity y      | -5.74       |
|    mean velocity z      | 18.4        |
|    mean_ep_length       | 251         |
|    mean_reward          | -2.2e+05    |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.005692371 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.000201    |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+08    |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00544    |
|    std                  | 0.979       |
|    value_loss           | 2.38e+08    |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=-180955.27 +/- 164149.53
Episode length: 188.40 +/- 161.36
------------------------------------
| eval/              |             |
|    mean action     | -0.28485397 |
|    mean velocity x | 1.28        |
|    mean velocity y | 1.87        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 188         |
|    mean_reward     | -1.81e+05   |
| time/              |             |
|    total_timesteps | 140000      |
------------------------------------
Eval num_timesteps=140500, episode_reward=-236890.58 +/- 167420.95
Episode length: 264.80 +/- 180.44
------------------------------------
| eval/              |             |
|    mean action     | -0.44939664 |
|    mean velocity x | 2.43        |
|    mean velocity y | 3.55        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 265         |
|    mean_reward     | -2.37e+05   |
| time/              |             |
|    total_timesteps | 140500      |
------------------------------------
Eval num_timesteps=141000, episode_reward=-196743.29 +/- 85996.51
Episode length: 199.00 +/- 92.85
----------------------------------
| eval/              |           |
|    mean action     | 0.4008242 |
|    mean velocity x | -5.62     |
|    mean velocity y | -6.16     |
|    mean velocity z | 20.3      |
|    mean_ep_length  | 199       |
|    mean_reward     | -1.97e+05 |
| time/              |           |
|    total_timesteps | 141000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.52e+03  |
|    ep_rew_mean     | -2.97e+06 |
| time/              |           |
|    fps             | 33        |
|    iterations      | 69        |
|    time_elapsed    | 4277      |
|    total_timesteps | 141312    |
----------------------------------
Eval num_timesteps=141500, episode_reward=-207313.10 +/- 79130.85
Episode length: 231.00 +/- 75.54
------------------------------------------
| eval/                   |              |
|    mean action          | -0.06811262  |
|    mean velocity x      | 0.472        |
|    mean velocity y      | 0.227        |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 231          |
|    mean_reward          | -2.07e+05    |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0058711963 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.00181      |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+08     |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00572     |
|    std                  | 0.976        |
|    value_loss           | 2.53e+08     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-184639.52 +/- 103113.30
Episode length: 204.40 +/- 146.18
----------------------------------
| eval/              |           |
|    mean action     | 0.5697494 |
|    mean velocity x | -6.08     |
|    mean velocity y | -7.78     |
|    mean velocity z | 22.6      |
|    mean_ep_length  | 204       |
|    mean_reward     | -1.85e+05 |
| time/              |           |
|    total_timesteps | 142000    |
----------------------------------
Eval num_timesteps=142500, episode_reward=-149275.11 +/- 105529.64
Episode length: 172.20 +/- 115.06
------------------------------------
| eval/              |             |
|    mean action     | -0.17796728 |
|    mean velocity x | 0.593       |
|    mean velocity y | 0.999       |
|    mean velocity z | 14          |
|    mean_ep_length  | 172         |
|    mean_reward     | -1.49e+05   |
| time/              |             |
|    total_timesteps | 142500      |
------------------------------------
Eval num_timesteps=143000, episode_reward=-157562.01 +/- 91642.52
Episode length: 190.40 +/- 99.01
------------------------------------
| eval/              |             |
|    mean action     | -0.34286758 |
|    mean velocity x | 4.14        |
|    mean velocity y | 4.67        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 190         |
|    mean_reward     | -1.58e+05   |
| time/              |             |
|    total_timesteps | 143000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.42e+03  |
|    ep_rew_mean     | -2.84e+06 |
| time/              |           |
|    fps             | 33        |
|    iterations      | 70        |
|    time_elapsed    | 4286      |
|    total_timesteps | 143360    |
----------------------------------
Eval num_timesteps=143500, episode_reward=-137313.05 +/- 87622.94
Episode length: 204.00 +/- 121.73
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08331929  |
|    mean velocity x      | -0.0192      |
|    mean velocity y      | 0.236        |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 204          |
|    mean_reward          | -1.37e+05    |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0046338304 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.00595      |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+08     |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00408     |
|    std                  | 0.977        |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-162084.48 +/- 65124.95
Episode length: 191.00 +/- 82.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.19285348 |
|    mean velocity x | -2.54      |
|    mean velocity y | -2.68      |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 191        |
|    mean_reward     | -1.62e+05  |
| time/              |            |
|    total_timesteps | 144000     |
-----------------------------------
Eval num_timesteps=144500, episode_reward=-168328.43 +/- 67725.85
Episode length: 194.00 +/- 89.14
------------------------------------
| eval/              |             |
|    mean action     | -0.19036773 |
|    mean velocity x | 0.822       |
|    mean velocity y | 1.83        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 194         |
|    mean_reward     | -1.68e+05   |
| time/              |             |
|    total_timesteps | 144500      |
------------------------------------
Eval num_timesteps=145000, episode_reward=-216782.27 +/- 94694.89
Episode length: 253.20 +/- 115.78
------------------------------------
| eval/              |             |
|    mean action     | 0.051017355 |
|    mean velocity x | -1.41       |
|    mean velocity y | -1.17       |
|    mean velocity z | 17          |
|    mean_ep_length  | 253         |
|    mean_reward     | -2.17e+05   |
| time/              |             |
|    total_timesteps | 145000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.3e+03   |
|    ep_rew_mean     | -2.69e+06 |
| time/              |           |
|    fps             | 33        |
|    iterations      | 71        |
|    time_elapsed    | 4294      |
|    total_timesteps | 145408    |
----------------------------------
Eval num_timesteps=145500, episode_reward=-149225.31 +/- 62603.83
Episode length: 173.20 +/- 56.15
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.5268755   |
|    mean velocity x      | -6.14       |
|    mean velocity y      | -8.11       |
|    mean velocity z      | 20.9        |
|    mean_ep_length       | 173         |
|    mean_reward          | -1.49e+05   |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.004959435 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.00995     |
|    learning_rate        | 0.001       |
|    loss                 | 1.03e+08    |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00381    |
|    std                  | 0.978       |
|    value_loss           | 2.59e+08    |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=-203311.72 +/- 137255.07
Episode length: 243.00 +/- 119.93
-------------------------------------
| eval/              |              |
|    mean action     | -0.036007557 |
|    mean velocity x | -0.215       |
|    mean velocity y | -0.168       |
|    mean velocity z | 18.1         |
|    mean_ep_length  | 243          |
|    mean_reward     | -2.03e+05    |
| time/              |              |
|    total_timesteps | 146000       |
-------------------------------------
Eval num_timesteps=146500, episode_reward=-189036.64 +/- 109133.71
Episode length: 216.40 +/- 133.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.19013834 |
|    mean velocity x | -1.82      |
|    mean velocity y | -2.69      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 216        |
|    mean_reward     | -1.89e+05  |
| time/              |            |
|    total_timesteps | 146500     |
-----------------------------------
Eval num_timesteps=147000, episode_reward=-269314.53 +/- 195762.64
Episode length: 293.60 +/- 231.27
------------------------------------
| eval/              |             |
|    mean action     | -0.38279572 |
|    mean velocity x | 2.04        |
|    mean velocity y | 3.15        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 294         |
|    mean_reward     | -2.69e+05   |
| time/              |             |
|    total_timesteps | 147000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 2.17e+03  |
|    ep_rew_mean     | -2.53e+06 |
| time/              |           |
|    fps             | 34        |
|    iterations      | 72        |
|    time_elapsed    | 4303      |
|    total_timesteps | 147456    |
----------------------------------
Eval num_timesteps=147500, episode_reward=-139702.20 +/- 89067.36
Episode length: 168.20 +/- 97.90
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13465133  |
|    mean velocity x      | -0.281       |
|    mean velocity y      | 0.39         |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 168          |
|    mean_reward          | -1.4e+05     |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0049142893 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.00445      |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+08     |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00451     |
|    std                  | 0.978        |
|    value_loss           | 2.59e+08     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-112100.67 +/- 32781.69
Episode length: 113.40 +/- 48.97
------------------------------------
| eval/              |             |
|    mean action     | -0.16976684 |
|    mean velocity x | -0.0349     |
|    mean velocity y | -0.0956     |
|    mean velocity z | 21          |
|    mean_ep_length  | 113         |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 148000      |
------------------------------------
Eval num_timesteps=148500, episode_reward=-208017.72 +/- 121127.84
Episode length: 238.20 +/- 136.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.11380281 |
|    mean velocity x | -1.2       |
|    mean velocity y | -1.8       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 238        |
|    mean_reward     | -2.08e+05  |
| time/              |            |
|    total_timesteps | 148500     |
-----------------------------------
Eval num_timesteps=149000, episode_reward=-240648.13 +/- 110079.55
Episode length: 287.40 +/- 125.56
----------------------------------
| eval/              |           |
|    mean action     | 0.5360338 |
|    mean velocity x | -7.25     |
|    mean velocity y | -8.2      |
|    mean velocity z | 20.2      |
|    mean_ep_length  | 287       |
|    mean_reward     | -2.41e+05 |
| time/              |           |
|    total_timesteps | 149000    |
----------------------------------
Eval num_timesteps=149500, episode_reward=-306288.79 +/- 143859.36
Episode length: 360.00 +/- 159.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.11056368 |
|    mean velocity x | -1.16      |
|    mean velocity y | -2.46      |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 360        |
|    mean_reward     | -3.06e+05  |
| time/              |            |
|    total_timesteps | 149500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.99e+03  |
|    ep_rew_mean     | -2.33e+06 |
| time/              |           |
|    fps             | 34        |
|    iterations      | 73        |
|    time_elapsed    | 4314      |
|    total_timesteps | 149504    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-108277.68 +/- 98300.60
Episode length: 147.00 +/- 85.10
------------------------------------------
| eval/                   |              |
|    mean action          | 0.27880332   |
|    mean velocity x      | -3.04        |
|    mean velocity y      | -4.2         |
|    mean velocity z      | 19.9         |
|    mean_ep_length       | 147          |
|    mean_reward          | -1.08e+05    |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0031539206 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.00989      |
|    learning_rate        | 0.001        |
|    loss                 | 1.89e+08     |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00206     |
|    std                  | 0.979        |
|    value_loss           | 3.16e+08     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-86418.13 +/- 36557.08
Episode length: 102.40 +/- 22.76
------------------------------------
| eval/              |             |
|    mean action     | 0.049328603 |
|    mean velocity x | -2.36       |
|    mean velocity y | -3.24       |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 102         |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 150500      |
------------------------------------
Eval num_timesteps=151000, episode_reward=-133873.98 +/- 41373.48
Episode length: 163.60 +/- 55.46
----------------------------------
| eval/              |           |
|    mean action     | 0.1125134 |
|    mean velocity x | -2.68     |
|    mean velocity y | -3.25     |
|    mean velocity z | 19.7      |
|    mean_ep_length  | 164       |
|    mean_reward     | -1.34e+05 |
| time/              |           |
|    total_timesteps | 151000    |
----------------------------------
Eval num_timesteps=151500, episode_reward=-213658.76 +/- 113196.40
Episode length: 265.00 +/- 128.14
----------------------------------
| eval/              |           |
|    mean action     | 0.5228009 |
|    mean velocity x | -4.87     |
|    mean velocity y | -6.53     |
|    mean velocity z | 19.7      |
|    mean_ep_length  | 265       |
|    mean_reward     | -2.14e+05 |
| time/              |           |
|    total_timesteps | 151500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.89e+03 |
|    ep_rew_mean     | -2.2e+06 |
| time/              |          |
|    fps             | 35       |
|    iterations      | 74       |
|    time_elapsed    | 4322     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-233453.91 +/- 227401.82
Episode length: 287.40 +/- 270.38
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.30749995  |
|    mean velocity x      | -3.53       |
|    mean velocity y      | -4.42       |
|    mean velocity z      | 18.2        |
|    mean_ep_length       | 287         |
|    mean_reward          | -2.33e+05   |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.004080726 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.0196      |
|    learning_rate        | 0.001       |
|    loss                 | 1.16e+08    |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.00252    |
|    std                  | 0.977       |
|    value_loss           | 2.68e+08    |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=-119982.16 +/- 46829.47
Episode length: 137.40 +/- 33.86
------------------------------------
| eval/              |             |
|    mean action     | 0.024727242 |
|    mean velocity x | -2.33       |
|    mean velocity y | -1.39       |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 137         |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 152500      |
------------------------------------
Eval num_timesteps=153000, episode_reward=-176267.74 +/- 131027.25
Episode length: 181.60 +/- 150.75
-------------------------------------
| eval/              |              |
|    mean action     | -0.029185718 |
|    mean velocity x | 0.47         |
|    mean velocity y | -0.389       |
|    mean velocity z | 15.4         |
|    mean_ep_length  | 182          |
|    mean_reward     | -1.76e+05    |
| time/              |              |
|    total_timesteps | 153000       |
-------------------------------------
Eval num_timesteps=153500, episode_reward=-235913.31 +/- 204935.11
Episode length: 265.60 +/- 265.14
-----------------------------------
| eval/              |            |
|    mean action     | 0.22294454 |
|    mean velocity x | -2.53      |
|    mean velocity y | -3.76      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 266        |
|    mean_reward     | -2.36e+05  |
| time/              |            |
|    total_timesteps | 153500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.74e+03  |
|    ep_rew_mean     | -2.03e+06 |
| time/              |           |
|    fps             | 35        |
|    iterations      | 75        |
|    time_elapsed    | 4331      |
|    total_timesteps | 153600    |
----------------------------------
Eval num_timesteps=154000, episode_reward=-162732.08 +/- 96774.63
Episode length: 211.20 +/- 106.17
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.37239805  |
|    mean velocity x      | -2.47       |
|    mean velocity y      | -3.47       |
|    mean velocity z      | 16.6        |
|    mean_ep_length       | 211         |
|    mean_reward          | -1.63e+05   |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.004354962 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.0101      |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+08    |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.974       |
|    value_loss           | 2.35e+08    |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=-234093.95 +/- 148050.82
Episode length: 243.60 +/- 189.80
----------------------------------
| eval/              |           |
|    mean action     | 0.4139761 |
|    mean velocity x | -5.69     |
|    mean velocity y | -6.73     |
|    mean velocity z | 20.3      |
|    mean_ep_length  | 244       |
|    mean_reward     | -2.34e+05 |
| time/              |           |
|    total_timesteps | 154500    |
----------------------------------
Eval num_timesteps=155000, episode_reward=-284525.55 +/- 83299.77
Episode length: 350.00 +/- 85.60
-----------------------------------
| eval/              |            |
|    mean action     | -0.2515193 |
|    mean velocity x | 0.555      |
|    mean velocity y | 2.28       |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 350        |
|    mean_reward     | -2.85e+05  |
| time/              |            |
|    total_timesteps | 155000     |
-----------------------------------
Eval num_timesteps=155500, episode_reward=-225159.76 +/- 125203.96
Episode length: 251.60 +/- 158.39
-------------------------------------
| eval/              |              |
|    mean action     | -0.107271194 |
|    mean velocity x | -0.0469      |
|    mean velocity y | -0.378       |
|    mean velocity z | 16.9         |
|    mean_ep_length  | 252          |
|    mean_reward     | -2.25e+05    |
| time/              |              |
|    total_timesteps | 155500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.65e+03  |
|    ep_rew_mean     | -1.92e+06 |
| time/              |           |
|    fps             | 35        |
|    iterations      | 76        |
|    time_elapsed    | 4340      |
|    total_timesteps | 155648    |
----------------------------------
Eval num_timesteps=156000, episode_reward=-117804.56 +/- 49049.14
Episode length: 106.40 +/- 43.89
------------------------------------------
| eval/                   |              |
|    mean action          | 0.079525836  |
|    mean velocity x      | -2.32        |
|    mean velocity y      | -1.91        |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 106          |
|    mean_reward          | -1.18e+05    |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0029725607 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.0264       |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+08     |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.975        |
|    value_loss           | 2.39e+08     |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-143690.76 +/- 60281.64
Episode length: 181.60 +/- 70.88
------------------------------------
| eval/              |             |
|    mean action     | -0.13633698 |
|    mean velocity x | -0.18       |
|    mean velocity y | -0.199      |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 182         |
|    mean_reward     | -1.44e+05   |
| time/              |             |
|    total_timesteps | 156500      |
------------------------------------
Eval num_timesteps=157000, episode_reward=-159417.17 +/- 72248.65
Episode length: 172.40 +/- 81.27
------------------------------------
| eval/              |             |
|    mean action     | -0.07926357 |
|    mean velocity x | -0.223      |
|    mean velocity y | 0.397       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 172         |
|    mean_reward     | -1.59e+05   |
| time/              |             |
|    total_timesteps | 157000      |
------------------------------------
Eval num_timesteps=157500, episode_reward=-110901.88 +/- 29021.96
Episode length: 111.80 +/- 41.93
------------------------------------
| eval/              |             |
|    mean action     | -0.06459378 |
|    mean velocity x | 0.115       |
|    mean velocity y | -0.149      |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 112         |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 157500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 651      |
|    ep_rew_mean     | -6.5e+05 |
| time/              |          |
|    fps             | 36       |
|    iterations      | 77       |
|    time_elapsed    | 4348     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-203784.26 +/- 99593.80
Episode length: 217.20 +/- 108.02
------------------------------------------
| eval/                   |              |
|    mean action          | 0.0037586899 |
|    mean velocity x      | 0.546        |
|    mean velocity y      | 0.154        |
|    mean velocity z      | 17.4         |
|    mean_ep_length       | 217          |
|    mean_reward          | -2.04e+05    |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0036082934 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.0359       |
|    learning_rate        | 0.001        |
|    loss                 | 9.24e+07     |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00345     |
|    std                  | 0.974        |
|    value_loss           | 2.68e+08     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-129103.30 +/- 51089.93
Episode length: 146.60 +/- 59.89
-------------------------------------
| eval/              |              |
|    mean action     | -0.033936232 |
|    mean velocity x | -0.056       |
|    mean velocity y | 0.149        |
|    mean velocity z | 15.8         |
|    mean_ep_length  | 147          |
|    mean_reward     | -1.29e+05    |
| time/              |              |
|    total_timesteps | 158500       |
-------------------------------------
Eval num_timesteps=159000, episode_reward=-117998.81 +/- 80349.23
Episode length: 137.60 +/- 103.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.09019113 |
|    mean velocity x | -1.18      |
|    mean velocity y | -1.19      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 138        |
|    mean_reward     | -1.18e+05  |
| time/              |            |
|    total_timesteps | 159000     |
-----------------------------------
Eval num_timesteps=159500, episode_reward=-198353.72 +/- 127005.54
Episode length: 237.80 +/- 150.58
------------------------------------
| eval/              |             |
|    mean action     | -0.01903415 |
|    mean velocity x | -0.669      |
|    mean velocity y | -1.04       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 238         |
|    mean_reward     | -1.98e+05   |
| time/              |             |
|    total_timesteps | 159500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 410       |
|    ep_rew_mean     | -3.98e+05 |
| time/              |           |
|    fps             | 36        |
|    iterations      | 78        |
|    time_elapsed    | 4357      |
|    total_timesteps | 159744    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-115404.51 +/- 34959.09
Episode length: 123.60 +/- 51.12
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18851574  |
|    mean velocity x      | 1.23         |
|    mean velocity y      | 2.92         |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 124          |
|    mean_reward          | -1.15e+05    |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0058632586 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0406       |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+08     |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00614     |
|    std                  | 0.972        |
|    value_loss           | 2.79e+08     |
------------------------------------------
Eval num_timesteps=160500, episode_reward=-71542.46 +/- 43637.63
Episode length: 87.60 +/- 32.60
------------------------------------
| eval/              |             |
|    mean action     | -0.23201846 |
|    mean velocity x | 1.68        |
|    mean velocity y | 2.18        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 87.6        |
|    mean_reward     | -7.15e+04   |
| time/              |             |
|    total_timesteps | 160500      |
------------------------------------
Eval num_timesteps=161000, episode_reward=-98965.56 +/- 51334.18
Episode length: 125.40 +/- 77.42
------------------------------------
| eval/              |             |
|    mean action     | 0.073524766 |
|    mean velocity x | 0.637       |
|    mean velocity y | -1.4        |
|    mean velocity z | 16.6        |
|    mean_ep_length  | 125         |
|    mean_reward     | -9.9e+04    |
| time/              |             |
|    total_timesteps | 161000      |
------------------------------------
Eval num_timesteps=161500, episode_reward=-127032.08 +/- 24732.15
Episode length: 147.20 +/- 51.39
-------------------------------------
| eval/              |              |
|    mean action     | -0.029682312 |
|    mean velocity x | -0.35        |
|    mean velocity y | -0.164       |
|    mean velocity z | 17.6         |
|    mean_ep_length  | 147          |
|    mean_reward     | -1.27e+05    |
| time/              |              |
|    total_timesteps | 161500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 343       |
|    ep_rew_mean     | -3.32e+05 |
| time/              |           |
|    fps             | 37        |
|    iterations      | 79        |
|    time_elapsed    | 4365      |
|    total_timesteps | 161792    |
----------------------------------
Eval num_timesteps=162000, episode_reward=-104296.00 +/- 38114.41
Episode length: 111.00 +/- 50.68
----------------------------------------
| eval/                   |            |
|    mean action          | 0.24739522 |
|    mean velocity x      | -3.63      |
|    mean velocity y      | -3.54      |
|    mean velocity z      | 16.1       |
|    mean_ep_length       | 111        |
|    mean_reward          | -1.04e+05  |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.00463835 |
|    clip_fraction        | 0.0196     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.0444     |
|    learning_rate        | 0.001      |
|    loss                 | 1.63e+08   |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.00389   |
|    std                  | 0.974      |
|    value_loss           | 2.77e+08   |
----------------------------------------
Eval num_timesteps=162500, episode_reward=-107574.57 +/- 27386.21
Episode length: 132.80 +/- 37.81
-----------------------------------
| eval/              |            |
|    mean action     | -0.3997979 |
|    mean velocity x | 2.54       |
|    mean velocity y | 3.4        |
|    mean velocity z | 16.1       |
|    mean_ep_length  | 133        |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 162500     |
-----------------------------------
Eval num_timesteps=163000, episode_reward=-181160.09 +/- 207547.35
Episode length: 197.40 +/- 256.27
-----------------------------------
| eval/              |            |
|    mean action     | 0.12540045 |
|    mean velocity x | -0.418     |
|    mean velocity y | -1.39      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 197        |
|    mean_reward     | -1.81e+05  |
| time/              |            |
|    total_timesteps | 163000     |
-----------------------------------
Eval num_timesteps=163500, episode_reward=-87769.56 +/- 53744.75
Episode length: 96.40 +/- 51.64
------------------------------------
| eval/              |             |
|    mean action     | -0.21469702 |
|    mean velocity x | 1.3         |
|    mean velocity y | 1.06        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 96.4        |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 163500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | -2.8e+05 |
| time/              |          |
|    fps             | 37       |
|    iterations      | 80       |
|    time_elapsed    | 4372     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-167385.97 +/- 90440.33
Episode length: 215.20 +/- 133.85
------------------------------------------
| eval/                   |              |
|    mean action          | 0.61580336   |
|    mean velocity x      | -4.98        |
|    mean velocity y      | -6.84        |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 215          |
|    mean_reward          | -1.67e+05    |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0024820643 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0295       |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+08     |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.973        |
|    value_loss           | 2.78e+08     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-118767.88 +/- 22292.15
Episode length: 126.20 +/- 51.37
------------------------------------
| eval/              |             |
|    mean action     | -0.16403137 |
|    mean velocity x | -0.606      |
|    mean velocity y | 1.26        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 126         |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 164500      |
------------------------------------
Eval num_timesteps=165000, episode_reward=-167181.57 +/- 118002.79
Episode length: 218.20 +/- 148.36
------------------------------------
| eval/              |             |
|    mean action     | 0.071006775 |
|    mean velocity x | -1.75       |
|    mean velocity y | -1.31       |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 218         |
|    mean_reward     | -1.67e+05   |
| time/              |             |
|    total_timesteps | 165000      |
------------------------------------
Eval num_timesteps=165500, episode_reward=-218323.54 +/- 64794.35
Episode length: 237.80 +/- 89.87
-------------------------------------
| eval/              |              |
|    mean action     | -0.076568075 |
|    mean velocity x | 0.127        |
|    mean velocity y | -0.183       |
|    mean velocity z | 18.2         |
|    mean_ep_length  | 238          |
|    mean_reward     | -2.18e+05    |
| time/              |              |
|    total_timesteps | 165500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 264       |
|    ep_rew_mean     | -2.57e+05 |
| time/              |           |
|    fps             | 37        |
|    iterations      | 81        |
|    time_elapsed    | 4382      |
|    total_timesteps | 165888    |
----------------------------------
Eval num_timesteps=166000, episode_reward=-85598.43 +/- 32174.20
Episode length: 85.80 +/- 11.77
------------------------------------------
| eval/                   |              |
|    mean action          | -0.7061367   |
|    mean velocity x      | 6.08         |
|    mean velocity y      | 8.23         |
|    mean velocity z      | 20.4         |
|    mean_ep_length       | 85.8         |
|    mean_reward          | -8.56e+04    |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0045052143 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0154       |
|    learning_rate        | 0.001        |
|    loss                 | 1e+08        |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.0033      |
|    std                  | 0.974        |
|    value_loss           | 2.51e+08     |
------------------------------------------
Eval num_timesteps=166500, episode_reward=-93141.26 +/- 8914.60
Episode length: 90.40 +/- 11.11
------------------------------------
| eval/              |             |
|    mean action     | 0.061439317 |
|    mean velocity x | -1.77       |
|    mean velocity y | -1.62       |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 90.4        |
|    mean_reward     | -9.31e+04   |
| time/              |             |
|    total_timesteps | 166500      |
------------------------------------
Eval num_timesteps=167000, episode_reward=-66206.21 +/- 40044.85
Episode length: 89.60 +/- 25.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.33229834 |
|    mean velocity x | -3.29      |
|    mean velocity y | -4.75      |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 89.6       |
|    mean_reward     | -6.62e+04  |
| time/              |            |
|    total_timesteps | 167000     |
-----------------------------------
New best mean reward!
Eval num_timesteps=167500, episode_reward=-138754.77 +/- 64071.01
Episode length: 136.00 +/- 80.82
----------------------------------
| eval/              |           |
|    mean action     | 0.5465973 |
|    mean velocity x | -4.56     |
|    mean velocity y | -6.01     |
|    mean velocity z | 15.8      |
|    mean_ep_length  | 136       |
|    mean_reward     | -1.39e+05 |
| time/              |           |
|    total_timesteps | 167500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 217       |
|    ep_rew_mean     | -2.12e+05 |
| time/              |           |
|    fps             | 38        |
|    iterations      | 82        |
|    time_elapsed    | 4389      |
|    total_timesteps | 167936    |
----------------------------------
Eval num_timesteps=168000, episode_reward=-127013.24 +/- 70782.50
Episode length: 138.20 +/- 85.57
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.0656701  |
|    mean velocity x      | 0.551       |
|    mean velocity y      | 0.261       |
|    mean velocity z      | 17.3        |
|    mean_ep_length       | 138         |
|    mean_reward          | -1.27e+05   |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.004183501 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.0378      |
|    learning_rate        | 0.001       |
|    loss                 | 7.94e+07    |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00337    |
|    std                  | 0.972       |
|    value_loss           | 2.24e+08    |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=-99318.42 +/- 51031.01
Episode length: 103.40 +/- 45.41
----------------------------------
| eval/              |           |
|    mean action     | 0.3986801 |
|    mean velocity x | -2.18     |
|    mean velocity y | -4.28     |
|    mean velocity z | 16.9      |
|    mean_ep_length  | 103       |
|    mean_reward     | -9.93e+04 |
| time/              |           |
|    total_timesteps | 168500    |
----------------------------------
Eval num_timesteps=169000, episode_reward=-91584.99 +/- 34140.52
Episode length: 114.00 +/- 41.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.25034928 |
|    mean velocity x | -1.92      |
|    mean velocity y | -2.89      |
|    mean velocity z | 15.4       |
|    mean_ep_length  | 114        |
|    mean_reward     | -9.16e+04  |
| time/              |            |
|    total_timesteps | 169000     |
-----------------------------------
Eval num_timesteps=169500, episode_reward=-121269.63 +/- 63472.74
Episode length: 157.60 +/- 84.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.08119144 |
|    mean velocity x | -1.43      |
|    mean velocity y | -1.22      |
|    mean velocity z | 15.7       |
|    mean_ep_length  | 158        |
|    mean_reward     | -1.21e+05  |
| time/              |            |
|    total_timesteps | 169500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 196       |
|    ep_rew_mean     | -1.87e+05 |
| time/              |           |
|    fps             | 38        |
|    iterations      | 83        |
|    time_elapsed    | 4397      |
|    total_timesteps | 169984    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-119228.64 +/- 69576.16
Episode length: 139.80 +/- 71.69
------------------------------------------
| eval/                   |              |
|    mean action          | 0.26912504   |
|    mean velocity x      | -3.14        |
|    mean velocity y      | -3.6         |
|    mean velocity z      | 17.3         |
|    mean_ep_length       | 140          |
|    mean_reward          | -1.19e+05    |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0032865922 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.047        |
|    learning_rate        | 0.001        |
|    loss                 | 9.12e+07     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.0027      |
|    std                  | 0.973        |
|    value_loss           | 1.92e+08     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-121073.18 +/- 63571.71
Episode length: 141.80 +/- 97.27
-----------------------------------
| eval/              |            |
|    mean action     | 0.20173906 |
|    mean velocity x | -3.17      |
|    mean velocity y | -3.18      |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 142        |
|    mean_reward     | -1.21e+05  |
| time/              |            |
|    total_timesteps | 170500     |
-----------------------------------
Eval num_timesteps=171000, episode_reward=-138631.48 +/- 34047.34
Episode length: 119.60 +/- 51.30
-----------------------------------
| eval/              |            |
|    mean action     | 0.19501996 |
|    mean velocity x | -1.69      |
|    mean velocity y | -2.53      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 120        |
|    mean_reward     | -1.39e+05  |
| time/              |            |
|    total_timesteps | 171000     |
-----------------------------------
Eval num_timesteps=171500, episode_reward=-139632.53 +/- 29401.67
Episode length: 143.40 +/- 54.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.33538318 |
|    mean velocity x | -2.02      |
|    mean velocity y | -3.61      |
|    mean velocity z | 14         |
|    mean_ep_length  | 143        |
|    mean_reward     | -1.4e+05   |
| time/              |            |
|    total_timesteps | 171500     |
-----------------------------------
Eval num_timesteps=172000, episode_reward=-89815.24 +/- 44814.41
Episode length: 116.00 +/- 58.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.24758667 |
|    mean velocity x | -2.34      |
|    mean velocity y | -3.28      |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 116        |
|    mean_reward     | -8.98e+04  |
| time/              |            |
|    total_timesteps | 172000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 192       |
|    ep_rew_mean     | -1.82e+05 |
| time/              |           |
|    fps             | 39        |
|    iterations      | 84        |
|    time_elapsed    | 4405      |
|    total_timesteps | 172032    |
----------------------------------
Eval num_timesteps=172500, episode_reward=-143564.12 +/- 81274.61
Episode length: 172.80 +/- 88.20
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.006682526  |
|    mean velocity x      | -0.838        |
|    mean velocity y      | -1.02         |
|    mean velocity z      | 19            |
|    mean_ep_length       | 173           |
|    mean_reward          | -1.44e+05     |
| time/                   |               |
|    total_timesteps      | 172500        |
| train/                  |               |
|    approx_kl            | 0.00055724673 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.17         |
|    explained_variance   | 0.0298        |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+08      |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000773     |
|    std                  | 0.973         |
|    value_loss           | 2.48e+08      |
-------------------------------------------
Eval num_timesteps=173000, episode_reward=-133294.30 +/- 106042.64
Episode length: 142.80 +/- 133.13
-----------------------------------
| eval/              |            |
|    mean action     | 0.21177082 |
|    mean velocity x | -2.18      |
|    mean velocity y | -3.45      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 143        |
|    mean_reward     | -1.33e+05  |
| time/              |            |
|    total_timesteps | 173000     |
-----------------------------------
Eval num_timesteps=173500, episode_reward=-112481.62 +/- 24993.32
Episode length: 126.60 +/- 41.33
------------------------------------
| eval/              |             |
|    mean action     | -0.16213383 |
|    mean velocity x | -0.757      |
|    mean velocity y | 0.454       |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 127         |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 173500      |
------------------------------------
Eval num_timesteps=174000, episode_reward=-108457.26 +/- 44288.27
Episode length: 130.40 +/- 56.26
------------------------------------
| eval/              |             |
|    mean action     | 0.020829666 |
|    mean velocity x | -2.27       |
|    mean velocity y | -2.35       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 130         |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 174000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 184       |
|    ep_rew_mean     | -1.76e+05 |
| time/              |           |
|    fps             | 39        |
|    iterations      | 85        |
|    time_elapsed    | 4413      |
|    total_timesteps | 174080    |
----------------------------------
Eval num_timesteps=174500, episode_reward=-113098.19 +/- 103141.16
Episode length: 137.80 +/- 112.30
------------------------------------------
| eval/                   |              |
|    mean action          | -0.00924905  |
|    mean velocity x      | -0.787       |
|    mean velocity y      | -0.0244      |
|    mean velocity z      | 17.5         |
|    mean_ep_length       | 138          |
|    mean_reward          | -1.13e+05    |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0028599156 |
|    clip_fraction        | 0.00474      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0223       |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+08     |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00233     |
|    std                  | 0.972        |
|    value_loss           | 2.79e+08     |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-104924.95 +/- 39784.51
Episode length: 119.40 +/- 41.35
------------------------------------
| eval/              |             |
|    mean action     | -0.37315392 |
|    mean velocity x | 0.255       |
|    mean velocity y | 1.81        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 119         |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 175000      |
------------------------------------
Eval num_timesteps=175500, episode_reward=-75137.91 +/- 35992.64
Episode length: 93.00 +/- 10.92
------------------------------------
| eval/              |             |
|    mean action     | -0.40075353 |
|    mean velocity x | 2.85        |
|    mean velocity y | 3.96        |
|    mean velocity z | 17.2        |
|    mean_ep_length  | 93          |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 175500      |
------------------------------------
Eval num_timesteps=176000, episode_reward=-129554.93 +/- 33851.88
Episode length: 154.60 +/- 68.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.33455762 |
|    mean velocity x | -4.29      |
|    mean velocity y | -4.77      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 155        |
|    mean_reward     | -1.3e+05   |
| time/              |            |
|    total_timesteps | 176000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 173       |
|    ep_rew_mean     | -1.66e+05 |
| time/              |           |
|    fps             | 39        |
|    iterations      | 86        |
|    time_elapsed    | 4421      |
|    total_timesteps | 176128    |
----------------------------------
Eval num_timesteps=176500, episode_reward=-84771.65 +/- 17072.87
Episode length: 111.60 +/- 40.47
------------------------------------------
| eval/                   |              |
|    mean action          | 0.015683498  |
|    mean velocity x      | -1.7         |
|    mean velocity y      | -0.708       |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 112          |
|    mean_reward          | -8.48e+04    |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0036259606 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0376       |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+08     |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.972        |
|    value_loss           | 2.5e+08      |
------------------------------------------
Eval num_timesteps=177000, episode_reward=-96340.42 +/- 25181.43
Episode length: 124.20 +/- 36.65
------------------------------------
| eval/              |             |
|    mean action     | -0.06121609 |
|    mean velocity x | 0.112       |
|    mean velocity y | 0.359       |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 124         |
|    mean_reward     | -9.63e+04   |
| time/              |             |
|    total_timesteps | 177000      |
------------------------------------
Eval num_timesteps=177500, episode_reward=-72211.47 +/- 38858.57
Episode length: 92.20 +/- 31.68
------------------------------------
| eval/              |             |
|    mean action     | 0.064118266 |
|    mean velocity x | -0.342      |
|    mean velocity y | -0.485      |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 92.2        |
|    mean_reward     | -7.22e+04   |
| time/              |             |
|    total_timesteps | 177500      |
------------------------------------
Eval num_timesteps=178000, episode_reward=-110126.07 +/- 47671.46
Episode length: 136.20 +/- 51.47
-----------------------------------
| eval/              |            |
|    mean action     | -0.5173827 |
|    mean velocity x | 2.09       |
|    mean velocity y | 3.3        |
|    mean velocity z | 20.9       |
|    mean_ep_length  | 136        |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 178000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 172       |
|    ep_rew_mean     | -1.64e+05 |
| time/              |           |
|    fps             | 40        |
|    iterations      | 87        |
|    time_elapsed    | 4429      |
|    total_timesteps | 178176    |
----------------------------------
Eval num_timesteps=178500, episode_reward=-144564.67 +/- 52046.38
Episode length: 141.00 +/- 60.16
------------------------------------------
| eval/                   |              |
|    mean action          | 0.32512146   |
|    mean velocity x      | -2.63        |
|    mean velocity y      | -3.27        |
|    mean velocity z      | 20.7         |
|    mean_ep_length       | 141          |
|    mean_reward          | -1.45e+05    |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0031261782 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0435       |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+08      |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00252     |
|    std                  | 0.973        |
|    value_loss           | 2.75e+08     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-68893.24 +/- 21221.32
Episode length: 88.60 +/- 11.96
-----------------------------------
| eval/              |            |
|    mean action     | 0.61581385 |
|    mean velocity x | -2.4       |
|    mean velocity y | -4.25      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 88.6       |
|    mean_reward     | -6.89e+04  |
| time/              |            |
|    total_timesteps | 179000     |
-----------------------------------
Eval num_timesteps=179500, episode_reward=-118507.30 +/- 56036.25
Episode length: 120.20 +/- 51.23
-------------------------------------
| eval/              |              |
|    mean action     | -0.055789627 |
|    mean velocity x | -0.532       |
|    mean velocity y | -0.305       |
|    mean velocity z | 16.4         |
|    mean_ep_length  | 120          |
|    mean_reward     | -1.19e+05    |
| time/              |              |
|    total_timesteps | 179500       |
-------------------------------------
Eval num_timesteps=180000, episode_reward=-109139.70 +/- 36601.64
Episode length: 160.00 +/- 60.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.03771127 |
|    mean velocity x | 0.217      |
|    mean velocity y | 0.72       |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 160        |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 180000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 158       |
|    ep_rew_mean     | -1.51e+05 |
| time/              |           |
|    fps             | 40        |
|    iterations      | 88        |
|    time_elapsed    | 4437      |
|    total_timesteps | 180224    |
----------------------------------
Eval num_timesteps=180500, episode_reward=-132320.66 +/- 52025.06
Episode length: 128.80 +/- 67.37
------------------------------------------
| eval/                   |              |
|    mean action          | 0.34341848   |
|    mean velocity x      | -2.36        |
|    mean velocity y      | -3.19        |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 129          |
|    mean_reward          | -1.32e+05    |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0031094234 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.0312       |
|    learning_rate        | 0.001        |
|    loss                 | 1.52e+08     |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00308     |
|    std                  | 0.971        |
|    value_loss           | 2.74e+08     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-129021.66 +/- 50823.33
Episode length: 141.40 +/- 64.07
-------------------------------------
| eval/              |              |
|    mean action     | -0.073826954 |
|    mean velocity x | 0.155        |
|    mean velocity y | 0.834        |
|    mean velocity z | 18.1         |
|    mean_ep_length  | 141          |
|    mean_reward     | -1.29e+05    |
| time/              |              |
|    total_timesteps | 181000       |
-------------------------------------
Eval num_timesteps=181500, episode_reward=-69751.73 +/- 44275.82
Episode length: 93.60 +/- 18.21
------------------------------------
| eval/              |             |
|    mean action     | -0.16683643 |
|    mean velocity x | 0.728       |
|    mean velocity y | 1.13        |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 93.6        |
|    mean_reward     | -6.98e+04   |
| time/              |             |
|    total_timesteps | 181500      |
------------------------------------
Eval num_timesteps=182000, episode_reward=-94913.03 +/- 34151.36
Episode length: 103.40 +/- 17.95
-----------------------------------
| eval/              |            |
|    mean action     | 0.53180164 |
|    mean velocity x | -3.04      |
|    mean velocity y | -5.14      |
|    mean velocity z | 15.3       |
|    mean_ep_length  | 103        |
|    mean_reward     | -9.49e+04  |
| time/              |            |
|    total_timesteps | 182000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 149       |
|    ep_rew_mean     | -1.43e+05 |
| time/              |           |
|    fps             | 41        |
|    iterations      | 89        |
|    time_elapsed    | 4444      |
|    total_timesteps | 182272    |
----------------------------------
Eval num_timesteps=182500, episode_reward=-103710.37 +/- 24221.63
Episode length: 97.00 +/- 7.48
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.08133317 |
|    mean velocity x      | -0.115      |
|    mean velocity y      | -0.562      |
|    mean velocity z      | 18.3        |
|    mean_ep_length       | 97          |
|    mean_reward          | -1.04e+05   |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.004091978 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.0468      |
|    learning_rate        | 0.001       |
|    loss                 | 9.74e+07    |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00419    |
|    std                  | 0.968       |
|    value_loss           | 2.31e+08    |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=-76700.04 +/- 40696.25
Episode length: 88.40 +/- 17.95
----------------------------------
| eval/              |           |
|    mean action     | 0.2331777 |
|    mean velocity x | -0.0666   |
|    mean velocity y | 0.79      |
|    mean velocity z | 17.9      |
|    mean_ep_length  | 88.4      |
|    mean_reward     | -7.67e+04 |
| time/              |           |
|    total_timesteps | 183000    |
----------------------------------
Eval num_timesteps=183500, episode_reward=-86578.58 +/- 14671.09
Episode length: 99.40 +/- 17.78
----------------------------------
| eval/              |           |
|    mean action     | 0.564954  |
|    mean velocity x | -2.61     |
|    mean velocity y | -4.71     |
|    mean velocity z | 17.2      |
|    mean_ep_length  | 99.4      |
|    mean_reward     | -8.66e+04 |
| time/              |           |
|    total_timesteps | 183500    |
----------------------------------
Eval num_timesteps=184000, episode_reward=-131716.88 +/- 66101.61
Episode length: 165.40 +/- 105.26
-----------------------------------
| eval/              |            |
|    mean action     | 0.34203547 |
|    mean velocity x | -1.95      |
|    mean velocity y | -2.37      |
|    mean velocity z | 16.9       |
|    mean_ep_length  | 165        |
|    mean_reward     | -1.32e+05  |
| time/              |            |
|    total_timesteps | 184000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 150       |
|    ep_rew_mean     | -1.46e+05 |
| time/              |           |
|    fps             | 41        |
|    iterations      | 90        |
|    time_elapsed    | 4452      |
|    total_timesteps | 184320    |
----------------------------------
Eval num_timesteps=184500, episode_reward=-106343.71 +/- 25945.60
Episode length: 93.60 +/- 7.23
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07040797  |
|    mean velocity x      | -0.751       |
|    mean velocity y      | -0.68        |
|    mean velocity z      | 20           |
|    mean_ep_length       | 93.6         |
|    mean_reward          | -1.06e+05    |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0016337552 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.0493       |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+08     |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.968        |
|    value_loss           | 2.57e+08     |
------------------------------------------
Eval num_timesteps=185000, episode_reward=-91096.14 +/- 12380.35
Episode length: 97.00 +/- 6.84
-----------------------------------
| eval/              |            |
|    mean action     | 0.33264634 |
|    mean velocity x | -0.446     |
|    mean velocity y | -2.32      |
|    mean velocity z | 14.8       |
|    mean_ep_length  | 97         |
|    mean_reward     | -9.11e+04  |
| time/              |            |
|    total_timesteps | 185000     |
-----------------------------------
Eval num_timesteps=185500, episode_reward=-161720.27 +/- 60738.29
Episode length: 185.00 +/- 89.49
-----------------------------------
| eval/              |            |
|    mean action     | 0.50557107 |
|    mean velocity x | -3.3       |
|    mean velocity y | -3.69      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 185        |
|    mean_reward     | -1.62e+05  |
| time/              |            |
|    total_timesteps | 185500     |
-----------------------------------
Eval num_timesteps=186000, episode_reward=-91538.33 +/- 45346.34
Episode length: 80.40 +/- 21.55
-------------------------------------
| eval/              |              |
|    mean action     | -0.006472656 |
|    mean velocity x | 0.398        |
|    mean velocity y | 0.597        |
|    mean velocity z | 17.2         |
|    mean_ep_length  | 80.4         |
|    mean_reward     | -9.15e+04    |
| time/              |              |
|    total_timesteps | 186000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 139       |
|    ep_rew_mean     | -1.38e+05 |
| time/              |           |
|    fps             | 41        |
|    iterations      | 91        |
|    time_elapsed    | 4459      |
|    total_timesteps | 186368    |
----------------------------------
Eval num_timesteps=186500, episode_reward=-107783.49 +/- 38443.25
Episode length: 127.80 +/- 39.88
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.27880394  |
|    mean velocity x      | -1.43       |
|    mean velocity y      | -2.45       |
|    mean velocity z      | 21.2        |
|    mean_ep_length       | 128         |
|    mean_reward          | -1.08e+05   |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.002397343 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.034       |
|    learning_rate        | 0.001       |
|    loss                 | 1.35e+08    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00251    |
|    std                  | 0.968       |
|    value_loss           | 2.6e+08     |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=-76108.98 +/- 17258.78
Episode length: 84.20 +/- 3.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.18694018 |
|    mean velocity x | -0.542     |
|    mean velocity y | -1.06      |
|    mean velocity z | 13         |
|    mean_ep_length  | 84.2       |
|    mean_reward     | -7.61e+04  |
| time/              |            |
|    total_timesteps | 187000     |
-----------------------------------
Eval num_timesteps=187500, episode_reward=-90333.99 +/- 21170.45
Episode length: 91.80 +/- 10.91
----------------------------------
| eval/              |           |
|    mean action     | 0.2755238 |
|    mean velocity x | -1.71     |
|    mean velocity y | -2.51     |
|    mean velocity z | 16.8      |
|    mean_ep_length  | 91.8      |
|    mean_reward     | -9.03e+04 |
| time/              |           |
|    total_timesteps | 187500    |
----------------------------------
Eval num_timesteps=188000, episode_reward=-100530.86 +/- 33827.24
Episode length: 87.80 +/- 8.08
-----------------------------------
| eval/              |            |
|    mean action     | 0.38176513 |
|    mean velocity x | -2.82      |
|    mean velocity y | -3.96      |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 87.8       |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 188000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 136       |
|    ep_rew_mean     | -1.33e+05 |
| time/              |           |
|    fps             | 42        |
|    iterations      | 92        |
|    time_elapsed    | 4467      |
|    total_timesteps | 188416    |
----------------------------------
Eval num_timesteps=188500, episode_reward=-99693.54 +/- 26237.91
Episode length: 100.00 +/- 20.95
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.30998015 |
|    mean velocity x      | 0.376       |
|    mean velocity y      | 1.4         |
|    mean velocity z      | 20.1        |
|    mean_ep_length       | 100         |
|    mean_reward          | -9.97e+04   |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.002853434 |
|    clip_fraction        | 0.00562     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.0374      |
|    learning_rate        | 0.001       |
|    loss                 | 2.2e+08     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.0017     |
|    std                  | 0.968       |
|    value_loss           | 2.6e+08     |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=-108516.52 +/- 91975.30
Episode length: 138.80 +/- 92.93
------------------------------------
| eval/              |             |
|    mean action     | -0.21471347 |
|    mean velocity x | 0.843       |
|    mean velocity y | 1.34        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 139         |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 189000      |
------------------------------------
Eval num_timesteps=189500, episode_reward=-51064.13 +/- 48661.75
Episode length: 58.60 +/- 36.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.19927524 |
|    mean velocity x | 0.0329     |
|    mean velocity y | -1.41      |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -5.11e+04  |
| time/              |            |
|    total_timesteps | 189500     |
-----------------------------------
New best mean reward!
Eval num_timesteps=190000, episode_reward=-109409.16 +/- 19275.76
Episode length: 96.60 +/- 8.50
------------------------------------
| eval/              |             |
|    mean action     | -0.08380682 |
|    mean velocity x | 1.29        |
|    mean velocity y | 1.77        |
|    mean velocity z | 15.2        |
|    mean_ep_length  | 96.6        |
|    mean_reward     | -1.09e+05   |
| time/              |             |
|    total_timesteps | 190000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 136       |
|    ep_rew_mean     | -1.32e+05 |
| time/              |           |
|    fps             | 42        |
|    iterations      | 93        |
|    time_elapsed    | 4474      |
|    total_timesteps | 190464    |
----------------------------------
Eval num_timesteps=190500, episode_reward=-119425.23 +/- 84731.94
Episode length: 131.20 +/- 96.44
------------------------------------------
| eval/                   |              |
|    mean action          | 0.202595     |
|    mean velocity x      | -1.76        |
|    mean velocity y      | -1.89        |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 131          |
|    mean_reward          | -1.19e+05    |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0026743414 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.0399       |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+08     |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00298     |
|    std                  | 0.963        |
|    value_loss           | 2.37e+08     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-68868.96 +/- 31651.88
Episode length: 86.60 +/- 15.97
------------------------------------
| eval/              |             |
|    mean action     | -0.33985063 |
|    mean velocity x | 2.1         |
|    mean velocity y | 2.18        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 86.6        |
|    mean_reward     | -6.89e+04   |
| time/              |             |
|    total_timesteps | 191000      |
------------------------------------
Eval num_timesteps=191500, episode_reward=-78683.99 +/- 30706.52
Episode length: 111.40 +/- 28.49
------------------------------------
| eval/              |             |
|    mean action     | -0.57142407 |
|    mean velocity x | 2.24        |
|    mean velocity y | 4.9         |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 111         |
|    mean_reward     | -7.87e+04   |
| time/              |             |
|    total_timesteps | 191500      |
------------------------------------
Eval num_timesteps=192000, episode_reward=-91682.33 +/- 50707.67
Episode length: 77.00 +/- 26.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.21384077 |
|    mean velocity x | -1.4       |
|    mean velocity y | -2.3       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 77         |
|    mean_reward     | -9.17e+04  |
| time/              |            |
|    total_timesteps | 192000     |
-----------------------------------
Eval num_timesteps=192500, episode_reward=-120810.66 +/- 5221.31
Episode length: 93.00 +/- 3.41
-------------------------------------
| eval/              |              |
|    mean action     | -0.021142846 |
|    mean velocity x | -2.19        |
|    mean velocity y | -2.17        |
|    mean velocity z | 16.6         |
|    mean_ep_length  | 93           |
|    mean_reward     | -1.21e+05    |
| time/              |              |
|    total_timesteps | 192500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 134       |
|    ep_rew_mean     | -1.27e+05 |
| time/              |           |
|    fps             | 42        |
|    iterations      | 94        |
|    time_elapsed    | 4482      |
|    total_timesteps | 192512    |
----------------------------------
Eval num_timesteps=193000, episode_reward=-110580.87 +/- 40169.56
Episode length: 110.20 +/- 51.81
------------------------------------------
| eval/                   |              |
|    mean action          | -0.053672317 |
|    mean velocity x      | -0.171       |
|    mean velocity y      | 0.254        |
|    mean velocity z      | 17.5         |
|    mean_ep_length       | 110          |
|    mean_reward          | -1.11e+05    |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0019900962 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.0434       |
|    learning_rate        | 0.001        |
|    loss                 | 7.63e+07     |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00103     |
|    std                  | 0.963        |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-128806.14 +/- 31974.99
Episode length: 109.20 +/- 37.09
-----------------------------------
| eval/              |            |
|    mean action     | -0.7915632 |
|    mean velocity x | 4.61       |
|    mean velocity y | 6.72       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 109        |
|    mean_reward     | -1.29e+05  |
| time/              |            |
|    total_timesteps | 193500     |
-----------------------------------
Eval num_timesteps=194000, episode_reward=-96120.07 +/- 36341.54
Episode length: 91.40 +/- 6.18
------------------------------------
| eval/              |             |
|    mean action     | -0.31921205 |
|    mean velocity x | 0.155       |
|    mean velocity y | 1.1         |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 91.4        |
|    mean_reward     | -9.61e+04   |
| time/              |             |
|    total_timesteps | 194000      |
------------------------------------
Eval num_timesteps=194500, episode_reward=-124720.24 +/- 48970.37
Episode length: 133.00 +/- 48.76
------------------------------------
| eval/              |             |
|    mean action     | -0.32340822 |
|    mean velocity x | 0.472       |
|    mean velocity y | 1.81        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 133         |
|    mean_reward     | -1.25e+05   |
| time/              |             |
|    total_timesteps | 194500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 137       |
|    ep_rew_mean     | -1.29e+05 |
| time/              |           |
|    fps             | 43        |
|    iterations      | 95        |
|    time_elapsed    | 4490      |
|    total_timesteps | 194560    |
----------------------------------
Eval num_timesteps=195000, episode_reward=-84145.24 +/- 26858.93
Episode length: 87.80 +/- 5.74
----------------------------------------
| eval/                   |            |
|    mean action          | 0.30617133 |
|    mean velocity x      | -1.27      |
|    mean velocity y      | -2.24      |
|    mean velocity z      | 19.3       |
|    mean_ep_length       | 87.8       |
|    mean_reward          | -8.41e+04  |
| time/                   |            |
|    total_timesteps      | 195000     |
| train/                  |            |
|    approx_kl            | 0.00287265 |
|    clip_fraction        | 0.00366    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.14      |
|    explained_variance   | 0.0454     |
|    learning_rate        | 0.001      |
|    loss                 | 1.5e+08    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.00354   |
|    std                  | 0.961      |
|    value_loss           | 2.29e+08   |
----------------------------------------
Eval num_timesteps=195500, episode_reward=-65240.66 +/- 51936.13
Episode length: 69.20 +/- 23.23
----------------------------------
| eval/              |           |
|    mean action     | 0.4556548 |
|    mean velocity x | -3.01     |
|    mean velocity y | -4.16     |
|    mean velocity z | 16.4      |
|    mean_ep_length  | 69.2      |
|    mean_reward     | -6.52e+04 |
| time/              |           |
|    total_timesteps | 195500    |
----------------------------------
Eval num_timesteps=196000, episode_reward=-107869.77 +/- 26624.46
Episode length: 111.00 +/- 41.12
-----------------------------------
| eval/              |            |
|    mean action     | 0.11471791 |
|    mean velocity x | -0.904     |
|    mean velocity y | -0.929     |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 111        |
|    mean_reward     | -1.08e+05  |
| time/              |            |
|    total_timesteps | 196000     |
-----------------------------------
Eval num_timesteps=196500, episode_reward=-111522.61 +/- 17705.82
Episode length: 91.20 +/- 3.71
------------------------------------
| eval/              |             |
|    mean action     | -0.25790685 |
|    mean velocity x | 1.74        |
|    mean velocity y | 1.37        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 91.2        |
|    mean_reward     | -1.12e+05   |
| time/              |             |
|    total_timesteps | 196500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | -1.3e+05 |
| time/              |          |
|    fps             | 43       |
|    iterations      | 96       |
|    time_elapsed    | 4499     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-95832.96 +/- 20626.53
Episode length: 93.40 +/- 5.31
------------------------------------------
| eval/                   |              |
|    mean action          | -0.19307038  |
|    mean velocity x      | -0.143       |
|    mean velocity y      | 0.198        |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 93.4         |
|    mean_reward          | -9.58e+04    |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0012241321 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.047        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00121     |
|    std                  | 0.96         |
|    value_loss           | 2.48e+08     |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-103435.70 +/- 24156.34
Episode length: 85.00 +/- 2.83
----------------------------------
| eval/              |           |
|    mean action     | 0.1290987 |
|    mean velocity x | -0.853    |
|    mean velocity y | -0.307    |
|    mean velocity z | 18.4      |
|    mean_ep_length  | 85        |
|    mean_reward     | -1.03e+05 |
| time/              |           |
|    total_timesteps | 197500    |
----------------------------------
Eval num_timesteps=198000, episode_reward=-74664.74 +/- 44297.87
Episode length: 86.80 +/- 14.46
------------------------------------
| eval/              |             |
|    mean action     | -0.17113858 |
|    mean velocity x | 0.567       |
|    mean velocity y | 0.652       |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 86.8        |
|    mean_reward     | -7.47e+04   |
| time/              |             |
|    total_timesteps | 198000      |
------------------------------------
Eval num_timesteps=198500, episode_reward=-76473.16 +/- 50615.15
Episode length: 90.20 +/- 48.48
------------------------------------
| eval/              |             |
|    mean action     | -0.07671692 |
|    mean velocity x | -0.483      |
|    mean velocity y | -0.0213     |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 90.2        |
|    mean_reward     | -7.65e+04   |
| time/              |             |
|    total_timesteps | 198500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 127       |
|    ep_rew_mean     | -1.23e+05 |
| time/              |           |
|    fps             | 44        |
|    iterations      | 97        |
|    time_elapsed    | 4506      |
|    total_timesteps | 198656    |
----------------------------------
Eval num_timesteps=199000, episode_reward=-95644.46 +/- 25457.20
Episode length: 88.60 +/- 9.89
------------------------------------------
| eval/                   |              |
|    mean action          | -0.16326566  |
|    mean velocity x      | 1.32         |
|    mean velocity y      | 1.79         |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 88.6         |
|    mean_reward          | -9.56e+04    |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0011246807 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.0537       |
|    learning_rate        | 0.001        |
|    loss                 | 2.05e+08     |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.962        |
|    value_loss           | 2.96e+08     |
------------------------------------------
Eval num_timesteps=199500, episode_reward=-101557.94 +/- 24171.65
Episode length: 86.20 +/- 2.23
-----------------------------------
| eval/              |            |
|    mean action     | 0.06545064 |
|    mean velocity x | -1.49      |
|    mean velocity y | -0.879     |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 86.2       |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 199500     |
-----------------------------------
Eval num_timesteps=200000, episode_reward=-108314.91 +/- 48372.87
Episode length: 110.60 +/- 39.41
----------------------------------
| eval/              |           |
|    mean action     | 0.4203673 |
|    mean velocity x | -1.97     |
|    mean velocity y | -3.15     |
|    mean velocity z | 14.2      |
|    mean_ep_length  | 111       |
|    mean_reward     | -1.08e+05 |
| time/              |           |
|    total_timesteps | 200000    |
----------------------------------
Eval num_timesteps=200500, episode_reward=-76552.11 +/- 66206.55
Episode length: 101.60 +/- 61.76
------------------------------------
| eval/              |             |
|    mean action     | -0.19291434 |
|    mean velocity x | 1.43        |
|    mean velocity y | 1.59        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 102         |
|    mean_reward     | -7.66e+04   |
| time/              |             |
|    total_timesteps | 200500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 127       |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    fps             | 44        |
|    iterations      | 98        |
|    time_elapsed    | 4513      |
|    total_timesteps | 200704    |
----------------------------------
Eval num_timesteps=201000, episode_reward=-104599.82 +/- 18408.94
Episode length: 85.80 +/- 3.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46461663  |
|    mean velocity x      | 1.99         |
|    mean velocity y      | 3.54         |
|    mean velocity z      | 22.4         |
|    mean_ep_length       | 85.8         |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0018349551 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.0535       |
|    learning_rate        | 0.001        |
|    loss                 | 8.78e+07     |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.962        |
|    value_loss           | 2.35e+08     |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-68445.78 +/- 30777.16
Episode length: 95.40 +/- 31.46
----------------------------------
| eval/              |           |
|    mean action     | 0.2726384 |
|    mean velocity x | -1.5      |
|    mean velocity y | -1.89     |
|    mean velocity z | 18.2      |
|    mean_ep_length  | 95.4      |
|    mean_reward     | -6.84e+04 |
| time/              |           |
|    total_timesteps | 201500    |
----------------------------------
Eval num_timesteps=202000, episode_reward=-86569.98 +/- 32930.57
Episode length: 87.20 +/- 10.65
------------------------------------
| eval/              |             |
|    mean action     | -0.16585812 |
|    mean velocity x | 1.21        |
|    mean velocity y | 2.33        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 87.2        |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 202000      |
------------------------------------
Eval num_timesteps=202500, episode_reward=-101697.46 +/- 52727.15
Episode length: 117.20 +/- 69.47
------------------------------------
| eval/              |             |
|    mean action     | -0.73155326 |
|    mean velocity x | 4.38        |
|    mean velocity y | 5.93        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 117         |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 202500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 130       |
|    ep_rew_mean     | -1.27e+05 |
| time/              |           |
|    fps             | 44        |
|    iterations      | 99        |
|    time_elapsed    | 4520      |
|    total_timesteps | 202752    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-52889.01 +/- 49121.05
Episode length: 64.40 +/- 24.86
------------------------------------------
| eval/                   |              |
|    mean action          | -0.074882604 |
|    mean velocity x      | -0.343       |
|    mean velocity y      | -1.28        |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 64.4         |
|    mean_reward          | -5.29e+04    |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.00235311   |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.0277       |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+08     |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00239     |
|    std                  | 0.959        |
|    value_loss           | 2.8e+08      |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-103712.68 +/- 85710.30
Episode length: 129.80 +/- 96.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.33797705 |
|    mean velocity x | -1.66      |
|    mean velocity y | -3.36      |
|    mean velocity z | 17         |
|    mean_ep_length  | 130        |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 203500     |
-----------------------------------
Eval num_timesteps=204000, episode_reward=-88646.10 +/- 38714.49
Episode length: 113.00 +/- 59.59
-------------------------------------
| eval/              |              |
|    mean action     | -0.018091861 |
|    mean velocity x | -0.15        |
|    mean velocity y | 0.032        |
|    mean velocity z | 17.2         |
|    mean_ep_length  | 113          |
|    mean_reward     | -8.86e+04    |
| time/              |              |
|    total_timesteps | 204000       |
-------------------------------------
Eval num_timesteps=204500, episode_reward=-94916.52 +/- 41880.39
Episode length: 82.00 +/- 8.17
------------------------------------
| eval/              |             |
|    mean action     | -0.11268128 |
|    mean velocity x | -0.678      |
|    mean velocity y | 0.734       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 82          |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 204500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 123       |
|    ep_rew_mean     | -1.21e+05 |
| time/              |           |
|    fps             | 45        |
|    iterations      | 100       |
|    time_elapsed    | 4528      |
|    total_timesteps | 204800    |
----------------------------------
Eval num_timesteps=205000, episode_reward=-106982.90 +/- 51429.29
Episode length: 125.40 +/- 62.81
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.24290861 |
|    mean velocity x      | 0.139       |
|    mean velocity y      | 0.632       |
|    mean velocity z      | 18.9        |
|    mean_ep_length       | 125         |
|    mean_reward          | -1.07e+05   |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.003968699 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.0545      |
|    learning_rate        | 0.001       |
|    loss                 | 6.96e+07    |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00373    |
|    std                  | 0.961       |
|    value_loss           | 2.1e+08     |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=-107978.20 +/- 69735.94
Episode length: 120.60 +/- 69.97
------------------------------------
| eval/              |             |
|    mean action     | -0.24591121 |
|    mean velocity x | -0.309      |
|    mean velocity y | 0.498       |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 121         |
|    mean_reward     | -1.08e+05   |
| time/              |             |
|    total_timesteps | 205500      |
------------------------------------
Eval num_timesteps=206000, episode_reward=-58462.40 +/- 35699.30
Episode length: 70.00 +/- 19.42
-----------------------------------
| eval/              |            |
|    mean action     | 0.27902517 |
|    mean velocity x | -2.94      |
|    mean velocity y | -2.96      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 70         |
|    mean_reward     | -5.85e+04  |
| time/              |            |
|    total_timesteps | 206000     |
-----------------------------------
Eval num_timesteps=206500, episode_reward=-97624.46 +/- 22281.28
Episode length: 95.80 +/- 19.37
-----------------------------------
| eval/              |            |
|    mean action     | -0.1802763 |
|    mean velocity x | 0.465      |
|    mean velocity y | 0.0851     |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 95.8       |
|    mean_reward     | -9.76e+04  |
| time/              |            |
|    total_timesteps | 206500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 115       |
|    ep_rew_mean     | -1.16e+05 |
| time/              |           |
|    fps             | 45        |
|    iterations      | 101       |
|    time_elapsed    | 4536      |
|    total_timesteps | 206848    |
----------------------------------
Eval num_timesteps=207000, episode_reward=-134576.80 +/- 70774.45
Episode length: 136.80 +/- 76.42
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.148875   |
|    mean velocity x      | -0.737      |
|    mean velocity y      | -0.533      |
|    mean velocity z      | 17.1        |
|    mean_ep_length       | 137         |
|    mean_reward          | -1.35e+05   |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.005411437 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.0548      |
|    learning_rate        | 0.001       |
|    loss                 | 1.6e+08     |
|    n_updates            | 1010        |
|    policy_gradient_loss | -0.00593    |
|    std                  | 0.962       |
|    value_loss           | 2.75e+08    |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=-77896.64 +/- 33869.52
Episode length: 83.60 +/- 14.69
------------------------------------
| eval/              |             |
|    mean action     | -0.20436163 |
|    mean velocity x | 0.1         |
|    mean velocity y | 0.999       |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 83.6        |
|    mean_reward     | -7.79e+04   |
| time/              |             |
|    total_timesteps | 207500      |
------------------------------------
Eval num_timesteps=208000, episode_reward=-48978.02 +/- 47383.17
Episode length: 58.80 +/- 25.79
-----------------------------------
| eval/              |            |
|    mean action     | 0.52533525 |
|    mean velocity x | -1.55      |
|    mean velocity y | -4.15      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -4.9e+04   |
| time/              |            |
|    total_timesteps | 208000     |
-----------------------------------
New best mean reward!
Eval num_timesteps=208500, episode_reward=-87679.92 +/- 39575.61
Episode length: 114.40 +/- 62.40
----------------------------------
| eval/              |           |
|    mean action     | 0.2562345 |
|    mean velocity x | -1.42     |
|    mean velocity y | -1.87     |
|    mean velocity z | 16.2      |
|    mean_ep_length  | 114       |
|    mean_reward     | -8.77e+04 |
| time/              |           |
|    total_timesteps | 208500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 117       |
|    ep_rew_mean     | -1.16e+05 |
| time/              |           |
|    fps             | 45        |
|    iterations      | 102       |
|    time_elapsed    | 4543      |
|    total_timesteps | 208896    |
----------------------------------
Eval num_timesteps=209000, episode_reward=-112185.00 +/- 86518.60
Episode length: 115.60 +/- 86.50
------------------------------------------
| eval/                   |              |
|    mean action          | -0.031677857 |
|    mean velocity x      | -0.559       |
|    mean velocity y      | -0.367       |
|    mean velocity z      | 19           |
|    mean_ep_length       | 116          |
|    mean_reward          | -1.12e+05    |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0032375176 |
|    clip_fraction        | 0.00894      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.0684       |
|    learning_rate        | 0.001        |
|    loss                 | 8.54e+07     |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00283     |
|    std                  | 0.959        |
|    value_loss           | 2.25e+08     |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-123479.21 +/- 115731.74
Episode length: 137.60 +/- 121.28
------------------------------------
| eval/              |             |
|    mean action     | -0.39098704 |
|    mean velocity x | 2.14        |
|    mean velocity y | 3.38        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 138         |
|    mean_reward     | -1.23e+05   |
| time/              |             |
|    total_timesteps | 209500      |
------------------------------------
Eval num_timesteps=210000, episode_reward=-110215.01 +/- 16981.35
Episode length: 80.60 +/- 1.96
-----------------------------------
| eval/              |            |
|    mean action     | 0.10926348 |
|    mean velocity x | -0.185     |
|    mean velocity y | 0.0543     |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 80.6       |
|    mean_reward     | -1.1e+05   |
| time/              |            |
|    total_timesteps | 210000     |
-----------------------------------
Eval num_timesteps=210500, episode_reward=-89302.02 +/- 19808.91
Episode length: 86.60 +/- 5.12
-----------------------------------
| eval/              |            |
|    mean action     | 0.07084367 |
|    mean velocity x | 0.263      |
|    mean velocity y | 0.334      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 86.6       |
|    mean_reward     | -8.93e+04  |
| time/              |            |
|    total_timesteps | 210500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 118       |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    fps             | 46        |
|    iterations      | 103       |
|    time_elapsed    | 4550      |
|    total_timesteps | 210944    |
----------------------------------
Eval num_timesteps=211000, episode_reward=-48186.93 +/- 39977.91
Episode length: 66.80 +/- 20.66
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.02046891  |
|    mean velocity x      | 1.73        |
|    mean velocity y      | 1.66        |
|    mean velocity z      | 18.9        |
|    mean_ep_length       | 66.8        |
|    mean_reward          | -4.82e+04   |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.005658051 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.0539      |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+08    |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00514    |
|    std                  | 0.96        |
|    value_loss           | 3.13e+08    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=211500, episode_reward=-79344.07 +/- 55912.85
Episode length: 110.40 +/- 66.62
-----------------------------------
| eval/              |            |
|    mean action     | 0.11717157 |
|    mean velocity x | -1.01      |
|    mean velocity y | -0.28      |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 110        |
|    mean_reward     | -7.93e+04  |
| time/              |            |
|    total_timesteps | 211500     |
-----------------------------------
Eval num_timesteps=212000, episode_reward=-55411.40 +/- 48528.43
Episode length: 57.60 +/- 31.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.09232805 |
|    mean velocity x | 1.43       |
|    mean velocity y | 0.0278     |
|    mean velocity z | 16.3       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -5.54e+04  |
| time/              |            |
|    total_timesteps | 212000     |
-----------------------------------
Eval num_timesteps=212500, episode_reward=-107383.19 +/- 19949.31
Episode length: 97.80 +/- 39.21
-----------------------------------
| eval/              |            |
|    mean action     | -0.3819254 |
|    mean velocity x | 2.07       |
|    mean velocity y | 2.92       |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 97.8       |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 212500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 113       |
|    ep_rew_mean     | -1.17e+05 |
| time/              |           |
|    fps             | 46        |
|    iterations      | 104       |
|    time_elapsed    | 4557      |
|    total_timesteps | 212992    |
----------------------------------
Eval num_timesteps=213000, episode_reward=-141613.77 +/- 74675.38
Episode length: 157.00 +/- 95.85
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.38393348 |
|    mean velocity x      | 1.91        |
|    mean velocity y      | 1.72        |
|    mean velocity z      | 18.6        |
|    mean_ep_length       | 157         |
|    mean_reward          | -1.42e+05   |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.003685775 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.0598      |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+08    |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00483    |
|    std                  | 0.96        |
|    value_loss           | 2.91e+08    |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=-93966.03 +/- 17657.84
Episode length: 88.00 +/- 9.53
------------------------------------
| eval/              |             |
|    mean action     | -0.20524508 |
|    mean velocity x | -0.126      |
|    mean velocity y | 0.316       |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 88          |
|    mean_reward     | -9.4e+04    |
| time/              |             |
|    total_timesteps | 213500      |
------------------------------------
Eval num_timesteps=214000, episode_reward=-176496.34 +/- 161093.60
Episode length: 206.80 +/- 163.74
------------------------------------
| eval/              |             |
|    mean action     | -0.09767698 |
|    mean velocity x | 0.762       |
|    mean velocity y | 1.45        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 207         |
|    mean_reward     | -1.76e+05   |
| time/              |             |
|    total_timesteps | 214000      |
------------------------------------
Eval num_timesteps=214500, episode_reward=-93226.81 +/- 17623.63
Episode length: 93.00 +/- 14.45
------------------------------------
| eval/              |             |
|    mean action     | -0.03902782 |
|    mean velocity x | 1.26        |
|    mean velocity y | 0.0479      |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 93          |
|    mean_reward     | -9.32e+04   |
| time/              |             |
|    total_timesteps | 214500      |
------------------------------------
Eval num_timesteps=215000, episode_reward=-57307.31 +/- 35652.32
Episode length: 79.00 +/- 31.05
------------------------------------
| eval/              |             |
|    mean action     | -0.35773852 |
|    mean velocity x | 2.63        |
|    mean velocity y | 3.46        |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 79          |
|    mean_reward     | -5.73e+04   |
| time/              |             |
|    total_timesteps | 215000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 113       |
|    ep_rew_mean     | -1.19e+05 |
| time/              |           |
|    fps             | 47        |
|    iterations      | 105       |
|    time_elapsed    | 4566      |
|    total_timesteps | 215040    |
----------------------------------
Eval num_timesteps=215500, episode_reward=-85930.90 +/- 27688.70
Episode length: 90.00 +/- 15.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26257077  |
|    mean velocity x      | 1.64         |
|    mean velocity y      | 2.74         |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 90           |
|    mean_reward          | -8.59e+04    |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0007312234 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.0558       |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+08     |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.961        |
|    value_loss           | 3.09e+08     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=-96328.74 +/- 64432.12
Episode length: 91.00 +/- 62.16
------------------------------------
| eval/              |             |
|    mean action     | -0.45404568 |
|    mean velocity x | 2.26        |
|    mean velocity y | 2.51        |
|    mean velocity z | 16.4        |
|    mean_ep_length  | 91          |
|    mean_reward     | -9.63e+04   |
| time/              |             |
|    total_timesteps | 216000      |
------------------------------------
Eval num_timesteps=216500, episode_reward=-93409.07 +/- 20456.10
Episode length: 87.40 +/- 7.12
----------------------------------
| eval/              |           |
|    mean action     | 0.4036233 |
|    mean velocity x | -1.98     |
|    mean velocity y | -3.22     |
|    mean velocity z | 20.8      |
|    mean_ep_length  | 87.4      |
|    mean_reward     | -9.34e+04 |
| time/              |           |
|    total_timesteps | 216500    |
----------------------------------
Eval num_timesteps=217000, episode_reward=-94005.64 +/- 25964.74
Episode length: 83.00 +/- 10.41
-----------------------------------
| eval/              |            |
|    mean action     | -0.5343175 |
|    mean velocity x | 2.57       |
|    mean velocity y | 3.75       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 83         |
|    mean_reward     | -9.4e+04   |
| time/              |            |
|    total_timesteps | 217000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 119       |
|    ep_rew_mean     | -1.25e+05 |
| time/              |           |
|    fps             | 47        |
|    iterations      | 106       |
|    time_elapsed    | 4573      |
|    total_timesteps | 217088    |
----------------------------------
Eval num_timesteps=217500, episode_reward=-133665.12 +/- 68328.71
Episode length: 127.60 +/- 92.32
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.32621333 |
|    mean velocity x      | 0.484       |
|    mean velocity y      | 1.41        |
|    mean velocity z      | 17.6        |
|    mean_ep_length       | 128         |
|    mean_reward          | -1.34e+05   |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.004383863 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.0475      |
|    learning_rate        | 0.001       |
|    loss                 | 1.24e+08    |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00342    |
|    std                  | 0.962       |
|    value_loss           | 2.33e+08    |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=-106788.67 +/- 142176.22
Episode length: 128.60 +/- 155.13
----------------------------------
| eval/              |           |
|    mean action     | 0.6022097 |
|    mean velocity x | -3        |
|    mean velocity y | -4.63     |
|    mean velocity z | 16.9      |
|    mean_ep_length  | 129       |
|    mean_reward     | -1.07e+05 |
| time/              |           |
|    total_timesteps | 218000    |
----------------------------------
Eval num_timesteps=218500, episode_reward=-80167.29 +/- 36163.70
Episode length: 72.40 +/- 14.81
------------------------------------
| eval/              |             |
|    mean action     | -0.59731704 |
|    mean velocity x | 3.02        |
|    mean velocity y | 4.37        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 72.4        |
|    mean_reward     | -8.02e+04   |
| time/              |             |
|    total_timesteps | 218500      |
------------------------------------
Eval num_timesteps=219000, episode_reward=-95435.91 +/- 40777.51
Episode length: 103.60 +/- 36.88
----------------------------------
| eval/              |           |
|    mean action     | -0.359095 |
|    mean velocity x | 0.768     |
|    mean velocity y | 2.55      |
|    mean velocity z | 19.3      |
|    mean_ep_length  | 104       |
|    mean_reward     | -9.54e+04 |
| time/              |           |
|    total_timesteps | 219000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 122       |
|    ep_rew_mean     | -1.28e+05 |
| time/              |           |
|    fps             | 47        |
|    iterations      | 107       |
|    time_elapsed    | 4581      |
|    total_timesteps | 219136    |
----------------------------------
Eval num_timesteps=219500, episode_reward=-81335.96 +/- 4752.41
Episode length: 92.60 +/- 8.96
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.45899525 |
|    mean velocity x      | 3.34        |
|    mean velocity y      | 4.11        |
|    mean velocity z      | 18.4        |
|    mean_ep_length       | 92.6        |
|    mean_reward          | -8.13e+04   |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.004641452 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.0524      |
|    learning_rate        | 0.001       |
|    loss                 | 1.19e+08    |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00477    |
|    std                  | 0.961       |
|    value_loss           | 2.69e+08    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=-80255.12 +/- 43278.16
Episode length: 73.60 +/- 10.65
-----------------------------------
| eval/              |            |
|    mean action     | -0.7872271 |
|    mean velocity x | 4.15       |
|    mean velocity y | 6.94       |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 73.6       |
|    mean_reward     | -8.03e+04  |
| time/              |            |
|    total_timesteps | 220000     |
-----------------------------------
Eval num_timesteps=220500, episode_reward=-93771.54 +/- 59726.81
Episode length: 92.00 +/- 51.80
-----------------------------------
| eval/              |            |
|    mean action     | 0.08306471 |
|    mean velocity x | 0.253      |
|    mean velocity y | -1.15      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 92         |
|    mean_reward     | -9.38e+04  |
| time/              |            |
|    total_timesteps | 220500     |
-----------------------------------
Eval num_timesteps=221000, episode_reward=-93470.97 +/- 16040.90
Episode length: 100.40 +/- 33.07
------------------------------------
| eval/              |             |
|    mean action     | -0.21897772 |
|    mean velocity x | 1.04        |
|    mean velocity y | 2.08        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 100         |
|    mean_reward     | -9.35e+04   |
| time/              |             |
|    total_timesteps | 221000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 124       |
|    ep_rew_mean     | -1.32e+05 |
| time/              |           |
|    fps             | 48        |
|    iterations      | 108       |
|    time_elapsed    | 4588      |
|    total_timesteps | 221184    |
----------------------------------
Eval num_timesteps=221500, episode_reward=-72393.50 +/- 30459.16
Episode length: 85.20 +/- 21.79
------------------------------------------
| eval/                   |              |
|    mean action          | -0.030825485 |
|    mean velocity x      | -1.5         |
|    mean velocity y      | -1.28        |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 85.2         |
|    mean_reward          | -7.24e+04    |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0018261102 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.0609       |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+08      |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.96         |
|    value_loss           | 2.5e+08      |
------------------------------------------
Eval num_timesteps=222000, episode_reward=-85808.16 +/- 44385.05
Episode length: 74.80 +/- 30.18
-----------------------------------
| eval/              |            |
|    mean action     | -0.3217688 |
|    mean velocity x | 2.25       |
|    mean velocity y | 3.17       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 74.8       |
|    mean_reward     | -8.58e+04  |
| time/              |            |
|    total_timesteps | 222000     |
-----------------------------------
Eval num_timesteps=222500, episode_reward=-133122.18 +/- 57242.85
Episode length: 136.00 +/- 85.73
-----------------------------------
| eval/              |            |
|    mean action     | -0.4020131 |
|    mean velocity x | 1.59       |
|    mean velocity y | 2.95       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 136        |
|    mean_reward     | -1.33e+05  |
| time/              |            |
|    total_timesteps | 222500     |
-----------------------------------
Eval num_timesteps=223000, episode_reward=-50366.76 +/- 35285.06
Episode length: 69.60 +/- 16.86
------------------------------------
| eval/              |             |
|    mean action     | -0.40693805 |
|    mean velocity x | 0.454       |
|    mean velocity y | 3.18        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 69.6        |
|    mean_reward     | -5.04e+04   |
| time/              |             |
|    total_timesteps | 223000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 125       |
|    ep_rew_mean     | -1.29e+05 |
| time/              |           |
|    fps             | 48        |
|    iterations      | 109       |
|    time_elapsed    | 4595      |
|    total_timesteps | 223232    |
----------------------------------
Eval num_timesteps=223500, episode_reward=-128369.04 +/- 77167.78
Episode length: 152.40 +/- 103.20
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.9838012  |
|    mean velocity x      | 5.74        |
|    mean velocity y      | 9.34        |
|    mean velocity z      | 22.2        |
|    mean_ep_length       | 152         |
|    mean_reward          | -1.28e+05   |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.004702679 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.0416      |
|    learning_rate        | 0.001       |
|    loss                 | 8.12e+07    |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00477    |
|    std                  | 0.96        |
|    value_loss           | 2.13e+08    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=-119652.32 +/- 98217.21
Episode length: 144.60 +/- 125.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.35621363 |
|    mean velocity x | -0.531     |
|    mean velocity y | -3.04      |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 145        |
|    mean_reward     | -1.2e+05   |
| time/              |            |
|    total_timesteps | 224000     |
-----------------------------------
Eval num_timesteps=224500, episode_reward=-109458.46 +/- 69870.18
Episode length: 108.00 +/- 79.95
-----------------------------------
| eval/              |            |
|    mean action     | -0.3510639 |
|    mean velocity x | 0.822      |
|    mean velocity y | 1.45       |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 108        |
|    mean_reward     | -1.09e+05  |
| time/              |            |
|    total_timesteps | 224500     |
-----------------------------------
Eval num_timesteps=225000, episode_reward=-76330.61 +/- 28387.11
Episode length: 93.40 +/- 23.10
-----------------------------------
| eval/              |            |
|    mean action     | -0.2590421 |
|    mean velocity x | 1.01       |
|    mean velocity y | 0.792      |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 93.4       |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 225000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 127       |
|    ep_rew_mean     | -1.29e+05 |
| time/              |           |
|    fps             | 48        |
|    iterations      | 110       |
|    time_elapsed    | 4603      |
|    total_timesteps | 225280    |
----------------------------------
Eval num_timesteps=225500, episode_reward=-119457.84 +/- 101261.62
Episode length: 147.60 +/- 127.35
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39472902   |
|    mean velocity x      | 0.941         |
|    mean velocity y      | 1.99          |
|    mean velocity z      | 15.5          |
|    mean_ep_length       | 148           |
|    mean_reward          | -1.19e+05     |
| time/                   |               |
|    total_timesteps      | 225500        |
| train/                  |               |
|    approx_kl            | 0.00051185885 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.13         |
|    explained_variance   | 0.0461        |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+08      |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 0.96          |
|    value_loss           | 2.37e+08      |
-------------------------------------------
Eval num_timesteps=226000, episode_reward=-126524.39 +/- 48065.89
Episode length: 133.40 +/- 67.08
-----------------------------------
| eval/              |            |
|    mean action     | -0.3329575 |
|    mean velocity x | 2.38       |
|    mean velocity y | 3.21       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 133        |
|    mean_reward     | -1.27e+05  |
| time/              |            |
|    total_timesteps | 226000     |
-----------------------------------
Eval num_timesteps=226500, episode_reward=-135669.50 +/- 48881.48
Episode length: 122.20 +/- 67.12
-----------------------------------
| eval/              |            |
|    mean action     | 0.07005458 |
|    mean velocity x | 1.05       |
|    mean velocity y | -0.827     |
|    mean velocity z | 15.9       |
|    mean_ep_length  | 122        |
|    mean_reward     | -1.36e+05  |
| time/              |            |
|    total_timesteps | 226500     |
-----------------------------------
Eval num_timesteps=227000, episode_reward=-83013.56 +/- 17304.81
Episode length: 83.60 +/- 3.93
-----------------------------------
| eval/              |            |
|    mean action     | -0.5473065 |
|    mean velocity x | 2.46       |
|    mean velocity y | 4.19       |
|    mean velocity z | 20         |
|    mean_ep_length  | 83.6       |
|    mean_reward     | -8.3e+04   |
| time/              |            |
|    total_timesteps | 227000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 130       |
|    ep_rew_mean     | -1.31e+05 |
| time/              |           |
|    fps             | 49        |
|    iterations      | 111       |
|    time_elapsed    | 4611      |
|    total_timesteps | 227328    |
----------------------------------
Eval num_timesteps=227500, episode_reward=-71755.94 +/- 97590.11
Episode length: 88.40 +/- 100.74
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26425663  |
|    mean velocity x      | -0.0936      |
|    mean velocity y      | 0.856        |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 88.4         |
|    mean_reward          | -7.18e+04    |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0033084569 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.0349       |
|    learning_rate        | 0.001        |
|    loss                 | 9.84e+07     |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00386     |
|    std                  | 0.962        |
|    value_loss           | 2.26e+08     |
------------------------------------------
Eval num_timesteps=228000, episode_reward=-82391.26 +/- 23354.77
Episode length: 80.80 +/- 6.21
------------------------------------
| eval/              |             |
|    mean action     | -0.11464476 |
|    mean velocity x | 0.317       |
|    mean velocity y | 0.826       |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 80.8        |
|    mean_reward     | -8.24e+04   |
| time/              |             |
|    total_timesteps | 228000      |
------------------------------------
Eval num_timesteps=228500, episode_reward=-92466.55 +/- 31348.44
Episode length: 98.40 +/- 38.37
------------------------------------
| eval/              |             |
|    mean action     | -0.31245363 |
|    mean velocity x | 0.988       |
|    mean velocity y | 1.51        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 98.4        |
|    mean_reward     | -9.25e+04   |
| time/              |             |
|    total_timesteps | 228500      |
------------------------------------
Eval num_timesteps=229000, episode_reward=-96139.70 +/- 47137.43
Episode length: 77.80 +/- 7.52
-----------------------------------
| eval/              |            |
|    mean action     | -0.8381882 |
|    mean velocity x | 4.69       |
|    mean velocity y | 6.92       |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 77.8       |
|    mean_reward     | -9.61e+04  |
| time/              |            |
|    total_timesteps | 229000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 128       |
|    ep_rew_mean     | -1.28e+05 |
| time/              |           |
|    fps             | 49        |
|    iterations      | 112       |
|    time_elapsed    | 4620      |
|    total_timesteps | 229376    |
----------------------------------
Eval num_timesteps=229500, episode_reward=-80642.14 +/- 43529.67
Episode length: 94.80 +/- 40.56
------------------------------------------
| eval/                   |              |
|    mean action          | -0.16123357  |
|    mean velocity x      | 0.209        |
|    mean velocity y      | 0.498        |
|    mean velocity z      | 18           |
|    mean_ep_length       | 94.8         |
|    mean_reward          | -8.06e+04    |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0018226729 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.0487       |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 0.964        |
|    value_loss           | 2.44e+08     |
------------------------------------------
Eval num_timesteps=230000, episode_reward=-53512.75 +/- 53922.24
Episode length: 54.40 +/- 33.02
------------------------------------
| eval/              |             |
|    mean action     | 0.014348885 |
|    mean velocity x | 0.0701      |
|    mean velocity y | 0.136       |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5.35e+04   |
| time/              |             |
|    total_timesteps | 230000      |
------------------------------------
Eval num_timesteps=230500, episode_reward=-59492.51 +/- 51668.46
Episode length: 53.80 +/- 32.52
------------------------------------
| eval/              |             |
|    mean action     | -0.34041262 |
|    mean velocity x | 1.41        |
|    mean velocity y | 3.2         |
|    mean velocity z | 16.5        |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -5.95e+04   |
| time/              |             |
|    total_timesteps | 230500      |
------------------------------------
Eval num_timesteps=231000, episode_reward=-80744.06 +/- 21577.37
Episode length: 85.40 +/- 13.41
----------------------------------
| eval/              |           |
|    mean action     | 0.5079317 |
|    mean velocity x | -2.24     |
|    mean velocity y | -2.96     |
|    mean velocity z | 15.5      |
|    mean_ep_length  | 85.4      |
|    mean_reward     | -8.07e+04 |
| time/              |           |
|    total_timesteps | 231000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 125       |
|    ep_rew_mean     | -1.25e+05 |
| time/              |           |
|    fps             | 50        |
|    iterations      | 113       |
|    time_elapsed    | 4627      |
|    total_timesteps | 231424    |
----------------------------------
Eval num_timesteps=231500, episode_reward=-94641.91 +/- 24163.01
Episode length: 87.40 +/- 15.33
------------------------------------------
| eval/                   |              |
|    mean action          | 0.38354516   |
|    mean velocity x      | -1.85        |
|    mean velocity y      | -2.89        |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 87.4         |
|    mean_reward          | -9.46e+04    |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0010143238 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.0635       |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+08     |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.964        |
|    value_loss           | 2.26e+08     |
------------------------------------------
Eval num_timesteps=232000, episode_reward=-85463.77 +/- 24402.08
Episode length: 87.80 +/- 17.29
------------------------------------
| eval/              |             |
|    mean action     | -0.27130717 |
|    mean velocity x | 2.03        |
|    mean velocity y | 2.9         |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 87.8        |
|    mean_reward     | -8.55e+04   |
| time/              |             |
|    total_timesteps | 232000      |
------------------------------------
Eval num_timesteps=232500, episode_reward=-98838.46 +/- 33070.39
Episode length: 116.20 +/- 50.32
------------------------------------
| eval/              |             |
|    mean action     | -0.90482736 |
|    mean velocity x | 5.59        |
|    mean velocity y | 8.46        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 116         |
|    mean_reward     | -9.88e+04   |
| time/              |             |
|    total_timesteps | 232500      |
------------------------------------
Eval num_timesteps=233000, episode_reward=-65274.53 +/- 28091.11
Episode length: 80.20 +/- 23.01
-------------------------------------
| eval/              |              |
|    mean action     | -0.042146277 |
|    mean velocity x | -0.075       |
|    mean velocity y | -0.17        |
|    mean velocity z | 21.9         |
|    mean_ep_length  | 80.2         |
|    mean_reward     | -6.53e+04    |
| time/              |              |
|    total_timesteps | 233000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 121       |
|    ep_rew_mean     | -1.21e+05 |
| time/              |           |
|    fps             | 50        |
|    iterations      | 114       |
|    time_elapsed    | 4634      |
|    total_timesteps | 233472    |
----------------------------------
Eval num_timesteps=233500, episode_reward=-81794.78 +/- 42865.84
Episode length: 72.00 +/- 12.13
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.23072249 |
|    mean velocity x      | 0.407       |
|    mean velocity y      | 1.01        |
|    mean velocity z      | 18.1        |
|    mean_ep_length       | 72          |
|    mean_reward          | -8.18e+04   |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.006593584 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.044       |
|    learning_rate        | 0.001       |
|    loss                 | 1.39e+08    |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00695    |
|    std                  | 0.958       |
|    value_loss           | 2.59e+08    |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=-83331.51 +/- 13456.98
Episode length: 83.40 +/- 18.60
-----------------------------------
| eval/              |            |
|    mean action     | -0.3121931 |
|    mean velocity x | 0.183      |
|    mean velocity y | 1.24       |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 83.4       |
|    mean_reward     | -8.33e+04  |
| time/              |            |
|    total_timesteps | 234000     |
-----------------------------------
Eval num_timesteps=234500, episode_reward=-214649.41 +/- 250657.41
Episode length: 235.40 +/- 304.35
------------------------------------
| eval/              |             |
|    mean action     | -0.41858733 |
|    mean velocity x | 2.02        |
|    mean velocity y | 3.2         |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 235         |
|    mean_reward     | -2.15e+05   |
| time/              |             |
|    total_timesteps | 234500      |
------------------------------------
Eval num_timesteps=235000, episode_reward=-60445.51 +/- 32467.17
Episode length: 68.60 +/- 22.03
-----------------------------------
| eval/              |            |
|    mean action     | 0.15220156 |
|    mean velocity x | -1.44      |
|    mean velocity y | -1.66      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 68.6       |
|    mean_reward     | -6.04e+04  |
| time/              |            |
|    total_timesteps | 235000     |
-----------------------------------
Eval num_timesteps=235500, episode_reward=-67582.74 +/- 36214.82
Episode length: 68.40 +/- 27.72
-----------------------------------
| eval/              |            |
|    mean action     | -0.8768117 |
|    mean velocity x | 5.9        |
|    mean velocity y | 8.58       |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 68.4       |
|    mean_reward     | -6.76e+04  |
| time/              |            |
|    total_timesteps | 235500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 113       |
|    ep_rew_mean     | -1.14e+05 |
| time/              |           |
|    fps             | 50        |
|    iterations      | 115       |
|    time_elapsed    | 4642      |
|    total_timesteps | 235520    |
----------------------------------
Eval num_timesteps=236000, episode_reward=-133451.80 +/- 106010.06
Episode length: 158.00 +/- 129.97
------------------------------------------
| eval/                   |              |
|    mean action          | 0.077240884  |
|    mean velocity x      | -0.444       |
|    mean velocity y      | -0.667       |
|    mean velocity z      | 18           |
|    mean_ep_length       | 158          |
|    mean_reward          | -1.33e+05    |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0016138825 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.0697       |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+08      |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.956        |
|    value_loss           | 2.2e+08      |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-135050.69 +/- 42823.63
Episode length: 124.80 +/- 57.75
------------------------------------
| eval/              |             |
|    mean action     | 0.003189064 |
|    mean velocity x | -0.492      |
|    mean velocity y | 0.194       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 125         |
|    mean_reward     | -1.35e+05   |
| time/              |             |
|    total_timesteps | 236500      |
------------------------------------
Eval num_timesteps=237000, episode_reward=-104007.96 +/- 16659.68
Episode length: 85.60 +/- 9.39
------------------------------------
| eval/              |             |
|    mean action     | -0.25184336 |
|    mean velocity x | 1.43        |
|    mean velocity y | 2.15        |
|    mean velocity z | 18          |
|    mean_ep_length  | 85.6        |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 237000      |
------------------------------------
Eval num_timesteps=237500, episode_reward=-84530.93 +/- 44181.50
Episode length: 100.20 +/- 44.58
------------------------------------
| eval/              |             |
|    mean action     | -0.07920978 |
|    mean velocity x | -0.00178    |
|    mean velocity y | 0.352       |
|    mean velocity z | 16.5        |
|    mean_ep_length  | 100         |
|    mean_reward     | -8.45e+04   |
| time/              |             |
|    total_timesteps | 237500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 111       |
|    ep_rew_mean     | -1.14e+05 |
| time/              |           |
|    fps             | 51        |
|    iterations      | 116       |
|    time_elapsed    | 4650      |
|    total_timesteps | 237568    |
----------------------------------
Eval num_timesteps=238000, episode_reward=-77978.15 +/- 41435.71
Episode length: 82.00 +/- 44.28
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.20984241 |
|    mean velocity x      | 0.532       |
|    mean velocity y      | 1.29        |
|    mean velocity z      | 16.7        |
|    mean_ep_length       | 82          |
|    mean_reward          | -7.8e+04    |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.001488406 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.0701      |
|    learning_rate        | 0.001       |
|    loss                 | 1.09e+08    |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 0.957       |
|    value_loss           | 2.42e+08    |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=-85539.28 +/- 14516.91
Episode length: 84.60 +/- 17.00
----------------------------------
| eval/              |           |
|    mean action     | 0.6504501 |
|    mean velocity x | -2.01     |
|    mean velocity y | -4.38     |
|    mean velocity z | 18.6      |
|    mean_ep_length  | 84.6      |
|    mean_reward     | -8.55e+04 |
| time/              |           |
|    total_timesteps | 238500    |
----------------------------------
Eval num_timesteps=239000, episode_reward=-73837.52 +/- 37766.57
Episode length: 75.40 +/- 5.89
------------------------------------
| eval/              |             |
|    mean action     | -0.65354466 |
|    mean velocity x | 3.62        |
|    mean velocity y | 4.36        |
|    mean velocity z | 16.2        |
|    mean_ep_length  | 75.4        |
|    mean_reward     | -7.38e+04   |
| time/              |             |
|    total_timesteps | 239000      |
------------------------------------
Eval num_timesteps=239500, episode_reward=-94821.41 +/- 31272.17
Episode length: 77.20 +/- 1.17
------------------------------------
| eval/              |             |
|    mean action     | -0.26705512 |
|    mean velocity x | 2.38        |
|    mean velocity y | 3.05        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 77.2        |
|    mean_reward     | -9.48e+04   |
| time/              |             |
|    total_timesteps | 239500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 115       |
|    ep_rew_mean     | -1.15e+05 |
| time/              |           |
|    fps             | 51        |
|    iterations      | 117       |
|    time_elapsed    | 4657      |
|    total_timesteps | 239616    |
----------------------------------
Eval num_timesteps=240000, episode_reward=-67401.74 +/- 61234.32
Episode length: 82.60 +/- 61.24
----------------------------------------
| eval/                   |            |
|    mean action          | 0.06894257 |
|    mean velocity x      | 1.31       |
|    mean velocity y      | 0.0849     |
|    mean velocity z      | 15.1       |
|    mean_ep_length       | 82.6       |
|    mean_reward          | -6.74e+04  |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.00088404 |
|    clip_fraction        | 0.000244   |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.12      |
|    explained_variance   | 0.0588     |
|    learning_rate        | 0.001      |
|    loss                 | 1.2e+08    |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.000793  |
|    std                  | 0.957      |
|    value_loss           | 2.37e+08   |
----------------------------------------
Eval num_timesteps=240500, episode_reward=-66989.71 +/- 51671.48
Episode length: 57.00 +/- 32.35
----------------------------------
| eval/              |           |
|    mean action     | 0.2753551 |
|    mean velocity x | 0.17      |
|    mean velocity y | -0.292    |
|    mean velocity z | 17.6      |
|    mean_ep_length  | 57        |
|    mean_reward     | -6.7e+04  |
| time/              |           |
|    total_timesteps | 240500    |
----------------------------------
Eval num_timesteps=241000, episode_reward=-98758.68 +/- 14910.39
Episode length: 93.40 +/- 18.54
-----------------------------------
| eval/              |            |
|    mean action     | -0.4912428 |
|    mean velocity x | 2.55       |
|    mean velocity y | 4.06       |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 93.4       |
|    mean_reward     | -9.88e+04  |
| time/              |            |
|    total_timesteps | 241000     |
-----------------------------------
Eval num_timesteps=241500, episode_reward=-83064.03 +/- 19465.24
Episode length: 97.40 +/- 33.04
------------------------------------
| eval/              |             |
|    mean action     | -0.61074746 |
|    mean velocity x | 3.9         |
|    mean velocity y | 5.68        |
|    mean velocity z | 18          |
|    mean_ep_length  | 97.4        |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 241500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 119       |
|    ep_rew_mean     | -1.17e+05 |
| time/              |           |
|    fps             | 51        |
|    iterations      | 118       |
|    time_elapsed    | 4664      |
|    total_timesteps | 241664    |
----------------------------------
Eval num_timesteps=242000, episode_reward=-117867.01 +/- 32889.08
Episode length: 104.60 +/- 42.61
------------------------------------------
| eval/                   |              |
|    mean action          | -0.06366715  |
|    mean velocity x      | 0.862        |
|    mean velocity y      | 0.468        |
|    mean velocity z      | 17.1         |
|    mean_ep_length       | 105          |
|    mean_reward          | -1.18e+05    |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0010452532 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.0477       |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+08     |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.957        |
|    value_loss           | 1.9e+08      |
------------------------------------------
Eval num_timesteps=242500, episode_reward=-71845.85 +/- 34579.42
Episode length: 73.20 +/- 17.02
----------------------------------
| eval/              |           |
|    mean action     | 0.3391736 |
|    mean velocity x | -1.91     |
|    mean velocity y | -3.81     |
|    mean velocity z | 21.3      |
|    mean_ep_length  | 73.2      |
|    mean_reward     | -7.18e+04 |
| time/              |           |
|    total_timesteps | 242500    |
----------------------------------
Eval num_timesteps=243000, episode_reward=-39586.32 +/- 45758.04
Episode length: 54.80 +/- 28.07
-----------------------------------
| eval/              |            |
|    mean action     | -0.5257415 |
|    mean velocity x | 3.01       |
|    mean velocity y | 4.58       |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 54.8       |
|    mean_reward     | -3.96e+04  |
| time/              |            |
|    total_timesteps | 243000     |
-----------------------------------
New best mean reward!
Eval num_timesteps=243500, episode_reward=-95816.04 +/- 21901.00
Episode length: 79.60 +/- 7.55
----------------------------------
| eval/              |           |
|    mean action     | -0.539714 |
|    mean velocity x | 1.91      |
|    mean velocity y | 3.1       |
|    mean velocity z | 16.9      |
|    mean_ep_length  | 79.6      |
|    mean_reward     | -9.58e+04 |
| time/              |           |
|    total_timesteps | 243500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 117       |
|    ep_rew_mean     | -1.16e+05 |
| time/              |           |
|    fps             | 52        |
|    iterations      | 119       |
|    time_elapsed    | 4671      |
|    total_timesteps | 243712    |
----------------------------------
Eval num_timesteps=244000, episode_reward=-61294.01 +/- 40841.31
Episode length: 76.60 +/- 36.36
------------------------------------------
| eval/                   |              |
|    mean action          | 0.4839254    |
|    mean velocity x      | -1.04        |
|    mean velocity y      | -2.25        |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 76.6         |
|    mean_reward          | -6.13e+04    |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0038547022 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.0502       |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00327     |
|    std                  | 0.954        |
|    value_loss           | 2.56e+08     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-94838.63 +/- 56405.06
Episode length: 95.80 +/- 62.18
-------------------------------------
| eval/              |              |
|    mean action     | -0.026106384 |
|    mean velocity x | 1.03         |
|    mean velocity y | 0.407        |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 95.8         |
|    mean_reward     | -9.48e+04    |
| time/              |              |
|    total_timesteps | 244500       |
-------------------------------------
Eval num_timesteps=245000, episode_reward=-88501.00 +/- 47784.46
Episode length: 68.40 +/- 18.21
-----------------------------------
| eval/              |            |
|    mean action     | -0.2715183 |
|    mean velocity x | 2.24       |
|    mean velocity y | 2.35       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 68.4       |
|    mean_reward     | -8.85e+04  |
| time/              |            |
|    total_timesteps | 245000     |
-----------------------------------
Eval num_timesteps=245500, episode_reward=-107394.34 +/- 30697.27
Episode length: 97.00 +/- 35.03
-----------------------------------
| eval/              |            |
|    mean action     | 0.05070975 |
|    mean velocity x | -0.164     |
|    mean velocity y | -1.57      |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 97         |
|    mean_reward     | -1.07e+05  |
| time/              |            |
|    total_timesteps | 245500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 116       |
|    ep_rew_mean     | -1.16e+05 |
| time/              |           |
|    fps             | 52        |
|    iterations      | 120       |
|    time_elapsed    | 4679      |
|    total_timesteps | 245760    |
----------------------------------
Eval num_timesteps=246000, episode_reward=-81350.17 +/- 43029.42
Episode length: 78.20 +/- 27.37
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29289967  |
|    mean velocity x      | 3.43         |
|    mean velocity y      | 3.43         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 78.2         |
|    mean_reward          | -8.14e+04    |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0009926884 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.0606       |
|    learning_rate        | 0.001        |
|    loss                 | 7.2e+07      |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.955        |
|    value_loss           | 2.6e+08      |
------------------------------------------
Eval num_timesteps=246500, episode_reward=-71040.55 +/- 32431.21
Episode length: 76.60 +/- 16.57
------------------------------------
| eval/              |             |
|    mean action     | -0.78128946 |
|    mean velocity x | 4.24        |
|    mean velocity y | 6.62        |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 76.6        |
|    mean_reward     | -7.1e+04    |
| time/              |             |
|    total_timesteps | 246500      |
------------------------------------
Eval num_timesteps=247000, episode_reward=-49086.57 +/- 33355.86
Episode length: 80.60 +/- 41.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.5848321 |
|    mean velocity x | 4.98       |
|    mean velocity y | 5.84       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 80.6       |
|    mean_reward     | -4.91e+04  |
| time/              |            |
|    total_timesteps | 247000     |
-----------------------------------
Eval num_timesteps=247500, episode_reward=-94268.49 +/- 47952.46
Episode length: 63.40 +/- 28.20
-----------------------------------
| eval/              |            |
|    mean action     | -0.2796732 |
|    mean velocity x | 1.54       |
|    mean velocity y | 2.26       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -9.43e+04  |
| time/              |            |
|    total_timesteps | 247500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 121       |
|    ep_rew_mean     | -1.22e+05 |
| time/              |           |
|    fps             | 52        |
|    iterations      | 121       |
|    time_elapsed    | 4685      |
|    total_timesteps | 247808    |
----------------------------------
Eval num_timesteps=248000, episode_reward=-51059.67 +/- 32855.93
Episode length: 62.20 +/- 19.57
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.11345623 |
|    mean velocity x      | 0.000257    |
|    mean velocity y      | 0.407       |
|    mean velocity z      | 18.6        |
|    mean_ep_length       | 62.2        |
|    mean_reward          | -5.11e+04   |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.006548106 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.0589      |
|    learning_rate        | 0.001       |
|    loss                 | 1.66e+08    |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00575    |
|    std                  | 0.954       |
|    value_loss           | 2.67e+08    |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=-76448.13 +/- 22288.83
Episode length: 82.80 +/- 15.61
------------------------------------
| eval/              |             |
|    mean action     | -0.21423875 |
|    mean velocity x | 0.513       |
|    mean velocity y | 1.88        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 82.8        |
|    mean_reward     | -7.64e+04   |
| time/              |             |
|    total_timesteps | 248500      |
------------------------------------
Eval num_timesteps=249000, episode_reward=-64227.12 +/- 33406.01
Episode length: 73.20 +/- 5.74
------------------------------------
| eval/              |             |
|    mean action     | -0.12655488 |
|    mean velocity x | 1.74        |
|    mean velocity y | 0.903       |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 249000      |
------------------------------------
Eval num_timesteps=249500, episode_reward=-181036.95 +/- 108114.77
Episode length: 179.60 +/- 125.70
-----------------------------------
| eval/              |            |
|    mean action     | -0.6789856 |
|    mean velocity x | 4.88       |
|    mean velocity y | 6.24       |
|    mean velocity z | 20.9       |
|    mean_ep_length  | 180        |
|    mean_reward     | -1.81e+05  |
| time/              |            |
|    total_timesteps | 249500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 121       |
|    ep_rew_mean     | -1.24e+05 |
| time/              |           |
|    fps             | 53        |
|    iterations      | 122       |
|    time_elapsed    | 4693      |
|    total_timesteps | 249856    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-103203.21 +/- 52192.68
Episode length: 92.00 +/- 62.54
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.62237245   |
|    mean velocity x      | 3.1           |
|    mean velocity y      | 4.74          |
|    mean velocity z      | 17.6          |
|    mean_ep_length       | 92            |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00095140707 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.11         |
|    explained_variance   | 0.0704        |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 1220          |
|    policy_gradient_loss | -0.00181      |
|    std                  | 0.954         |
|    value_loss           | 2.5e+08       |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=-126173.79 +/- 68189.08
Episode length: 117.60 +/- 85.71
------------------------------------
| eval/              |             |
|    mean action     | -0.32714602 |
|    mean velocity x | 2.19        |
|    mean velocity y | 3.15        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 118         |
|    mean_reward     | -1.26e+05   |
| time/              |             |
|    total_timesteps | 250500      |
------------------------------------
Eval num_timesteps=251000, episode_reward=-118586.29 +/- 38004.60
Episode length: 103.60 +/- 52.30
------------------------------------
| eval/              |             |
|    mean action     | 0.028179115 |
|    mean velocity x | 1.34        |
|    mean velocity y | 0.819       |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 104         |
|    mean_reward     | -1.19e+05   |
| time/              |             |
|    total_timesteps | 251000      |
------------------------------------
Eval num_timesteps=251500, episode_reward=-93002.53 +/- 54350.17
Episode length: 94.00 +/- 44.43
------------------------------------
| eval/              |             |
|    mean action     | -0.48865408 |
|    mean velocity x | 4.37        |
|    mean velocity y | 4.06        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 94          |
|    mean_reward     | -9.3e+04    |
| time/              |             |
|    total_timesteps | 251500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 121       |
|    ep_rew_mean     | -1.25e+05 |
| time/              |           |
|    fps             | 53        |
|    iterations      | 123       |
|    time_elapsed    | 4700      |
|    total_timesteps | 251904    |
----------------------------------
Eval num_timesteps=252000, episode_reward=-74489.23 +/- 38391.29
Episode length: 70.80 +/- 27.17
------------------------------------------
| eval/                   |              |
|    mean action          | 0.7005274    |
|    mean velocity x      | -2.56        |
|    mean velocity y      | -5.42        |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 70.8         |
|    mean_reward          | -7.45e+04    |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0020944246 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0553       |
|    learning_rate        | 0.001        |
|    loss                 | 9.93e+07     |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.953        |
|    value_loss           | 2.45e+08     |
------------------------------------------
Eval num_timesteps=252500, episode_reward=-73012.53 +/- 19463.97
Episode length: 84.00 +/- 25.51
-----------------------------------
| eval/              |            |
|    mean action     | -0.1848426 |
|    mean velocity x | 0.333      |
|    mean velocity y | 1.58       |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 84         |
|    mean_reward     | -7.3e+04   |
| time/              |            |
|    total_timesteps | 252500     |
-----------------------------------
Eval num_timesteps=253000, episode_reward=-62185.45 +/- 33606.74
Episode length: 68.80 +/- 15.99
----------------------------------
| eval/              |           |
|    mean action     | 0.3355029 |
|    mean velocity x | -1.8      |
|    mean velocity y | -2.74     |
|    mean velocity z | 19.3      |
|    mean_ep_length  | 68.8      |
|    mean_reward     | -6.22e+04 |
| time/              |           |
|    total_timesteps | 253000    |
----------------------------------
Eval num_timesteps=253500, episode_reward=-80519.72 +/- 43086.42
Episode length: 66.20 +/- 29.24
------------------------------------
| eval/              |             |
|    mean action     | -0.13092938 |
|    mean velocity x | 0.195       |
|    mean velocity y | -0.668      |
|    mean velocity z | 20          |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 253500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 118       |
|    ep_rew_mean     | -1.26e+05 |
| time/              |           |
|    fps             | 53        |
|    iterations      | 124       |
|    time_elapsed    | 4707      |
|    total_timesteps | 253952    |
----------------------------------
Eval num_timesteps=254000, episode_reward=-69317.19 +/- 37576.30
Episode length: 75.00 +/- 22.24
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.9525332  |
|    mean velocity x      | 4.29        |
|    mean velocity y      | 7.82        |
|    mean velocity z      | 20.1        |
|    mean_ep_length       | 75          |
|    mean_reward          | -6.93e+04   |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.003983221 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.0577      |
|    learning_rate        | 0.001       |
|    loss                 | 8.08e+07    |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00299    |
|    std                  | 0.954       |
|    value_loss           | 2.45e+08    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-128316.36 +/- 88582.76
Episode length: 143.60 +/- 94.54
------------------------------------
| eval/              |             |
|    mean action     | -0.90627074 |
|    mean velocity x | 3.67        |
|    mean velocity y | 6.66        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 144         |
|    mean_reward     | -1.28e+05   |
| time/              |             |
|    total_timesteps | 254500      |
------------------------------------
Eval num_timesteps=255000, episode_reward=-51263.81 +/- 36349.16
Episode length: 77.60 +/- 25.75
-----------------------------------
| eval/              |            |
|    mean action     | 0.25138578 |
|    mean velocity x | -0.713     |
|    mean velocity y | -0.213     |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 77.6       |
|    mean_reward     | -5.13e+04  |
| time/              |            |
|    total_timesteps | 255000     |
-----------------------------------
Eval num_timesteps=255500, episode_reward=-55290.69 +/- 30342.84
Episode length: 65.00 +/- 22.04
----------------------------------
| eval/              |           |
|    mean action     | 0.3979404 |
|    mean velocity x | -2.43     |
|    mean velocity y | -3.16     |
|    mean velocity z | 18.2      |
|    mean_ep_length  | 65        |
|    mean_reward     | -5.53e+04 |
| time/              |           |
|    total_timesteps | 255500    |
----------------------------------
Eval num_timesteps=256000, episode_reward=-81500.31 +/- 44348.50
Episode length: 65.00 +/- 29.41
------------------------------------
| eval/              |             |
|    mean action     | -0.24931228 |
|    mean velocity x | -0.798      |
|    mean velocity y | 0.167       |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 65          |
|    mean_reward     | -8.15e+04   |
| time/              |             |
|    total_timesteps | 256000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 120       |
|    ep_rew_mean     | -1.28e+05 |
| time/              |           |
|    fps             | 54        |
|    iterations      | 125       |
|    time_elapsed    | 4715      |
|    total_timesteps | 256000    |
----------------------------------
Eval num_timesteps=256500, episode_reward=-114345.69 +/- 75528.44
Episode length: 123.80 +/- 94.22
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.8135924  |
|    mean velocity x      | 4.11        |
|    mean velocity y      | 6.72        |
|    mean velocity z      | 20.4        |
|    mean_ep_length       | 124         |
|    mean_reward          | -1.14e+05   |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.001411244 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.0598      |
|    learning_rate        | 0.001       |
|    loss                 | 1.43e+08    |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 0.953       |
|    value_loss           | 2.35e+08    |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=-102509.13 +/- 101175.97
Episode length: 122.00 +/- 111.56
-------------------------------------
| eval/              |              |
|    mean action     | -0.014665268 |
|    mean velocity x | -0.353       |
|    mean velocity y | -0.271       |
|    mean velocity z | 18.6         |
|    mean_ep_length  | 122          |
|    mean_reward     | -1.03e+05    |
| time/              |              |
|    total_timesteps | 257000       |
-------------------------------------
Eval num_timesteps=257500, episode_reward=-60632.04 +/- 45910.39
Episode length: 57.40 +/- 25.24
--------------------------------------
| eval/              |               |
|    mean action     | -0.0045114057 |
|    mean velocity x | -1.17         |
|    mean velocity y | -1.91         |
|    mean velocity z | 21.1          |
|    mean_ep_length  | 57.4          |
|    mean_reward     | -6.06e+04     |
| time/              |               |
|    total_timesteps | 257500        |
--------------------------------------
Eval num_timesteps=258000, episode_reward=-62278.01 +/- 30450.64
Episode length: 71.40 +/- 15.55
------------------------------------
| eval/              |             |
|    mean action     | -0.18178882 |
|    mean velocity x | -0.741      |
|    mean velocity y | -0.366      |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 71.4        |
|    mean_reward     | -6.23e+04   |
| time/              |             |
|    total_timesteps | 258000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | -1.3e+05 |
| time/              |          |
|    fps             | 54       |
|    iterations      | 126      |
|    time_elapsed    | 4722     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-80532.60 +/- 27298.38
Episode length: 70.80 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean action          | -0.112821214 |
|    mean velocity x      | 0.0353       |
|    mean velocity y      | -0.464       |
|    mean velocity z      | 20.5         |
|    mean_ep_length       | 70.8         |
|    mean_reward          | -8.05e+04    |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0008935228 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0433       |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+08     |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.000992    |
|    std                  | 0.952        |
|    value_loss           | 2.87e+08     |
------------------------------------------
Eval num_timesteps=259000, episode_reward=-74221.46 +/- 25218.47
Episode length: 71.20 +/- 10.93
------------------------------------
| eval/              |             |
|    mean action     | -0.24743369 |
|    mean velocity x | 1.11        |
|    mean velocity y | 1.62        |
|    mean velocity z | 15.9        |
|    mean_ep_length  | 71.2        |
|    mean_reward     | -7.42e+04   |
| time/              |             |
|    total_timesteps | 259000      |
------------------------------------
Eval num_timesteps=259500, episode_reward=-92110.78 +/- 44651.24
Episode length: 69.80 +/- 14.73
----------------------------------
| eval/              |           |
|    mean action     | 0.3411365 |
|    mean velocity x | -0.919    |
|    mean velocity y | -2.37     |
|    mean velocity z | 15.2      |
|    mean_ep_length  | 69.8      |
|    mean_reward     | -9.21e+04 |
| time/              |           |
|    total_timesteps | 259500    |
----------------------------------
Eval num_timesteps=260000, episode_reward=-72435.91 +/- 24769.71
Episode length: 80.40 +/- 15.87
------------------------------------
| eval/              |             |
|    mean action     | -0.56648886 |
|    mean velocity x | 2.41        |
|    mean velocity y | 4.56        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 80.4        |
|    mean_reward     | -7.24e+04   |
| time/              |             |
|    total_timesteps | 260000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 113       |
|    ep_rew_mean     | -1.19e+05 |
| time/              |           |
|    fps             | 54        |
|    iterations      | 127       |
|    time_elapsed    | 4729      |
|    total_timesteps | 260096    |
----------------------------------
Eval num_timesteps=260500, episode_reward=-87151.54 +/- 46097.11
Episode length: 63.80 +/- 24.85
------------------------------------------
| eval/                   |              |
|    mean action          | 0.29041564   |
|    mean velocity x      | -1.56        |
|    mean velocity y      | -2.44        |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 63.8         |
|    mean_reward          | -8.72e+04    |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0024181644 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0747       |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+08     |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 0.953        |
|    value_loss           | 2.11e+08     |
------------------------------------------
Eval num_timesteps=261000, episode_reward=-73973.60 +/- 45192.18
Episode length: 64.20 +/- 16.02
------------------------------------
| eval/              |             |
|    mean action     | -0.40405884 |
|    mean velocity x | 1.06        |
|    mean velocity y | 3.26        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 64.2        |
|    mean_reward     | -7.4e+04    |
| time/              |             |
|    total_timesteps | 261000      |
------------------------------------
Eval num_timesteps=261500, episode_reward=-90688.54 +/- 20970.53
Episode length: 89.00 +/- 15.17
------------------------------------
| eval/              |             |
|    mean action     | -0.60667956 |
|    mean velocity x | 3.41        |
|    mean velocity y | 5.04        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 89          |
|    mean_reward     | -9.07e+04   |
| time/              |             |
|    total_timesteps | 261500      |
------------------------------------
Eval num_timesteps=262000, episode_reward=-76822.15 +/- 46538.54
Episode length: 64.40 +/- 24.25
------------------------------------
| eval/              |             |
|    mean action     | -0.87257504 |
|    mean velocity x | 5.27        |
|    mean velocity y | 7.62        |
|    mean velocity z | 21          |
|    mean_ep_length  | 64.4        |
|    mean_reward     | -7.68e+04   |
| time/              |             |
|    total_timesteps | 262000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | -1.2e+05 |
| time/              |          |
|    fps             | 55       |
|    iterations      | 128      |
|    time_elapsed    | 4736     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-117327.36 +/- 112918.75
Episode length: 155.20 +/- 130.76
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.40641508 |
|    mean velocity x      | 0.562       |
|    mean velocity y      | 2.42        |
|    mean velocity z      | 19.4        |
|    mean_ep_length       | 155         |
|    mean_reward          | -1.17e+05   |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.002796012 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.0774      |
|    learning_rate        | 0.001       |
|    loss                 | 1.33e+08    |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00171    |
|    std                  | 0.952       |
|    value_loss           | 2.36e+08    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=-82651.15 +/- 32439.11
Episode length: 77.20 +/- 10.46
------------------------------------
| eval/              |             |
|    mean action     | -0.22747335 |
|    mean velocity x | 0.111       |
|    mean velocity y | 0.741       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 77.2        |
|    mean_reward     | -8.27e+04   |
| time/              |             |
|    total_timesteps | 263000      |
------------------------------------
Eval num_timesteps=263500, episode_reward=-74842.13 +/- 38691.31
Episode length: 78.40 +/- 26.06
-----------------------------------
| eval/              |            |
|    mean action     | -0.8986903 |
|    mean velocity x | 3.08       |
|    mean velocity y | 5.49       |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 78.4       |
|    mean_reward     | -7.48e+04  |
| time/              |            |
|    total_timesteps | 263500     |
-----------------------------------
Eval num_timesteps=264000, episode_reward=-44376.42 +/- 33573.01
Episode length: 61.80 +/- 27.67
-----------------------------------
| eval/              |            |
|    mean action     | -0.4992472 |
|    mean velocity x | 0.934      |
|    mean velocity y | 3.05       |
|    mean velocity z | 18         |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -4.44e+04  |
| time/              |            |
|    total_timesteps | 264000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 115       |
|    ep_rew_mean     | -1.21e+05 |
| time/              |           |
|    fps             | 55        |
|    iterations      | 129       |
|    time_elapsed    | 4743      |
|    total_timesteps | 264192    |
----------------------------------
Eval num_timesteps=264500, episode_reward=-65867.21 +/- 33681.35
Episode length: 77.20 +/- 29.18
------------------------------------------
| eval/                   |              |
|    mean action          | 0.06375849   |
|    mean velocity x      | -2.51        |
|    mean velocity y      | -1.61        |
|    mean velocity z      | 20.4         |
|    mean_ep_length       | 77.2         |
|    mean_reward          | -6.59e+04    |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0039409185 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0702       |
|    learning_rate        | 0.001        |
|    loss                 | 9.17e+07     |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00284     |
|    std                  | 0.953        |
|    value_loss           | 2.47e+08     |
------------------------------------------
Eval num_timesteps=265000, episode_reward=-83817.38 +/- 43949.82
Episode length: 68.00 +/- 28.77
------------------------------------
| eval/              |             |
|    mean action     | -0.37843925 |
|    mean velocity x | 1.39        |
|    mean velocity y | 2.82        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 68          |
|    mean_reward     | -8.38e+04   |
| time/              |             |
|    total_timesteps | 265000      |
------------------------------------
Eval num_timesteps=265500, episode_reward=-156058.64 +/- 70421.47
Episode length: 159.60 +/- 114.31
------------------------------------
| eval/              |             |
|    mean action     | -0.44538134 |
|    mean velocity x | 1.38        |
|    mean velocity y | 3.21        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 160         |
|    mean_reward     | -1.56e+05   |
| time/              |             |
|    total_timesteps | 265500      |
------------------------------------
Eval num_timesteps=266000, episode_reward=-74207.25 +/- 43495.43
Episode length: 75.60 +/- 26.30
-----------------------------------
| eval/              |            |
|    mean action     | -0.4514782 |
|    mean velocity x | 2.65       |
|    mean velocity y | 4.23       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 75.6       |
|    mean_reward     | -7.42e+04  |
| time/              |            |
|    total_timesteps | 266000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 108       |
|    ep_rew_mean     | -1.18e+05 |
| time/              |           |
|    fps             | 56        |
|    iterations      | 130       |
|    time_elapsed    | 4751      |
|    total_timesteps | 266240    |
----------------------------------
Eval num_timesteps=266500, episode_reward=-43667.79 +/- 47693.24
Episode length: 51.00 +/- 35.43
------------------------------------------
| eval/                   |              |
|    mean action          | 0.30962762   |
|    mean velocity x      | 0.438        |
|    mean velocity y      | -1.01        |
|    mean velocity z      | 19.8         |
|    mean_ep_length       | 51           |
|    mean_reward          | -4.37e+04    |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0011637879 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0598       |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+08     |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.953        |
|    value_loss           | 3.48e+08     |
------------------------------------------
Eval num_timesteps=267000, episode_reward=-110753.68 +/- 27601.99
Episode length: 116.60 +/- 42.10
------------------------------------
| eval/              |             |
|    mean action     | -0.13870761 |
|    mean velocity x | 0.508       |
|    mean velocity y | 0.927       |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 117         |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 267000      |
------------------------------------
Eval num_timesteps=267500, episode_reward=-107418.37 +/- 64945.30
Episode length: 117.40 +/- 94.83
------------------------------------
| eval/              |             |
|    mean action     | 0.008386028 |
|    mean velocity x | 0.868       |
|    mean velocity y | 1.79        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 117         |
|    mean_reward     | -1.07e+05   |
| time/              |             |
|    total_timesteps | 267500      |
------------------------------------
Eval num_timesteps=268000, episode_reward=-98920.35 +/- 44623.30
Episode length: 91.00 +/- 40.44
-------------------------------------
| eval/              |              |
|    mean action     | -0.101253465 |
|    mean velocity x | -1.26        |
|    mean velocity y | 0.558        |
|    mean velocity z | 18.4         |
|    mean_ep_length  | 91           |
|    mean_reward     | -9.89e+04    |
| time/              |              |
|    total_timesteps | 268000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 105       |
|    ep_rew_mean     | -1.11e+05 |
| time/              |           |
|    fps             | 56        |
|    iterations      | 131       |
|    time_elapsed    | 4758      |
|    total_timesteps | 268288    |
----------------------------------
Eval num_timesteps=268500, episode_reward=-78036.02 +/- 24552.45
Episode length: 73.40 +/- 8.96
------------------------------------------
| eval/                   |              |
|    mean action          | 0.0098835835 |
|    mean velocity x      | 0.51         |
|    mean velocity y      | 1.04         |
|    mean velocity z      | 18           |
|    mean_ep_length       | 73.4         |
|    mean_reward          | -7.8e+04     |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0009246152 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.042        |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+08     |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.952        |
|    value_loss           | 2.55e+08     |
------------------------------------------
Eval num_timesteps=269000, episode_reward=-73774.65 +/- 6686.13
Episode length: 78.80 +/- 7.28
------------------------------------
| eval/              |             |
|    mean action     | -0.20909588 |
|    mean velocity x | 0.631       |
|    mean velocity y | 0.919       |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 78.8        |
|    mean_reward     | -7.38e+04   |
| time/              |             |
|    total_timesteps | 269000      |
------------------------------------
Eval num_timesteps=269500, episode_reward=-99092.06 +/- 55800.59
Episode length: 92.40 +/- 44.33
------------------------------------
| eval/              |             |
|    mean action     | -0.33580816 |
|    mean velocity x | 1.73        |
|    mean velocity y | 1.99        |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 92.4        |
|    mean_reward     | -9.91e+04   |
| time/              |             |
|    total_timesteps | 269500      |
------------------------------------
Eval num_timesteps=270000, episode_reward=-77929.39 +/- 40539.04
Episode length: 87.20 +/- 39.35
------------------------------------
| eval/              |             |
|    mean action     | -0.05772937 |
|    mean velocity x | 1.89        |
|    mean velocity y | 0.18        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 87.2        |
|    mean_reward     | -7.79e+04   |
| time/              |             |
|    total_timesteps | 270000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 107       |
|    ep_rew_mean     | -1.15e+05 |
| time/              |           |
|    fps             | 56        |
|    iterations      | 132       |
|    time_elapsed    | 4766      |
|    total_timesteps | 270336    |
----------------------------------
Eval num_timesteps=270500, episode_reward=-78522.20 +/- 45844.21
Episode length: 71.40 +/- 12.61
------------------------------------------
| eval/                   |              |
|    mean action          | -0.7406381   |
|    mean velocity x      | 5.07         |
|    mean velocity y      | 7.15         |
|    mean velocity z      | 20.8         |
|    mean_ep_length       | 71.4         |
|    mean_reward          | -7.85e+04    |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0027828333 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0351       |
|    learning_rate        | 0.001        |
|    loss                 | 1.81e+08     |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.951        |
|    value_loss           | 2.41e+08     |
------------------------------------------
Eval num_timesteps=271000, episode_reward=-111037.34 +/- 44522.15
Episode length: 99.80 +/- 40.71
------------------------------------
| eval/              |             |
|    mean action     | -0.81755006 |
|    mean velocity x | 4.12        |
|    mean velocity y | 6.95        |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 99.8        |
|    mean_reward     | -1.11e+05   |
| time/              |             |
|    total_timesteps | 271000      |
------------------------------------
Eval num_timesteps=271500, episode_reward=-186026.26 +/- 138354.96
Episode length: 215.40 +/- 189.30
------------------------------------
| eval/              |             |
|    mean action     | -0.33252087 |
|    mean velocity x | 3.09        |
|    mean velocity y | 4.13        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 215         |
|    mean_reward     | -1.86e+05   |
| time/              |             |
|    total_timesteps | 271500      |
------------------------------------
Eval num_timesteps=272000, episode_reward=-87818.94 +/- 15436.78
Episode length: 78.80 +/- 4.79
------------------------------------
| eval/              |             |
|    mean action     | -0.61982006 |
|    mean velocity x | 4.5         |
|    mean velocity y | 5.11        |
|    mean velocity z | 16.8        |
|    mean_ep_length  | 78.8        |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 272000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 109       |
|    ep_rew_mean     | -1.15e+05 |
| time/              |           |
|    fps             | 57        |
|    iterations      | 133       |
|    time_elapsed    | 4776      |
|    total_timesteps | 272384    |
----------------------------------
Eval num_timesteps=272500, episode_reward=-69339.59 +/- 34221.20
Episode length: 67.60 +/- 18.60
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.76178724   |
|    mean velocity x      | 4.2           |
|    mean velocity y      | 5.71          |
|    mean velocity z      | 17.9          |
|    mean_ep_length       | 67.6          |
|    mean_reward          | -6.93e+04     |
| time/                   |               |
|    total_timesteps      | 272500        |
| train/                  |               |
|    approx_kl            | 0.00031327983 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.11         |
|    explained_variance   | 0.0707        |
|    learning_rate        | 0.001         |
|    loss                 | 7.68e+07      |
|    n_updates            | 1330          |
|    policy_gradient_loss | -0.000858     |
|    std                  | 0.952         |
|    value_loss           | 1.95e+08      |
-------------------------------------------
Eval num_timesteps=273000, episode_reward=-201582.24 +/- 262415.30
Episode length: 226.60 +/- 309.02
------------------------------------
| eval/              |             |
|    mean action     | -0.26418087 |
|    mean velocity x | -0.226      |
|    mean velocity y | 0.855       |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 227         |
|    mean_reward     | -2.02e+05   |
| time/              |             |
|    total_timesteps | 273000      |
------------------------------------
Eval num_timesteps=273500, episode_reward=-86118.90 +/- 89887.13
Episode length: 99.00 +/- 95.48
-----------------------------------
| eval/              |            |
|    mean action     | -0.6949182 |
|    mean velocity x | 2.74       |
|    mean velocity y | 4.97       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 99         |
|    mean_reward     | -8.61e+04  |
| time/              |            |
|    total_timesteps | 273500     |
-----------------------------------
Eval num_timesteps=274000, episode_reward=-81210.91 +/- 42799.65
Episode length: 65.40 +/- 26.11
------------------------------------
| eval/              |             |
|    mean action     | -0.19712614 |
|    mean velocity x | 0.22        |
|    mean velocity y | 1.22        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 274000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 105       |
|    ep_rew_mean     | -1.11e+05 |
| time/              |           |
|    fps             | 57        |
|    iterations      | 134       |
|    time_elapsed    | 4783      |
|    total_timesteps | 274432    |
----------------------------------
Eval num_timesteps=274500, episode_reward=-171138.58 +/- 210805.75
Episode length: 194.20 +/- 253.19
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.023694625   |
|    mean velocity x      | 1.38          |
|    mean velocity y      | 1.3           |
|    mean velocity z      | 20            |
|    mean_ep_length       | 194           |
|    mean_reward          | -1.71e+05     |
| time/                   |               |
|    total_timesteps      | 274500        |
| train/                  |               |
|    approx_kl            | 0.00085031317 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.11         |
|    explained_variance   | 0.0527        |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+08      |
|    n_updates            | 1340          |
|    policy_gradient_loss | -0.00146      |
|    std                  | 0.952         |
|    value_loss           | 2.56e+08      |
-------------------------------------------
Eval num_timesteps=275000, episode_reward=-93848.89 +/- 5987.37
Episode length: 78.20 +/- 5.23
------------------------------------
| eval/              |             |
|    mean action     | -0.30157802 |
|    mean velocity x | 1.46        |
|    mean velocity y | 2.37        |
|    mean velocity z | 20          |
|    mean_ep_length  | 78.2        |
|    mean_reward     | -9.38e+04   |
| time/              |             |
|    total_timesteps | 275000      |
------------------------------------
Eval num_timesteps=275500, episode_reward=-70759.13 +/- 34383.21
Episode length: 68.80 +/- 13.14
------------------------------------
| eval/              |             |
|    mean action     | 0.044044282 |
|    mean velocity x | 0.306       |
|    mean velocity y | 0.765       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 68.8        |
|    mean_reward     | -7.08e+04   |
| time/              |             |
|    total_timesteps | 275500      |
------------------------------------
Eval num_timesteps=276000, episode_reward=-88777.60 +/- 28674.22
Episode length: 95.20 +/- 48.92
------------------------------------
| eval/              |             |
|    mean action     | 0.024723595 |
|    mean velocity x | -0.93       |
|    mean velocity y | -0.782      |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 95.2        |
|    mean_reward     | -8.88e+04   |
| time/              |             |
|    total_timesteps | 276000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 106       |
|    ep_rew_mean     | -1.11e+05 |
| time/              |           |
|    fps             | 57        |
|    iterations      | 135       |
|    time_elapsed    | 4791      |
|    total_timesteps | 276480    |
----------------------------------
Eval num_timesteps=276500, episode_reward=-49671.35 +/- 44459.00
Episode length: 55.40 +/- 25.08
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3595596    |
|    mean velocity x      | 1             |
|    mean velocity y      | 2.94          |
|    mean velocity z      | 22.2          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -4.97e+04     |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 0.00059789466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.11         |
|    explained_variance   | 0.0436        |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+08      |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000827     |
|    std                  | 0.953         |
|    value_loss           | 2.76e+08      |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=-72808.63 +/- 30766.89
Episode length: 76.60 +/- 24.89
-------------------------------------
| eval/              |              |
|    mean action     | -0.067770675 |
|    mean velocity x | 0.22         |
|    mean velocity y | -0.323       |
|    mean velocity z | 18.7         |
|    mean_ep_length  | 76.6         |
|    mean_reward     | -7.28e+04    |
| time/              |              |
|    total_timesteps | 277000       |
-------------------------------------
Eval num_timesteps=277500, episode_reward=-78818.24 +/- 40045.35
Episode length: 80.20 +/- 38.27
----------------------------------
| eval/              |           |
|    mean action     | 0.3284475 |
|    mean velocity x | 0.131     |
|    mean velocity y | -0.782    |
|    mean velocity z | 20.2      |
|    mean_ep_length  | 80.2      |
|    mean_reward     | -7.88e+04 |
| time/              |           |
|    total_timesteps | 277500    |
----------------------------------
Eval num_timesteps=278000, episode_reward=-75532.95 +/- 14884.44
Episode length: 81.60 +/- 20.54
------------------------------------
| eval/              |             |
|    mean action     | 0.021188354 |
|    mean velocity x | -0.0698     |
|    mean velocity y | -0.705      |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 81.6        |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 278000      |
------------------------------------
Eval num_timesteps=278500, episode_reward=-92644.07 +/- 31163.40
Episode length: 71.00 +/- 6.84
-------------------------------------
| eval/              |              |
|    mean action     | -0.087801084 |
|    mean velocity x | 1.4          |
|    mean velocity y | 1.49         |
|    mean velocity z | 18           |
|    mean_ep_length  | 71           |
|    mean_reward     | -9.26e+04    |
| time/              |              |
|    total_timesteps | 278500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 107       |
|    ep_rew_mean     | -1.13e+05 |
| time/              |           |
|    fps             | 58        |
|    iterations      | 136       |
|    time_elapsed    | 4799      |
|    total_timesteps | 278528    |
----------------------------------
Eval num_timesteps=279000, episode_reward=-74163.49 +/- 26516.79
Episode length: 113.80 +/- 41.24
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20449515  |
|    mean velocity x      | 1.57         |
|    mean velocity y      | 1.51         |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 114          |
|    mean_reward          | -7.42e+04    |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0004931288 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0547       |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+08     |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 0.953        |
|    value_loss           | 2.71e+08     |
------------------------------------------
Eval num_timesteps=279500, episode_reward=-56716.59 +/- 35053.35
Episode length: 64.80 +/- 18.72
------------------------------------
| eval/              |             |
|    mean action     | -0.34203884 |
|    mean velocity x | 1.1         |
|    mean velocity y | 2.99        |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -5.67e+04   |
| time/              |             |
|    total_timesteps | 279500      |
------------------------------------
Eval num_timesteps=280000, episode_reward=-80950.51 +/- 37009.34
Episode length: 68.00 +/- 12.55
-----------------------------------
| eval/              |            |
|    mean action     | -0.6566748 |
|    mean velocity x | 3.28       |
|    mean velocity y | 6.1        |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 68         |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 280000     |
-----------------------------------
Eval num_timesteps=280500, episode_reward=-96996.32 +/- 20593.21
Episode length: 80.60 +/- 14.44
------------------------------------
| eval/              |             |
|    mean action     | -0.21850248 |
|    mean velocity x | 1.6         |
|    mean velocity y | 1.5         |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 80.6        |
|    mean_reward     | -9.7e+04    |
| time/              |             |
|    total_timesteps | 280500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 107       |
|    ep_rew_mean     | -1.16e+05 |
| time/              |           |
|    fps             | 58        |
|    iterations      | 137       |
|    time_elapsed    | 4806      |
|    total_timesteps | 280576    |
----------------------------------
Eval num_timesteps=281000, episode_reward=-63095.87 +/- 24059.53
Episode length: 69.80 +/- 15.25
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5626247   |
|    mean velocity x      | 3.67         |
|    mean velocity y      | 5.95         |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 69.8         |
|    mean_reward          | -6.31e+04    |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0010608186 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0583       |
|    learning_rate        | 0.001        |
|    loss                 | 1.56e+08     |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 0.955        |
|    value_loss           | 3.12e+08     |
------------------------------------------
Eval num_timesteps=281500, episode_reward=-120249.50 +/- 72384.43
Episode length: 120.80 +/- 93.61
------------------------------------
| eval/              |             |
|    mean action     | -0.40989172 |
|    mean velocity x | 2.26        |
|    mean velocity y | 2.86        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 121         |
|    mean_reward     | -1.2e+05    |
| time/              |             |
|    total_timesteps | 281500      |
------------------------------------
Eval num_timesteps=282000, episode_reward=-123935.82 +/- 66378.08
Episode length: 130.60 +/- 58.54
-----------------------------------
| eval/              |            |
|    mean action     | -0.3106105 |
|    mean velocity x | 1.34       |
|    mean velocity y | 1.49       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 131        |
|    mean_reward     | -1.24e+05  |
| time/              |            |
|    total_timesteps | 282000     |
-----------------------------------
Eval num_timesteps=282500, episode_reward=-74801.79 +/- 26744.39
Episode length: 71.60 +/- 7.12
-------------------------------------
| eval/              |              |
|    mean action     | -0.054095335 |
|    mean velocity x | -0.845       |
|    mean velocity y | -0.607       |
|    mean velocity z | 19.7         |
|    mean_ep_length  | 71.6         |
|    mean_reward     | -7.48e+04    |
| time/              |              |
|    total_timesteps | 282500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 102       |
|    ep_rew_mean     | -1.13e+05 |
| time/              |           |
|    fps             | 58        |
|    iterations      | 138       |
|    time_elapsed    | 4813      |
|    total_timesteps | 282624    |
----------------------------------
Eval num_timesteps=283000, episode_reward=-44707.02 +/- 23559.79
Episode length: 66.40 +/- 15.07
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2026587    |
|    mean velocity x      | 1.53         |
|    mean velocity y      | 0.54         |
|    mean velocity z      | 17.5         |
|    mean_ep_length       | 66.4         |
|    mean_reward          | -4.47e+04    |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0017692829 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.0758       |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+08     |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 0.955        |
|    value_loss           | 2.32e+08     |
------------------------------------------
Eval num_timesteps=283500, episode_reward=-83519.46 +/- 17882.39
Episode length: 70.60 +/- 4.63
------------------------------------
| eval/              |             |
|    mean action     | -0.06923395 |
|    mean velocity x | 1.37        |
|    mean velocity y | 1.18        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 70.6        |
|    mean_reward     | -8.35e+04   |
| time/              |             |
|    total_timesteps | 283500      |
------------------------------------
Eval num_timesteps=284000, episode_reward=-49467.64 +/- 34957.90
Episode length: 59.00 +/- 25.94
-------------------------------------
| eval/              |              |
|    mean action     | -0.004397705 |
|    mean velocity x | -1.67        |
|    mean velocity y | -1.5         |
|    mean velocity z | 21.4         |
|    mean_ep_length  | 59           |
|    mean_reward     | -4.95e+04    |
| time/              |              |
|    total_timesteps | 284000       |
-------------------------------------
Eval num_timesteps=284500, episode_reward=-82162.83 +/- 20900.89
Episode length: 82.60 +/- 27.51
------------------------------------
| eval/              |             |
|    mean action     | -0.41256627 |
|    mean velocity x | 2.06        |
|    mean velocity y | 4.36        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 82.6        |
|    mean_reward     | -8.22e+04   |
| time/              |             |
|    total_timesteps | 284500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 102       |
|    ep_rew_mean     | -1.13e+05 |
| time/              |           |
|    fps             | 59        |
|    iterations      | 139       |
|    time_elapsed    | 4820      |
|    total_timesteps | 284672    |
----------------------------------
Eval num_timesteps=285000, episode_reward=-93008.36 +/- 21267.36
Episode length: 76.40 +/- 5.75
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4350395   |
|    mean velocity x      | 2.78         |
|    mean velocity y      | 4.34         |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 76.4         |
|    mean_reward          | -9.3e+04     |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0011554307 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0593       |
|    learning_rate        | 0.001        |
|    loss                 | 8.1e+07      |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.954        |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=285500, episode_reward=-81858.62 +/- 20850.38
Episode length: 74.20 +/- 4.79
-----------------------------------
| eval/              |            |
|    mean action     | -0.7073579 |
|    mean velocity x | 1.91       |
|    mean velocity y | 4.48       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 74.2       |
|    mean_reward     | -8.19e+04  |
| time/              |            |
|    total_timesteps | 285500     |
-----------------------------------
Eval num_timesteps=286000, episode_reward=-66190.66 +/- 36198.84
Episode length: 58.40 +/- 22.93
------------------------------------
| eval/              |             |
|    mean action     | -0.31748906 |
|    mean velocity x | 1.81        |
|    mean velocity y | 3.36        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -6.62e+04   |
| time/              |             |
|    total_timesteps | 286000      |
------------------------------------
Eval num_timesteps=286500, episode_reward=-71885.10 +/- 32616.58
Episode length: 71.00 +/- 15.24
-----------------------------------
| eval/              |            |
|    mean action     | -0.1538333 |
|    mean velocity x | 1.4        |
|    mean velocity y | 1.79       |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 71         |
|    mean_reward     | -7.19e+04  |
| time/              |            |
|    total_timesteps | 286500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 102       |
|    ep_rew_mean     | -1.11e+05 |
| time/              |           |
|    fps             | 59        |
|    iterations      | 140       |
|    time_elapsed    | 4827      |
|    total_timesteps | 286720    |
----------------------------------
Eval num_timesteps=287000, episode_reward=-75859.81 +/- 34369.81
Episode length: 83.40 +/- 13.81
------------------------------------------
| eval/                   |              |
|    mean action          | 0.04105027   |
|    mean velocity x      | -1.86        |
|    mean velocity y      | -2.05        |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 83.4         |
|    mean_reward          | -7.59e+04    |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0022130758 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0668       |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00392     |
|    std                  | 0.953        |
|    value_loss           | 2.32e+08     |
------------------------------------------
Eval num_timesteps=287500, episode_reward=-89553.24 +/- 16265.40
Episode length: 89.20 +/- 24.95
------------------------------------
| eval/              |             |
|    mean action     | -0.11805223 |
|    mean velocity x | -0.12       |
|    mean velocity y | 0.194       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 89.2        |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 287500      |
------------------------------------
Eval num_timesteps=288000, episode_reward=-81139.36 +/- 47541.22
Episode length: 90.20 +/- 39.22
------------------------------------
| eval/              |             |
|    mean action     | -0.22565082 |
|    mean velocity x | 0.912       |
|    mean velocity y | 2.06        |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 90.2        |
|    mean_reward     | -8.11e+04   |
| time/              |             |
|    total_timesteps | 288000      |
------------------------------------
Eval num_timesteps=288500, episode_reward=-66422.75 +/- 34446.36
Episode length: 67.40 +/- 10.38
-----------------------------------
| eval/              |            |
|    mean action     | 0.10927582 |
|    mean velocity x | -0.2       |
|    mean velocity y | -1.22      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 67.4       |
|    mean_reward     | -6.64e+04  |
| time/              |            |
|    total_timesteps | 288500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99        |
|    ep_rew_mean     | -1.08e+05 |
| time/              |           |
|    fps             | 59        |
|    iterations      | 141       |
|    time_elapsed    | 4835      |
|    total_timesteps | 288768    |
----------------------------------
Eval num_timesteps=289000, episode_reward=-71819.88 +/- 41488.38
Episode length: 58.80 +/- 23.40
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4261325   |
|    mean velocity x      | 1.67         |
|    mean velocity y      | 2.93         |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 58.8         |
|    mean_reward          | -7.18e+04    |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0024718442 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.0611       |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+08     |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.954        |
|    value_loss           | 2.98e+08     |
------------------------------------------
Eval num_timesteps=289500, episode_reward=-28280.65 +/- 31803.63
Episode length: 50.40 +/- 25.38
------------------------------------
| eval/              |             |
|    mean action     | -0.07174445 |
|    mean velocity x | -0.991      |
|    mean velocity y | -0.888      |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 50.4        |
|    mean_reward     | -2.83e+04   |
| time/              |             |
|    total_timesteps | 289500      |
------------------------------------
New best mean reward!
Eval num_timesteps=290000, episode_reward=-91257.26 +/- 16439.69
Episode length: 85.00 +/- 14.13
------------------------------------
| eval/              |             |
|    mean action     | 0.031821776 |
|    mean velocity x | 1.7         |
|    mean velocity y | 0.824       |
|    mean velocity z | 17          |
|    mean_ep_length  | 85          |
|    mean_reward     | -9.13e+04   |
| time/              |             |
|    total_timesteps | 290000      |
------------------------------------
Eval num_timesteps=290500, episode_reward=-77835.44 +/- 35488.83
Episode length: 93.60 +/- 31.28
------------------------------------
| eval/              |             |
|    mean action     | -0.34729657 |
|    mean velocity x | 1.22        |
|    mean velocity y | 2.09        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 93.6        |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 290500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.1      |
|    ep_rew_mean     | -9.86e+04 |
| time/              |           |
|    fps             | 60        |
|    iterations      | 142       |
|    time_elapsed    | 4842      |
|    total_timesteps | 290816    |
----------------------------------
Eval num_timesteps=291000, episode_reward=-85206.99 +/- 17045.32
Episode length: 76.20 +/- 10.17
------------------------------------------
| eval/                   |              |
|    mean action          | -0.750465    |
|    mean velocity x      | 3.46         |
|    mean velocity y      | 5.47         |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 76.2         |
|    mean_reward          | -8.52e+04    |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0039194776 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0582       |
|    learning_rate        | 0.001        |
|    loss                 | 6.69e+07     |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00366     |
|    std                  | 0.949        |
|    value_loss           | 2.05e+08     |
------------------------------------------
Eval num_timesteps=291500, episode_reward=-78417.67 +/- 36603.05
Episode length: 84.40 +/- 35.25
-----------------------------------
| eval/              |            |
|    mean action     | -0.7892373 |
|    mean velocity x | 3.25       |
|    mean velocity y | 5.92       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 84.4       |
|    mean_reward     | -7.84e+04  |
| time/              |            |
|    total_timesteps | 291500     |
-----------------------------------
Eval num_timesteps=292000, episode_reward=-113379.14 +/- 62990.81
Episode length: 108.80 +/- 70.21
------------------------------------
| eval/              |             |
|    mean action     | -0.11309987 |
|    mean velocity x | 0.247       |
|    mean velocity y | -0.0717     |
|    mean velocity z | 18          |
|    mean_ep_length  | 109         |
|    mean_reward     | -1.13e+05   |
| time/              |             |
|    total_timesteps | 292000      |
------------------------------------
Eval num_timesteps=292500, episode_reward=-36473.08 +/- 30769.63
Episode length: 48.60 +/- 34.68
------------------------------------
| eval/              |             |
|    mean action     | -0.53118265 |
|    mean velocity x | 1.57        |
|    mean velocity y | 3.53        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -3.65e+04   |
| time/              |             |
|    total_timesteps | 292500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.6      |
|    ep_rew_mean     | -9.98e+04 |
| time/              |           |
|    fps             | 60        |
|    iterations      | 143       |
|    time_elapsed    | 4849      |
|    total_timesteps | 292864    |
----------------------------------
Eval num_timesteps=293000, episode_reward=-109022.05 +/- 45035.75
Episode length: 106.80 +/- 70.79
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.7723612  |
|    mean velocity x      | 4.17        |
|    mean velocity y      | 6.49        |
|    mean velocity z      | 16.5        |
|    mean_ep_length       | 107         |
|    mean_reward          | -1.09e+05   |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.002352662 |
|    clip_fraction        | 0.00518     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.0754      |
|    learning_rate        | 0.001       |
|    loss                 | 1.3e+08     |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00208    |
|    std                  | 0.95        |
|    value_loss           | 2.27e+08    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=-91313.29 +/- 13529.83
Episode length: 91.40 +/- 21.57
-------------------------------------
| eval/              |              |
|    mean action     | -0.027903229 |
|    mean velocity x | -0.299       |
|    mean velocity y | -0.695       |
|    mean velocity z | 16           |
|    mean_ep_length  | 91.4         |
|    mean_reward     | -9.13e+04    |
| time/              |              |
|    total_timesteps | 293500       |
-------------------------------------
Eval num_timesteps=294000, episode_reward=-112154.07 +/- 86869.10
Episode length: 109.00 +/- 105.79
-----------------------------------
| eval/              |            |
|    mean action     | 0.29191405 |
|    mean velocity x | -1.95      |
|    mean velocity y | -2.36      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 109        |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 294000     |
-----------------------------------
Eval num_timesteps=294500, episode_reward=-80605.32 +/- 12312.45
Episode length: 90.20 +/- 26.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.08979024 |
|    mean velocity x | 0.516      |
|    mean velocity y | -0.0212    |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 90.2       |
|    mean_reward     | -8.06e+04  |
| time/              |            |
|    total_timesteps | 294500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 89.2      |
|    ep_rew_mean     | -9.12e+04 |
| time/              |           |
|    fps             | 60        |
|    iterations      | 144       |
|    time_elapsed    | 4857      |
|    total_timesteps | 294912    |
----------------------------------
Eval num_timesteps=295000, episode_reward=-85392.93 +/- 19620.40
Episode length: 75.00 +/- 7.07
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.7022196   |
|    mean velocity x      | -3.26       |
|    mean velocity y      | -5.2        |
|    mean velocity z      | 15.8        |
|    mean_ep_length       | 75          |
|    mean_reward          | -8.54e+04   |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.002237612 |
|    clip_fraction        | 0.00225     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.076       |
|    learning_rate        | 0.001       |
|    loss                 | 8.82e+07    |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00237    |
|    std                  | 0.95        |
|    value_loss           | 2.02e+08    |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=-34449.41 +/- 29954.50
Episode length: 57.60 +/- 31.83
------------------------------------
| eval/              |             |
|    mean action     | 0.093484975 |
|    mean velocity x | 0.0399      |
|    mean velocity y | -1.17       |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -3.44e+04   |
| time/              |             |
|    total_timesteps | 295500      |
------------------------------------
Eval num_timesteps=296000, episode_reward=-109670.52 +/- 19912.65
Episode length: 100.20 +/- 33.26
------------------------------------
| eval/              |             |
|    mean action     | -0.07003047 |
|    mean velocity x | -0.163      |
|    mean velocity y | -0.385      |
|    mean velocity z | 17          |
|    mean_ep_length  | 100         |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 296000      |
------------------------------------
Eval num_timesteps=296500, episode_reward=-55509.05 +/- 38512.61
Episode length: 66.20 +/- 13.42
------------------------------------
| eval/              |             |
|    mean action     | -0.50985074 |
|    mean velocity x | 3.04        |
|    mean velocity y | 3.78        |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -5.55e+04   |
| time/              |             |
|    total_timesteps | 296500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 91.3      |
|    ep_rew_mean     | -9.27e+04 |
| time/              |           |
|    fps             | 61        |
|    iterations      | 145       |
|    time_elapsed    | 4864      |
|    total_timesteps | 296960    |
----------------------------------
Eval num_timesteps=297000, episode_reward=-74124.72 +/- 44546.50
Episode length: 60.60 +/- 19.89
------------------------------------------
| eval/                   |              |
|    mean action          | 0.6669677    |
|    mean velocity x      | -2.52        |
|    mean velocity y      | -5.31        |
|    mean velocity z      | 19.9         |
|    mean_ep_length       | 60.6         |
|    mean_reward          | -7.41e+04    |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0025877652 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0829       |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+08     |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 0.949        |
|    value_loss           | 2.27e+08     |
------------------------------------------
Eval num_timesteps=297500, episode_reward=-112986.42 +/- 21113.92
Episode length: 94.80 +/- 39.33
-------------------------------------
| eval/              |              |
|    mean action     | -0.033175774 |
|    mean velocity x | 0.831        |
|    mean velocity y | -0.397       |
|    mean velocity z | 19.1         |
|    mean_ep_length  | 94.8         |
|    mean_reward     | -1.13e+05    |
| time/              |              |
|    total_timesteps | 297500       |
-------------------------------------
Eval num_timesteps=298000, episode_reward=-68227.83 +/- 32805.98
Episode length: 75.00 +/- 12.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.9356672 |
|    mean velocity x | 4.36       |
|    mean velocity y | 7.52       |
|    mean velocity z | 16.5       |
|    mean_ep_length  | 75         |
|    mean_reward     | -6.82e+04  |
| time/              |            |
|    total_timesteps | 298000     |
-----------------------------------
Eval num_timesteps=298500, episode_reward=-61899.39 +/- 40569.59
Episode length: 81.40 +/- 47.99
-----------------------------------
| eval/              |            |
|    mean action     | -0.5699668 |
|    mean velocity x | 2.88       |
|    mean velocity y | 4.43       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 81.4       |
|    mean_reward     | -6.19e+04  |
| time/              |            |
|    total_timesteps | 298500     |
-----------------------------------
Eval num_timesteps=299000, episode_reward=-70519.68 +/- 35750.66
Episode length: 72.00 +/- 33.35
------------------------------------
| eval/              |             |
|    mean action     | -0.08961531 |
|    mean velocity x | 1.27        |
|    mean velocity y | 0.901       |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 72          |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 299000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.5      |
|    ep_rew_mean     | -9.51e+04 |
| time/              |           |
|    fps             | 61        |
|    iterations      | 146       |
|    time_elapsed    | 4871      |
|    total_timesteps | 299008    |
----------------------------------
Eval num_timesteps=299500, episode_reward=-93737.65 +/- 78857.14
Episode length: 110.80 +/- 72.95
------------------------------------------
| eval/                   |              |
|    mean action          | -0.9466957   |
|    mean velocity x      | 3.06         |
|    mean velocity y      | 6.06         |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 111          |
|    mean_reward          | -9.37e+04    |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0044989306 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0952       |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+08     |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00465     |
|    std                  | 0.95         |
|    value_loss           | 2.42e+08     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-101482.38 +/- 23488.13
Episode length: 70.80 +/- 4.26
------------------------------------
| eval/              |             |
|    mean action     | -0.19079313 |
|    mean velocity x | 0.578       |
|    mean velocity y | 0.56        |
|    mean velocity z | 19          |
|    mean_ep_length  | 70.8        |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 300000      |
------------------------------------
Eval num_timesteps=300500, episode_reward=-81037.43 +/- 18704.61
Episode length: 78.20 +/- 11.87
-----------------------------------
| eval/              |            |
|    mean action     | -0.4696933 |
|    mean velocity x | 1.33       |
|    mean velocity y | 1.83       |
|    mean velocity z | 16         |
|    mean_ep_length  | 78.2       |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 300500     |
-----------------------------------
Eval num_timesteps=301000, episode_reward=-83771.22 +/- 76870.73
Episode length: 97.40 +/- 86.15
-------------------------------------
| eval/              |              |
|    mean action     | -0.112132154 |
|    mean velocity x | 0.0864       |
|    mean velocity y | -0.194       |
|    mean velocity z | 19.3         |
|    mean_ep_length  | 97.4         |
|    mean_reward     | -8.38e+04    |
| time/              |              |
|    total_timesteps | 301000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.5      |
|    ep_rew_mean     | -9.42e+04 |
| time/              |           |
|    fps             | 61        |
|    iterations      | 147       |
|    time_elapsed    | 4879      |
|    total_timesteps | 301056    |
----------------------------------
Eval num_timesteps=301500, episode_reward=-85353.13 +/- 22861.28
Episode length: 84.60 +/- 29.21
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.04408696    |
|    mean velocity x      | -0.979        |
|    mean velocity y      | -2.1          |
|    mean velocity z      | 21            |
|    mean_ep_length       | 84.6          |
|    mean_reward          | -8.54e+04     |
| time/                   |               |
|    total_timesteps      | 301500        |
| train/                  |               |
|    approx_kl            | 0.00056065636 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.1          |
|    explained_variance   | 0.0821        |
|    learning_rate        | 0.001         |
|    loss                 | 7.74e+07      |
|    n_updates            | 1470          |
|    policy_gradient_loss | -0.00138      |
|    std                  | 0.95          |
|    value_loss           | 2.47e+08      |
-------------------------------------------
Eval num_timesteps=302000, episode_reward=-55956.04 +/- 45470.43
Episode length: 47.00 +/- 29.89
------------------------------------
| eval/              |             |
|    mean action     | -0.27136555 |
|    mean velocity x | 2.34        |
|    mean velocity y | 2.06        |
|    mean velocity z | 20          |
|    mean_ep_length  | 47          |
|    mean_reward     | -5.6e+04    |
| time/              |             |
|    total_timesteps | 302000      |
------------------------------------
Eval num_timesteps=302500, episode_reward=-84528.33 +/- 38644.50
Episode length: 67.60 +/- 12.34
-----------------------------------
| eval/              |            |
|    mean action     | -0.7622151 |
|    mean velocity x | 3.23       |
|    mean velocity y | 4.61       |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 67.6       |
|    mean_reward     | -8.45e+04  |
| time/              |            |
|    total_timesteps | 302500     |
-----------------------------------
Eval num_timesteps=303000, episode_reward=-47705.37 +/- 10577.71
Episode length: 61.80 +/- 3.87
-----------------------------------
| eval/              |            |
|    mean action     | -0.1903986 |
|    mean velocity x | 1.2        |
|    mean velocity y | 1.41       |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -4.77e+04  |
| time/              |            |
|    total_timesteps | 303000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | -1e+05   |
| time/              |          |
|    fps             | 62       |
|    iterations      | 148      |
|    time_elapsed    | 4886     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=-67766.54 +/- 30123.33
Episode length: 71.60 +/- 23.68
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.9214337  |
|    mean velocity x      | 4.44        |
|    mean velocity y      | 6.32        |
|    mean velocity z      | 19.5        |
|    mean_ep_length       | 71.6        |
|    mean_reward          | -6.78e+04   |
| time/                   |             |
|    total_timesteps      | 303500      |
| train/                  |             |
|    approx_kl            | 0.003598604 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.0463      |
|    learning_rate        | 0.001       |
|    loss                 | 9.66e+07    |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.00401    |
|    std                  | 0.948       |
|    value_loss           | 2.32e+08    |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=-54021.19 +/- 25430.23
Episode length: 60.20 +/- 8.06
-----------------------------------
| eval/              |            |
|    mean action     | -0.3147347 |
|    mean velocity x | 1.44       |
|    mean velocity y | 1.76       |
|    mean velocity z | 18         |
|    mean_ep_length  | 60.2       |
|    mean_reward     | -5.4e+04   |
| time/              |            |
|    total_timesteps | 304000     |
-----------------------------------
Eval num_timesteps=304500, episode_reward=-79640.42 +/- 16106.11
Episode length: 76.00 +/- 11.66
------------------------------------
| eval/              |             |
|    mean action     | -0.18387881 |
|    mean velocity x | 0.137       |
|    mean velocity y | 1.65        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 76          |
|    mean_reward     | -7.96e+04   |
| time/              |             |
|    total_timesteps | 304500      |
------------------------------------
Eval num_timesteps=305000, episode_reward=-66154.86 +/- 61583.39
Episode length: 85.80 +/- 54.76
------------------------------------
| eval/              |             |
|    mean action     | -0.21181518 |
|    mean velocity x | -0.894      |
|    mean velocity y | 0.57        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 85.8        |
|    mean_reward     | -6.62e+04   |
| time/              |             |
|    total_timesteps | 305000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98       |
|    ep_rew_mean     | -1e+05   |
| time/              |          |
|    fps             | 62       |
|    iterations      | 149      |
|    time_elapsed    | 4893     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-77922.15 +/- 19282.20
Episode length: 71.60 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6950624   |
|    mean velocity x      | 2.11         |
|    mean velocity y      | 4.32         |
|    mean velocity z      | 18.8         |
|    mean_ep_length       | 71.6         |
|    mean_reward          | -7.79e+04    |
| time/                   |              |
|    total_timesteps      | 305500       |
| train/                  |              |
|    approx_kl            | 0.0017139924 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0654       |
|    learning_rate        | 0.001        |
|    loss                 | 9.67e+07     |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.95         |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=306000, episode_reward=-95419.78 +/- 12603.71
Episode length: 78.80 +/- 6.73
-----------------------------------
| eval/              |            |
|    mean action     | 0.35886672 |
|    mean velocity x | -1.6       |
|    mean velocity y | -2.51      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 78.8       |
|    mean_reward     | -9.54e+04  |
| time/              |            |
|    total_timesteps | 306000     |
-----------------------------------
Eval num_timesteps=306500, episode_reward=-70722.66 +/- 9865.87
Episode length: 74.60 +/- 10.61
------------------------------------
| eval/              |             |
|    mean action     | -0.20297149 |
|    mean velocity x | -0.787      |
|    mean velocity y | 1.08        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 74.6        |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 306500      |
------------------------------------
Eval num_timesteps=307000, episode_reward=-68273.12 +/- 21736.87
Episode length: 74.60 +/- 11.83
-----------------------------------
| eval/              |            |
|    mean action     | 0.41507158 |
|    mean velocity x | -1.7       |
|    mean velocity y | -1.34      |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 74.6       |
|    mean_reward     | -6.83e+04  |
| time/              |            |
|    total_timesteps | 307000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.2     |
|    ep_rew_mean     | -1e+05   |
| time/              |          |
|    fps             | 62       |
|    iterations      | 150      |
|    time_elapsed    | 4900     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=-108004.27 +/- 11140.79
Episode length: 71.40 +/- 1.36
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.25600687 |
|    mean velocity x      | 1.01        |
|    mean velocity y      | 2.59        |
|    mean velocity z      | 16.7        |
|    mean_ep_length       | 71.4        |
|    mean_reward          | -1.08e+05   |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.002521905 |
|    clip_fraction        | 0.00522     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.0755      |
|    learning_rate        | 0.001       |
|    loss                 | 1.65e+08    |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00331    |
|    std                  | 0.949       |
|    value_loss           | 2.19e+08    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=-55231.59 +/- 38926.91
Episode length: 58.60 +/- 23.81
------------------------------------
| eval/              |             |
|    mean action     | -0.83697194 |
|    mean velocity x | 3.56        |
|    mean velocity y | 6.81        |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -5.52e+04   |
| time/              |             |
|    total_timesteps | 308000      |
------------------------------------
Eval num_timesteps=308500, episode_reward=-66500.69 +/- 37701.80
Episode length: 71.20 +/- 25.28
-----------------------------------
| eval/              |            |
|    mean action     | -0.2590055 |
|    mean velocity x | 0.0169     |
|    mean velocity y | 1.83       |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 71.2       |
|    mean_reward     | -6.65e+04  |
| time/              |            |
|    total_timesteps | 308500     |
-----------------------------------
Eval num_timesteps=309000, episode_reward=-100227.62 +/- 11445.33
Episode length: 77.60 +/- 7.26
------------------------------------
| eval/              |             |
|    mean action     | -0.20426612 |
|    mean velocity x | 1.15        |
|    mean velocity y | 1.39        |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 77.6        |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 309000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.5      |
|    ep_rew_mean     | -1.01e+05 |
| time/              |           |
|    fps             | 63        |
|    iterations      | 151       |
|    time_elapsed    | 4907      |
|    total_timesteps | 309248    |
----------------------------------
Eval num_timesteps=309500, episode_reward=-61327.29 +/- 47590.58
Episode length: 66.60 +/- 22.69
------------------------------------------
| eval/                   |              |
|    mean action          | -0.11433679  |
|    mean velocity x      | -1.22        |
|    mean velocity y      | -1.51        |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 66.6         |
|    mean_reward          | -6.13e+04    |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 0.0009449987 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0712       |
|    learning_rate        | 0.001        |
|    loss                 | 2.02e+08     |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 0.95         |
|    value_loss           | 3.1e+08      |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-88435.92 +/- 17401.51
Episode length: 77.60 +/- 10.05
------------------------------------
| eval/              |             |
|    mean action     | -0.13268961 |
|    mean velocity x | 1.76        |
|    mean velocity y | 1.4         |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 77.6        |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 310000      |
------------------------------------
Eval num_timesteps=310500, episode_reward=-97658.02 +/- 15507.30
Episode length: 78.80 +/- 7.78
-----------------------------------
| eval/              |            |
|    mean action     | 0.26053345 |
|    mean velocity x | -0.719     |
|    mean velocity y | -1.95      |
|    mean velocity z | 14.9       |
|    mean_ep_length  | 78.8       |
|    mean_reward     | -9.77e+04  |
| time/              |            |
|    total_timesteps | 310500     |
-----------------------------------
Eval num_timesteps=311000, episode_reward=-91025.46 +/- 18026.61
Episode length: 77.00 +/- 8.65
------------------------------------
| eval/              |             |
|    mean action     | -0.64633626 |
|    mean velocity x | 4.2         |
|    mean velocity y | 4.11        |
|    mean velocity z | 16.4        |
|    mean_ep_length  | 77          |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 311000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.9      |
|    ep_rew_mean     | -9.83e+04 |
| time/              |           |
|    fps             | 63        |
|    iterations      | 152       |
|    time_elapsed    | 4914      |
|    total_timesteps | 311296    |
----------------------------------
Eval num_timesteps=311500, episode_reward=-84452.51 +/- 23349.68
Episode length: 74.20 +/- 9.95
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.5781336  |
|    mean velocity x      | 4.38        |
|    mean velocity y      | 4.59        |
|    mean velocity z      | 19.6        |
|    mean_ep_length       | 74.2        |
|    mean_reward          | -8.45e+04   |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.004038964 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.0994      |
|    learning_rate        | 0.001       |
|    loss                 | 8.94e+07    |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00393    |
|    std                  | 0.951       |
|    value_loss           | 1.97e+08    |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=-58142.81 +/- 39152.50
Episode length: 57.80 +/- 14.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.7140431 |
|    mean velocity x | 1.78       |
|    mean velocity y | 4.61       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -5.81e+04  |
| time/              |            |
|    total_timesteps | 312000     |
-----------------------------------
Eval num_timesteps=312500, episode_reward=-115828.59 +/- 48867.20
Episode length: 105.60 +/- 72.24
-------------------------------------
| eval/              |              |
|    mean action     | -0.037035838 |
|    mean velocity x | -1.46        |
|    mean velocity y | -0.228       |
|    mean velocity z | 21.4         |
|    mean_ep_length  | 106          |
|    mean_reward     | -1.16e+05    |
| time/              |              |
|    total_timesteps | 312500       |
-------------------------------------
Eval num_timesteps=313000, episode_reward=-88199.61 +/- 36012.26
Episode length: 66.60 +/- 8.33
-----------------------------------
| eval/              |            |
|    mean action     | -0.8766907 |
|    mean velocity x | 3.12       |
|    mean velocity y | 5.19       |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 66.6       |
|    mean_reward     | -8.82e+04  |
| time/              |            |
|    total_timesteps | 313000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 91.8      |
|    ep_rew_mean     | -9.56e+04 |
| time/              |           |
|    fps             | 63        |
|    iterations      | 153       |
|    time_elapsed    | 4922      |
|    total_timesteps | 313344    |
----------------------------------
Eval num_timesteps=313500, episode_reward=-84374.00 +/- 31421.89
Episode length: 84.00 +/- 31.70
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19108905   |
|    mean velocity x      | 1.44          |
|    mean velocity y      | 2.39          |
|    mean velocity z      | 17.4          |
|    mean_ep_length       | 84            |
|    mean_reward          | -8.44e+04     |
| time/                   |               |
|    total_timesteps      | 313500        |
| train/                  |               |
|    approx_kl            | 0.00096371537 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.1          |
|    explained_variance   | 0.0742        |
|    learning_rate        | 0.001         |
|    loss                 | 1.42e+08      |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.00105      |
|    std                  | 0.951         |
|    value_loss           | 2.66e+08      |
-------------------------------------------
Eval num_timesteps=314000, episode_reward=-82578.94 +/- 28880.64
Episode length: 73.40 +/- 14.09
-----------------------------------
| eval/              |            |
|    mean action     | 0.37396368 |
|    mean velocity x | -1.17      |
|    mean velocity y | -2.38      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 73.4       |
|    mean_reward     | -8.26e+04  |
| time/              |            |
|    total_timesteps | 314000     |
-----------------------------------
Eval num_timesteps=314500, episode_reward=-83798.77 +/- 17982.64
Episode length: 70.60 +/- 4.03
-----------------------------------
| eval/              |            |
|    mean action     | -0.4559508 |
|    mean velocity x | 2.84       |
|    mean velocity y | 2.82       |
|    mean velocity z | 15.5       |
|    mean_ep_length  | 70.6       |
|    mean_reward     | -8.38e+04  |
| time/              |            |
|    total_timesteps | 314500     |
-----------------------------------
Eval num_timesteps=315000, episode_reward=-75429.61 +/- 39467.39
Episode length: 72.20 +/- 21.27
-------------------------------------
| eval/              |              |
|    mean action     | -0.060135085 |
|    mean velocity x | -0.642       |
|    mean velocity y | -1.33        |
|    mean velocity z | 19.2         |
|    mean_ep_length  | 72.2         |
|    mean_reward     | -7.54e+04    |
| time/              |              |
|    total_timesteps | 315000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 89.7      |
|    ep_rew_mean     | -9.43e+04 |
| time/              |           |
|    fps             | 63        |
|    iterations      | 154       |
|    time_elapsed    | 4929      |
|    total_timesteps | 315392    |
----------------------------------
Eval num_timesteps=315500, episode_reward=-86552.14 +/- 21109.71
Episode length: 71.20 +/- 4.87
------------------------------------------
| eval/                   |              |
|    mean action          | -0.031774748 |
|    mean velocity x      | 0.382        |
|    mean velocity y      | 0.839        |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 71.2         |
|    mean_reward          | -8.66e+04    |
| time/                   |              |
|    total_timesteps      | 315500       |
| train/                  |              |
|    approx_kl            | 0.0012326031 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0758       |
|    learning_rate        | 0.001        |
|    loss                 | 1.31e+08     |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 0.951        |
|    value_loss           | 2.49e+08     |
------------------------------------------
Eval num_timesteps=316000, episode_reward=-80432.49 +/- 34769.13
Episode length: 71.00 +/- 12.65
-----------------------------------
| eval/              |            |
|    mean action     | -0.2713411 |
|    mean velocity x | 0.888      |
|    mean velocity y | 1.87       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 71         |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 316000     |
-----------------------------------
Eval num_timesteps=316500, episode_reward=-55525.79 +/- 44560.01
Episode length: 51.80 +/- 25.02
-----------------------------------
| eval/              |            |
|    mean action     | -0.3847207 |
|    mean velocity x | 0.547      |
|    mean velocity y | 2.54       |
|    mean velocity z | 17         |
|    mean_ep_length  | 51.8       |
|    mean_reward     | -5.55e+04  |
| time/              |            |
|    total_timesteps | 316500     |
-----------------------------------
Eval num_timesteps=317000, episode_reward=-76298.45 +/- 18250.88
Episode length: 80.20 +/- 14.27
-----------------------------------
| eval/              |            |
|    mean action     | -0.6140577 |
|    mean velocity x | 2.58       |
|    mean velocity y | 4.52       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 80.2       |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 317000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 86.3      |
|    ep_rew_mean     | -8.92e+04 |
| time/              |           |
|    fps             | 64        |
|    iterations      | 155       |
|    time_elapsed    | 4936      |
|    total_timesteps | 317440    |
----------------------------------
Eval num_timesteps=317500, episode_reward=-78732.04 +/- 38996.21
Episode length: 65.40 +/- 7.17
-----------------------------------------
| eval/                   |             |
|    mean action          | -1.0649927  |
|    mean velocity x      | 3.94        |
|    mean velocity y      | 7.13        |
|    mean velocity z      | 18.2        |
|    mean_ep_length       | 65.4        |
|    mean_reward          | -7.87e+04   |
| time/                   |             |
|    total_timesteps      | 317500      |
| train/                  |             |
|    approx_kl            | 0.005329172 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.0868      |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+08    |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00596    |
|    std                  | 0.949       |
|    value_loss           | 2.07e+08    |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=-91519.50 +/- 28146.19
Episode length: 88.00 +/- 29.44
-----------------------------------
| eval/              |            |
|    mean action     | -0.2641708 |
|    mean velocity x | -0.158     |
|    mean velocity y | 1.08       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 88         |
|    mean_reward     | -9.15e+04  |
| time/              |            |
|    total_timesteps | 318000     |
-----------------------------------
Eval num_timesteps=318500, episode_reward=-80556.10 +/- 24081.66
Episode length: 77.60 +/- 11.64
------------------------------------
| eval/              |             |
|    mean action     | -0.12065093 |
|    mean velocity x | 0.532       |
|    mean velocity y | 0.00418     |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 77.6        |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 318500      |
------------------------------------
Eval num_timesteps=319000, episode_reward=-61635.43 +/- 51001.56
Episode length: 52.20 +/- 30.41
------------------------------------
| eval/              |             |
|    mean action     | -0.31553918 |
|    mean velocity x | 2.32        |
|    mean velocity y | 3.79        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -6.16e+04   |
| time/              |             |
|    total_timesteps | 319000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | -9.4e+04 |
| time/              |          |
|    fps             | 64       |
|    iterations      | 156      |
|    time_elapsed    | 4946     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-103323.93 +/- 16254.09
Episode length: 73.00 +/- 3.16
------------------------------------------
| eval/                   |              |
|    mean action          | 0.10535997   |
|    mean velocity x      | -1.99        |
|    mean velocity y      | -2.23        |
|    mean velocity z      | 16.1         |
|    mean_ep_length       | 73           |
|    mean_reward          | -1.03e+05    |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0012597011 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0766       |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+08     |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.948        |
|    value_loss           | 2.81e+08     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-67522.52 +/- 39876.30
Episode length: 74.20 +/- 15.46
-----------------------------------
| eval/              |            |
|    mean action     | 0.24339217 |
|    mean velocity x | 0.654      |
|    mean velocity y | -1.44      |
|    mean velocity z | 13.9       |
|    mean_ep_length  | 74.2       |
|    mean_reward     | -6.75e+04  |
| time/              |            |
|    total_timesteps | 320000     |
-----------------------------------
Eval num_timesteps=320500, episode_reward=-88553.62 +/- 29472.89
Episode length: 69.20 +/- 1.72
------------------------------------
| eval/              |             |
|    mean action     | -0.13461685 |
|    mean velocity x | -0.12       |
|    mean velocity y | 0.869       |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 69.2        |
|    mean_reward     | -8.86e+04   |
| time/              |             |
|    total_timesteps | 320500      |
------------------------------------
Eval num_timesteps=321000, episode_reward=-23052.96 +/- 22796.99
Episode length: 42.80 +/- 19.49
----------------------------------
| eval/              |           |
|    mean action     | -0.599007 |
|    mean velocity x | 0.248     |
|    mean velocity y | 2.74      |
|    mean velocity z | 17.4      |
|    mean_ep_length  | 42.8      |
|    mean_reward     | -2.31e+04 |
| time/              |           |
|    total_timesteps | 321000    |
----------------------------------
New best mean reward!
Eval num_timesteps=321500, episode_reward=-49704.95 +/- 32790.47
Episode length: 59.40 +/- 28.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.07008636 |
|    mean velocity x | -0.868     |
|    mean velocity y | 0.0439     |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -4.97e+04  |
| time/              |            |
|    total_timesteps | 321500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.6      |
|    ep_rew_mean     | -8.54e+04 |
| time/              |           |
|    fps             | 64        |
|    iterations      | 157       |
|    time_elapsed    | 4954      |
|    total_timesteps | 321536    |
----------------------------------
Eval num_timesteps=322000, episode_reward=-85881.26 +/- 28568.83
Episode length: 70.60 +/- 12.48
------------------------------------------
| eval/                   |              |
|    mean action          | -0.018296424 |
|    mean velocity x      | 0.377        |
|    mean velocity y      | -0.098       |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 70.6         |
|    mean_reward          | -8.59e+04    |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0010296395 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.001        |
|    loss                 | 7.16e+07     |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.948        |
|    value_loss           | 1.59e+08     |
------------------------------------------
Eval num_timesteps=322500, episode_reward=-57156.31 +/- 47754.30
Episode length: 66.60 +/- 42.43
------------------------------------
| eval/              |             |
|    mean action     | 0.035354935 |
|    mean velocity x | 0.0144      |
|    mean velocity y | -0.464      |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 66.6        |
|    mean_reward     | -5.72e+04   |
| time/              |             |
|    total_timesteps | 322500      |
------------------------------------
Eval num_timesteps=323000, episode_reward=-48391.23 +/- 27634.94
Episode length: 57.40 +/- 23.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.15015386 |
|    mean velocity x | -1.7       |
|    mean velocity y | -1.73      |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 57.4       |
|    mean_reward     | -4.84e+04  |
| time/              |            |
|    total_timesteps | 323000     |
-----------------------------------
Eval num_timesteps=323500, episode_reward=-73308.55 +/- 23568.84
Episode length: 66.00 +/- 5.97
------------------------------------
| eval/              |             |
|    mean action     | -0.15542845 |
|    mean velocity x | 1.89        |
|    mean velocity y | 1.43        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 66          |
|    mean_reward     | -7.33e+04   |
| time/              |             |
|    total_timesteps | 323500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.1      |
|    ep_rew_mean     | -8.53e+04 |
| time/              |           |
|    fps             | 65        |
|    iterations      | 158       |
|    time_elapsed    | 4961      |
|    total_timesteps | 323584    |
----------------------------------
Eval num_timesteps=324000, episode_reward=-64474.38 +/- 39196.04
Episode length: 57.80 +/- 20.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.051094167 |
|    mean velocity x      | -0.972       |
|    mean velocity y      | -0.448       |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 57.8         |
|    mean_reward          | -6.45e+04    |
| time/                   |              |
|    total_timesteps      | 324000       |
| train/                  |              |
|    approx_kl            | 0.0002649508 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.105        |
|    learning_rate        | 0.001        |
|    loss                 | 7.2e+07      |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00067     |
|    std                  | 0.949        |
|    value_loss           | 2.19e+08     |
------------------------------------------
Eval num_timesteps=324500, episode_reward=-56962.85 +/- 35351.14
Episode length: 69.80 +/- 30.24
------------------------------------
| eval/              |             |
|    mean action     | -0.48637563 |
|    mean velocity x | 1.22        |
|    mean velocity y | 2.9         |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 69.8        |
|    mean_reward     | -5.7e+04    |
| time/              |             |
|    total_timesteps | 324500      |
------------------------------------
Eval num_timesteps=325000, episode_reward=-38412.09 +/- 32240.30
Episode length: 56.20 +/- 27.61
-----------------------------------
| eval/              |            |
|    mean action     | -0.6685653 |
|    mean velocity x | 1.59       |
|    mean velocity y | 3.11       |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -3.84e+04  |
| time/              |            |
|    total_timesteps | 325000     |
-----------------------------------
Eval num_timesteps=325500, episode_reward=-71304.09 +/- 11549.71
Episode length: 75.60 +/- 11.29
------------------------------------
| eval/              |             |
|    mean action     | -0.52937084 |
|    mean velocity x | 2.02        |
|    mean velocity y | 4.15        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 75.6        |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 325500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.3      |
|    ep_rew_mean     | -8.94e+04 |
| time/              |           |
|    fps             | 65        |
|    iterations      | 159       |
|    time_elapsed    | 4968      |
|    total_timesteps | 325632    |
----------------------------------
Eval num_timesteps=326000, episode_reward=-74176.27 +/- 44517.14
Episode length: 60.00 +/- 17.38
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.21350226   |
|    mean velocity x      | 0.916         |
|    mean velocity y      | 1.23          |
|    mean velocity z      | 17.9          |
|    mean_ep_length       | 60            |
|    mean_reward          | -7.42e+04     |
| time/                   |               |
|    total_timesteps      | 326000        |
| train/                  |               |
|    approx_kl            | 0.00087701285 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.1          |
|    explained_variance   | 0.0761        |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+08      |
|    n_updates            | 1590          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 0.948         |
|    value_loss           | 2.58e+08      |
-------------------------------------------
Eval num_timesteps=326500, episode_reward=-40666.79 +/- 38200.20
Episode length: 56.80 +/- 28.78
------------------------------------
| eval/              |             |
|    mean action     | 0.040364623 |
|    mean velocity x | 0.857       |
|    mean velocity y | -0.774      |
|    mean velocity z | 16.6        |
|    mean_ep_length  | 56.8        |
|    mean_reward     | -4.07e+04   |
| time/              |             |
|    total_timesteps | 326500      |
------------------------------------
Eval num_timesteps=327000, episode_reward=-67031.78 +/- 35703.42
Episode length: 66.20 +/- 19.36
------------------------------------
| eval/              |             |
|    mean action     | 0.031330977 |
|    mean velocity x | 0.183       |
|    mean velocity y | 0.158       |
|    mean velocity z | 18          |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 327000      |
------------------------------------
Eval num_timesteps=327500, episode_reward=-55630.57 +/- 34825.96
Episode length: 61.40 +/- 8.06
------------------------------------
| eval/              |             |
|    mean action     | -0.30048704 |
|    mean velocity x | 0.892       |
|    mean velocity y | 2.53        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 61.4        |
|    mean_reward     | -5.56e+04   |
| time/              |             |
|    total_timesteps | 327500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80        |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 65        |
|    iterations      | 160       |
|    time_elapsed    | 4975      |
|    total_timesteps | 327680    |
----------------------------------
Eval num_timesteps=328000, episode_reward=-86772.93 +/- 22913.58
Episode length: 82.80 +/- 27.24
------------------------------------------
| eval/                   |              |
|    mean action          | -0.8406901   |
|    mean velocity x      | 4.46         |
|    mean velocity y      | 6.6          |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 82.8         |
|    mean_reward          | -8.68e+04    |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0028001103 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0825       |
|    learning_rate        | 0.001        |
|    loss                 | 5.51e+07     |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00274     |
|    std                  | 0.947        |
|    value_loss           | 1.88e+08     |
------------------------------------------
Eval num_timesteps=328500, episode_reward=-90177.66 +/- 17345.65
Episode length: 72.00 +/- 5.22
------------------------------------
| eval/              |             |
|    mean action     | -0.45938444 |
|    mean velocity x | 1.29        |
|    mean velocity y | 3.05        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 72          |
|    mean_reward     | -9.02e+04   |
| time/              |             |
|    total_timesteps | 328500      |
------------------------------------
Eval num_timesteps=329000, episode_reward=-43670.71 +/- 29298.74
Episode length: 54.80 +/- 14.12
------------------------------------
| eval/              |             |
|    mean action     | 0.025386643 |
|    mean velocity x | -0.843      |
|    mean velocity y | -0.0331     |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -4.37e+04   |
| time/              |             |
|    total_timesteps | 329000      |
------------------------------------
Eval num_timesteps=329500, episode_reward=-54726.65 +/- 35907.27
Episode length: 58.80 +/- 25.58
-----------------------------------
| eval/              |            |
|    mean action     | -0.3342608 |
|    mean velocity x | 2.28       |
|    mean velocity y | 3.23       |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -5.47e+04  |
| time/              |            |
|    total_timesteps | 329500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.3      |
|    ep_rew_mean     | -8.58e+04 |
| time/              |           |
|    fps             | 66        |
|    iterations      | 161       |
|    time_elapsed    | 4982      |
|    total_timesteps | 329728    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-64282.01 +/- 46838.59
Episode length: 60.20 +/- 26.06
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32234752  |
|    mean velocity x      | 1.16         |
|    mean velocity y      | 0.682        |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 60.2         |
|    mean_reward          | -6.43e+04    |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0023017197 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0859       |
|    learning_rate        | 0.001        |
|    loss                 | 8.89e+07     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.946        |
|    value_loss           | 2.51e+08     |
------------------------------------------
Eval num_timesteps=330500, episode_reward=-68225.51 +/- 35311.54
Episode length: 65.20 +/- 10.07
----------------------------------
| eval/              |           |
|    mean action     | 0.2058552 |
|    mean velocity x | -1.76     |
|    mean velocity y | -2.74     |
|    mean velocity z | 20.3      |
|    mean_ep_length  | 65.2      |
|    mean_reward     | -6.82e+04 |
| time/              |           |
|    total_timesteps | 330500    |
----------------------------------
Eval num_timesteps=331000, episode_reward=-76324.40 +/- 33462.18
Episode length: 64.40 +/- 18.70
----------------------------------
| eval/              |           |
|    mean action     | 0.3491456 |
|    mean velocity x | -1.12     |
|    mean velocity y | -2.63     |
|    mean velocity z | 19        |
|    mean_ep_length  | 64.4      |
|    mean_reward     | -7.63e+04 |
| time/              |           |
|    total_timesteps | 331000    |
----------------------------------
Eval num_timesteps=331500, episode_reward=-54279.85 +/- 24704.76
Episode length: 80.20 +/- 21.01
-----------------------------------
| eval/              |            |
|    mean action     | 0.38357335 |
|    mean velocity x | -0.978     |
|    mean velocity y | -2.36      |
|    mean velocity z | 19         |
|    mean_ep_length  | 80.2       |
|    mean_reward     | -5.43e+04  |
| time/              |            |
|    total_timesteps | 331500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.5      |
|    ep_rew_mean     | -8.97e+04 |
| time/              |           |
|    fps             | 66        |
|    iterations      | 162       |
|    time_elapsed    | 4989      |
|    total_timesteps | 331776    |
----------------------------------
Eval num_timesteps=332000, episode_reward=-70697.82 +/- 24187.71
Episode length: 73.80 +/- 13.33
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50762165  |
|    mean velocity x      | 2.79         |
|    mean velocity y      | 3.5          |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 73.8         |
|    mean_reward          | -7.07e+04    |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0012701126 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0891       |
|    learning_rate        | 0.001        |
|    loss                 | 2.04e+08     |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 0.946        |
|    value_loss           | 2.72e+08     |
------------------------------------------
Eval num_timesteps=332500, episode_reward=-78612.55 +/- 35299.03
Episode length: 66.20 +/- 17.34
-----------------------------------
| eval/              |            |
|    mean action     | -0.1520868 |
|    mean velocity x | -0.252     |
|    mean velocity y | 0.819      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 66.2       |
|    mean_reward     | -7.86e+04  |
| time/              |            |
|    total_timesteps | 332500     |
-----------------------------------
Eval num_timesteps=333000, episode_reward=-80846.44 +/- 43978.93
Episode length: 61.20 +/- 21.10
-----------------------------------
| eval/              |            |
|    mean action     | -0.5520284 |
|    mean velocity x | 1.17       |
|    mean velocity y | 3.63       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -8.08e+04  |
| time/              |            |
|    total_timesteps | 333000     |
-----------------------------------
Eval num_timesteps=333500, episode_reward=-101060.12 +/- 23208.72
Episode length: 70.20 +/- 4.07
------------------------------------
| eval/              |             |
|    mean action     | -0.12605056 |
|    mean velocity x | 0.427       |
|    mean velocity y | -0.565      |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 70.2        |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 333500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.2      |
|    ep_rew_mean     | -8.88e+04 |
| time/              |           |
|    fps             | 66        |
|    iterations      | 163       |
|    time_elapsed    | 4996      |
|    total_timesteps | 333824    |
----------------------------------
Eval num_timesteps=334000, episode_reward=-56387.57 +/- 30554.56
Episode length: 71.80 +/- 37.79
------------------------------------------
| eval/                   |              |
|    mean action          | -0.50232464  |
|    mean velocity x      | 2            |
|    mean velocity y      | 2.86         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 71.8         |
|    mean_reward          | -5.64e+04    |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0019252863 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0799       |
|    learning_rate        | 0.001        |
|    loss                 | 1.64e+08     |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 0.946        |
|    value_loss           | 2.53e+08     |
------------------------------------------
Eval num_timesteps=334500, episode_reward=-90969.10 +/- 10318.29
Episode length: 81.00 +/- 10.62
------------------------------------
| eval/              |             |
|    mean action     | -0.24838233 |
|    mean velocity x | 1.48        |
|    mean velocity y | 3.04        |
|    mean velocity z | 18          |
|    mean_ep_length  | 81          |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 334500      |
------------------------------------
Eval num_timesteps=335000, episode_reward=-58034.84 +/- 29385.50
Episode length: 73.00 +/- 22.14
------------------------------------
| eval/              |             |
|    mean action     | -0.16485444 |
|    mean velocity x | 0.833       |
|    mean velocity y | 1.65        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 73          |
|    mean_reward     | -5.8e+04    |
| time/              |             |
|    total_timesteps | 335000      |
------------------------------------
Eval num_timesteps=335500, episode_reward=-85296.11 +/- 37060.88
Episode length: 83.20 +/- 42.86
------------------------------------
| eval/              |             |
|    mean action     | -0.21840581 |
|    mean velocity x | 2.16        |
|    mean velocity y | 2           |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 83.2        |
|    mean_reward     | -8.53e+04   |
| time/              |             |
|    total_timesteps | 335500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 89.9      |
|    ep_rew_mean     | -9.68e+04 |
| time/              |           |
|    fps             | 67        |
|    iterations      | 164       |
|    time_elapsed    | 5003      |
|    total_timesteps | 335872    |
----------------------------------
Eval num_timesteps=336000, episode_reward=-55203.01 +/- 40153.53
Episode length: 56.80 +/- 25.79
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4089853   |
|    mean velocity x      | 1.17         |
|    mean velocity y      | 2.68         |
|    mean velocity z      | 19.5         |
|    mean_ep_length       | 56.8         |
|    mean_reward          | -5.52e+04    |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0002835155 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.0774       |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.944        |
|    value_loss           | 2.55e+08     |
------------------------------------------
Eval num_timesteps=336500, episode_reward=-100752.51 +/- 32885.86
Episode length: 99.00 +/- 34.06
------------------------------------
| eval/              |             |
|    mean action     | -0.14127114 |
|    mean velocity x | 2.07        |
|    mean velocity y | 2.44        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 99          |
|    mean_reward     | -1.01e+05   |
| time/              |             |
|    total_timesteps | 336500      |
------------------------------------
Eval num_timesteps=337000, episode_reward=-61147.64 +/- 48046.06
Episode length: 55.60 +/- 21.12
-----------------------------------
| eval/              |            |
|    mean action     | 0.03701943 |
|    mean velocity x | -0.994     |
|    mean velocity y | -1.8       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 55.6       |
|    mean_reward     | -6.11e+04  |
| time/              |            |
|    total_timesteps | 337000     |
-----------------------------------
Eval num_timesteps=337500, episode_reward=-88297.45 +/- 29135.32
Episode length: 71.80 +/- 7.68
-----------------------------------
| eval/              |            |
|    mean action     | 0.10370109 |
|    mean velocity x | -0.772     |
|    mean velocity y | -1.3       |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 71.8       |
|    mean_reward     | -8.83e+04  |
| time/              |            |
|    total_timesteps | 337500     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88       |
|    ep_rew_mean     | -9.7e+04 |
| time/              |          |
|    fps             | 67       |
|    iterations      | 165      |
|    time_elapsed    | 5011     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=-73076.15 +/- 48051.03
Episode length: 58.00 +/- 21.61
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1661783   |
|    mean velocity x      | 1.08         |
|    mean velocity y      | 2.01         |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 58           |
|    mean_reward          | -7.31e+04    |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0023829849 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.0726       |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+08     |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00379     |
|    std                  | 0.945        |
|    value_loss           | 2.81e+08     |
------------------------------------------
Eval num_timesteps=338500, episode_reward=-79083.20 +/- 32233.10
Episode length: 67.60 +/- 4.59
------------------------------------
| eval/              |             |
|    mean action     | -0.23395367 |
|    mean velocity x | 2.14        |
|    mean velocity y | 2.33        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 67.6        |
|    mean_reward     | -7.91e+04   |
| time/              |             |
|    total_timesteps | 338500      |
------------------------------------
Eval num_timesteps=339000, episode_reward=-92055.53 +/- 16467.08
Episode length: 70.60 +/- 0.80
-----------------------------------
| eval/              |            |
|    mean action     | -0.2787785 |
|    mean velocity x | 1.27       |
|    mean velocity y | 1.15       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 70.6       |
|    mean_reward     | -9.21e+04  |
| time/              |            |
|    total_timesteps | 339000     |
-----------------------------------
Eval num_timesteps=339500, episode_reward=-44061.10 +/- 42316.13
Episode length: 62.20 +/- 44.63
------------------------------------
| eval/              |             |
|    mean action     | -0.18645862 |
|    mean velocity x | -0.877      |
|    mean velocity y | -0.0465     |
|    mean velocity z | 15.4        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -4.41e+04   |
| time/              |             |
|    total_timesteps | 339500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.8      |
|    ep_rew_mean     | -9.27e+04 |
| time/              |           |
|    fps             | 67        |
|    iterations      | 166       |
|    time_elapsed    | 5018      |
|    total_timesteps | 339968    |
----------------------------------
Eval num_timesteps=340000, episode_reward=-52460.16 +/- 44745.45
Episode length: 55.20 +/- 22.87
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13943027  |
|    mean velocity x      | 1.48         |
|    mean velocity y      | 1.71         |
|    mean velocity z      | 15.7         |
|    mean_ep_length       | 55.2         |
|    mean_reward          | -5.25e+04    |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0042885095 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+08      |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.00604     |
|    std                  | 0.949        |
|    value_loss           | 1.95e+08     |
------------------------------------------
Eval num_timesteps=340500, episode_reward=-78347.26 +/- 39616.09
Episode length: 87.80 +/- 41.05
------------------------------------
| eval/              |             |
|    mean action     | -0.09830287 |
|    mean velocity x | -0.152      |
|    mean velocity y | 0.111       |
|    mean velocity z | 17          |
|    mean_ep_length  | 87.8        |
|    mean_reward     | -7.83e+04   |
| time/              |             |
|    total_timesteps | 340500      |
------------------------------------
Eval num_timesteps=341000, episode_reward=-60683.93 +/- 28581.59
Episode length: 65.40 +/- 17.62
------------------------------------
| eval/              |             |
|    mean action     | -0.34465423 |
|    mean velocity x | 1.2         |
|    mean velocity y | 1.26        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -6.07e+04   |
| time/              |             |
|    total_timesteps | 341000      |
------------------------------------
Eval num_timesteps=341500, episode_reward=-99322.00 +/- 14825.97
Episode length: 73.00 +/- 2.10
-----------------------------------
| eval/              |            |
|    mean action     | 0.33452675 |
|    mean velocity x | -2.2       |
|    mean velocity y | -2.77      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 73         |
|    mean_reward     | -9.93e+04  |
| time/              |            |
|    total_timesteps | 341500     |
-----------------------------------
Eval num_timesteps=342000, episode_reward=-80652.43 +/- 33771.82
Episode length: 72.80 +/- 11.30
------------------------------------
| eval/              |             |
|    mean action     | -0.58026516 |
|    mean velocity x | 1.29        |
|    mean velocity y | 3.87        |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 72.8        |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 342000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87        |
|    ep_rew_mean     | -9.27e+04 |
| time/              |           |
|    fps             | 68        |
|    iterations      | 167       |
|    time_elapsed    | 5025      |
|    total_timesteps | 342016    |
----------------------------------
Eval num_timesteps=342500, episode_reward=-66761.20 +/- 45134.49
Episode length: 65.80 +/- 35.67
------------------------------------------
| eval/                   |              |
|    mean action          | -0.43843898  |
|    mean velocity x      | 0.837        |
|    mean velocity y      | 3.27         |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 65.8         |
|    mean_reward          | -6.68e+04    |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0009721309 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.0972       |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+08     |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.949        |
|    value_loss           | 2.58e+08     |
------------------------------------------
Eval num_timesteps=343000, episode_reward=-79521.37 +/- 36465.52
Episode length: 69.20 +/- 19.04
-------------------------------------
| eval/              |              |
|    mean action     | -0.010599939 |
|    mean velocity x | -0.206       |
|    mean velocity y | -0.0602      |
|    mean velocity z | 16.4         |
|    mean_ep_length  | 69.2         |
|    mean_reward     | -7.95e+04    |
| time/              |              |
|    total_timesteps | 343000       |
-------------------------------------
Eval num_timesteps=343500, episode_reward=-69120.42 +/- 40940.28
Episode length: 62.40 +/- 13.72
------------------------------------
| eval/              |             |
|    mean action     | -0.15696757 |
|    mean velocity x | -0.771      |
|    mean velocity y | 0.134       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 62.4        |
|    mean_reward     | -6.91e+04   |
| time/              |             |
|    total_timesteps | 343500      |
------------------------------------
Eval num_timesteps=344000, episode_reward=-48001.45 +/- 35259.25
Episode length: 58.80 +/- 17.69
------------------------------------
| eval/              |             |
|    mean action     | -0.12897453 |
|    mean velocity x | 0.807       |
|    mean velocity y | 1.08        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 58.8        |
|    mean_reward     | -4.8e+04    |
| time/              |             |
|    total_timesteps | 344000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.2      |
|    ep_rew_mean     | -8.51e+04 |
| time/              |           |
|    fps             | 68        |
|    iterations      | 168       |
|    time_elapsed    | 5033      |
|    total_timesteps | 344064    |
----------------------------------
Eval num_timesteps=344500, episode_reward=-65190.54 +/- 21177.95
Episode length: 69.00 +/- 6.23
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21819383  |
|    mean velocity x      | -0.267       |
|    mean velocity y      | 1.97         |
|    mean velocity z      | 15.8         |
|    mean_ep_length       | 69           |
|    mean_reward          | -6.52e+04    |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 0.0013016826 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.105        |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+08     |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.949        |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=345000, episode_reward=-99115.14 +/- 23431.34
Episode length: 76.20 +/- 6.24
----------------------------------
| eval/              |           |
|    mean action     | 0.5529981 |
|    mean velocity x | -1.89     |
|    mean velocity y | -4.3      |
|    mean velocity z | 18.1      |
|    mean_ep_length  | 76.2      |
|    mean_reward     | -9.91e+04 |
| time/              |           |
|    total_timesteps | 345000    |
----------------------------------
Eval num_timesteps=345500, episode_reward=-84785.66 +/- 17310.24
Episode length: 72.00 +/- 7.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.06883096 |
|    mean velocity x | -0.314     |
|    mean velocity y | -1.73      |
|    mean velocity z | 18         |
|    mean_ep_length  | 72         |
|    mean_reward     | -8.48e+04  |
| time/              |            |
|    total_timesteps | 345500     |
-----------------------------------
Eval num_timesteps=346000, episode_reward=-79786.36 +/- 43323.94
Episode length: 63.80 +/- 17.20
------------------------------------
| eval/              |             |
|    mean action     | -0.18812744 |
|    mean velocity x | -0.453      |
|    mean velocity y | -0.255      |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -7.98e+04   |
| time/              |             |
|    total_timesteps | 346000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80        |
|    ep_rew_mean     | -8.23e+04 |
| time/              |           |
|    fps             | 68        |
|    iterations      | 169       |
|    time_elapsed    | 5040      |
|    total_timesteps | 346112    |
----------------------------------
Eval num_timesteps=346500, episode_reward=-77495.64 +/- 30890.24
Episode length: 79.60 +/- 24.45
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.46266517   |
|    mean velocity x      | 0.00128       |
|    mean velocity y      | 2.46          |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 79.6          |
|    mean_reward          | -7.75e+04     |
| time/                   |               |
|    total_timesteps      | 346500        |
| train/                  |               |
|    approx_kl            | 0.00023880845 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.1          |
|    explained_variance   | 0.0869        |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+08      |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000733     |
|    std                  | 0.949         |
|    value_loss           | 2.4e+08       |
-------------------------------------------
Eval num_timesteps=347000, episode_reward=-72334.87 +/- 34920.25
Episode length: 71.00 +/- 10.14
------------------------------------
| eval/              |             |
|    mean action     | -0.36160594 |
|    mean velocity x | 1.43        |
|    mean velocity y | 1.74        |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 71          |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 347000      |
------------------------------------
Eval num_timesteps=347500, episode_reward=-92622.24 +/- 21054.91
Episode length: 85.40 +/- 19.21
-----------------------------------
| eval/              |            |
|    mean action     | -0.6710748 |
|    mean velocity x | 2.19       |
|    mean velocity y | 3.84       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 85.4       |
|    mean_reward     | -9.26e+04  |
| time/              |            |
|    total_timesteps | 347500     |
-----------------------------------
Eval num_timesteps=348000, episode_reward=-70554.43 +/- 43807.26
Episode length: 57.20 +/- 20.57
-----------------------------------
| eval/              |            |
|    mean action     | -0.6053456 |
|    mean velocity x | 2.03       |
|    mean velocity y | 3.48       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 57.2       |
|    mean_reward     | -7.06e+04  |
| time/              |            |
|    total_timesteps | 348000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.5      |
|    ep_rew_mean     | -8.61e+04 |
| time/              |           |
|    fps             | 68        |
|    iterations      | 170       |
|    time_elapsed    | 5047      |
|    total_timesteps | 348160    |
----------------------------------
Eval num_timesteps=348500, episode_reward=-58369.90 +/- 32663.60
Episode length: 58.00 +/- 13.46
------------------------------------------
| eval/                   |              |
|    mean action          | 0.17243549   |
|    mean velocity x      | -0.579       |
|    mean velocity y      | -1.55        |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 58           |
|    mean_reward          | -5.84e+04    |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0042463536 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0607       |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+08      |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00419     |
|    std                  | 0.948        |
|    value_loss           | 2.5e+08      |
------------------------------------------
Eval num_timesteps=349000, episode_reward=-82843.91 +/- 15401.22
Episode length: 81.20 +/- 9.47
------------------------------------
| eval/              |             |
|    mean action     | -0.16003002 |
|    mean velocity x | -0.255      |
|    mean velocity y | 0.948       |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 81.2        |
|    mean_reward     | -8.28e+04   |
| time/              |             |
|    total_timesteps | 349000      |
------------------------------------
Eval num_timesteps=349500, episode_reward=-71301.97 +/- 36883.65
Episode length: 65.20 +/- 27.44
----------------------------------
| eval/              |           |
|    mean action     | 0.1997574 |
|    mean velocity x | 0.722     |
|    mean velocity y | -0.258    |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 65.2      |
|    mean_reward     | -7.13e+04 |
| time/              |           |
|    total_timesteps | 349500    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-90434.39 +/- 24037.35
Episode length: 70.60 +/- 5.64
-----------------------------------
| eval/              |            |
|    mean action     | 0.18205792 |
|    mean velocity x | -2.25      |
|    mean velocity y | -0.867     |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 70.6       |
|    mean_reward     | -9.04e+04  |
| time/              |            |
|    total_timesteps | 350000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.4     |
|    ep_rew_mean     | -8.4e+04 |
| time/              |          |
|    fps             | 69       |
|    iterations      | 171      |
|    time_elapsed    | 5054     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-72525.71 +/- 33635.76
Episode length: 84.60 +/- 30.70
------------------------------------------
| eval/                   |              |
|    mean action          | 0.36101738   |
|    mean velocity x      | -2.37        |
|    mean velocity y      | -2.92        |
|    mean velocity z      | 17.3         |
|    mean_ep_length       | 84.6         |
|    mean_reward          | -7.25e+04    |
| time/                   |              |
|    total_timesteps      | 350500       |
| train/                  |              |
|    approx_kl            | 0.0024467153 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0944       |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+08     |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 0.947        |
|    value_loss           | 2e+08        |
------------------------------------------
Eval num_timesteps=351000, episode_reward=-86037.50 +/- 43187.60
Episode length: 93.00 +/- 47.23
------------------------------------
| eval/              |             |
|    mean action     | -0.22262634 |
|    mean velocity x | 1.83        |
|    mean velocity y | 2.05        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 93          |
|    mean_reward     | -8.6e+04    |
| time/              |             |
|    total_timesteps | 351000      |
------------------------------------
Eval num_timesteps=351500, episode_reward=-47776.99 +/- 31380.02
Episode length: 62.20 +/- 20.06
------------------------------------
| eval/              |             |
|    mean action     | -0.28100133 |
|    mean velocity x | 0.109       |
|    mean velocity y | 1.2         |
|    mean velocity z | 20          |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -4.78e+04   |
| time/              |             |
|    total_timesteps | 351500      |
------------------------------------
Eval num_timesteps=352000, episode_reward=-85437.27 +/- 27410.10
Episode length: 68.00 +/- 8.41
------------------------------------
| eval/              |             |
|    mean action     | -0.06902751 |
|    mean velocity x | 1.6         |
|    mean velocity y | 0.69        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 68          |
|    mean_reward     | -8.54e+04   |
| time/              |             |
|    total_timesteps | 352000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.1      |
|    ep_rew_mean     | -8.64e+04 |
| time/              |           |
|    fps             | 69        |
|    iterations      | 172       |
|    time_elapsed    | 5061      |
|    total_timesteps | 352256    |
----------------------------------
Eval num_timesteps=352500, episode_reward=-83252.25 +/- 26872.38
Episode length: 74.40 +/- 15.47
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.20392187   |
|    mean velocity x      | 0.439         |
|    mean velocity y      | 0.914         |
|    mean velocity z      | 16.8          |
|    mean_ep_length       | 74.4          |
|    mean_reward          | -8.33e+04     |
| time/                   |               |
|    total_timesteps      | 352500        |
| train/                  |               |
|    approx_kl            | 0.00034282132 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.09         |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+08      |
|    n_updates            | 1720          |
|    policy_gradient_loss | -0.000492     |
|    std                  | 0.947         |
|    value_loss           | 2.1e+08       |
-------------------------------------------
Eval num_timesteps=353000, episode_reward=-64577.09 +/- 16049.63
Episode length: 82.20 +/- 24.73
-----------------------------------
| eval/              |            |
|    mean action     | -0.5587954 |
|    mean velocity x | 2.25       |
|    mean velocity y | 3.24       |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 82.2       |
|    mean_reward     | -6.46e+04  |
| time/              |            |
|    total_timesteps | 353000     |
-----------------------------------
Eval num_timesteps=353500, episode_reward=-48722.26 +/- 43506.51
Episode length: 45.60 +/- 26.84
------------------------------------
| eval/              |             |
|    mean action     | -0.16391398 |
|    mean velocity x | 0.606       |
|    mean velocity y | 0.994       |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 45.6        |
|    mean_reward     | -4.87e+04   |
| time/              |             |
|    total_timesteps | 353500      |
------------------------------------
Eval num_timesteps=354000, episode_reward=-30823.18 +/- 31503.64
Episode length: 45.00 +/- 20.82
-----------------------------------
| eval/              |            |
|    mean action     | -0.7720211 |
|    mean velocity x | 2.78       |
|    mean velocity y | 4.48       |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 45         |
|    mean_reward     | -3.08e+04  |
| time/              |            |
|    total_timesteps | 354000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | -8.6e+04 |
| time/              |          |
|    fps             | 69       |
|    iterations      | 173      |
|    time_elapsed    | 5069     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=-84824.16 +/- 26755.24
Episode length: 74.40 +/- 16.17
------------------------------------------
| eval/                   |              |
|    mean action          | 0.22367251   |
|    mean velocity x      | 0.248        |
|    mean velocity y      | -1.15        |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 74.4         |
|    mean_reward          | -8.48e+04    |
| time/                   |              |
|    total_timesteps      | 354500       |
| train/                  |              |
|    approx_kl            | 0.0006242426 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0805       |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+08     |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.947        |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=355000, episode_reward=-89003.56 +/- 19241.58
Episode length: 72.20 +/- 5.64
------------------------------------
| eval/              |             |
|    mean action     | -0.29029912 |
|    mean velocity x | -0.0733     |
|    mean velocity y | 1.43        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 72.2        |
|    mean_reward     | -8.9e+04    |
| time/              |             |
|    total_timesteps | 355000      |
------------------------------------
Eval num_timesteps=355500, episode_reward=-94984.16 +/- 29091.63
Episode length: 85.80 +/- 30.80
-----------------------------------
| eval/              |            |
|    mean action     | 0.08916125 |
|    mean velocity x | -2.25      |
|    mean velocity y | -2.21      |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 85.8       |
|    mean_reward     | -9.5e+04   |
| time/              |            |
|    total_timesteps | 355500     |
-----------------------------------
Eval num_timesteps=356000, episode_reward=-46302.99 +/- 30357.55
Episode length: 64.80 +/- 16.95
-------------------------------------
| eval/              |              |
|    mean action     | -0.036589388 |
|    mean velocity x | -0.0951      |
|    mean velocity y | -0.045       |
|    mean velocity z | 18.7         |
|    mean_ep_length  | 64.8         |
|    mean_reward     | -4.63e+04    |
| time/              |              |
|    total_timesteps | 356000       |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | -8.8e+04 |
| time/              |          |
|    fps             | 70       |
|    iterations      | 174      |
|    time_elapsed    | 5076     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=-31102.47 +/- 27805.87
Episode length: 49.20 +/- 22.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1862627   |
|    mean velocity x      | 1.3          |
|    mean velocity y      | 1.3          |
|    mean velocity z      | 21.5         |
|    mean_ep_length       | 49.2         |
|    mean_reward          | -3.11e+04    |
| time/                   |              |
|    total_timesteps      | 356500       |
| train/                  |              |
|    approx_kl            | 0.0006587204 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0924       |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+08     |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 0.947        |
|    value_loss           | 2.34e+08     |
------------------------------------------
Eval num_timesteps=357000, episode_reward=-65517.26 +/- 39397.21
Episode length: 58.80 +/- 27.07
-----------------------------------
| eval/              |            |
|    mean action     | 0.23763898 |
|    mean velocity x | -0.517     |
|    mean velocity y | -1.93      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -6.55e+04  |
| time/              |            |
|    total_timesteps | 357000     |
-----------------------------------
Eval num_timesteps=357500, episode_reward=-71015.03 +/- 52921.07
Episode length: 54.60 +/- 23.24
-----------------------------------
| eval/              |            |
|    mean action     | -0.2735421 |
|    mean velocity x | 1.03       |
|    mean velocity y | 1.05       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -7.1e+04   |
| time/              |            |
|    total_timesteps | 357500     |
-----------------------------------
Eval num_timesteps=358000, episode_reward=-81908.15 +/- 6218.50
Episode length: 82.20 +/- 9.33
------------------------------------
| eval/              |             |
|    mean action     | -0.32496494 |
|    mean velocity x | 0.244       |
|    mean velocity y | 1.03        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 82.2        |
|    mean_reward     | -8.19e+04   |
| time/              |             |
|    total_timesteps | 358000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.7      |
|    ep_rew_mean     | -8.81e+04 |
| time/              |           |
|    fps             | 70        |
|    iterations      | 175       |
|    time_elapsed    | 5083      |
|    total_timesteps | 358400    |
----------------------------------
Eval num_timesteps=358500, episode_reward=-68756.06 +/- 24109.09
Episode length: 69.00 +/- 10.22
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.5882737     |
|    mean velocity x      | -2.5          |
|    mean velocity y      | -3.86         |
|    mean velocity z      | 18.6          |
|    mean_ep_length       | 69            |
|    mean_reward          | -6.88e+04     |
| time/                   |               |
|    total_timesteps      | 358500        |
| train/                  |               |
|    approx_kl            | 0.00097373815 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.09         |
|    explained_variance   | 0.0841        |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+08      |
|    n_updates            | 1750          |
|    policy_gradient_loss | -0.00133      |
|    std                  | 0.947         |
|    value_loss           | 2.8e+08       |
-------------------------------------------
Eval num_timesteps=359000, episode_reward=-87313.37 +/- 20625.85
Episode length: 70.60 +/- 2.50
------------------------------------
| eval/              |             |
|    mean action     | -0.12031903 |
|    mean velocity x | -0.797      |
|    mean velocity y | -0.746      |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 70.6        |
|    mean_reward     | -8.73e+04   |
| time/              |             |
|    total_timesteps | 359000      |
------------------------------------
Eval num_timesteps=359500, episode_reward=-59213.97 +/- 30094.85
Episode length: 74.40 +/- 31.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.05207937 |
|    mean velocity x | -0.469     |
|    mean velocity y | -0.698     |
|    mean velocity z | 19         |
|    mean_ep_length  | 74.4       |
|    mean_reward     | -5.92e+04  |
| time/              |            |
|    total_timesteps | 359500     |
-----------------------------------
Eval num_timesteps=360000, episode_reward=-67001.74 +/- 35174.93
Episode length: 63.40 +/- 14.32
------------------------------------
| eval/              |             |
|    mean action     | -0.65107155 |
|    mean velocity x | 4.14        |
|    mean velocity y | 5.17        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 360000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.3      |
|    ep_rew_mean     | -8.84e+04 |
| time/              |           |
|    fps             | 70        |
|    iterations      | 176       |
|    time_elapsed    | 5090      |
|    total_timesteps | 360448    |
----------------------------------
Eval num_timesteps=360500, episode_reward=-98137.45 +/- 14005.03
Episode length: 70.60 +/- 1.36
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4533399    |
|    mean velocity x      | 2             |
|    mean velocity y      | 3.59          |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 70.6          |
|    mean_reward          | -9.81e+04     |
| time/                   |               |
|    total_timesteps      | 360500        |
| train/                  |               |
|    approx_kl            | 0.00017560573 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.09         |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+08      |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.000544     |
|    std                  | 0.947         |
|    value_loss           | 2.44e+08      |
-------------------------------------------
Eval num_timesteps=361000, episode_reward=-77417.33 +/- 15088.64
Episode length: 66.40 +/- 3.44
------------------------------------
| eval/              |             |
|    mean action     | 0.057971373 |
|    mean velocity x | -0.703      |
|    mean velocity y | -0.553      |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -7.74e+04   |
| time/              |             |
|    total_timesteps | 361000      |
------------------------------------
Eval num_timesteps=361500, episode_reward=-80535.81 +/- 41053.23
Episode length: 68.00 +/- 25.67
------------------------------------
| eval/              |             |
|    mean action     | 0.028117286 |
|    mean velocity x | -0.192      |
|    mean velocity y | -0.114      |
|    mean velocity z | 22.2        |
|    mean_ep_length  | 68          |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 361500      |
------------------------------------
Eval num_timesteps=362000, episode_reward=-30023.14 +/- 22358.64
Episode length: 46.60 +/- 22.83
-----------------------------------
| eval/              |            |
|    mean action     | 0.15298942 |
|    mean velocity x | -1.03      |
|    mean velocity y | -1.84      |
|    mean velocity z | 19         |
|    mean_ep_length  | 46.6       |
|    mean_reward     | -3e+04     |
| time/              |            |
|    total_timesteps | 362000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.4      |
|    ep_rew_mean     | -9.24e+04 |
| time/              |           |
|    fps             | 71        |
|    iterations      | 177       |
|    time_elapsed    | 5097      |
|    total_timesteps | 362496    |
----------------------------------
Eval num_timesteps=362500, episode_reward=-43281.97 +/- 44419.10
Episode length: 47.80 +/- 21.42
------------------------------------------
| eval/                   |              |
|    mean action          | -0.080416106 |
|    mean velocity x      | -0.682       |
|    mean velocity y      | -1.31        |
|    mean velocity z      | 21           |
|    mean_ep_length       | 47.8         |
|    mean_reward          | -4.33e+04    |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0027580764 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.0707       |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+08     |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00364     |
|    std                  | 0.947        |
|    value_loss           | 3.08e+08     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=-90027.16 +/- 9123.93
Episode length: 75.60 +/- 13.69
------------------------------------
| eval/              |             |
|    mean action     | -0.50830257 |
|    mean velocity x | -0.177      |
|    mean velocity y | 1.8         |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 75.6        |
|    mean_reward     | -9e+04      |
| time/              |             |
|    total_timesteps | 363000      |
------------------------------------
Eval num_timesteps=363500, episode_reward=-71152.12 +/- 35992.51
Episode length: 62.00 +/- 9.80
-----------------------------------
| eval/              |            |
|    mean action     | 0.18589284 |
|    mean velocity x | -1.53      |
|    mean velocity y | -1.62      |
|    mean velocity z | 20         |
|    mean_ep_length  | 62         |
|    mean_reward     | -7.12e+04  |
| time/              |            |
|    total_timesteps | 363500     |
-----------------------------------
Eval num_timesteps=364000, episode_reward=-51520.29 +/- 41604.49
Episode length: 49.80 +/- 17.94
------------------------------------
| eval/              |             |
|    mean action     | -0.35459363 |
|    mean velocity x | 0.135       |
|    mean velocity y | 1.42        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 49.8        |
|    mean_reward     | -5.15e+04   |
| time/              |             |
|    total_timesteps | 364000      |
------------------------------------
Eval num_timesteps=364500, episode_reward=-56887.77 +/- 35849.32
Episode length: 62.80 +/- 15.30
------------------------------------
| eval/              |             |
|    mean action     | -0.16759521 |
|    mean velocity x | -1.06       |
|    mean velocity y | 0.6         |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 62.8        |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 364500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.7      |
|    ep_rew_mean     | -9.09e+04 |
| time/              |           |
|    fps             | 71        |
|    iterations      | 178       |
|    time_elapsed    | 5105      |
|    total_timesteps | 364544    |
----------------------------------
Eval num_timesteps=365000, episode_reward=-51988.48 +/- 33147.10
Episode length: 61.20 +/- 19.58
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3567912  |
|    mean velocity x      | 0.358       |
|    mean velocity y      | 1.8         |
|    mean velocity z      | 20.2        |
|    mean_ep_length       | 61.2        |
|    mean_reward          | -5.2e+04    |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.002613893 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.0907      |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+08    |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00267    |
|    std                  | 0.946       |
|    value_loss           | 2.91e+08    |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=-51649.62 +/- 41224.90
Episode length: 52.20 +/- 20.03
------------------------------------
| eval/              |             |
|    mean action     | -0.36754745 |
|    mean velocity x | -0.361      |
|    mean velocity y | 1.35        |
|    mean velocity z | 16.6        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -5.16e+04   |
| time/              |             |
|    total_timesteps | 365500      |
------------------------------------
Eval num_timesteps=366000, episode_reward=-58392.95 +/- 32557.49
Episode length: 69.60 +/- 32.38
-----------------------------------
| eval/              |            |
|    mean action     | 0.43988234 |
|    mean velocity x | -3.6       |
|    mean velocity y | -4.51      |
|    mean velocity z | 16         |
|    mean_ep_length  | 69.6       |
|    mean_reward     | -5.84e+04  |
| time/              |            |
|    total_timesteps | 366000     |
-----------------------------------
Eval num_timesteps=366500, episode_reward=-99448.79 +/- 7812.34
Episode length: 71.60 +/- 3.50
------------------------------------
| eval/              |             |
|    mean action     | -0.47946516 |
|    mean velocity x | 1.29        |
|    mean velocity y | 2.58        |
|    mean velocity z | 22.7        |
|    mean_ep_length  | 71.6        |
|    mean_reward     | -9.94e+04   |
| time/              |             |
|    total_timesteps | 366500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.6      |
|    ep_rew_mean     | -9.36e+04 |
| time/              |           |
|    fps             | 71        |
|    iterations      | 179       |
|    time_elapsed    | 5112      |
|    total_timesteps | 366592    |
----------------------------------
Eval num_timesteps=367000, episode_reward=-66869.33 +/- 25443.10
Episode length: 70.00 +/- 16.52
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.73810506 |
|    mean velocity x      | 3.1         |
|    mean velocity y      | 4.42        |
|    mean velocity z      | 20.2        |
|    mean_ep_length       | 70          |
|    mean_reward          | -6.69e+04   |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.001323392 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.0945      |
|    learning_rate        | 0.001       |
|    loss                 | 1.64e+08    |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 0.946       |
|    value_loss           | 2.5e+08     |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=-60700.85 +/- 38467.83
Episode length: 53.80 +/- 20.05
------------------------------------
| eval/              |             |
|    mean action     | -0.43770105 |
|    mean velocity x | 1.85        |
|    mean velocity y | 2.58        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -6.07e+04   |
| time/              |             |
|    total_timesteps | 367500      |
------------------------------------
Eval num_timesteps=368000, episode_reward=-65886.21 +/- 33524.74
Episode length: 61.60 +/- 11.15
------------------------------------
| eval/              |             |
|    mean action     | -0.09769437 |
|    mean velocity x | 0.784       |
|    mean velocity y | 1.01        |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -6.59e+04   |
| time/              |             |
|    total_timesteps | 368000      |
------------------------------------
Eval num_timesteps=368500, episode_reward=-72640.40 +/- 43870.23
Episode length: 58.20 +/- 13.41
------------------------------------
| eval/              |             |
|    mean action     | -0.41370478 |
|    mean velocity x | 0.351       |
|    mean velocity y | 1.58        |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -7.26e+04   |
| time/              |             |
|    total_timesteps | 368500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 85        |
|    ep_rew_mean     | -9.81e+04 |
| time/              |           |
|    fps             | 72        |
|    iterations      | 180       |
|    time_elapsed    | 5119      |
|    total_timesteps | 368640    |
----------------------------------
Eval num_timesteps=369000, episode_reward=-74275.70 +/- 17617.43
Episode length: 66.80 +/- 5.23
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46462998  |
|    mean velocity x      | 1.71         |
|    mean velocity y      | 1.48         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 66.8         |
|    mean_reward          | -7.43e+04    |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0050889566 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.085        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+08     |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00533     |
|    std                  | 0.943        |
|    value_loss           | 2.47e+08     |
------------------------------------------
Eval num_timesteps=369500, episode_reward=-85574.56 +/- 20658.78
Episode length: 81.40 +/- 25.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.11285759 |
|    mean velocity x | -2.17      |
|    mean velocity y | -1.93      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 81.4       |
|    mean_reward     | -8.56e+04  |
| time/              |            |
|    total_timesteps | 369500     |
-----------------------------------
Eval num_timesteps=370000, episode_reward=-90389.22 +/- 34457.23
Episode length: 63.80 +/- 8.45
-----------------------------------
| eval/              |            |
|    mean action     | -0.2614565 |
|    mean velocity x | 1.05       |
|    mean velocity y | 2.58       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 63.8       |
|    mean_reward     | -9.04e+04  |
| time/              |            |
|    total_timesteps | 370000     |
-----------------------------------
Eval num_timesteps=370500, episode_reward=-91693.24 +/- 26666.11
Episode length: 67.80 +/- 4.53
------------------------------------
| eval/              |             |
|    mean action     | -0.15959531 |
|    mean velocity x | -0.027      |
|    mean velocity y | 0.315       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 67.8        |
|    mean_reward     | -9.17e+04   |
| time/              |             |
|    total_timesteps | 370500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.7      |
|    ep_rew_mean     | -9.26e+04 |
| time/              |           |
|    fps             | 72        |
|    iterations      | 181       |
|    time_elapsed    | 5126      |
|    total_timesteps | 370688    |
----------------------------------
Eval num_timesteps=371000, episode_reward=-87095.46 +/- 21251.29
Episode length: 68.00 +/- 6.72
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.2132947  |
|    mean velocity x      | 1.07        |
|    mean velocity y      | 1.56        |
|    mean velocity z      | 16.7        |
|    mean_ep_length       | 68          |
|    mean_reward          | -8.71e+04   |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.003763938 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.109       |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+08    |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00407    |
|    std                  | 0.943       |
|    value_loss           | 2.09e+08    |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=-48679.40 +/- 20762.96
Episode length: 58.80 +/- 2.93
------------------------------------
| eval/              |             |
|    mean action     | -0.44151348 |
|    mean velocity x | 1.87        |
|    mean velocity y | 4.18        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 58.8        |
|    mean_reward     | -4.87e+04   |
| time/              |             |
|    total_timesteps | 371500      |
------------------------------------
Eval num_timesteps=372000, episode_reward=-78875.23 +/- 33314.78
Episode length: 67.60 +/- 17.90
------------------------------------
| eval/              |             |
|    mean action     | -0.23845156 |
|    mean velocity x | 0.805       |
|    mean velocity y | 2.35        |
|    mean velocity z | 22.4        |
|    mean_ep_length  | 67.6        |
|    mean_reward     | -7.89e+04   |
| time/              |             |
|    total_timesteps | 372000      |
------------------------------------
Eval num_timesteps=372500, episode_reward=-80674.43 +/- 21107.33
Episode length: 74.80 +/- 9.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.08079919 |
|    mean velocity x | 0.854      |
|    mean velocity y | -0.704     |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 74.8       |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 372500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.1      |
|    ep_rew_mean     | -9.05e+04 |
| time/              |           |
|    fps             | 72        |
|    iterations      | 182       |
|    time_elapsed    | 5133      |
|    total_timesteps | 372736    |
----------------------------------
Eval num_timesteps=373000, episode_reward=-75514.92 +/- 31417.90
Episode length: 74.00 +/- 27.31
------------------------------------------
| eval/                   |              |
|    mean action          | -0.020828834 |
|    mean velocity x      | -0.669       |
|    mean velocity y      | -0.47        |
|    mean velocity z      | 20.4         |
|    mean_ep_length       | 74           |
|    mean_reward          | -7.55e+04    |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0034003174 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+08     |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00361     |
|    std                  | 0.943        |
|    value_loss           | 2.45e+08     |
------------------------------------------
Eval num_timesteps=373500, episode_reward=-85029.89 +/- 19882.34
Episode length: 68.40 +/- 7.17
------------------------------------
| eval/              |             |
|    mean action     | -0.21873084 |
|    mean velocity x | -0.141      |
|    mean velocity y | 1.42        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 373500      |
------------------------------------
Eval num_timesteps=374000, episode_reward=-86698.47 +/- 18133.34
Episode length: 64.00 +/- 3.95
----------------------------------
| eval/              |           |
|    mean action     | 0.1519826 |
|    mean velocity x | -1.46     |
|    mean velocity y | -1.69     |
|    mean velocity z | 20.6      |
|    mean_ep_length  | 64        |
|    mean_reward     | -8.67e+04 |
| time/              |           |
|    total_timesteps | 374000    |
----------------------------------
Eval num_timesteps=374500, episode_reward=-44853.03 +/- 31900.13
Episode length: 58.20 +/- 25.11
------------------------------------
| eval/              |             |
|    mean action     | -0.38232136 |
|    mean velocity x | 3.01        |
|    mean velocity y | 3.01        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -4.49e+04   |
| time/              |             |
|    total_timesteps | 374500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.9      |
|    ep_rew_mean     | -9.05e+04 |
| time/              |           |
|    fps             | 72        |
|    iterations      | 183       |
|    time_elapsed    | 5144      |
|    total_timesteps | 374784    |
----------------------------------
Eval num_timesteps=375000, episode_reward=-53363.14 +/- 28675.65
Episode length: 58.40 +/- 25.91
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31449473  |
|    mean velocity x      | 3.04         |
|    mean velocity y      | 2.32         |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 58.4         |
|    mean_reward          | -5.34e+04    |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0020045196 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.001        |
|    loss                 | 7.14e+07     |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.943        |
|    value_loss           | 2.47e+08     |
------------------------------------------
Eval num_timesteps=375500, episode_reward=-62537.93 +/- 52350.98
Episode length: 62.60 +/- 42.38
------------------------------------
| eval/              |             |
|    mean action     | 0.046337273 |
|    mean velocity x | -1.8        |
|    mean velocity y | -1.25       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 62.6        |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 375500      |
------------------------------------
Eval num_timesteps=376000, episode_reward=-58859.64 +/- 48075.17
Episode length: 74.20 +/- 56.45
-----------------------------------
| eval/              |            |
|    mean action     | -0.4603626 |
|    mean velocity x | 3.12       |
|    mean velocity y | 2.99       |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 74.2       |
|    mean_reward     | -5.89e+04  |
| time/              |            |
|    total_timesteps | 376000     |
-----------------------------------
Eval num_timesteps=376500, episode_reward=-61067.34 +/- 31746.78
Episode length: 57.20 +/- 9.85
------------------------------------
| eval/              |             |
|    mean action     | -0.30127662 |
|    mean velocity x | 0.161       |
|    mean velocity y | 2.27        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.11e+04   |
| time/              |             |
|    total_timesteps | 376500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.2      |
|    ep_rew_mean     | -8.77e+04 |
| time/              |           |
|    fps             | 73        |
|    iterations      | 184       |
|    time_elapsed    | 5151      |
|    total_timesteps | 376832    |
----------------------------------
Eval num_timesteps=377000, episode_reward=-71426.62 +/- 23810.72
Episode length: 60.80 +/- 7.36
------------------------------------------
| eval/                   |              |
|    mean action          | -0.44963795  |
|    mean velocity x      | 2.86         |
|    mean velocity y      | 3.44         |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 60.8         |
|    mean_reward          | -7.14e+04    |
| time/                   |              |
|    total_timesteps      | 377000       |
| train/                  |              |
|    approx_kl            | 0.0025132708 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+08     |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.00244     |
|    std                  | 0.941        |
|    value_loss           | 2.05e+08     |
------------------------------------------
Eval num_timesteps=377500, episode_reward=-68621.76 +/- 24095.68
Episode length: 69.00 +/- 13.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.56317526 |
|    mean velocity x | -0.098     |
|    mean velocity y | -2.78      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 69         |
|    mean_reward     | -6.86e+04  |
| time/              |            |
|    total_timesteps | 377500     |
-----------------------------------
Eval num_timesteps=378000, episode_reward=-67272.92 +/- 35437.15
Episode length: 55.00 +/- 16.06
-----------------------------------
| eval/              |            |
|    mean action     | -0.4210691 |
|    mean velocity x | 1.27       |
|    mean velocity y | 2.96       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 55         |
|    mean_reward     | -6.73e+04  |
| time/              |            |
|    total_timesteps | 378000     |
-----------------------------------
Eval num_timesteps=378500, episode_reward=-65975.06 +/- 37576.57
Episode length: 56.60 +/- 20.64
-----------------------------------
| eval/              |            |
|    mean action     | 0.49528405 |
|    mean velocity x | -1.09      |
|    mean velocity y | -3.13      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -6.6e+04   |
| time/              |            |
|    total_timesteps | 378500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.1      |
|    ep_rew_mean     | -8.61e+04 |
| time/              |           |
|    fps             | 73        |
|    iterations      | 185       |
|    time_elapsed    | 5158      |
|    total_timesteps | 378880    |
----------------------------------
Eval num_timesteps=379000, episode_reward=-62110.78 +/- 33966.11
Episode length: 59.80 +/- 24.49
------------------------------------------
| eval/                   |              |
|    mean action          | 0.1143296    |
|    mean velocity x      | -0.119       |
|    mean velocity y      | 0.373        |
|    mean velocity z      | 17.2         |
|    mean_ep_length       | 59.8         |
|    mean_reward          | -6.21e+04    |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0024547903 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.001        |
|    loss                 | 1.64e+08     |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 0.938        |
|    value_loss           | 2.18e+08     |
------------------------------------------
Eval num_timesteps=379500, episode_reward=-67936.06 +/- 36481.33
Episode length: 66.00 +/- 25.23
-----------------------------------
| eval/              |            |
|    mean action     | 0.04982077 |
|    mean velocity x | 0.281      |
|    mean velocity y | 0.212      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 66         |
|    mean_reward     | -6.79e+04  |
| time/              |            |
|    total_timesteps | 379500     |
-----------------------------------
Eval num_timesteps=380000, episode_reward=-66231.71 +/- 40141.85
Episode length: 57.20 +/- 17.29
------------------------------------
| eval/              |             |
|    mean action     | 0.035331164 |
|    mean velocity x | -0.238      |
|    mean velocity y | 0.214       |
|    mean velocity z | 20          |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.62e+04   |
| time/              |             |
|    total_timesteps | 380000      |
------------------------------------
Eval num_timesteps=380500, episode_reward=-84465.80 +/- 35652.30
Episode length: 61.00 +/- 9.61
-----------------------------------
| eval/              |            |
|    mean action     | -0.2581695 |
|    mean velocity x | 0.382      |
|    mean velocity y | 1.74       |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 61         |
|    mean_reward     | -8.45e+04  |
| time/              |            |
|    total_timesteps | 380500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.2      |
|    ep_rew_mean     | -8.03e+04 |
| time/              |           |
|    fps             | 73        |
|    iterations      | 186       |
|    time_elapsed    | 5165      |
|    total_timesteps | 380928    |
----------------------------------
Eval num_timesteps=381000, episode_reward=-82950.97 +/- 31456.80
Episode length: 63.80 +/- 9.58
------------------------------------------
| eval/                   |              |
|    mean action          | 0.16336684   |
|    mean velocity x      | -0.239       |
|    mean velocity y      | -0.718       |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 63.8         |
|    mean_reward          | -8.3e+04     |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0014613123 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.001        |
|    loss                 | 8.79e+07     |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.938        |
|    value_loss           | 2.11e+08     |
------------------------------------------
Eval num_timesteps=381500, episode_reward=-67119.69 +/- 26942.84
Episode length: 60.00 +/- 6.16
------------------------------------
| eval/              |             |
|    mean action     | -0.06657503 |
|    mean velocity x | 0.913       |
|    mean velocity y | 1.35        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 60          |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 381500      |
------------------------------------
Eval num_timesteps=382000, episode_reward=-70625.91 +/- 38582.44
Episode length: 66.80 +/- 20.99
------------------------------------
| eval/              |             |
|    mean action     | -0.31951118 |
|    mean velocity x | 3.08        |
|    mean velocity y | 3.24        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -7.06e+04   |
| time/              |             |
|    total_timesteps | 382000      |
------------------------------------
Eval num_timesteps=382500, episode_reward=-60810.85 +/- 35385.55
Episode length: 57.20 +/- 22.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.07658717 |
|    mean velocity x | 0.872      |
|    mean velocity y | 0.617      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 57.2       |
|    mean_reward     | -6.08e+04  |
| time/              |            |
|    total_timesteps | 382500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.6      |
|    ep_rew_mean     | -7.66e+04 |
| time/              |           |
|    fps             | 74        |
|    iterations      | 187       |
|    time_elapsed    | 5173      |
|    total_timesteps | 382976    |
----------------------------------
Eval num_timesteps=383000, episode_reward=-64244.60 +/- 32926.99
Episode length: 63.60 +/- 24.15
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37975731  |
|    mean velocity x      | 2.44         |
|    mean velocity y      | 2.51         |
|    mean velocity z      | 18.3         |
|    mean_ep_length       | 63.6         |
|    mean_reward          | -6.42e+04    |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0031213027 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.104        |
|    learning_rate        | 0.001        |
|    loss                 | 9.95e+07     |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.937        |
|    value_loss           | 2.22e+08     |
------------------------------------------
Eval num_timesteps=383500, episode_reward=-72296.57 +/- 31631.58
Episode length: 65.80 +/- 11.72
------------------------------------
| eval/              |             |
|    mean action     | -0.57074696 |
|    mean velocity x | 2.94        |
|    mean velocity y | 4.82        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 65.8        |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 383500      |
------------------------------------
Eval num_timesteps=384000, episode_reward=-80654.60 +/- 42878.07
Episode length: 60.80 +/- 25.89
-----------------------------------
| eval/              |            |
|    mean action     | -0.3655427 |
|    mean velocity x | 3.05       |
|    mean velocity y | 3          |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 60.8       |
|    mean_reward     | -8.07e+04  |
| time/              |            |
|    total_timesteps | 384000     |
-----------------------------------
Eval num_timesteps=384500, episode_reward=-65208.99 +/- 27614.57
Episode length: 72.60 +/- 15.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.19994593 |
|    mean velocity x | -0.645     |
|    mean velocity y | -1.14      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 72.6       |
|    mean_reward     | -6.52e+04  |
| time/              |            |
|    total_timesteps | 384500     |
-----------------------------------
Eval num_timesteps=385000, episode_reward=-61849.92 +/- 36927.12
Episode length: 55.20 +/- 14.55
------------------------------------
| eval/              |             |
|    mean action     | -0.21516965 |
|    mean velocity x | 1.85        |
|    mean velocity y | 1.94        |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 385000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.9      |
|    ep_rew_mean     | -7.82e+04 |
| time/              |           |
|    fps             | 74        |
|    iterations      | 188       |
|    time_elapsed    | 5180      |
|    total_timesteps | 385024    |
----------------------------------
Eval num_timesteps=385500, episode_reward=-52053.98 +/- 30939.03
Episode length: 54.00 +/- 10.14
------------------------------------------
| eval/                   |              |
|    mean action          | 0.07690543   |
|    mean velocity x      | 0.9          |
|    mean velocity y      | -0.809       |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 54           |
|    mean_reward          | -5.21e+04    |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 0.0034260899 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+08     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00339     |
|    std                  | 0.935        |
|    value_loss           | 2.35e+08     |
------------------------------------------
Eval num_timesteps=386000, episode_reward=-101839.90 +/- 29061.64
Episode length: 90.80 +/- 32.35
------------------------------------
| eval/              |             |
|    mean action     | -0.41940507 |
|    mean velocity x | 3.07        |
|    mean velocity y | 4.13        |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 90.8        |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 386000      |
------------------------------------
Eval num_timesteps=386500, episode_reward=-71386.28 +/- 43409.09
Episode length: 55.00 +/- 23.18
------------------------------------
| eval/              |             |
|    mean action     | -0.16017658 |
|    mean velocity x | -0.184      |
|    mean velocity y | 1.21        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 55          |
|    mean_reward     | -7.14e+04   |
| time/              |             |
|    total_timesteps | 386500      |
------------------------------------
Eval num_timesteps=387000, episode_reward=-50554.63 +/- 40520.14
Episode length: 55.60 +/- 30.81
-------------------------------------
| eval/              |              |
|    mean action     | -0.060361844 |
|    mean velocity x | 1.54         |
|    mean velocity y | 0.619        |
|    mean velocity z | 18.9         |
|    mean_ep_length  | 55.6         |
|    mean_reward     | -5.06e+04    |
| time/              |              |
|    total_timesteps | 387000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.23e+04 |
| time/              |           |
|    fps             | 74        |
|    iterations      | 189       |
|    time_elapsed    | 5187      |
|    total_timesteps | 387072    |
----------------------------------
Eval num_timesteps=387500, episode_reward=-74654.20 +/- 15681.23
Episode length: 65.60 +/- 8.87
----------------------------------------
| eval/                   |            |
|    mean action          | 0.49684212 |
|    mean velocity x      | -3.01      |
|    mean velocity y      | -3.94      |
|    mean velocity z      | 17.1       |
|    mean_ep_length       | 65.6       |
|    mean_reward          | -7.47e+04  |
| time/                   |            |
|    total_timesteps      | 387500     |
| train/                  |            |
|    approx_kl            | 0.00056627 |
|    clip_fraction        | 0.000244   |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.05      |
|    explained_variance   | 0.107      |
|    learning_rate        | 0.001      |
|    loss                 | 7.12e+07   |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.00126   |
|    std                  | 0.935      |
|    value_loss           | 2.69e+08   |
----------------------------------------
Eval num_timesteps=388000, episode_reward=-49875.48 +/- 38853.94
Episode length: 48.80 +/- 28.51
------------------------------------
| eval/              |             |
|    mean action     | -0.19761026 |
|    mean velocity x | -1.17       |
|    mean velocity y | -0.882      |
|    mean velocity z | 18          |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -4.99e+04   |
| time/              |             |
|    total_timesteps | 388000      |
------------------------------------
Eval num_timesteps=388500, episode_reward=-85694.23 +/- 42709.47
Episode length: 61.00 +/- 16.76
----------------------------------
| eval/              |           |
|    mean action     | 0.2913676 |
|    mean velocity x | 0.14      |
|    mean velocity y | -2.23     |
|    mean velocity z | 18.1      |
|    mean_ep_length  | 61        |
|    mean_reward     | -8.57e+04 |
| time/              |           |
|    total_timesteps | 388500    |
----------------------------------
Eval num_timesteps=389000, episode_reward=-93440.82 +/- 10775.45
Episode length: 66.60 +/- 2.58
------------------------------------
| eval/              |             |
|    mean action     | -0.11200001 |
|    mean velocity x | -0.251      |
|    mean velocity y | 0.458       |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 66.6        |
|    mean_reward     | -9.34e+04   |
| time/              |             |
|    total_timesteps | 389000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | -8.6e+04 |
| time/              |          |
|    fps             | 74       |
|    iterations      | 190      |
|    time_elapsed    | 5194     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=-84046.84 +/- 15995.73
Episode length: 72.40 +/- 8.96
------------------------------------------
| eval/                   |              |
|    mean action          | -0.8009001   |
|    mean velocity x      | 2.05         |
|    mean velocity y      | 4.52         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 72.4         |
|    mean_reward          | -8.4e+04     |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0007811938 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+08     |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 0.935        |
|    value_loss           | 2.37e+08     |
------------------------------------------
Eval num_timesteps=390000, episode_reward=-76484.60 +/- 22408.79
Episode length: 68.60 +/- 10.11
------------------------------------
| eval/              |             |
|    mean action     | -0.31667697 |
|    mean velocity x | 1.94        |
|    mean velocity y | 3.28        |
|    mean velocity z | 17          |
|    mean_ep_length  | 68.6        |
|    mean_reward     | -7.65e+04   |
| time/              |             |
|    total_timesteps | 390000      |
------------------------------------
Eval num_timesteps=390500, episode_reward=-35318.00 +/- 30772.94
Episode length: 49.40 +/- 32.65
-----------------------------------
| eval/              |            |
|    mean action     | 0.08320087 |
|    mean velocity x | 0.755      |
|    mean velocity y | 0.68       |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 49.4       |
|    mean_reward     | -3.53e+04  |
| time/              |            |
|    total_timesteps | 390500     |
-----------------------------------
Eval num_timesteps=391000, episode_reward=-68959.15 +/- 32360.02
Episode length: 63.80 +/- 16.12
-----------------------------------
| eval/              |            |
|    mean action     | -0.5008215 |
|    mean velocity x | 3.34       |
|    mean velocity y | 3.91       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 63.8       |
|    mean_reward     | -6.9e+04   |
| time/              |            |
|    total_timesteps | 391000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | -9.1e+04 |
| time/              |          |
|    fps             | 75       |
|    iterations      | 191      |
|    time_elapsed    | 5201     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=-78120.45 +/- 43052.83
Episode length: 103.80 +/- 54.36
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2025621    |
|    mean velocity x      | -1.06        |
|    mean velocity y      | -1.57        |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 104          |
|    mean_reward          | -7.81e+04    |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0018945647 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.0891       |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+08     |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.935        |
|    value_loss           | 2.61e+08     |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-72101.98 +/- 13332.84
Episode length: 72.20 +/- 3.66
------------------------------------
| eval/              |             |
|    mean action     | -0.48707566 |
|    mean velocity x | 2.77        |
|    mean velocity y | 4.25        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 72.2        |
|    mean_reward     | -7.21e+04   |
| time/              |             |
|    total_timesteps | 392000      |
------------------------------------
Eval num_timesteps=392500, episode_reward=-78528.73 +/- 23336.57
Episode length: 65.60 +/- 10.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.63358414 |
|    mean velocity x | -2.4       |
|    mean velocity y | -4.88      |
|    mean velocity z | 15.1       |
|    mean_ep_length  | 65.6       |
|    mean_reward     | -7.85e+04  |
| time/              |            |
|    total_timesteps | 392500     |
-----------------------------------
Eval num_timesteps=393000, episode_reward=-62294.49 +/- 19068.84
Episode length: 60.20 +/- 6.65
------------------------------------
| eval/              |             |
|    mean action     | -0.16223703 |
|    mean velocity x | -0.211      |
|    mean velocity y | 0.262       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 60.2        |
|    mean_reward     | -6.23e+04   |
| time/              |             |
|    total_timesteps | 393000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | -8.6e+04 |
| time/              |          |
|    fps             | 75       |
|    iterations      | 192      |
|    time_elapsed    | 5209     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=-69070.75 +/- 28170.00
Episode length: 69.20 +/- 14.69
------------------------------------------
| eval/                   |              |
|    mean action          | -0.041614305 |
|    mean velocity x      | 1.33         |
|    mean velocity y      | 1.82         |
|    mean velocity z      | 18           |
|    mean_ep_length       | 69.2         |
|    mean_reward          | -6.91e+04    |
| time/                   |              |
|    total_timesteps      | 393500       |
| train/                  |              |
|    approx_kl            | 0.0023271306 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.001        |
|    loss                 | 9.81e+07     |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 0.934        |
|    value_loss           | 1.96e+08     |
------------------------------------------
Eval num_timesteps=394000, episode_reward=-77394.26 +/- 32924.13
Episode length: 62.20 +/- 9.37
------------------------------------
| eval/              |             |
|    mean action     | -0.04576004 |
|    mean velocity x | -1.03       |
|    mean velocity y | -0.501      |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -7.74e+04   |
| time/              |             |
|    total_timesteps | 394000      |
------------------------------------
Eval num_timesteps=394500, episode_reward=-50379.08 +/- 35781.46
Episode length: 65.20 +/- 28.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.21288763 |
|    mean velocity x | -0.381     |
|    mean velocity y | -0.567     |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 65.2       |
|    mean_reward     | -5.04e+04  |
| time/              |            |
|    total_timesteps | 394500     |
-----------------------------------
Eval num_timesteps=395000, episode_reward=-81340.14 +/- 20310.80
Episode length: 80.80 +/- 22.14
----------------------------------
| eval/              |           |
|    mean action     | 0.3286245 |
|    mean velocity x | -1.16     |
|    mean velocity y | -1.8      |
|    mean velocity z | 17.6      |
|    mean_ep_length  | 80.8      |
|    mean_reward     | -8.13e+04 |
| time/              |           |
|    total_timesteps | 395000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.6      |
|    ep_rew_mean     | -8.43e+04 |
| time/              |           |
|    fps             | 75        |
|    iterations      | 193       |
|    time_elapsed    | 5216      |
|    total_timesteps | 395264    |
----------------------------------
Eval num_timesteps=395500, episode_reward=-89164.70 +/- 34052.07
Episode length: 62.60 +/- 7.23
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18866555  |
|    mean velocity x      | 0.701        |
|    mean velocity y      | 0.155        |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -8.92e+04    |
| time/                   |              |
|    total_timesteps      | 395500       |
| train/                  |              |
|    approx_kl            | 0.0042173145 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.112        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+08     |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00473     |
|    std                  | 0.936        |
|    value_loss           | 2.42e+08     |
------------------------------------------
Eval num_timesteps=396000, episode_reward=-87214.67 +/- 18615.48
Episode length: 64.40 +/- 4.32
-----------------------------------
| eval/              |            |
|    mean action     | 0.34719503 |
|    mean velocity x | -1.44      |
|    mean velocity y | -1.64      |
|    mean velocity z | 19         |
|    mean_ep_length  | 64.4       |
|    mean_reward     | -8.72e+04  |
| time/              |            |
|    total_timesteps | 396000     |
-----------------------------------
Eval num_timesteps=396500, episode_reward=-79080.55 +/- 31362.49
Episode length: 62.20 +/- 10.28
------------------------------------
| eval/              |             |
|    mean action     | 0.018396752 |
|    mean velocity x | -0.914      |
|    mean velocity y | -0.415      |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -7.91e+04   |
| time/              |             |
|    total_timesteps | 396500      |
------------------------------------
Eval num_timesteps=397000, episode_reward=-62354.05 +/- 38290.50
Episode length: 58.40 +/- 10.74
------------------------------------
| eval/              |             |
|    mean action     | 0.031573486 |
|    mean velocity x | -0.412      |
|    mean velocity y | -0.123      |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -6.24e+04   |
| time/              |             |
|    total_timesteps | 397000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.16e+04 |
| time/              |           |
|    fps             | 76        |
|    iterations      | 194       |
|    time_elapsed    | 5223      |
|    total_timesteps | 397312    |
----------------------------------
Eval num_timesteps=397500, episode_reward=-75929.84 +/- 9469.39
Episode length: 64.40 +/- 3.61
----------------------------------------
| eval/                   |            |
|    mean action          | 0.20439178 |
|    mean velocity x      | -0.0906    |
|    mean velocity y      | -0.814     |
|    mean velocity z      | 20.1       |
|    mean_ep_length       | 64.4       |
|    mean_reward          | -7.59e+04  |
| time/                   |            |
|    total_timesteps      | 397500     |
| train/                  |            |
|    approx_kl            | 0.00196073 |
|    clip_fraction        | 0.00176    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 0.11       |
|    learning_rate        | 0.001      |
|    loss                 | 2.36e+08   |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.00228   |
|    std                  | 0.936      |
|    value_loss           | 2.47e+08   |
----------------------------------------
Eval num_timesteps=398000, episode_reward=-77202.31 +/- 21505.93
Episode length: 72.20 +/- 18.21
-----------------------------------
| eval/              |            |
|    mean action     | 0.27245837 |
|    mean velocity x | 1.06       |
|    mean velocity y | -1.44      |
|    mean velocity z | 21.5       |
|    mean_ep_length  | 72.2       |
|    mean_reward     | -7.72e+04  |
| time/              |            |
|    total_timesteps | 398000     |
-----------------------------------
Eval num_timesteps=398500, episode_reward=-47510.63 +/- 33341.36
Episode length: 59.00 +/- 23.52
-----------------------------------
| eval/              |            |
|    mean action     | -0.3919766 |
|    mean velocity x | 0.664      |
|    mean velocity y | 3.21       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 59         |
|    mean_reward     | -4.75e+04  |
| time/              |            |
|    total_timesteps | 398500     |
-----------------------------------
Eval num_timesteps=399000, episode_reward=-82114.27 +/- 23136.55
Episode length: 67.00 +/- 6.81
------------------------------------
| eval/              |             |
|    mean action     | 0.053605285 |
|    mean velocity x | -1.71       |
|    mean velocity y | -0.632      |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 67          |
|    mean_reward     | -8.21e+04   |
| time/              |             |
|    total_timesteps | 399000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75        |
|    ep_rew_mean     | -8.14e+04 |
| time/              |           |
|    fps             | 76        |
|    iterations      | 195       |
|    time_elapsed    | 5230      |
|    total_timesteps | 399360    |
----------------------------------
Eval num_timesteps=399500, episode_reward=-55470.75 +/- 36099.88
Episode length: 53.60 +/- 20.65
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5686579   |
|    mean velocity x      | 0.579        |
|    mean velocity y      | 1.54         |
|    mean velocity z      | 17           |
|    mean_ep_length       | 53.6         |
|    mean_reward          | -5.55e+04    |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0017144287 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.101        |
|    learning_rate        | 0.001        |
|    loss                 | 9.17e+07     |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00228     |
|    std                  | 0.937        |
|    value_loss           | 2.25e+08     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-62188.14 +/- 31305.13
Episode length: 60.80 +/- 19.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.44143713 |
|    mean velocity x | -2.74      |
|    mean velocity y | -1.45      |
|    mean velocity z | 14.6       |
|    mean_ep_length  | 60.8       |
|    mean_reward     | -6.22e+04  |
| time/              |            |
|    total_timesteps | 400000     |
-----------------------------------
Eval num_timesteps=400500, episode_reward=-55248.74 +/- 39306.02
Episode length: 52.00 +/- 14.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.05214139 |
|    mean velocity x | 0.379      |
|    mean velocity y | 0.426      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 52         |
|    mean_reward     | -5.52e+04  |
| time/              |            |
|    total_timesteps | 400500     |
-----------------------------------
Eval num_timesteps=401000, episode_reward=-43815.69 +/- 38281.95
Episode length: 51.20 +/- 24.81
------------------------------------
| eval/              |             |
|    mean action     | -0.13776483 |
|    mean velocity x | 2.32        |
|    mean velocity y | 1.62        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 51.2        |
|    mean_reward     | -4.38e+04   |
| time/              |             |
|    total_timesteps | 401000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.4      |
|    ep_rew_mean     | -7.98e+04 |
| time/              |           |
|    fps             | 76        |
|    iterations      | 196       |
|    time_elapsed    | 5237      |
|    total_timesteps | 401408    |
----------------------------------
Eval num_timesteps=401500, episode_reward=-83705.47 +/- 11200.28
Episode length: 75.20 +/- 3.60
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.1027048   |
|    mean velocity x      | -0.232      |
|    mean velocity y      | 0.0898      |
|    mean velocity z      | 17.4        |
|    mean_ep_length       | 75.2        |
|    mean_reward          | -8.37e+04   |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.002597872 |
|    clip_fraction        | 0.00586     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.0884      |
|    learning_rate        | 0.001       |
|    loss                 | 9.18e+07    |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00275    |
|    std                  | 0.937       |
|    value_loss           | 1.95e+08    |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-67159.89 +/- 27130.21
Episode length: 69.60 +/- 15.89
------------------------------------
| eval/              |             |
|    mean action     | -0.39414805 |
|    mean velocity x | 1.21        |
|    mean velocity y | 2.91        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 69.6        |
|    mean_reward     | -6.72e+04   |
| time/              |             |
|    total_timesteps | 402000      |
------------------------------------
Eval num_timesteps=402500, episode_reward=-54558.20 +/- 34677.10
Episode length: 57.60 +/- 16.60
------------------------------------
| eval/              |             |
|    mean action     | 0.075159654 |
|    mean velocity x | -0.427      |
|    mean velocity y | -0.98       |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -5.46e+04   |
| time/              |             |
|    total_timesteps | 402500      |
------------------------------------
Eval num_timesteps=403000, episode_reward=-70496.16 +/- 43859.62
Episode length: 56.00 +/- 23.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.40351605 |
|    mean velocity x | -2.47      |
|    mean velocity y | -2.35      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 56         |
|    mean_reward     | -7.05e+04  |
| time/              |            |
|    total_timesteps | 403000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.7      |
|    ep_rew_mean     | -7.97e+04 |
| time/              |           |
|    fps             | 76        |
|    iterations      | 197       |
|    time_elapsed    | 5244      |
|    total_timesteps | 403456    |
----------------------------------
Eval num_timesteps=403500, episode_reward=-56212.67 +/- 38516.76
Episode length: 76.40 +/- 47.37
------------------------------------------
| eval/                   |              |
|    mean action          | 0.0141511    |
|    mean velocity x      | 1.16         |
|    mean velocity y      | 0.782        |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 76.4         |
|    mean_reward          | -5.62e+04    |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0034396406 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.937        |
|    value_loss           | 2.41e+08     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-56056.55 +/- 43822.18
Episode length: 57.00 +/- 30.70
-----------------------------------
| eval/              |            |
|    mean action     | 0.04474859 |
|    mean velocity x | 0.607      |
|    mean velocity y | 0.858      |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 57         |
|    mean_reward     | -5.61e+04  |
| time/              |            |
|    total_timesteps | 404000     |
-----------------------------------
Eval num_timesteps=404500, episode_reward=-51355.91 +/- 46703.59
Episode length: 74.80 +/- 41.07
-----------------------------------
| eval/              |            |
|    mean action     | 0.36530823 |
|    mean velocity x | -0.49      |
|    mean velocity y | -2.13      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 74.8       |
|    mean_reward     | -5.14e+04  |
| time/              |            |
|    total_timesteps | 404500     |
-----------------------------------
Eval num_timesteps=405000, episode_reward=-71932.36 +/- 27764.54
Episode length: 64.80 +/- 7.96
-----------------------------------
| eval/              |            |
|    mean action     | 0.13814247 |
|    mean velocity x | -1.61      |
|    mean velocity y | -0.761     |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 64.8       |
|    mean_reward     | -7.19e+04  |
| time/              |            |
|    total_timesteps | 405000     |
-----------------------------------
Eval num_timesteps=405500, episode_reward=-68038.89 +/- 42863.08
Episode length: 77.40 +/- 37.46
------------------------------------
| eval/              |             |
|    mean action     | -0.41645458 |
|    mean velocity x | 2.36        |
|    mean velocity y | 3.13        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 77.4        |
|    mean_reward     | -6.8e+04    |
| time/              |             |
|    total_timesteps | 405500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.2      |
|    ep_rew_mean     | -7.94e+04 |
| time/              |           |
|    fps             | 77        |
|    iterations      | 198       |
|    time_elapsed    | 5252      |
|    total_timesteps | 405504    |
----------------------------------
Eval num_timesteps=406000, episode_reward=-73278.33 +/- 25618.78
Episode length: 77.40 +/- 20.64
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.12367021 |
|    mean velocity x      | 2.3         |
|    mean velocity y      | 1.7         |
|    mean velocity z      | 22.2        |
|    mean_ep_length       | 77.4        |
|    mean_reward          | -7.33e+04   |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.002978168 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.11        |
|    learning_rate        | 0.001       |
|    loss                 | 1.33e+08    |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.937       |
|    value_loss           | 2.93e+08    |
-----------------------------------------
Eval num_timesteps=406500, episode_reward=-67314.87 +/- 41495.97
Episode length: 57.00 +/- 12.79
------------------------------------
| eval/              |             |
|    mean action     | -0.63593894 |
|    mean velocity x | 3.55        |
|    mean velocity y | 5.34        |
|    mean velocity z | 18          |
|    mean_ep_length  | 57          |
|    mean_reward     | -6.73e+04   |
| time/              |             |
|    total_timesteps | 406500      |
------------------------------------
Eval num_timesteps=407000, episode_reward=-52781.85 +/- 44536.33
Episode length: 45.00 +/- 25.26
-----------------------------------
| eval/              |            |
|    mean action     | 0.40821016 |
|    mean velocity x | -0.856     |
|    mean velocity y | -1.32      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 45         |
|    mean_reward     | -5.28e+04  |
| time/              |            |
|    total_timesteps | 407000     |
-----------------------------------
Eval num_timesteps=407500, episode_reward=-59377.55 +/- 37139.33
Episode length: 59.20 +/- 22.09
------------------------------------
| eval/              |             |
|    mean action     | -0.28401616 |
|    mean velocity x | 3.35        |
|    mean velocity y | 4.04        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -5.94e+04   |
| time/              |             |
|    total_timesteps | 407500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.3      |
|    ep_rew_mean     | -8.43e+04 |
| time/              |           |
|    fps             | 77        |
|    iterations      | 199       |
|    time_elapsed    | 5259      |
|    total_timesteps | 407552    |
----------------------------------
Eval num_timesteps=408000, episode_reward=-74405.95 +/- 43727.68
Episode length: 63.40 +/- 25.75
----------------------------------------
| eval/                   |            |
|    mean action          | 0.43481377 |
|    mean velocity x      | -0.0789    |
|    mean velocity y      | -1.22      |
|    mean velocity z      | 16.6       |
|    mean_ep_length       | 63.4       |
|    mean_reward          | -7.44e+04  |
| time/                   |            |
|    total_timesteps      | 408000     |
| train/                  |            |
|    approx_kl            | 0.00432585 |
|    clip_fraction        | 0.0158     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 0.122      |
|    learning_rate        | 0.001      |
|    loss                 | 1.16e+08   |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.00441   |
|    std                  | 0.937      |
|    value_loss           | 2.31e+08   |
----------------------------------------
Eval num_timesteps=408500, episode_reward=-88101.30 +/- 16446.00
Episode length: 66.20 +/- 3.66
-------------------------------------
| eval/              |              |
|    mean action     | -0.040667467 |
|    mean velocity x | 2.11         |
|    mean velocity y | 0.878        |
|    mean velocity z | 18.2         |
|    mean_ep_length  | 66.2         |
|    mean_reward     | -8.81e+04    |
| time/              |              |
|    total_timesteps | 408500       |
-------------------------------------
Eval num_timesteps=409000, episode_reward=-40891.56 +/- 34622.93
Episode length: 59.60 +/- 18.49
------------------------------------
| eval/              |             |
|    mean action     | -0.35475484 |
|    mean velocity x | 3.14        |
|    mean velocity y | 3.43        |
|    mean velocity z | 18          |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -4.09e+04   |
| time/              |             |
|    total_timesteps | 409000      |
------------------------------------
Eval num_timesteps=409500, episode_reward=-62406.30 +/- 31731.81
Episode length: 61.80 +/- 21.48
------------------------------------
| eval/              |             |
|    mean action     | -0.29555768 |
|    mean velocity x | 1.07        |
|    mean velocity y | 1.93        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -6.24e+04   |
| time/              |             |
|    total_timesteps | 409500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.6      |
|    ep_rew_mean     | -8.61e+04 |
| time/              |           |
|    fps             | 77        |
|    iterations      | 200       |
|    time_elapsed    | 5266      |
|    total_timesteps | 409600    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-79429.66 +/- 29302.32
Episode length: 66.00 +/- 13.94
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18151133  |
|    mean velocity x      | 0.519        |
|    mean velocity y      | 0.168        |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 66           |
|    mean_reward          | -7.94e+04    |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0034293914 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.122        |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+08     |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00428     |
|    std                  | 0.935        |
|    value_loss           | 2.27e+08     |
------------------------------------------
Eval num_timesteps=410500, episode_reward=-56582.07 +/- 46943.50
Episode length: 51.80 +/- 26.77
------------------------------------
| eval/              |             |
|    mean action     | 0.032677766 |
|    mean velocity x | 0.315       |
|    mean velocity y | -0.0656     |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 51.8        |
|    mean_reward     | -5.66e+04   |
| time/              |             |
|    total_timesteps | 410500      |
------------------------------------
Eval num_timesteps=411000, episode_reward=-62161.63 +/- 41696.88
Episode length: 53.60 +/- 15.40
----------------------------------
| eval/              |           |
|    mean action     | 0.2660199 |
|    mean velocity x | 0.237     |
|    mean velocity y | -0.701    |
|    mean velocity z | 18.9      |
|    mean_ep_length  | 53.6      |
|    mean_reward     | -6.22e+04 |
| time/              |           |
|    total_timesteps | 411000    |
----------------------------------
Eval num_timesteps=411500, episode_reward=-45754.69 +/- 39917.86
Episode length: 46.00 +/- 28.43
----------------------------------
| eval/              |           |
|    mean action     | 0.2291182 |
|    mean velocity x | 1.09      |
|    mean velocity y | 1.12      |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 46        |
|    mean_reward     | -4.58e+04 |
| time/              |           |
|    total_timesteps | 411500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.5      |
|    ep_rew_mean     | -8.56e+04 |
| time/              |           |
|    fps             | 78        |
|    iterations      | 201       |
|    time_elapsed    | 5273      |
|    total_timesteps | 411648    |
----------------------------------
Eval num_timesteps=412000, episode_reward=-83977.35 +/- 40714.03
Episode length: 62.00 +/- 8.56
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5468324    |
|    mean velocity x      | 1.75          |
|    mean velocity y      | 3.18          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 62            |
|    mean_reward          | -8.4e+04      |
| time/                   |               |
|    total_timesteps      | 412000        |
| train/                  |               |
|    approx_kl            | 0.00028497016 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.05         |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+08      |
|    n_updates            | 2010          |
|    policy_gradient_loss | -0.00139      |
|    std                  | 0.935         |
|    value_loss           | 3.06e+08      |
-------------------------------------------
Eval num_timesteps=412500, episode_reward=-57373.69 +/- 23139.68
Episode length: 75.60 +/- 22.35
-----------------------------------
| eval/              |            |
|    mean action     | -0.3245862 |
|    mean velocity x | 1.81       |
|    mean velocity y | 3.38       |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 75.6       |
|    mean_reward     | -5.74e+04  |
| time/              |            |
|    total_timesteps | 412500     |
-----------------------------------
Eval num_timesteps=413000, episode_reward=-80432.96 +/- 32297.67
Episode length: 65.80 +/- 8.54
------------------------------------
| eval/              |             |
|    mean action     | -0.17311414 |
|    mean velocity x | 1.54        |
|    mean velocity y | 1.35        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 65.8        |
|    mean_reward     | -8.04e+04   |
| time/              |             |
|    total_timesteps | 413000      |
------------------------------------
Eval num_timesteps=413500, episode_reward=-75741.22 +/- 40456.55
Episode length: 63.20 +/- 24.16
------------------------------------
| eval/              |             |
|    mean action     | -0.27796417 |
|    mean velocity x | 1.57        |
|    mean velocity y | 2.68        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 63.2        |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 413500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.8      |
|    ep_rew_mean     | -8.57e+04 |
| time/              |           |
|    fps             | 78        |
|    iterations      | 202       |
|    time_elapsed    | 5281      |
|    total_timesteps | 413696    |
----------------------------------
Eval num_timesteps=414000, episode_reward=-62952.81 +/- 36037.29
Episode length: 59.80 +/- 17.43
------------------------------------------
| eval/                   |              |
|    mean action          | 0.11243191   |
|    mean velocity x      | 1.7          |
|    mean velocity y      | 0.55         |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 59.8         |
|    mean_reward          | -6.3e+04     |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0032113604 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.127        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+08     |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00425     |
|    std                  | 0.939        |
|    value_loss           | 2.38e+08     |
------------------------------------------
Eval num_timesteps=414500, episode_reward=-56264.36 +/- 27429.47
Episode length: 65.60 +/- 17.95
------------------------------------
| eval/              |             |
|    mean action     | 0.037937637 |
|    mean velocity x | 0.29        |
|    mean velocity y | -0.464      |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 65.6        |
|    mean_reward     | -5.63e+04   |
| time/              |             |
|    total_timesteps | 414500      |
------------------------------------
Eval num_timesteps=415000, episode_reward=-65572.27 +/- 34853.03
Episode length: 68.80 +/- 17.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.16694072 |
|    mean velocity x | -0.459     |
|    mean velocity y | -1.97      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 68.8       |
|    mean_reward     | -6.56e+04  |
| time/              |            |
|    total_timesteps | 415000     |
-----------------------------------
Eval num_timesteps=415500, episode_reward=-51273.91 +/- 38479.18
Episode length: 58.00 +/- 24.48
-----------------------------------
| eval/              |            |
|    mean action     | -0.4039395 |
|    mean velocity x | 2.83       |
|    mean velocity y | 4.06       |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 58         |
|    mean_reward     | -5.13e+04  |
| time/              |            |
|    total_timesteps | 415500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.46e+04 |
| time/              |           |
|    fps             | 78        |
|    iterations      | 203       |
|    time_elapsed    | 5288      |
|    total_timesteps | 415744    |
----------------------------------
Eval num_timesteps=416000, episode_reward=-85943.59 +/- 18664.94
Episode length: 72.20 +/- 9.62
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1681229     |
|    mean velocity x      | -0.886        |
|    mean velocity y      | -2.5          |
|    mean velocity z      | 22.6          |
|    mean_ep_length       | 72.2          |
|    mean_reward          | -8.59e+04     |
| time/                   |               |
|    total_timesteps      | 416000        |
| train/                  |               |
|    approx_kl            | 0.00044238407 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.07         |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+08      |
|    n_updates            | 2030          |
|    policy_gradient_loss | -0.00043      |
|    std                  | 0.939         |
|    value_loss           | 2.51e+08      |
-------------------------------------------
Eval num_timesteps=416500, episode_reward=-48349.36 +/- 34061.45
Episode length: 55.20 +/- 21.40
------------------------------------
| eval/              |             |
|    mean action     | -0.07331314 |
|    mean velocity x | 1.13        |
|    mean velocity y | 0.925       |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -4.83e+04   |
| time/              |             |
|    total_timesteps | 416500      |
------------------------------------
Eval num_timesteps=417000, episode_reward=-69526.40 +/- 19893.60
Episode length: 74.80 +/- 15.56
------------------------------------
| eval/              |             |
|    mean action     | -0.15561076 |
|    mean velocity x | 0.588       |
|    mean velocity y | 1.46        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 74.8        |
|    mean_reward     | -6.95e+04   |
| time/              |             |
|    total_timesteps | 417000      |
------------------------------------
Eval num_timesteps=417500, episode_reward=-85623.24 +/- 18638.71
Episode length: 79.40 +/- 12.45
-----------------------------------
| eval/              |            |
|    mean action     | 0.19798854 |
|    mean velocity x | -0.0962    |
|    mean velocity y | -0.391     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 79.4       |
|    mean_reward     | -8.56e+04  |
| time/              |            |
|    total_timesteps | 417500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.8      |
|    ep_rew_mean     | -8.88e+04 |
| time/              |           |
|    fps             | 78        |
|    iterations      | 204       |
|    time_elapsed    | 5295      |
|    total_timesteps | 417792    |
----------------------------------
Eval num_timesteps=418000, episode_reward=-64676.67 +/- 19860.69
Episode length: 65.60 +/- 10.74
------------------------------------------
| eval/                   |              |
|    mean action          | -0.54673016  |
|    mean velocity x      | 2.85         |
|    mean velocity y      | 4.22         |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 65.6         |
|    mean_reward          | -6.47e+04    |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0003331894 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00088     |
|    std                  | 0.939        |
|    value_loss           | 2.35e+08     |
------------------------------------------
Eval num_timesteps=418500, episode_reward=-87841.34 +/- 22256.51
Episode length: 69.60 +/- 6.28
------------------------------------
| eval/              |             |
|    mean action     | -0.35541987 |
|    mean velocity x | 1.78        |
|    mean velocity y | 2.22        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 69.6        |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 418500      |
------------------------------------
Eval num_timesteps=419000, episode_reward=-66618.70 +/- 37875.67
Episode length: 58.60 +/- 24.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.1713215 |
|    mean velocity x | 1.3        |
|    mean velocity y | 2.08       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -6.66e+04  |
| time/              |            |
|    total_timesteps | 419000     |
-----------------------------------
Eval num_timesteps=419500, episode_reward=-67517.80 +/- 38980.97
Episode length: 60.00 +/- 11.68
------------------------------------
| eval/              |             |
|    mean action     | 0.027920889 |
|    mean velocity x | 0.0389      |
|    mean velocity y | -0.134      |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 60          |
|    mean_reward     | -6.75e+04   |
| time/              |             |
|    total_timesteps | 419500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.5      |
|    ep_rew_mean     | -9.36e+04 |
| time/              |           |
|    fps             | 79        |
|    iterations      | 205       |
|    time_elapsed    | 5302      |
|    total_timesteps | 419840    |
----------------------------------
Eval num_timesteps=420000, episode_reward=-73416.55 +/- 48790.19
Episode length: 70.80 +/- 33.67
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.04816188 |
|    mean velocity x      | -0.117      |
|    mean velocity y      | -0.526      |
|    mean velocity z      | 21          |
|    mean_ep_length       | 70.8        |
|    mean_reward          | -7.34e+04   |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.003592859 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.0977      |
|    learning_rate        | 0.001       |
|    loss                 | 1.02e+08    |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00405    |
|    std                  | 0.939       |
|    value_loss           | 3.31e+08    |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=-51184.79 +/- 40674.86
Episode length: 54.40 +/- 17.04
------------------------------------
| eval/              |             |
|    mean action     | -0.25452206 |
|    mean velocity x | 1.9         |
|    mean velocity y | 3.07        |
|    mean velocity z | 18          |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5.12e+04   |
| time/              |             |
|    total_timesteps | 420500      |
------------------------------------
Eval num_timesteps=421000, episode_reward=-60467.86 +/- 26143.10
Episode length: 68.40 +/- 16.48
------------------------------------
| eval/              |             |
|    mean action     | 0.021016816 |
|    mean velocity x | -0.0653     |
|    mean velocity y | -0.898      |
|    mean velocity z | 17.2        |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -6.05e+04   |
| time/              |             |
|    total_timesteps | 421000      |
------------------------------------
Eval num_timesteps=421500, episode_reward=-55262.67 +/- 39234.20
Episode length: 55.00 +/- 23.44
------------------------------------
| eval/              |             |
|    mean action     | -0.10366242 |
|    mean velocity x | 2.68        |
|    mean velocity y | 2.27        |
|    mean velocity z | 18          |
|    mean_ep_length  | 55          |
|    mean_reward     | -5.53e+04   |
| time/              |             |
|    total_timesteps | 421500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.7      |
|    ep_rew_mean     | -9.26e+04 |
| time/              |           |
|    fps             | 79        |
|    iterations      | 206       |
|    time_elapsed    | 5309      |
|    total_timesteps | 421888    |
----------------------------------
Eval num_timesteps=422000, episode_reward=-33239.96 +/- 32809.73
Episode length: 54.40 +/- 42.42
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.27468464    |
|    mean velocity x      | -0.917        |
|    mean velocity y      | -1.42         |
|    mean velocity z      | 21.3          |
|    mean_ep_length       | 54.4          |
|    mean_reward          | -3.32e+04     |
| time/                   |               |
|    total_timesteps      | 422000        |
| train/                  |               |
|    approx_kl            | 0.00016018952 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.07         |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.001         |
|    loss                 | 1.63e+08      |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.000325     |
|    std                  | 0.939         |
|    value_loss           | 2.17e+08      |
-------------------------------------------
Eval num_timesteps=422500, episode_reward=-62816.19 +/- 37117.91
Episode length: 68.40 +/- 15.94
-----------------------------------
| eval/              |            |
|    mean action     | 0.34077966 |
|    mean velocity x | -0.691     |
|    mean velocity y | -1.06      |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 68.4       |
|    mean_reward     | -6.28e+04  |
| time/              |            |
|    total_timesteps | 422500     |
-----------------------------------
Eval num_timesteps=423000, episode_reward=-75955.69 +/- 17852.82
Episode length: 72.80 +/- 10.72
----------------------------------
| eval/              |           |
|    mean action     | 0.3570843 |
|    mean velocity x | -1.45     |
|    mean velocity y | -2.47     |
|    mean velocity z | 20.4      |
|    mean_ep_length  | 72.8      |
|    mean_reward     | -7.6e+04  |
| time/              |           |
|    total_timesteps | 423000    |
----------------------------------
Eval num_timesteps=423500, episode_reward=-99151.80 +/- 17965.24
Episode length: 93.80 +/- 45.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.30847272 |
|    mean velocity x | -1.01      |
|    mean velocity y | -2.16      |
|    mean velocity z | 20         |
|    mean_ep_length  | 93.8       |
|    mean_reward     | -9.92e+04  |
| time/              |            |
|    total_timesteps | 423500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.6      |
|    ep_rew_mean     | -9.25e+04 |
| time/              |           |
|    fps             | 79        |
|    iterations      | 207       |
|    time_elapsed    | 5316      |
|    total_timesteps | 423936    |
----------------------------------
Eval num_timesteps=424000, episode_reward=-53099.14 +/- 37286.40
Episode length: 50.40 +/- 22.58
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.27051142 |
|    mean velocity x      | 1.43        |
|    mean velocity y      | 2.35        |
|    mean velocity z      | 17.1        |
|    mean_ep_length       | 50.4        |
|    mean_reward          | -5.31e+04   |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.004182727 |
|    clip_fraction        | 0.0113      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.0975      |
|    learning_rate        | 0.001       |
|    loss                 | 1.63e+08    |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00422    |
|    std                  | 0.938       |
|    value_loss           | 2.44e+08    |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=-76799.32 +/- 12109.39
Episode length: 76.20 +/- 17.97
-------------------------------------
| eval/              |              |
|    mean action     | -0.023285618 |
|    mean velocity x | 0.0885       |
|    mean velocity y | 0.733        |
|    mean velocity z | 18.5         |
|    mean_ep_length  | 76.2         |
|    mean_reward     | -7.68e+04    |
| time/              |              |
|    total_timesteps | 424500       |
-------------------------------------
Eval num_timesteps=425000, episode_reward=-58398.42 +/- 23482.14
Episode length: 63.40 +/- 14.80
------------------------------------
| eval/              |             |
|    mean action     | -0.12743138 |
|    mean velocity x | 1.06        |
|    mean velocity y | 1.74        |
|    mean velocity z | 21          |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -5.84e+04   |
| time/              |             |
|    total_timesteps | 425000      |
------------------------------------
Eval num_timesteps=425500, episode_reward=-36941.41 +/- 46081.21
Episode length: 35.80 +/- 23.67
------------------------------------
| eval/              |             |
|    mean action     | -0.04058529 |
|    mean velocity x | -0.132      |
|    mean velocity y | 1.26        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 35.8        |
|    mean_reward     | -3.69e+04   |
| time/              |             |
|    total_timesteps | 425500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.5      |
|    ep_rew_mean     | -9.24e+04 |
| time/              |           |
|    fps             | 80        |
|    iterations      | 208       |
|    time_elapsed    | 5323      |
|    total_timesteps | 425984    |
----------------------------------
Eval num_timesteps=426000, episode_reward=-69012.71 +/- 20340.36
Episode length: 66.20 +/- 5.64
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.033103827   |
|    mean velocity x      | -1.03         |
|    mean velocity y      | -0.854        |
|    mean velocity z      | 21.2          |
|    mean_ep_length       | 66.2          |
|    mean_reward          | -6.9e+04      |
| time/                   |               |
|    total_timesteps      | 426000        |
| train/                  |               |
|    approx_kl            | 0.00013630168 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.06         |
|    explained_variance   | 0.113         |
|    learning_rate        | 0.001         |
|    loss                 | 8.81e+07      |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.000641     |
|    std                  | 0.939         |
|    value_loss           | 2.51e+08      |
-------------------------------------------
Eval num_timesteps=426500, episode_reward=-80269.21 +/- 35677.33
Episode length: 81.80 +/- 39.61
-------------------------------------
| eval/              |              |
|    mean action     | -0.035451446 |
|    mean velocity x | -0.501       |
|    mean velocity y | -1.27        |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 81.8         |
|    mean_reward     | -8.03e+04    |
| time/              |              |
|    total_timesteps | 426500       |
-------------------------------------
Eval num_timesteps=427000, episode_reward=-53598.39 +/- 30489.55
Episode length: 58.20 +/- 7.52
------------------------------------
| eval/              |             |
|    mean action     | -0.46185547 |
|    mean velocity x | 1.42        |
|    mean velocity y | 3.04        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -5.36e+04   |
| time/              |             |
|    total_timesteps | 427000      |
------------------------------------
Eval num_timesteps=427500, episode_reward=-76143.69 +/- 22598.06
Episode length: 70.80 +/- 11.87
------------------------------------
| eval/              |             |
|    mean action     | -0.18188834 |
|    mean velocity x | 1.19        |
|    mean velocity y | 1.14        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 70.8        |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 427500      |
------------------------------------
Eval num_timesteps=428000, episode_reward=-41349.63 +/- 30174.34
Episode length: 52.80 +/- 22.68
-------------------------------------
| eval/              |              |
|    mean action     | -0.110508084 |
|    mean velocity x | 1.08         |
|    mean velocity y | 0.43         |
|    mean velocity z | 18           |
|    mean_ep_length  | 52.8         |
|    mean_reward     | -4.13e+04    |
| time/              |              |
|    total_timesteps | 428000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.6      |
|    ep_rew_mean     | -8.72e+04 |
| time/              |           |
|    fps             | 80        |
|    iterations      | 209       |
|    time_elapsed    | 5331      |
|    total_timesteps | 428032    |
----------------------------------
Eval num_timesteps=428500, episode_reward=-68949.26 +/- 34103.77
Episode length: 61.40 +/- 16.68
------------------------------------------
| eval/                   |              |
|    mean action          | 0.28076208   |
|    mean velocity x      | 0.652        |
|    mean velocity y      | -2.31        |
|    mean velocity z      | 18.8         |
|    mean_ep_length       | 61.4         |
|    mean_reward          | -6.89e+04    |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 0.0039365087 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.118        |
|    learning_rate        | 0.001        |
|    loss                 | 1.36e+08     |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00324     |
|    std                  | 0.938        |
|    value_loss           | 2.2e+08      |
------------------------------------------
Eval num_timesteps=429000, episode_reward=-78692.82 +/- 23508.03
Episode length: 68.40 +/- 6.18
------------------------------------
| eval/              |             |
|    mean action     | -0.14805351 |
|    mean velocity x | 0.593       |
|    mean velocity y | 1.63        |
|    mean velocity z | 20          |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -7.87e+04   |
| time/              |             |
|    total_timesteps | 429000      |
------------------------------------
Eval num_timesteps=429500, episode_reward=-112454.54 +/- 15556.95
Episode length: 84.00 +/- 38.50
----------------------------------
| eval/              |           |
|    mean action     | 0.0367717 |
|    mean velocity x | 1.25      |
|    mean velocity y | 0.522     |
|    mean velocity z | 17.9      |
|    mean_ep_length  | 84        |
|    mean_reward     | -1.12e+05 |
| time/              |           |
|    total_timesteps | 429500    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-56216.35 +/- 34618.28
Episode length: 54.40 +/- 14.43
------------------------------------
| eval/              |             |
|    mean action     | -0.31925908 |
|    mean velocity x | 1.93        |
|    mean velocity y | 3.46        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5.62e+04   |
| time/              |             |
|    total_timesteps | 430000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81        |
|    ep_rew_mean     | -8.91e+04 |
| time/              |           |
|    fps             | 80        |
|    iterations      | 210       |
|    time_elapsed    | 5338      |
|    total_timesteps | 430080    |
----------------------------------
Eval num_timesteps=430500, episode_reward=-70937.63 +/- 35009.20
Episode length: 60.60 +/- 13.81
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.39836952    |
|    mean velocity x      | -1.46         |
|    mean velocity y      | -2.63         |
|    mean velocity z      | 19.3          |
|    mean_ep_length       | 60.6          |
|    mean_reward          | -7.09e+04     |
| time/                   |               |
|    total_timesteps      | 430500        |
| train/                  |               |
|    approx_kl            | 0.00025835293 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.06         |
|    explained_variance   | 0.12          |
|    learning_rate        | 0.001         |
|    loss                 | 6.01e+07      |
|    n_updates            | 2100          |
|    policy_gradient_loss | -0.000952     |
|    std                  | 0.938         |
|    value_loss           | 2.32e+08      |
-------------------------------------------
Eval num_timesteps=431000, episode_reward=-83161.52 +/- 16218.66
Episode length: 70.60 +/- 12.77
----------------------------------
| eval/              |           |
|    mean action     | 0.3261085 |
|    mean velocity x | -0.175    |
|    mean velocity y | -0.464    |
|    mean velocity z | 18.3      |
|    mean_ep_length  | 70.6      |
|    mean_reward     | -8.32e+04 |
| time/              |           |
|    total_timesteps | 431000    |
----------------------------------
Eval num_timesteps=431500, episode_reward=-109624.00 +/- 74410.19
Episode length: 99.60 +/- 90.46
------------------------------------
| eval/              |             |
|    mean action     | -0.42921695 |
|    mean velocity x | 2.84        |
|    mean velocity y | 4.08        |
|    mean velocity z | 18          |
|    mean_ep_length  | 99.6        |
|    mean_reward     | -1.1e+05    |
| time/              |             |
|    total_timesteps | 431500      |
------------------------------------
Eval num_timesteps=432000, episode_reward=-92159.98 +/- 15726.66
Episode length: 67.00 +/- 3.22
------------------------------------
| eval/              |             |
|    mean action     | -0.18865742 |
|    mean velocity x | 1.59        |
|    mean velocity y | 2.11        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 67          |
|    mean_reward     | -9.22e+04   |
| time/              |             |
|    total_timesteps | 432000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.2      |
|    ep_rew_mean     | -8.67e+04 |
| time/              |           |
|    fps             | 80        |
|    iterations      | 211       |
|    time_elapsed    | 5346      |
|    total_timesteps | 432128    |
----------------------------------
Eval num_timesteps=432500, episode_reward=-65974.91 +/- 36750.44
Episode length: 54.00 +/- 12.44
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.061820358  |
|    mean velocity x      | 0.513         |
|    mean velocity y      | 0.681         |
|    mean velocity z      | 17.6          |
|    mean_ep_length       | 54            |
|    mean_reward          | -6.6e+04      |
| time/                   |               |
|    total_timesteps      | 432500        |
| train/                  |               |
|    approx_kl            | 0.00024979675 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.06         |
|    explained_variance   | 0.0924        |
|    learning_rate        | 0.001         |
|    loss                 | 1.71e+08      |
|    n_updates            | 2110          |
|    policy_gradient_loss | -0.000714     |
|    std                  | 0.938         |
|    value_loss           | 2.34e+08      |
-------------------------------------------
Eval num_timesteps=433000, episode_reward=-81693.34 +/- 20871.00
Episode length: 68.80 +/- 8.82
------------------------------------
| eval/              |             |
|    mean action     | -0.27406394 |
|    mean velocity x | 1.59        |
|    mean velocity y | 3.02        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 68.8        |
|    mean_reward     | -8.17e+04   |
| time/              |             |
|    total_timesteps | 433000      |
------------------------------------
Eval num_timesteps=433500, episode_reward=-56186.38 +/- 26864.09
Episode length: 66.60 +/- 18.34
------------------------------------
| eval/              |             |
|    mean action     | -0.10908112 |
|    mean velocity x | 0.829       |
|    mean velocity y | 1.55        |
|    mean velocity z | 15.4        |
|    mean_ep_length  | 66.6        |
|    mean_reward     | -5.62e+04   |
| time/              |             |
|    total_timesteps | 433500      |
------------------------------------
Eval num_timesteps=434000, episode_reward=-103840.22 +/- 8434.81
Episode length: 79.00 +/- 21.46
------------------------------------
| eval/              |             |
|    mean action     | -0.27000707 |
|    mean velocity x | 1.81        |
|    mean velocity y | 1.46        |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 79          |
|    mean_reward     | -1.04e+05   |
| time/              |             |
|    total_timesteps | 434000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.7      |
|    ep_rew_mean     | -8.19e+04 |
| time/              |           |
|    fps             | 81        |
|    iterations      | 212       |
|    time_elapsed    | 5353      |
|    total_timesteps | 434176    |
----------------------------------
Eval num_timesteps=434500, episode_reward=-43610.11 +/- 40901.12
Episode length: 49.40 +/- 27.51
------------------------------------------
| eval/                   |              |
|    mean action          | 0.012750855  |
|    mean velocity x      | 0.933        |
|    mean velocity y      | -0.0153      |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 49.4         |
|    mean_reward          | -4.36e+04    |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0049707377 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.0935       |
|    learning_rate        | 0.001        |
|    loss                 | 5.92e+07     |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00378     |
|    std                  | 0.937        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-55325.02 +/- 34140.06
Episode length: 56.80 +/- 20.99
-----------------------------------
| eval/              |            |
|    mean action     | 0.07842851 |
|    mean velocity x | 0.151      |
|    mean velocity y | -0.174     |
|    mean velocity z | 19         |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -5.53e+04  |
| time/              |            |
|    total_timesteps | 435000     |
-----------------------------------
Eval num_timesteps=435500, episode_reward=-72188.00 +/- 39073.90
Episode length: 59.00 +/- 17.79
-------------------------------------
| eval/              |              |
|    mean action     | -0.011644371 |
|    mean velocity x | -0.309       |
|    mean velocity y | 0.56         |
|    mean velocity z | 19.5         |
|    mean_ep_length  | 59           |
|    mean_reward     | -7.22e+04    |
| time/              |              |
|    total_timesteps | 435500       |
-------------------------------------
Eval num_timesteps=436000, episode_reward=-65749.94 +/- 36865.13
Episode length: 56.60 +/- 13.95
-----------------------------------
| eval/              |            |
|    mean action     | -0.5415631 |
|    mean velocity x | 2.36       |
|    mean velocity y | 4.31       |
|    mean velocity z | 21.7       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -6.57e+04  |
| time/              |            |
|    total_timesteps | 436000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.4      |
|    ep_rew_mean     | -8.34e+04 |
| time/              |           |
|    fps             | 81        |
|    iterations      | 213       |
|    time_elapsed    | 5360      |
|    total_timesteps | 436224    |
----------------------------------
Eval num_timesteps=436500, episode_reward=-61063.72 +/- 38906.63
Episode length: 54.80 +/- 21.50
------------------------------------------
| eval/                   |              |
|    mean action          | 0.14216526   |
|    mean velocity x      | -0.928       |
|    mean velocity y      | -0.625       |
|    mean velocity z      | 20.5         |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -6.11e+04    |
| time/                   |              |
|    total_timesteps      | 436500       |
| train/                  |              |
|    approx_kl            | 0.0032961373 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.105        |
|    learning_rate        | 0.001        |
|    loss                 | 2.25e+08     |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.938        |
|    value_loss           | 2.78e+08     |
------------------------------------------
Eval num_timesteps=437000, episode_reward=-83117.60 +/- 8566.97
Episode length: 75.40 +/- 14.64
------------------------------------
| eval/              |             |
|    mean action     | 0.009615967 |
|    mean velocity x | -0.323      |
|    mean velocity y | -0.158      |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 75.4        |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 437000      |
------------------------------------
Eval num_timesteps=437500, episode_reward=-85945.77 +/- 18181.96
Episode length: 74.00 +/- 12.02
------------------------------------
| eval/              |             |
|    mean action     | -0.20368466 |
|    mean velocity x | 1.67        |
|    mean velocity y | 2.28        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 74          |
|    mean_reward     | -8.59e+04   |
| time/              |             |
|    total_timesteps | 437500      |
------------------------------------
Eval num_timesteps=438000, episode_reward=-84196.88 +/- 28191.48
Episode length: 62.40 +/- 4.13
------------------------------------
| eval/              |             |
|    mean action     | -0.21689841 |
|    mean velocity x | 2.59        |
|    mean velocity y | 2.62        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 62.4        |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 438000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.45e+04 |
| time/              |           |
|    fps             | 81        |
|    iterations      | 214       |
|    time_elapsed    | 5367      |
|    total_timesteps | 438272    |
----------------------------------
Eval num_timesteps=438500, episode_reward=-89205.10 +/- 23195.09
Episode length: 76.80 +/- 18.45
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.68789196    |
|    mean velocity x      | -1.84         |
|    mean velocity y      | -5.12         |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 76.8          |
|    mean_reward          | -8.92e+04     |
| time/                   |               |
|    total_timesteps      | 438500        |
| train/                  |               |
|    approx_kl            | 0.00034999222 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.06         |
|    explained_variance   | 0.114         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+08      |
|    n_updates            | 2140          |
|    policy_gradient_loss | -0.00078      |
|    std                  | 0.938         |
|    value_loss           | 2.74e+08      |
-------------------------------------------
Eval num_timesteps=439000, episode_reward=-96166.19 +/- 36631.40
Episode length: 86.80 +/- 40.52
------------------------------------
| eval/              |             |
|    mean action     | -0.20676821 |
|    mean velocity x | 0.624       |
|    mean velocity y | 1.67        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 86.8        |
|    mean_reward     | -9.62e+04   |
| time/              |             |
|    total_timesteps | 439000      |
------------------------------------
Eval num_timesteps=439500, episode_reward=-106388.72 +/- 22236.66
Episode length: 87.80 +/- 30.20
------------------------------------
| eval/              |             |
|    mean action     | -0.09612742 |
|    mean velocity x | -0.619      |
|    mean velocity y | -0.243      |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 87.8        |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 439500      |
------------------------------------
Eval num_timesteps=440000, episode_reward=-78046.37 +/- 40249.07
Episode length: 77.00 +/- 26.50
----------------------------------
| eval/              |           |
|    mean action     | 0.2068541 |
|    mean velocity x | 0.437     |
|    mean velocity y | -1.25     |
|    mean velocity z | 19.5      |
|    mean_ep_length  | 77        |
|    mean_reward     | -7.8e+04  |
| time/              |           |
|    total_timesteps | 440000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.4      |
|    ep_rew_mean     | -8.83e+04 |
| time/              |           |
|    fps             | 81        |
|    iterations      | 215       |
|    time_elapsed    | 5375      |
|    total_timesteps | 440320    |
----------------------------------
Eval num_timesteps=440500, episode_reward=-91991.64 +/- 13164.25
Episode length: 74.40 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.00030491638 |
|    mean velocity x      | 2.2           |
|    mean velocity y      | 2.27          |
|    mean velocity z      | 20            |
|    mean_ep_length       | 74.4          |
|    mean_reward          | -9.2e+04      |
| time/                   |               |
|    total_timesteps      | 440500        |
| train/                  |               |
|    approx_kl            | 0.0010929022  |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.06         |
|    explained_variance   | 0.103         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 2150          |
|    policy_gradient_loss | -0.00102      |
|    std                  | 0.937         |
|    value_loss           | 2.56e+08      |
-------------------------------------------
Eval num_timesteps=441000, episode_reward=-76379.08 +/- 22108.08
Episode length: 68.00 +/- 3.85
------------------------------------
| eval/              |             |
|    mean action     | -0.32456958 |
|    mean velocity x | 0.277       |
|    mean velocity y | 1.66        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 68          |
|    mean_reward     | -7.64e+04   |
| time/              |             |
|    total_timesteps | 441000      |
------------------------------------
Eval num_timesteps=441500, episode_reward=-68675.58 +/- 27991.89
Episode length: 60.00 +/- 6.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.32381257 |
|    mean velocity x | -0.422     |
|    mean velocity y | -1.1       |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 60         |
|    mean_reward     | -6.87e+04  |
| time/              |            |
|    total_timesteps | 441500     |
-----------------------------------
Eval num_timesteps=442000, episode_reward=-72221.58 +/- 11621.16
Episode length: 68.20 +/- 5.56
-------------------------------------
| eval/              |              |
|    mean action     | -0.029785244 |
|    mean velocity x | 1.3          |
|    mean velocity y | 0.252        |
|    mean velocity z | 20           |
|    mean_ep_length  | 68.2         |
|    mean_reward     | -7.22e+04    |
| time/              |              |
|    total_timesteps | 442000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.6      |
|    ep_rew_mean     | -9.32e+04 |
| time/              |           |
|    fps             | 82        |
|    iterations      | 216       |
|    time_elapsed    | 5387      |
|    total_timesteps | 442368    |
----------------------------------
Eval num_timesteps=442500, episode_reward=-68663.12 +/- 34229.03
Episode length: 60.00 +/- 11.90
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14364038  |
|    mean velocity x      | 2            |
|    mean velocity y      | 1.92         |
|    mean velocity z      | 17           |
|    mean_ep_length       | 60           |
|    mean_reward          | -6.87e+04    |
| time/                   |              |
|    total_timesteps      | 442500       |
| train/                  |              |
|    approx_kl            | 0.0023888499 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.0951       |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+08     |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 0.937        |
|    value_loss           | 2.83e+08     |
------------------------------------------
Eval num_timesteps=443000, episode_reward=-71650.63 +/- 30266.99
Episode length: 74.40 +/- 15.56
------------------------------------
| eval/              |             |
|    mean action     | -0.81573373 |
|    mean velocity x | 3.24        |
|    mean velocity y | 6.06        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 74.4        |
|    mean_reward     | -7.17e+04   |
| time/              |             |
|    total_timesteps | 443000      |
------------------------------------
Eval num_timesteps=443500, episode_reward=-80420.24 +/- 15476.56
Episode length: 63.00 +/- 3.46
-----------------------------------
| eval/              |            |
|    mean action     | 0.10683175 |
|    mean velocity x | 0.667      |
|    mean velocity y | 0.148      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 63         |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 443500     |
-----------------------------------
Eval num_timesteps=444000, episode_reward=-42231.45 +/- 31011.45
Episode length: 48.60 +/- 11.77
------------------------------------
| eval/              |             |
|    mean action     | -0.16313693 |
|    mean velocity x | 1.47        |
|    mean velocity y | 0.951       |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -4.22e+04   |
| time/              |             |
|    total_timesteps | 444000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82        |
|    ep_rew_mean     | -9.23e+04 |
| time/              |           |
|    fps             | 82        |
|    iterations      | 217       |
|    time_elapsed    | 5393      |
|    total_timesteps | 444416    |
----------------------------------
Eval num_timesteps=444500, episode_reward=-41099.47 +/- 42137.51
Episode length: 49.40 +/- 20.83
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5052587   |
|    mean velocity x      | 1.98         |
|    mean velocity y      | 3.97         |
|    mean velocity z      | 20.9         |
|    mean_ep_length       | 49.4         |
|    mean_reward          | -4.11e+04    |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 0.0037952096 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.137        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00392     |
|    std                  | 0.939        |
|    value_loss           | 1.91e+08     |
------------------------------------------
Eval num_timesteps=445000, episode_reward=-66431.87 +/- 35599.94
Episode length: 63.20 +/- 26.48
-----------------------------------
| eval/              |            |
|    mean action     | 0.35618868 |
|    mean velocity x | -1.4       |
|    mean velocity y | -0.935     |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -6.64e+04  |
| time/              |            |
|    total_timesteps | 445000     |
-----------------------------------
Eval num_timesteps=445500, episode_reward=-75886.96 +/- 23851.62
Episode length: 61.60 +/- 5.54
-----------------------------------
| eval/              |            |
|    mean action     | 0.43898603 |
|    mean velocity x | -2.09      |
|    mean velocity y | -3.08      |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -7.59e+04  |
| time/              |            |
|    total_timesteps | 445500     |
-----------------------------------
Eval num_timesteps=446000, episode_reward=-47637.92 +/- 37480.29
Episode length: 53.40 +/- 23.78
-----------------------------------
| eval/              |            |
|    mean action     | 0.13150866 |
|    mean velocity x | 0.838      |
|    mean velocity y | -0.432     |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 53.4       |
|    mean_reward     | -4.76e+04  |
| time/              |            |
|    total_timesteps | 446000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.4      |
|    ep_rew_mean     | -8.74e+04 |
| time/              |           |
|    fps             | 82        |
|    iterations      | 218       |
|    time_elapsed    | 5401      |
|    total_timesteps | 446464    |
----------------------------------
Eval num_timesteps=446500, episode_reward=-53638.11 +/- 51578.06
Episode length: 46.20 +/- 15.83
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.2564389  |
|    mean velocity x      | 1.41        |
|    mean velocity y      | 2.86        |
|    mean velocity z      | 18.9        |
|    mean_ep_length       | 46.2        |
|    mean_reward          | -5.36e+04   |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.002119685 |
|    clip_fraction        | 0.00967     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.001       |
|    loss                 | 1.26e+08    |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.938       |
|    value_loss           | 2.18e+08    |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=-79124.56 +/- 35697.16
Episode length: 58.60 +/- 20.09
------------------------------------
| eval/              |             |
|    mean action     | -0.25318253 |
|    mean velocity x | 0.856       |
|    mean velocity y | 1.9         |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -7.91e+04   |
| time/              |             |
|    total_timesteps | 447000      |
------------------------------------
Eval num_timesteps=447500, episode_reward=-70021.57 +/- 20595.84
Episode length: 68.00 +/- 10.66
------------------------------------
| eval/              |             |
|    mean action     | -0.32943144 |
|    mean velocity x | 1.36        |
|    mean velocity y | 1.55        |
|    mean velocity z | 20          |
|    mean_ep_length  | 68          |
|    mean_reward     | -7e+04      |
| time/              |             |
|    total_timesteps | 447500      |
------------------------------------
Eval num_timesteps=448000, episode_reward=-80198.31 +/- 40356.93
Episode length: 53.20 +/- 21.10
------------------------------------
| eval/              |             |
|    mean action     | -0.14854853 |
|    mean velocity x | 0.259       |
|    mean velocity y | 1.84        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 53.2        |
|    mean_reward     | -8.02e+04   |
| time/              |             |
|    total_timesteps | 448000      |
------------------------------------
Eval num_timesteps=448500, episode_reward=-96292.63 +/- 23516.53
Episode length: 104.00 +/- 34.97
-----------------------------------
| eval/              |            |
|    mean action     | 0.10240975 |
|    mean velocity x | -0.413     |
|    mean velocity y | -0.448     |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 104        |
|    mean_reward     | -9.63e+04  |
| time/              |            |
|    total_timesteps | 448500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.3      |
|    ep_rew_mean     | -8.84e+04 |
| time/              |           |
|    fps             | 82        |
|    iterations      | 219       |
|    time_elapsed    | 5408      |
|    total_timesteps | 448512    |
----------------------------------
Eval num_timesteps=449000, episode_reward=-71866.18 +/- 49877.90
Episode length: 75.40 +/- 49.22
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20827533  |
|    mean velocity x      | 1.78         |
|    mean velocity y      | 2.2          |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 75.4         |
|    mean_reward          | -7.19e+04    |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 9.665417e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+08     |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.000389    |
|    std                  | 0.938        |
|    value_loss           | 2.7e+08      |
------------------------------------------
Eval num_timesteps=449500, episode_reward=-105986.83 +/- 47838.07
Episode length: 94.80 +/- 61.15
------------------------------------
| eval/              |             |
|    mean action     | -0.42543712 |
|    mean velocity x | 0.295       |
|    mean velocity y | 1.66        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 94.8        |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 449500      |
------------------------------------
Eval num_timesteps=450000, episode_reward=-51393.57 +/- 39183.50
Episode length: 53.40 +/- 16.74
-------------------------------------
| eval/              |              |
|    mean action     | -0.032653335 |
|    mean velocity x | -1.01        |
|    mean velocity y | -0.368       |
|    mean velocity z | 19           |
|    mean_ep_length  | 53.4         |
|    mean_reward     | -5.14e+04    |
| time/              |              |
|    total_timesteps | 450000       |
-------------------------------------
Eval num_timesteps=450500, episode_reward=-42655.35 +/- 40232.09
Episode length: 46.60 +/- 22.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.15821227 |
|    mean velocity x | -0.441     |
|    mean velocity y | -1.09      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 46.6       |
|    mean_reward     | -4.27e+04  |
| time/              |            |
|    total_timesteps | 450500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.3      |
|    ep_rew_mean     | -8.58e+04 |
| time/              |           |
|    fps             | 83        |
|    iterations      | 220       |
|    time_elapsed    | 5415      |
|    total_timesteps | 450560    |
----------------------------------
Eval num_timesteps=451000, episode_reward=-98884.56 +/- 21480.77
Episode length: 97.20 +/- 45.49
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29040644  |
|    mean velocity x      | 1.41         |
|    mean velocity y      | 1.72         |
|    mean velocity z      | 21.6         |
|    mean_ep_length       | 97.2         |
|    mean_reward          | -9.89e+04    |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0026334361 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.082        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+08     |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.00088     |
|    std                  | 0.938        |
|    value_loss           | 2.07e+08     |
------------------------------------------
Eval num_timesteps=451500, episode_reward=-98903.45 +/- 16404.82
Episode length: 80.60 +/- 20.87
------------------------------------
| eval/              |             |
|    mean action     | -0.10972202 |
|    mean velocity x | -0.591      |
|    mean velocity y | 0.762       |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 80.6        |
|    mean_reward     | -9.89e+04   |
| time/              |             |
|    total_timesteps | 451500      |
------------------------------------
Eval num_timesteps=452000, episode_reward=-80708.91 +/- 30930.63
Episode length: 68.20 +/- 18.59
------------------------------------
| eval/              |             |
|    mean action     | 0.092764325 |
|    mean velocity x | 0.938       |
|    mean velocity y | 0.368       |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 68.2        |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 452000      |
------------------------------------
Eval num_timesteps=452500, episode_reward=-44052.89 +/- 36580.74
Episode length: 55.20 +/- 21.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.44618443 |
|    mean velocity x | -0.946     |
|    mean velocity y | -2.16      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 55.2       |
|    mean_reward     | -4.41e+04  |
| time/              |            |
|    total_timesteps | 452500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.4      |
|    ep_rew_mean     | -8.69e+04 |
| time/              |           |
|    fps             | 83        |
|    iterations      | 221       |
|    time_elapsed    | 5423      |
|    total_timesteps | 452608    |
----------------------------------
Eval num_timesteps=453000, episode_reward=-58221.51 +/- 43508.13
Episode length: 52.40 +/- 20.46
------------------------------------------
| eval/                   |              |
|    mean action          | 0.1729534    |
|    mean velocity x      | -0.2         |
|    mean velocity y      | -0.643       |
|    mean velocity z      | 20           |
|    mean_ep_length       | 52.4         |
|    mean_reward          | -5.82e+04    |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0008856602 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.109        |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+08     |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 0.938        |
|    value_loss           | 2.34e+08     |
------------------------------------------
Eval num_timesteps=453500, episode_reward=-84188.92 +/- 39295.47
Episode length: 60.80 +/- 11.77
------------------------------------
| eval/              |             |
|    mean action     | -0.02480967 |
|    mean velocity x | 0.19        |
|    mean velocity y | -0.555      |
|    mean velocity z | 20          |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 453500      |
------------------------------------
Eval num_timesteps=454000, episode_reward=-71838.10 +/- 39479.07
Episode length: 54.60 +/- 20.07
-----------------------------------
| eval/              |            |
|    mean action     | 0.12739351 |
|    mean velocity x | -0.242     |
|    mean velocity y | -1.8       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -7.18e+04  |
| time/              |            |
|    total_timesteps | 454000     |
-----------------------------------
Eval num_timesteps=454500, episode_reward=-88847.28 +/- 16282.65
Episode length: 68.60 +/- 14.84
----------------------------------
| eval/              |           |
|    mean action     | 0.4768485 |
|    mean velocity x | -1.56     |
|    mean velocity y | -3.3      |
|    mean velocity z | 17.9      |
|    mean_ep_length  | 68.6      |
|    mean_reward     | -8.88e+04 |
| time/              |           |
|    total_timesteps | 454500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.6      |
|    ep_rew_mean     | -8.81e+04 |
| time/              |           |
|    fps             | 83        |
|    iterations      | 222       |
|    time_elapsed    | 5430      |
|    total_timesteps | 454656    |
----------------------------------
Eval num_timesteps=455000, episode_reward=-87635.24 +/- 17996.64
Episode length: 68.20 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.042308733 |
|    mean velocity x      | -1.11       |
|    mean velocity y      | 0.195       |
|    mean velocity z      | 21.5        |
|    mean_ep_length       | 68.2        |
|    mean_reward          | -8.76e+04   |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.001421733 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.104       |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+08     |
|    n_updates            | 2220        |
|    policy_gradient_loss | -0.0011     |
|    std                  | 0.939       |
|    value_loss           | 2.63e+08    |
-----------------------------------------
Eval num_timesteps=455500, episode_reward=-78149.42 +/- 27798.79
Episode length: 64.80 +/- 5.19
------------------------------------
| eval/              |             |
|    mean action     | -0.05503314 |
|    mean velocity x | 0.204       |
|    mean velocity y | -0.0167     |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.81e+04   |
| time/              |             |
|    total_timesteps | 455500      |
------------------------------------
Eval num_timesteps=456000, episode_reward=-73663.09 +/- 46754.86
Episode length: 87.80 +/- 58.86
------------------------------------
| eval/              |             |
|    mean action     | -0.40901282 |
|    mean velocity x | 2.02        |
|    mean velocity y | 3.3         |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 87.8        |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 456000      |
------------------------------------
Eval num_timesteps=456500, episode_reward=-99981.32 +/- 20180.29
Episode length: 65.20 +/- 2.64
------------------------------------
| eval/              |             |
|    mean action     | -0.19462398 |
|    mean velocity x | -0.0974     |
|    mean velocity y | 1.54        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 65.2        |
|    mean_reward     | -1e+05      |
| time/              |             |
|    total_timesteps | 456500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.6      |
|    ep_rew_mean     | -8.49e+04 |
| time/              |           |
|    fps             | 83        |
|    iterations      | 223       |
|    time_elapsed    | 5437      |
|    total_timesteps | 456704    |
----------------------------------
Eval num_timesteps=457000, episode_reward=-93759.79 +/- 15179.31
Episode length: 66.20 +/- 5.31
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21725202  |
|    mean velocity x      | -0.126       |
|    mean velocity y      | 1.19         |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 66.2         |
|    mean_reward          | -9.38e+04    |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0011161387 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+08     |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.94         |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=457500, episode_reward=-83568.98 +/- 36857.28
Episode length: 56.80 +/- 17.81
-----------------------------------
| eval/              |            |
|    mean action     | 0.16803654 |
|    mean velocity x | -0.466     |
|    mean velocity y | -0.747     |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -8.36e+04  |
| time/              |            |
|    total_timesteps | 457500     |
-----------------------------------
Eval num_timesteps=458000, episode_reward=-47303.58 +/- 42181.93
Episode length: 48.60 +/- 21.92
-----------------------------------
| eval/              |            |
|    mean action     | -0.5385127 |
|    mean velocity x | 1.32       |
|    mean velocity y | 3.99       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 48.6       |
|    mean_reward     | -4.73e+04  |
| time/              |            |
|    total_timesteps | 458000     |
-----------------------------------
Eval num_timesteps=458500, episode_reward=-58187.79 +/- 48481.72
Episode length: 61.80 +/- 41.76
-----------------------------------
| eval/              |            |
|    mean action     | -0.5529276 |
|    mean velocity x | 2.98       |
|    mean velocity y | 4.7        |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -5.82e+04  |
| time/              |            |
|    total_timesteps | 458500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.6      |
|    ep_rew_mean     | -8.53e+04 |
| time/              |           |
|    fps             | 84        |
|    iterations      | 224       |
|    time_elapsed    | 5444      |
|    total_timesteps | 458752    |
----------------------------------
Eval num_timesteps=459000, episode_reward=-94764.02 +/- 12363.10
Episode length: 90.00 +/- 42.12
------------------------------------------
| eval/                   |              |
|    mean action          | 0.0059306948 |
|    mean velocity x      | 0.631        |
|    mean velocity y      | 1.16         |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 90           |
|    mean_reward          | -9.48e+04    |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.000520071  |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+08     |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00121     |
|    std                  | 0.939        |
|    value_loss           | 2.3e+08      |
------------------------------------------
Eval num_timesteps=459500, episode_reward=-49114.33 +/- 52152.73
Episode length: 43.00 +/- 18.29
-------------------------------------
| eval/              |              |
|    mean action     | -0.022861056 |
|    mean velocity x | -0.314       |
|    mean velocity y | -0.0609      |
|    mean velocity z | 21.4         |
|    mean_ep_length  | 43           |
|    mean_reward     | -4.91e+04    |
| time/              |              |
|    total_timesteps | 459500       |
-------------------------------------
Eval num_timesteps=460000, episode_reward=-73129.98 +/- 25511.14
Episode length: 64.80 +/- 5.98
------------------------------------
| eval/              |             |
|    mean action     | -0.16270135 |
|    mean velocity x | 1.92        |
|    mean velocity y | 2.64        |
|    mean velocity z | 20          |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.31e+04   |
| time/              |             |
|    total_timesteps | 460000      |
------------------------------------
Eval num_timesteps=460500, episode_reward=-84826.74 +/- 38925.93
Episode length: 61.40 +/- 9.65
----------------------------------
| eval/              |           |
|    mean action     | -0.199496 |
|    mean velocity x | 1.38      |
|    mean velocity y | 1.41      |
|    mean velocity z | 15.6      |
|    mean_ep_length  | 61.4      |
|    mean_reward     | -8.48e+04 |
| time/              |           |
|    total_timesteps | 460500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -8.16e+04 |
| time/              |           |
|    fps             | 84        |
|    iterations      | 225       |
|    time_elapsed    | 5451      |
|    total_timesteps | 460800    |
----------------------------------
Eval num_timesteps=461000, episode_reward=-94026.73 +/- 9132.51
Episode length: 74.60 +/- 16.32
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15712781  |
|    mean velocity x      | -0.111       |
|    mean velocity y      | 0.945        |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 74.6         |
|    mean_reward          | -9.4e+04     |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0011195935 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.001        |
|    loss                 | 7.43e+07     |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.00254     |
|    std                  | 0.938        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=461500, episode_reward=-81787.62 +/- 21413.84
Episode length: 65.20 +/- 3.31
----------------------------------
| eval/              |           |
|    mean action     | 0.2636829 |
|    mean velocity x | -0.592    |
|    mean velocity y | -1.52     |
|    mean velocity z | 19.6      |
|    mean_ep_length  | 65.2      |
|    mean_reward     | -8.18e+04 |
| time/              |           |
|    total_timesteps | 461500    |
----------------------------------
Eval num_timesteps=462000, episode_reward=-72672.60 +/- 23023.19
Episode length: 65.00 +/- 11.76
------------------------------------
| eval/              |             |
|    mean action     | -0.27992436 |
|    mean velocity x | 1.04        |
|    mean velocity y | 1.76        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 65          |
|    mean_reward     | -7.27e+04   |
| time/              |             |
|    total_timesteps | 462000      |
------------------------------------
Eval num_timesteps=462500, episode_reward=-50792.08 +/- 25971.69
Episode length: 58.80 +/- 27.18
-----------------------------------
| eval/              |            |
|    mean action     | 0.34310398 |
|    mean velocity x | 0.0888     |
|    mean velocity y | -0.694     |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -5.08e+04  |
| time/              |            |
|    total_timesteps | 462500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.25e+04 |
| time/              |           |
|    fps             | 84        |
|    iterations      | 226       |
|    time_elapsed    | 5459      |
|    total_timesteps | 462848    |
----------------------------------
Eval num_timesteps=463000, episode_reward=-59083.78 +/- 29071.49
Episode length: 56.00 +/- 14.93
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.061399557   |
|    mean velocity x      | 0.493         |
|    mean velocity y      | -0.0339       |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 56            |
|    mean_reward          | -5.91e+04     |
| time/                   |               |
|    total_timesteps      | 463000        |
| train/                  |               |
|    approx_kl            | 0.00018243564 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.06         |
|    explained_variance   | 0.132         |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+08      |
|    n_updates            | 2260          |
|    policy_gradient_loss | -0.000246     |
|    std                  | 0.938         |
|    value_loss           | 2.33e+08      |
-------------------------------------------
Eval num_timesteps=463500, episode_reward=-65016.67 +/- 37789.38
Episode length: 56.80 +/- 15.50
-----------------------------------
| eval/              |            |
|    mean action     | -0.5887678 |
|    mean velocity x | 3.36       |
|    mean velocity y | 4.11       |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -6.5e+04   |
| time/              |            |
|    total_timesteps | 463500     |
-----------------------------------
Eval num_timesteps=464000, episode_reward=-87233.46 +/- 19130.43
Episode length: 79.20 +/- 32.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.34216994 |
|    mean velocity x | -0.618     |
|    mean velocity y | -2.66      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 79.2       |
|    mean_reward     | -8.72e+04  |
| time/              |            |
|    total_timesteps | 464000     |
-----------------------------------
Eval num_timesteps=464500, episode_reward=-53191.56 +/- 32443.12
Episode length: 58.20 +/- 25.76
----------------------------------
| eval/              |           |
|    mean action     | 0.5235408 |
|    mean velocity x | -1.76     |
|    mean velocity y | -2.9      |
|    mean velocity z | 19.5      |
|    mean_ep_length  | 58.2      |
|    mean_reward     | -5.32e+04 |
| time/              |           |
|    total_timesteps | 464500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76       |
|    ep_rew_mean     | -8.1e+04 |
| time/              |          |
|    fps             | 85       |
|    iterations      | 227      |
|    time_elapsed    | 5466     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=-50623.81 +/- 24960.26
Episode length: 53.40 +/- 7.81
------------------------------------------
| eval/                   |              |
|    mean action          | 0.15277313   |
|    mean velocity x      | 0.0144       |
|    mean velocity y      | 0.0868       |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 53.4         |
|    mean_reward          | -5.06e+04    |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0010095838 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.105        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 0.938        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=465500, episode_reward=-56105.61 +/- 19321.56
Episode length: 54.60 +/- 4.84
-----------------------------------
| eval/              |            |
|    mean action     | 0.04315671 |
|    mean velocity x | 0.169      |
|    mean velocity y | 0.514      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -5.61e+04  |
| time/              |            |
|    total_timesteps | 465500     |
-----------------------------------
Eval num_timesteps=466000, episode_reward=-58725.98 +/- 25003.70
Episode length: 58.60 +/- 11.43
-------------------------------------
| eval/              |              |
|    mean action     | -0.009570572 |
|    mean velocity x | 1.61         |
|    mean velocity y | 0.989        |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 58.6         |
|    mean_reward     | -5.87e+04    |
| time/              |              |
|    total_timesteps | 466000       |
-------------------------------------
Eval num_timesteps=466500, episode_reward=-60388.78 +/- 47035.22
Episode length: 50.20 +/- 16.69
-------------------------------------
| eval/              |              |
|    mean action     | -0.021551179 |
|    mean velocity x | 1.51         |
|    mean velocity y | 0.93         |
|    mean velocity z | 18.4         |
|    mean_ep_length  | 50.2         |
|    mean_reward     | -6.04e+04    |
| time/              |              |
|    total_timesteps | 466500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.9      |
|    ep_rew_mean     | -7.61e+04 |
| time/              |           |
|    fps             | 85        |
|    iterations      | 228       |
|    time_elapsed    | 5473      |
|    total_timesteps | 466944    |
----------------------------------
Eval num_timesteps=467000, episode_reward=-100463.36 +/- 17740.81
Episode length: 70.00 +/- 14.09
------------------------------------------
| eval/                   |              |
|    mean action          | 0.06532062   |
|    mean velocity x      | -1.76        |
|    mean velocity y      | -1.19        |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 70           |
|    mean_reward          | -1e+05       |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0019629796 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.111        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+08     |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.936        |
|    value_loss           | 2.07e+08     |
------------------------------------------
Eval num_timesteps=467500, episode_reward=-61316.87 +/- 50674.16
Episode length: 45.80 +/- 24.46
-------------------------------------
| eval/              |              |
|    mean action     | -0.115960196 |
|    mean velocity x | 1.48         |
|    mean velocity y | 1.3          |
|    mean velocity z | 17.6         |
|    mean_ep_length  | 45.8         |
|    mean_reward     | -6.13e+04    |
| time/              |              |
|    total_timesteps | 467500       |
-------------------------------------
Eval num_timesteps=468000, episode_reward=-62089.46 +/- 34712.23
Episode length: 65.40 +/- 26.72
------------------------------------
| eval/              |             |
|    mean action     | 0.053570922 |
|    mean velocity x | -0.00749    |
|    mean velocity y | -0.559      |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -6.21e+04   |
| time/              |             |
|    total_timesteps | 468000      |
------------------------------------
Eval num_timesteps=468500, episode_reward=-83211.93 +/- 31945.30
Episode length: 72.40 +/- 8.78
-----------------------------------
| eval/              |            |
|    mean action     | 0.21268532 |
|    mean velocity x | -1.09      |
|    mean velocity y | -1.21      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 72.4       |
|    mean_reward     | -8.32e+04  |
| time/              |            |
|    total_timesteps | 468500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.6      |
|    ep_rew_mean     | -7.54e+04 |
| time/              |           |
|    fps             | 85        |
|    iterations      | 229       |
|    time_elapsed    | 5480      |
|    total_timesteps | 468992    |
----------------------------------
Eval num_timesteps=469000, episode_reward=-84664.96 +/- 60636.87
Episode length: 81.20 +/- 62.11
------------------------------------------
| eval/                   |              |
|    mean action          | 0.17936599   |
|    mean velocity x      | -0.137       |
|    mean velocity y      | -0.9         |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 81.2         |
|    mean_reward          | -8.47e+04    |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0020713327 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.001        |
|    loss                 | 7.12e+07     |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.935        |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=469500, episode_reward=-61596.95 +/- 37698.84
Episode length: 54.60 +/- 19.48
-------------------------------------
| eval/              |              |
|    mean action     | -0.040001985 |
|    mean velocity x | -0.641       |
|    mean velocity y | 0.044        |
|    mean velocity z | 20.6         |
|    mean_ep_length  | 54.6         |
|    mean_reward     | -6.16e+04    |
| time/              |              |
|    total_timesteps | 469500       |
-------------------------------------
Eval num_timesteps=470000, episode_reward=-56434.00 +/- 35023.67
Episode length: 56.80 +/- 14.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.02198089 |
|    mean velocity x | -0.173     |
|    mean velocity y | -0.0285    |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -5.64e+04  |
| time/              |            |
|    total_timesteps | 470000     |
-----------------------------------
Eval num_timesteps=470500, episode_reward=-91142.23 +/- 14657.26
Episode length: 74.00 +/- 12.21
------------------------------------
| eval/              |             |
|    mean action     | -0.22395174 |
|    mean velocity x | -1.09       |
|    mean velocity y | 0.594       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 74          |
|    mean_reward     | -9.11e+04   |
| time/              |             |
|    total_timesteps | 470500      |
------------------------------------
Eval num_timesteps=471000, episode_reward=-81980.44 +/- 17850.98
Episode length: 77.00 +/- 14.76
------------------------------------
| eval/              |             |
|    mean action     | -0.34161335 |
|    mean velocity x | 0.804       |
|    mean velocity y | 2.03        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 77          |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 471000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | -7.8e+04 |
| time/              |          |
|    fps             | 85       |
|    iterations      | 230      |
|    time_elapsed    | 5488     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=-86076.07 +/- 23826.49
Episode length: 62.60 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean action          | 0.05114791   |
|    mean velocity x      | -0.195       |
|    mean velocity y      | 0.994        |
|    mean velocity z      | 19.8         |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -8.61e+04    |
| time/                   |              |
|    total_timesteps      | 471500       |
| train/                  |              |
|    approx_kl            | 0.0004834259 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.000864    |
|    std                  | 0.933        |
|    value_loss           | 2.73e+08     |
------------------------------------------
Eval num_timesteps=472000, episode_reward=-45509.09 +/- 30848.41
Episode length: 52.20 +/- 11.75
------------------------------------
| eval/              |             |
|    mean action     | -0.63297385 |
|    mean velocity x | 2.67        |
|    mean velocity y | 4.21        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -4.55e+04   |
| time/              |             |
|    total_timesteps | 472000      |
------------------------------------
Eval num_timesteps=472500, episode_reward=-74467.01 +/- 35370.06
Episode length: 69.00 +/- 7.13
------------------------------------
| eval/              |             |
|    mean action     | -0.19815238 |
|    mean velocity x | 2.02        |
|    mean velocity y | 0.667       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 69          |
|    mean_reward     | -7.45e+04   |
| time/              |             |
|    total_timesteps | 472500      |
------------------------------------
Eval num_timesteps=473000, episode_reward=-96886.61 +/- 9318.08
Episode length: 81.00 +/- 34.51
------------------------------------
| eval/              |             |
|    mean action     | 0.038508117 |
|    mean velocity x | -0.136      |
|    mean velocity y | 0.188       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 81          |
|    mean_reward     | -9.69e+04   |
| time/              |             |
|    total_timesteps | 473000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.8      |
|    ep_rew_mean     | -8.03e+04 |
| time/              |           |
|    fps             | 86        |
|    iterations      | 231       |
|    time_elapsed    | 5495      |
|    total_timesteps | 473088    |
----------------------------------
Eval num_timesteps=473500, episode_reward=-84989.85 +/- 38769.14
Episode length: 63.60 +/- 7.36
------------------------------------------
| eval/                   |              |
|    mean action          | 0.40092665   |
|    mean velocity x      | -0.143       |
|    mean velocity y      | -3.38        |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 63.6         |
|    mean_reward          | -8.5e+04     |
| time/                   |              |
|    total_timesteps      | 473500       |
| train/                  |              |
|    approx_kl            | 0.0031733108 |
|    clip_fraction        | 0.00698      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.001        |
|    loss                 | 8.07e+07     |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.0031      |
|    std                  | 0.932        |
|    value_loss           | 2.44e+08     |
------------------------------------------
Eval num_timesteps=474000, episode_reward=-58638.02 +/- 37039.22
Episode length: 47.80 +/- 21.70
-----------------------------------
| eval/              |            |
|    mean action     | 0.08422281 |
|    mean velocity x | 1.76       |
|    mean velocity y | 1.06       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 47.8       |
|    mean_reward     | -5.86e+04  |
| time/              |            |
|    total_timesteps | 474000     |
-----------------------------------
Eval num_timesteps=474500, episode_reward=-54631.56 +/- 31336.15
Episode length: 50.40 +/- 17.36
-----------------------------------
| eval/              |            |
|    mean action     | 0.21775909 |
|    mean velocity x | 0.932      |
|    mean velocity y | -0.555     |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 50.4       |
|    mean_reward     | -5.46e+04  |
| time/              |            |
|    total_timesteps | 474500     |
-----------------------------------
Eval num_timesteps=475000, episode_reward=-54380.06 +/- 36105.87
Episode length: 56.60 +/- 21.94
-----------------------------------
| eval/              |            |
|    mean action     | 0.18767047 |
|    mean velocity x | 0.869      |
|    mean velocity y | -0.676     |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -5.44e+04  |
| time/              |            |
|    total_timesteps | 475000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -8.29e+04 |
| time/              |           |
|    fps             | 86        |
|    iterations      | 232       |
|    time_elapsed    | 5502      |
|    total_timesteps | 475136    |
----------------------------------
Eval num_timesteps=475500, episode_reward=-91330.79 +/- 31884.47
Episode length: 86.20 +/- 46.31
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.20347074    |
|    mean velocity x      | -1.71         |
|    mean velocity y      | -2.47         |
|    mean velocity z      | 21.2          |
|    mean_ep_length       | 86.2          |
|    mean_reward          | -9.13e+04     |
| time/                   |               |
|    total_timesteps      | 475500        |
| train/                  |               |
|    approx_kl            | 0.00024210449 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.04         |
|    explained_variance   | 0.126         |
|    learning_rate        | 0.001         |
|    loss                 | 2.12e+08      |
|    n_updates            | 2320          |
|    policy_gradient_loss | -0.00103      |
|    std                  | 0.932         |
|    value_loss           | 2.32e+08      |
-------------------------------------------
Eval num_timesteps=476000, episode_reward=-87984.50 +/- 19457.88
Episode length: 75.40 +/- 23.16
------------------------------------
| eval/              |             |
|    mean action     | -0.23903221 |
|    mean velocity x | 2.04        |
|    mean velocity y | 2.62        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 75.4        |
|    mean_reward     | -8.8e+04    |
| time/              |             |
|    total_timesteps | 476000      |
------------------------------------
Eval num_timesteps=476500, episode_reward=-78719.05 +/- 42584.32
Episode length: 55.60 +/- 13.89
----------------------------------
| eval/              |           |
|    mean action     | 0.4891381 |
|    mean velocity x | -0.0225   |
|    mean velocity y | -2.24     |
|    mean velocity z | 15.9      |
|    mean_ep_length  | 55.6      |
|    mean_reward     | -7.87e+04 |
| time/              |           |
|    total_timesteps | 476500    |
----------------------------------
Eval num_timesteps=477000, episode_reward=-81481.27 +/- 37140.25
Episode length: 62.00 +/- 8.29
-----------------------------------
| eval/              |            |
|    mean action     | 0.54276985 |
|    mean velocity x | -2.72      |
|    mean velocity y | -4.65      |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 62         |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 477000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.48e+04 |
| time/              |           |
|    fps             | 86        |
|    iterations      | 233       |
|    time_elapsed    | 5509      |
|    total_timesteps | 477184    |
----------------------------------
Eval num_timesteps=477500, episode_reward=-80437.77 +/- 62790.26
Episode length: 102.40 +/- 69.19
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1934687     |
|    mean velocity x      | -1.07         |
|    mean velocity y      | -2            |
|    mean velocity z      | 21.9          |
|    mean_ep_length       | 102           |
|    mean_reward          | -8.04e+04     |
| time/                   |               |
|    total_timesteps      | 477500        |
| train/                  |               |
|    approx_kl            | 0.00030071093 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.04         |
|    explained_variance   | 0.127         |
|    learning_rate        | 0.001         |
|    loss                 | 4.83e+07      |
|    n_updates            | 2330          |
|    policy_gradient_loss | -0.000939     |
|    std                  | 0.931         |
|    value_loss           | 2.55e+08      |
-------------------------------------------
Eval num_timesteps=478000, episode_reward=-68382.71 +/- 39916.05
Episode length: 54.60 +/- 22.89
------------------------------------
| eval/              |             |
|    mean action     | -0.17476287 |
|    mean velocity x | -0.0909     |
|    mean velocity y | 0.272       |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 478000      |
------------------------------------
Eval num_timesteps=478500, episode_reward=-52514.71 +/- 34024.58
Episode length: 58.60 +/- 29.83
------------------------------------
| eval/              |             |
|    mean action     | -0.22112398 |
|    mean velocity x | 2.91        |
|    mean velocity y | 2.42        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -5.25e+04   |
| time/              |             |
|    total_timesteps | 478500      |
------------------------------------
Eval num_timesteps=479000, episode_reward=-58649.52 +/- 41174.47
Episode length: 56.00 +/- 12.59
-----------------------------------
| eval/              |            |
|    mean action     | -0.5130111 |
|    mean velocity x | 1.35       |
|    mean velocity y | 3.85       |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 56         |
|    mean_reward     | -5.86e+04  |
| time/              |            |
|    total_timesteps | 479000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.1      |
|    ep_rew_mean     | -8.99e+04 |
| time/              |           |
|    fps             | 86        |
|    iterations      | 234       |
|    time_elapsed    | 5516      |
|    total_timesteps | 479232    |
----------------------------------
Eval num_timesteps=479500, episode_reward=-93053.05 +/- 21835.02
Episode length: 64.40 +/- 2.33
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.04684297 |
|    mean velocity x      | 0.348       |
|    mean velocity y      | 0.24        |
|    mean velocity z      | 22.1        |
|    mean_ep_length       | 64.4        |
|    mean_reward          | -9.31e+04   |
| time/                   |             |
|    total_timesteps      | 479500      |
| train/                  |             |
|    approx_kl            | 0.003505467 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.001       |
|    loss                 | 9.51e+07    |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00381    |
|    std                  | 0.931       |
|    value_loss           | 2.81e+08    |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=-44412.68 +/- 40192.58
Episode length: 50.00 +/- 23.64
------------------------------------
| eval/              |             |
|    mean action     | -0.32673162 |
|    mean velocity x | 1.83        |
|    mean velocity y | 2.5         |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 50          |
|    mean_reward     | -4.44e+04   |
| time/              |             |
|    total_timesteps | 480000      |
------------------------------------
Eval num_timesteps=480500, episode_reward=-69935.55 +/- 5023.64
Episode length: 67.00 +/- 11.17
------------------------------------
| eval/              |             |
|    mean action     | -0.30898052 |
|    mean velocity x | 0.286       |
|    mean velocity y | 0.836       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 67          |
|    mean_reward     | -6.99e+04   |
| time/              |             |
|    total_timesteps | 480500      |
------------------------------------
Eval num_timesteps=481000, episode_reward=-85614.11 +/- 14154.83
Episode length: 72.60 +/- 16.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.6440482 |
|    mean velocity x | 2.67       |
|    mean velocity y | 4.16       |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 72.6       |
|    mean_reward     | -8.56e+04  |
| time/              |            |
|    total_timesteps | 481000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.2      |
|    ep_rew_mean     | -9.49e+04 |
| time/              |           |
|    fps             | 87        |
|    iterations      | 235       |
|    time_elapsed    | 5523      |
|    total_timesteps | 481280    |
----------------------------------
Eval num_timesteps=481500, episode_reward=-94587.72 +/- 50245.28
Episode length: 73.00 +/- 44.92
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13461459   |
|    mean velocity x      | 3.38          |
|    mean velocity y      | 2.66          |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 73            |
|    mean_reward          | -9.46e+04     |
| time/                   |               |
|    total_timesteps      | 481500        |
| train/                  |               |
|    approx_kl            | 0.00055254274 |
|    clip_fraction        | 0.00151       |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.04         |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+08      |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.00159      |
|    std                  | 0.931         |
|    value_loss           | 2.87e+08      |
-------------------------------------------
Eval num_timesteps=482000, episode_reward=-72942.96 +/- 30302.24
Episode length: 65.80 +/- 19.21
-----------------------------------
| eval/              |            |
|    mean action     | 0.16238092 |
|    mean velocity x | -0.144     |
|    mean velocity y | -1.02      |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -7.29e+04  |
| time/              |            |
|    total_timesteps | 482000     |
-----------------------------------
Eval num_timesteps=482500, episode_reward=-89405.15 +/- 44448.17
Episode length: 75.40 +/- 41.43
-----------------------------------
| eval/              |            |
|    mean action     | -0.3377983 |
|    mean velocity x | 0.804      |
|    mean velocity y | 2.38       |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 75.4       |
|    mean_reward     | -8.94e+04  |
| time/              |            |
|    total_timesteps | 482500     |
-----------------------------------
Eval num_timesteps=483000, episode_reward=-82587.27 +/- 27251.62
Episode length: 61.20 +/- 9.87
-----------------------------------
| eval/              |            |
|    mean action     | 0.38409215 |
|    mean velocity x | -0.577     |
|    mean velocity y | -2.66      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -8.26e+04  |
| time/              |            |
|    total_timesteps | 483000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.5      |
|    ep_rew_mean     | -9.76e+04 |
| time/              |           |
|    fps             | 87        |
|    iterations      | 236       |
|    time_elapsed    | 5531      |
|    total_timesteps | 483328    |
----------------------------------
Eval num_timesteps=483500, episode_reward=-89224.30 +/- 39534.30
Episode length: 80.00 +/- 32.94
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.02423146   |
|    mean velocity x      | -0.521        |
|    mean velocity y      | 0.0414        |
|    mean velocity z      | 18.8          |
|    mean_ep_length       | 80            |
|    mean_reward          | -8.92e+04     |
| time/                   |               |
|    total_timesteps      | 483500        |
| train/                  |               |
|    approx_kl            | 0.00023606134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.04         |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.001         |
|    loss                 | 1.1e+08       |
|    n_updates            | 2360          |
|    policy_gradient_loss | -0.000447     |
|    std                  | 0.931         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=484000, episode_reward=-84683.51 +/- 17659.26
Episode length: 69.60 +/- 8.40
----------------------------------
| eval/              |           |
|    mean action     | 0.4062863 |
|    mean velocity x | -1.82     |
|    mean velocity y | -2.91     |
|    mean velocity z | 18.8      |
|    mean_ep_length  | 69.6      |
|    mean_reward     | -8.47e+04 |
| time/              |           |
|    total_timesteps | 484000    |
----------------------------------
Eval num_timesteps=484500, episode_reward=-61517.75 +/- 26672.92
Episode length: 60.20 +/- 12.92
----------------------------------
| eval/              |           |
|    mean action     | 0.0462064 |
|    mean velocity x | 0.628     |
|    mean velocity y | -0.503    |
|    mean velocity z | 20        |
|    mean_ep_length  | 60.2      |
|    mean_reward     | -6.15e+04 |
| time/              |           |
|    total_timesteps | 484500    |
----------------------------------
Eval num_timesteps=485000, episode_reward=-75989.68 +/- 21914.01
Episode length: 70.60 +/- 9.22
-----------------------------------
| eval/              |            |
|    mean action     | -0.3049115 |
|    mean velocity x | -0.406     |
|    mean velocity y | 0.639      |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 70.6       |
|    mean_reward     | -7.6e+04   |
| time/              |            |
|    total_timesteps | 485000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 85.5      |
|    ep_rew_mean     | -1.03e+05 |
| time/              |           |
|    fps             | 87        |
|    iterations      | 237       |
|    time_elapsed    | 5538      |
|    total_timesteps | 485376    |
----------------------------------
Eval num_timesteps=485500, episode_reward=-89476.96 +/- 22530.50
Episode length: 64.40 +/- 7.45
----------------------------------------
| eval/                   |            |
|    mean action          | 0.07332919 |
|    mean velocity x      | 0.477      |
|    mean velocity y      | 0.14       |
|    mean velocity z      | 22.5       |
|    mean_ep_length       | 64.4       |
|    mean_reward          | -8.95e+04  |
| time/                   |            |
|    total_timesteps      | 485500     |
| train/                  |            |
|    approx_kl            | 0.00346578 |
|    clip_fraction        | 0.00937    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.04      |
|    explained_variance   | 0.0918     |
|    learning_rate        | 0.001      |
|    loss                 | 1.75e+08   |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.00261   |
|    std                  | 0.931      |
|    value_loss           | 2.91e+08   |
----------------------------------------
Eval num_timesteps=486000, episode_reward=-74607.87 +/- 28012.66
Episode length: 67.20 +/- 20.17
-----------------------------------
| eval/              |            |
|    mean action     | 0.08953908 |
|    mean velocity x | 0.201      |
|    mean velocity y | -0.934     |
|    mean velocity z | 21.5       |
|    mean_ep_length  | 67.2       |
|    mean_reward     | -7.46e+04  |
| time/              |            |
|    total_timesteps | 486000     |
-----------------------------------
Eval num_timesteps=486500, episode_reward=-67036.10 +/- 37474.31
Episode length: 55.60 +/- 26.85
------------------------------------
| eval/              |             |
|    mean action     | -0.31984454 |
|    mean velocity x | 1.52        |
|    mean velocity y | 2.71        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 55.6        |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 486500      |
------------------------------------
Eval num_timesteps=487000, episode_reward=-50787.96 +/- 28877.05
Episode length: 50.00 +/- 15.68
------------------------------------
| eval/              |             |
|    mean action     | -0.04183617 |
|    mean velocity x | 0.194       |
|    mean velocity y | 0.642       |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 50          |
|    mean_reward     | -5.08e+04   |
| time/              |             |
|    total_timesteps | 487000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82.2      |
|    ep_rew_mean     | -9.65e+04 |
| time/              |           |
|    fps             | 87        |
|    iterations      | 238       |
|    time_elapsed    | 5545      |
|    total_timesteps | 487424    |
----------------------------------
Eval num_timesteps=487500, episode_reward=-77753.72 +/- 27411.53
Episode length: 65.00 +/- 9.84
------------------------------------------
| eval/                   |              |
|    mean action          | -0.39571545  |
|    mean velocity x      | 3.74         |
|    mean velocity y      | 5.44         |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 65           |
|    mean_reward          | -7.78e+04    |
| time/                   |              |
|    total_timesteps      | 487500       |
| train/                  |              |
|    approx_kl            | 0.0030310608 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.127        |
|    learning_rate        | 0.001        |
|    loss                 | 7.23e+07     |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00299     |
|    std                  | 0.93         |
|    value_loss           | 2.59e+08     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=-79982.82 +/- 35567.10
Episode length: 63.80 +/- 9.50
-----------------------------------
| eval/              |            |
|    mean action     | -0.7739504 |
|    mean velocity x | 3.44       |
|    mean velocity y | 5.95       |
|    mean velocity z | 20         |
|    mean_ep_length  | 63.8       |
|    mean_reward     | -8e+04     |
| time/              |            |
|    total_timesteps | 488000     |
-----------------------------------
Eval num_timesteps=488500, episode_reward=-90995.64 +/- 27907.05
Episode length: 71.40 +/- 16.40
-------------------------------------
| eval/              |              |
|    mean action     | -0.122385554 |
|    mean velocity x | 0.18         |
|    mean velocity y | 0.89         |
|    mean velocity z | 19.9         |
|    mean_ep_length  | 71.4         |
|    mean_reward     | -9.1e+04     |
| time/              |              |
|    total_timesteps | 488500       |
-------------------------------------
Eval num_timesteps=489000, episode_reward=-65101.57 +/- 35993.81
Episode length: 58.40 +/- 10.73
-------------------------------------
| eval/              |              |
|    mean action     | -0.019648269 |
|    mean velocity x | 0.589        |
|    mean velocity y | 0.0294       |
|    mean velocity z | 21.6         |
|    mean_ep_length  | 58.4         |
|    mean_reward     | -6.51e+04    |
| time/              |              |
|    total_timesteps | 489000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 85.9      |
|    ep_rew_mean     | -9.88e+04 |
| time/              |           |
|    fps             | 88        |
|    iterations      | 239       |
|    time_elapsed    | 5552      |
|    total_timesteps | 489472    |
----------------------------------
Eval num_timesteps=489500, episode_reward=-74700.00 +/- 44413.94
Episode length: 60.20 +/- 10.70
------------------------------------------
| eval/                   |              |
|    mean action          | 0.7640801    |
|    mean velocity x      | -1.75        |
|    mean velocity y      | -3.62        |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 60.2         |
|    mean_reward          | -7.47e+04    |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0024413601 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.133        |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+08     |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00317     |
|    std                  | 0.928        |
|    value_loss           | 2.49e+08     |
------------------------------------------
Eval num_timesteps=490000, episode_reward=-88916.07 +/- 22466.49
Episode length: 69.00 +/- 3.29
-----------------------------------
| eval/              |            |
|    mean action     | -0.7181278 |
|    mean velocity x | 3.41       |
|    mean velocity y | 4.48       |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 69         |
|    mean_reward     | -8.89e+04  |
| time/              |            |
|    total_timesteps | 490000     |
-----------------------------------
Eval num_timesteps=490500, episode_reward=-61849.98 +/- 36545.55
Episode length: 65.00 +/- 23.86
------------------------------------
| eval/              |             |
|    mean action     | -0.11370708 |
|    mean velocity x | 0.0657      |
|    mean velocity y | 1.25        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 65          |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 490500      |
------------------------------------
Eval num_timesteps=491000, episode_reward=-75748.20 +/- 39625.51
Episode length: 78.20 +/- 45.91
------------------------------------
| eval/              |             |
|    mean action     | -0.11593811 |
|    mean velocity x | 1.79        |
|    mean velocity y | 1.49        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 78.2        |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 491000      |
------------------------------------
Eval num_timesteps=491500, episode_reward=-53259.94 +/- 41293.00
Episode length: 53.00 +/- 22.65
------------------------------------
| eval/              |             |
|    mean action     | -0.26920405 |
|    mean velocity x | 1.48        |
|    mean velocity y | 2.06        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 53          |
|    mean_reward     | -5.33e+04   |
| time/              |             |
|    total_timesteps | 491500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 85.6      |
|    ep_rew_mean     | -1.01e+05 |
| time/              |           |
|    fps             | 88        |
|    iterations      | 240       |
|    time_elapsed    | 5560      |
|    total_timesteps | 491520    |
----------------------------------
Eval num_timesteps=492000, episode_reward=-75642.40 +/- 42463.82
Episode length: 64.60 +/- 24.74
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13088328  |
|    mean velocity x      | 2.33         |
|    mean velocity y      | 2.76         |
|    mean velocity z      | 22.4         |
|    mean_ep_length       | 64.6         |
|    mean_reward          | -7.56e+04    |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0011678715 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.126        |
|    learning_rate        | 0.001        |
|    loss                 | 8.83e+07     |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.928        |
|    value_loss           | 2.65e+08     |
------------------------------------------
Eval num_timesteps=492500, episode_reward=-66549.49 +/- 38137.33
Episode length: 57.20 +/- 15.51
------------------------------------
| eval/              |             |
|    mean action     | -0.08788893 |
|    mean velocity x | 0.274       |
|    mean velocity y | 0.855       |
|    mean velocity z | 18          |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.65e+04   |
| time/              |             |
|    total_timesteps | 492500      |
------------------------------------
Eval num_timesteps=493000, episode_reward=-56986.75 +/- 44185.05
Episode length: 54.40 +/- 16.03
------------------------------------
| eval/              |             |
|    mean action     | -0.14679536 |
|    mean velocity x | 1.06        |
|    mean velocity y | 1.06        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5.7e+04    |
| time/              |             |
|    total_timesteps | 493000      |
------------------------------------
Eval num_timesteps=493500, episode_reward=-62429.15 +/- 48448.87
Episode length: 47.60 +/- 23.75
------------------------------------
| eval/              |             |
|    mean action     | -0.22968018 |
|    mean velocity x | 1.71        |
|    mean velocity y | 2.56        |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 47.6        |
|    mean_reward     | -6.24e+04   |
| time/              |             |
|    total_timesteps | 493500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 86        |
|    ep_rew_mean     | -9.94e+04 |
| time/              |           |
|    fps             | 88        |
|    iterations      | 241       |
|    time_elapsed    | 5566      |
|    total_timesteps | 493568    |
----------------------------------
Eval num_timesteps=494000, episode_reward=-43623.65 +/- 37150.55
Episode length: 55.00 +/- 33.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07999915  |
|    mean velocity x      | 1.72         |
|    mean velocity y      | 1.39         |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 55           |
|    mean_reward          | -4.36e+04    |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0008812904 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.14         |
|    learning_rate        | 0.001        |
|    loss                 | 6.26e+07     |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.928        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=494500, episode_reward=-85214.22 +/- 15338.36
Episode length: 66.20 +/- 5.56
-----------------------------------
| eval/              |            |
|    mean action     | -1.1432456 |
|    mean velocity x | 5.36       |
|    mean velocity y | 8.34       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 66.2       |
|    mean_reward     | -8.52e+04  |
| time/              |            |
|    total_timesteps | 494500     |
-----------------------------------
Eval num_timesteps=495000, episode_reward=-86907.45 +/- 28754.28
Episode length: 65.60 +/- 7.34
------------------------------------
| eval/              |             |
|    mean action     | -0.14911486 |
|    mean velocity x | 2.55        |
|    mean velocity y | 3.05        |
|    mean velocity z | 14.4        |
|    mean_ep_length  | 65.6        |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 495000      |
------------------------------------
Eval num_timesteps=495500, episode_reward=-62838.68 +/- 52423.79
Episode length: 47.40 +/- 26.23
-----------------------------------
| eval/              |            |
|    mean action     | 0.15760806 |
|    mean velocity x | 0.00802    |
|    mean velocity y | -0.0247    |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 47.4       |
|    mean_reward     | -6.28e+04  |
| time/              |            |
|    total_timesteps | 495500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 88.3      |
|    ep_rew_mean     | -9.47e+04 |
| time/              |           |
|    fps             | 88        |
|    iterations      | 242       |
|    time_elapsed    | 5573      |
|    total_timesteps | 495616    |
----------------------------------
Eval num_timesteps=496000, episode_reward=-56259.77 +/- 29075.55
Episode length: 62.60 +/- 16.21
------------------------------------------
| eval/                   |              |
|    mean action          | 0.27845737   |
|    mean velocity x      | -0.223       |
|    mean velocity y      | -0.792       |
|    mean velocity z      | 20.8         |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -5.63e+04    |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0019100737 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.001        |
|    loss                 | 6.26e+07     |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.929        |
|    value_loss           | 1.95e+08     |
------------------------------------------
Eval num_timesteps=496500, episode_reward=-72107.61 +/- 41639.91
Episode length: 77.60 +/- 41.11
------------------------------------
| eval/              |             |
|    mean action     | 0.013435784 |
|    mean velocity x | 1           |
|    mean velocity y | 0.496       |
|    mean velocity z | 19          |
|    mean_ep_length  | 77.6        |
|    mean_reward     | -7.21e+04   |
| time/              |             |
|    total_timesteps | 496500      |
------------------------------------
Eval num_timesteps=497000, episode_reward=-83872.36 +/- 20894.05
Episode length: 73.00 +/- 13.18
----------------------------------
| eval/              |           |
|    mean action     | 0.1155338 |
|    mean velocity x | 0.916     |
|    mean velocity y | -0.156    |
|    mean velocity z | 17.3      |
|    mean_ep_length  | 73        |
|    mean_reward     | -8.39e+04 |
| time/              |           |
|    total_timesteps | 497000    |
----------------------------------
Eval num_timesteps=497500, episode_reward=-60930.32 +/- 49454.36
Episode length: 47.80 +/- 24.69
-----------------------------------
| eval/              |            |
|    mean action     | 0.17440368 |
|    mean velocity x | -0.302     |
|    mean velocity y | -1.3       |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 47.8       |
|    mean_reward     | -6.09e+04  |
| time/              |            |
|    total_timesteps | 497500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82.3      |
|    ep_rew_mean     | -8.78e+04 |
| time/              |           |
|    fps             | 89        |
|    iterations      | 243       |
|    time_elapsed    | 5581      |
|    total_timesteps | 497664    |
----------------------------------
Eval num_timesteps=498000, episode_reward=-60384.72 +/- 35941.74
Episode length: 65.00 +/- 17.12
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1280204   |
|    mean velocity x      | 2.13         |
|    mean velocity y      | 0.731        |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 65           |
|    mean_reward          | -6.04e+04    |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0008372052 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.001        |
|    loss                 | 1.26e+08     |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.000643    |
|    std                  | 0.929        |
|    value_loss           | 2.49e+08     |
------------------------------------------
Eval num_timesteps=498500, episode_reward=-70039.64 +/- 40836.57
Episode length: 54.40 +/- 24.30
----------------------------------
| eval/              |           |
|    mean action     | 0.1597446 |
|    mean velocity x | -0.445    |
|    mean velocity y | -1.81     |
|    mean velocity z | 20        |
|    mean_ep_length  | 54.4      |
|    mean_reward     | -7e+04    |
| time/              |           |
|    total_timesteps | 498500    |
----------------------------------
Eval num_timesteps=499000, episode_reward=-59924.44 +/- 35283.13
Episode length: 59.00 +/- 22.57
------------------------------------
| eval/              |             |
|    mean action     | -0.12010373 |
|    mean velocity x | 0.218       |
|    mean velocity y | 0.895       |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 59          |
|    mean_reward     | -5.99e+04   |
| time/              |             |
|    total_timesteps | 499000      |
------------------------------------
Eval num_timesteps=499500, episode_reward=-89688.78 +/- 20941.79
Episode length: 71.00 +/- 9.06
------------------------------------
| eval/              |             |
|    mean action     | -0.15523155 |
|    mean velocity x | 2.43        |
|    mean velocity y | 2.61        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 71          |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 499500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.5      |
|    ep_rew_mean     | -8.37e+04 |
| time/              |           |
|    fps             | 89        |
|    iterations      | 244       |
|    time_elapsed    | 5588      |
|    total_timesteps | 499712    |
----------------------------------
Eval num_timesteps=500000, episode_reward=-65137.27 +/- 26498.67
Episode length: 62.00 +/- 8.74
------------------------------------------
| eval/                   |              |
|    mean action          | 0.20118503   |
|    mean velocity x      | -0.603       |
|    mean velocity y      | -1.07        |
|    mean velocity z      | 20           |
|    mean_ep_length       | 62           |
|    mean_reward          | -6.51e+04    |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0008800017 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+08     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.928        |
|    value_loss           | 2.34e+08     |
------------------------------------------
Eval num_timesteps=500500, episode_reward=-89793.31 +/- 44820.85
Episode length: 57.00 +/- 19.58
-------------------------------------
| eval/              |              |
|    mean action     | -0.046964448 |
|    mean velocity x | 0.654        |
|    mean velocity y | 0.832        |
|    mean velocity z | 20.9         |
|    mean_ep_length  | 57           |
|    mean_reward     | -8.98e+04    |
| time/              |              |
|    total_timesteps | 500500       |
-------------------------------------
Eval num_timesteps=501000, episode_reward=-73128.65 +/- 38969.21
Episode length: 59.20 +/- 12.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.24059486 |
|    mean velocity x | -1.4       |
|    mean velocity y | -2.17      |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 59.2       |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 501000     |
-----------------------------------
Eval num_timesteps=501500, episode_reward=-58889.10 +/- 31703.94
Episode length: 55.20 +/- 11.55
------------------------------------
| eval/              |             |
|    mean action     | -0.81293124 |
|    mean velocity x | 3.22        |
|    mean velocity y | 6.04        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -5.89e+04   |
| time/              |             |
|    total_timesteps | 501500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.5      |
|    ep_rew_mean     | -8.49e+04 |
| time/              |           |
|    fps             | 89        |
|    iterations      | 245       |
|    time_elapsed    | 5594      |
|    total_timesteps | 501760    |
----------------------------------
Eval num_timesteps=502000, episode_reward=-82989.32 +/- 39581.96
Episode length: 63.00 +/- 19.17
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21522802  |
|    mean velocity x      | 1.84         |
|    mean velocity y      | 2.33         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 63           |
|    mean_reward          | -8.3e+04     |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0031878194 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.124        |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+08     |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00317     |
|    std                  | 0.926        |
|    value_loss           | 2.61e+08     |
------------------------------------------
Eval num_timesteps=502500, episode_reward=-58790.06 +/- 40552.22
Episode length: 65.40 +/- 37.13
------------------------------------
| eval/              |             |
|    mean action     | -0.26053038 |
|    mean velocity x | 2.46        |
|    mean velocity y | 2.8         |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -5.88e+04   |
| time/              |             |
|    total_timesteps | 502500      |
------------------------------------
Eval num_timesteps=503000, episode_reward=-40333.84 +/- 39993.43
Episode length: 45.60 +/- 16.13
----------------------------------
| eval/              |           |
|    mean action     | 0.4902735 |
|    mean velocity x | -2.11     |
|    mean velocity y | -2.85     |
|    mean velocity z | 17.6      |
|    mean_ep_length  | 45.6      |
|    mean_reward     | -4.03e+04 |
| time/              |           |
|    total_timesteps | 503000    |
----------------------------------
Eval num_timesteps=503500, episode_reward=-52710.19 +/- 29150.38
Episode length: 62.20 +/- 13.57
------------------------------------
| eval/              |             |
|    mean action     | -0.35976252 |
|    mean velocity x | 1.59        |
|    mean velocity y | 2.32        |
|    mean velocity z | 19          |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -5.27e+04   |
| time/              |             |
|    total_timesteps | 503500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.2      |
|    ep_rew_mean     | -8.61e+04 |
| time/              |           |
|    fps             | 89        |
|    iterations      | 246       |
|    time_elapsed    | 5602      |
|    total_timesteps | 503808    |
----------------------------------
Eval num_timesteps=504000, episode_reward=-74381.97 +/- 12288.40
Episode length: 71.40 +/- 9.46
------------------------------------------
| eval/                   |              |
|    mean action          | -0.042995017 |
|    mean velocity x      | 1.69         |
|    mean velocity y      | 1.19         |
|    mean velocity z      | 20.8         |
|    mean_ep_length       | 71.4         |
|    mean_reward          | -7.44e+04    |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0027192521 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+08     |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 0.926        |
|    value_loss           | 1.97e+08     |
------------------------------------------
Eval num_timesteps=504500, episode_reward=-65122.15 +/- 43030.41
Episode length: 52.60 +/- 19.87
------------------------------------
| eval/              |             |
|    mean action     | -0.05619769 |
|    mean velocity x | 0.536       |
|    mean velocity y | 0.324       |
|    mean velocity z | 17.2        |
|    mean_ep_length  | 52.6        |
|    mean_reward     | -6.51e+04   |
| time/              |             |
|    total_timesteps | 504500      |
------------------------------------
Eval num_timesteps=505000, episode_reward=-24772.39 +/- 26908.21
Episode length: 37.40 +/- 19.09
------------------------------------
| eval/              |             |
|    mean action     | 0.031283423 |
|    mean velocity x | -0.266      |
|    mean velocity y | -0.308      |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 37.4        |
|    mean_reward     | -2.48e+04   |
| time/              |             |
|    total_timesteps | 505000      |
------------------------------------
Eval num_timesteps=505500, episode_reward=-93724.57 +/- 18694.79
Episode length: 67.00 +/- 4.34
-------------------------------------
| eval/              |              |
|    mean action     | -0.112140894 |
|    mean velocity x | -0.942       |
|    mean velocity y | -0.471       |
|    mean velocity z | 21           |
|    mean_ep_length  | 67           |
|    mean_reward     | -9.37e+04    |
| time/              |              |
|    total_timesteps | 505500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.4      |
|    ep_rew_mean     | -8.98e+04 |
| time/              |           |
|    fps             | 90        |
|    iterations      | 247       |
|    time_elapsed    | 5609      |
|    total_timesteps | 505856    |
----------------------------------
Eval num_timesteps=506000, episode_reward=-64254.50 +/- 34405.07
Episode length: 55.60 +/- 18.35
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.03461139    |
|    mean velocity x      | 2.42          |
|    mean velocity y      | 1.38          |
|    mean velocity z      | 19            |
|    mean_ep_length       | 55.6          |
|    mean_reward          | -6.43e+04     |
| time/                   |               |
|    total_timesteps      | 506000        |
| train/                  |               |
|    approx_kl            | 0.00016317322 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.02         |
|    explained_variance   | 0.121         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+08      |
|    n_updates            | 2470          |
|    policy_gradient_loss | -0.000428     |
|    std                  | 0.926         |
|    value_loss           | 2.86e+08      |
-------------------------------------------
Eval num_timesteps=506500, episode_reward=-81321.20 +/- 25134.51
Episode length: 71.20 +/- 9.70
------------------------------------
| eval/              |             |
|    mean action     | -0.21681531 |
|    mean velocity x | 0.586       |
|    mean velocity y | 2.34        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 71.2        |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 506500      |
------------------------------------
Eval num_timesteps=507000, episode_reward=-64485.46 +/- 32500.21
Episode length: 63.20 +/- 20.35
-----------------------------------
| eval/              |            |
|    mean action     | 0.14919828 |
|    mean velocity x | -0.515     |
|    mean velocity y | -1.6       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -6.45e+04  |
| time/              |            |
|    total_timesteps | 507000     |
-----------------------------------
Eval num_timesteps=507500, episode_reward=-60814.38 +/- 43585.92
Episode length: 53.40 +/- 19.17
------------------------------------
| eval/              |             |
|    mean action     | -0.15993185 |
|    mean velocity x | 1.59        |
|    mean velocity y | 2.12        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 53.4        |
|    mean_reward     | -6.08e+04   |
| time/              |             |
|    total_timesteps | 507500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.7      |
|    ep_rew_mean     | -8.65e+04 |
| time/              |           |
|    fps             | 90        |
|    iterations      | 248       |
|    time_elapsed    | 5616      |
|    total_timesteps | 507904    |
----------------------------------
Eval num_timesteps=508000, episode_reward=-45370.53 +/- 54210.50
Episode length: 35.60 +/- 24.48
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.59003985  |
|    mean velocity x      | -2.35       |
|    mean velocity y      | -4.89       |
|    mean velocity z      | 20.9        |
|    mean_ep_length       | 35.6        |
|    mean_reward          | -4.54e+04   |
| time/                   |             |
|    total_timesteps      | 508000      |
| train/                  |             |
|    approx_kl            | 0.003010346 |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.001       |
|    loss                 | 1.35e+08    |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00312    |
|    std                  | 0.926       |
|    value_loss           | 2.63e+08    |
-----------------------------------------
Eval num_timesteps=508500, episode_reward=-82867.82 +/- 9730.01
Episode length: 68.40 +/- 9.11
------------------------------------
| eval/              |             |
|    mean action     | -0.16797893 |
|    mean velocity x | 1.45        |
|    mean velocity y | 1.24        |
|    mean velocity z | 22          |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 508500      |
------------------------------------
Eval num_timesteps=509000, episode_reward=-77535.07 +/- 37596.62
Episode length: 58.60 +/- 12.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.20035107 |
|    mean velocity x | -0.932     |
|    mean velocity y | -1.31      |
|    mean velocity z | 16         |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -7.75e+04  |
| time/              |            |
|    total_timesteps | 509000     |
-----------------------------------
Eval num_timesteps=509500, episode_reward=-92326.73 +/- 15313.57
Episode length: 73.00 +/- 15.23
------------------------------------
| eval/              |             |
|    mean action     | 0.044820152 |
|    mean velocity x | 0.842       |
|    mean velocity y | -0.195      |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 73          |
|    mean_reward     | -9.23e+04   |
| time/              |             |
|    total_timesteps | 509500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.4      |
|    ep_rew_mean     | -8.28e+04 |
| time/              |           |
|    fps             | 90        |
|    iterations      | 249       |
|    time_elapsed    | 5623      |
|    total_timesteps | 509952    |
----------------------------------
Eval num_timesteps=510000, episode_reward=-66167.84 +/- 36663.32
Episode length: 56.20 +/- 16.87
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19159497   |
|    mean velocity x      | 0.762         |
|    mean velocity y      | 1.51          |
|    mean velocity z      | 18            |
|    mean_ep_length       | 56.2          |
|    mean_reward          | -6.62e+04     |
| time/                   |               |
|    total_timesteps      | 510000        |
| train/                  |               |
|    approx_kl            | 0.00039223002 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.02         |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.001         |
|    loss                 | 8.12e+07      |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.0011       |
|    std                  | 0.925         |
|    value_loss           | 2.52e+08      |
-------------------------------------------
Eval num_timesteps=510500, episode_reward=-50597.76 +/- 26728.20
Episode length: 63.20 +/- 14.08
------------------------------------
| eval/              |             |
|    mean action     | -0.49237338 |
|    mean velocity x | 2.16        |
|    mean velocity y | 3.44        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 63.2        |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 510500      |
------------------------------------
Eval num_timesteps=511000, episode_reward=-59081.73 +/- 22841.91
Episode length: 59.20 +/- 12.02
------------------------------------
| eval/              |             |
|    mean action     | 0.108220294 |
|    mean velocity x | 0.531       |
|    mean velocity y | 0.48        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -5.91e+04   |
| time/              |             |
|    total_timesteps | 511000      |
------------------------------------
Eval num_timesteps=511500, episode_reward=-48406.54 +/- 28328.37
Episode length: 54.00 +/- 18.47
------------------------------------
| eval/              |             |
|    mean action     | -0.22978105 |
|    mean velocity x | 1.26        |
|    mean velocity y | 1.32        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 54          |
|    mean_reward     | -4.84e+04   |
| time/              |             |
|    total_timesteps | 511500      |
------------------------------------
Eval num_timesteps=512000, episode_reward=-39706.27 +/- 29498.00
Episode length: 49.40 +/- 25.27
------------------------------------
| eval/              |             |
|    mean action     | -0.32999346 |
|    mean velocity x | 1.03        |
|    mean velocity y | 3.21        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 49.4        |
|    mean_reward     | -3.97e+04   |
| time/              |             |
|    total_timesteps | 512000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.8      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 90        |
|    iterations      | 250       |
|    time_elapsed    | 5630      |
|    total_timesteps | 512000    |
----------------------------------
Eval num_timesteps=512500, episode_reward=-74397.38 +/- 25285.71
Episode length: 65.60 +/- 12.18
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6048412   |
|    mean velocity x      | 2.53         |
|    mean velocity y      | 3.99         |
|    mean velocity z      | 21.6         |
|    mean_ep_length       | 65.6         |
|    mean_reward          | -7.44e+04    |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0010271899 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.143        |
|    learning_rate        | 0.001        |
|    loss                 | 1.23e+08     |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.926        |
|    value_loss           | 2.38e+08     |
------------------------------------------
Eval num_timesteps=513000, episode_reward=-80830.49 +/- 13314.49
Episode length: 68.60 +/- 8.96
------------------------------------
| eval/              |             |
|    mean action     | -0.56138396 |
|    mean velocity x | -0.399      |
|    mean velocity y | 3.04        |
|    mean velocity z | 19          |
|    mean_ep_length  | 68.6        |
|    mean_reward     | -8.08e+04   |
| time/              |             |
|    total_timesteps | 513000      |
------------------------------------
Eval num_timesteps=513500, episode_reward=-88144.95 +/- 14026.39
Episode length: 68.00 +/- 6.23
------------------------------------
| eval/              |             |
|    mean action     | -0.16402279 |
|    mean velocity x | 1.26        |
|    mean velocity y | 1.9         |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 68          |
|    mean_reward     | -8.81e+04   |
| time/              |             |
|    total_timesteps | 513500      |
------------------------------------
Eval num_timesteps=514000, episode_reward=-58754.40 +/- 29947.40
Episode length: 56.20 +/- 19.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.23171717 |
|    mean velocity x | 0.413      |
|    mean velocity y | 0.238      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -5.88e+04  |
| time/              |            |
|    total_timesteps | 514000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.9      |
|    ep_rew_mean     | -8.47e+04 |
| time/              |           |
|    fps             | 91        |
|    iterations      | 251       |
|    time_elapsed    | 5638      |
|    total_timesteps | 514048    |
----------------------------------
Eval num_timesteps=514500, episode_reward=-47372.32 +/- 42371.11
Episode length: 46.20 +/- 24.15
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.267747      |
|    mean velocity x      | 0.628         |
|    mean velocity y      | -1.16         |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 46.2          |
|    mean_reward          | -4.74e+04     |
| time/                   |               |
|    total_timesteps      | 514500        |
| train/                  |               |
|    approx_kl            | 0.00020799952 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.03         |
|    explained_variance   | 0.129         |
|    learning_rate        | 0.001         |
|    loss                 | 7.28e+07      |
|    n_updates            | 2510          |
|    policy_gradient_loss | -0.000423     |
|    std                  | 0.926         |
|    value_loss           | 2.66e+08      |
-------------------------------------------
Eval num_timesteps=515000, episode_reward=-46307.04 +/- 38037.52
Episode length: 45.20 +/- 23.09
------------------------------------
| eval/              |             |
|    mean action     | -0.29456863 |
|    mean velocity x | 1.19        |
|    mean velocity y | 2.26        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 45.2        |
|    mean_reward     | -4.63e+04   |
| time/              |             |
|    total_timesteps | 515000      |
------------------------------------
Eval num_timesteps=515500, episode_reward=-124490.69 +/- 38490.43
Episode length: 118.80 +/- 65.49
----------------------------------
| eval/              |           |
|    mean action     | 0.5039183 |
|    mean velocity x | -0.372    |
|    mean velocity y | -2.77     |
|    mean velocity z | 21.3      |
|    mean_ep_length  | 119       |
|    mean_reward     | -1.24e+05 |
| time/              |           |
|    total_timesteps | 515500    |
----------------------------------
Eval num_timesteps=516000, episode_reward=-91752.36 +/- 13377.29
Episode length: 76.80 +/- 17.97
-----------------------------------
| eval/              |            |
|    mean action     | 0.40303516 |
|    mean velocity x | -1.42      |
|    mean velocity y | -1.74      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 76.8       |
|    mean_reward     | -9.18e+04  |
| time/              |            |
|    total_timesteps | 516000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.4      |
|    ep_rew_mean     | -8.53e+04 |
| time/              |           |
|    fps             | 91        |
|    iterations      | 252       |
|    time_elapsed    | 5645      |
|    total_timesteps | 516096    |
----------------------------------
Eval num_timesteps=516500, episode_reward=-70079.44 +/- 32428.25
Episode length: 58.60 +/- 11.98
------------------------------------------
| eval/                   |              |
|    mean action          | 0.22388998   |
|    mean velocity x      | -0.257       |
|    mean velocity y      | -1.55        |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 58.6         |
|    mean_reward          | -7.01e+04    |
| time/                   |              |
|    total_timesteps      | 516500       |
| train/                  |              |
|    approx_kl            | 0.0005525948 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.128        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 0.927        |
|    value_loss           | 2.22e+08     |
------------------------------------------
Eval num_timesteps=517000, episode_reward=-59440.32 +/- 30496.09
Episode length: 59.60 +/- 24.10
-------------------------------------
| eval/              |              |
|    mean action     | -0.018034821 |
|    mean velocity x | 0.566        |
|    mean velocity y | 0.725        |
|    mean velocity z | 19           |
|    mean_ep_length  | 59.6         |
|    mean_reward     | -5.94e+04    |
| time/              |              |
|    total_timesteps | 517000       |
-------------------------------------
Eval num_timesteps=517500, episode_reward=-69514.38 +/- 44844.98
Episode length: 51.40 +/- 21.97
-------------------------------------
| eval/              |              |
|    mean action     | -0.011702561 |
|    mean velocity x | -0.109       |
|    mean velocity y | 0.48         |
|    mean velocity z | 17.4         |
|    mean_ep_length  | 51.4         |
|    mean_reward     | -6.95e+04    |
| time/              |              |
|    total_timesteps | 517500       |
-------------------------------------
Eval num_timesteps=518000, episode_reward=-85448.31 +/- 7418.49
Episode length: 80.40 +/- 24.40
----------------------------------
| eval/              |           |
|    mean action     | 0.2710089 |
|    mean velocity x | -1.1      |
|    mean velocity y | -2.13     |
|    mean velocity z | 20.7      |
|    mean_ep_length  | 80.4      |
|    mean_reward     | -8.54e+04 |
| time/              |           |
|    total_timesteps | 518000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.58e+04 |
| time/              |           |
|    fps             | 91        |
|    iterations      | 253       |
|    time_elapsed    | 5652      |
|    total_timesteps | 518144    |
----------------------------------
Eval num_timesteps=518500, episode_reward=-78130.09 +/- 35929.64
Episode length: 58.80 +/- 17.51
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.11415295 |
|    mean velocity x      | 0.648       |
|    mean velocity y      | 0.613       |
|    mean velocity z      | 17.9        |
|    mean_ep_length       | 58.8        |
|    mean_reward          | -7.81e+04   |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.0043009   |
|    clip_fraction        | 0.00962     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+08    |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00392    |
|    std                  | 0.925       |
|    value_loss           | 2.21e+08    |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=-64792.14 +/- 23203.81
Episode length: 57.20 +/- 11.30
------------------------------------
| eval/              |             |
|    mean action     | -0.39592695 |
|    mean velocity x | 0.932       |
|    mean velocity y | 3.06        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 519000      |
------------------------------------
Eval num_timesteps=519500, episode_reward=-80980.90 +/- 18630.39
Episode length: 66.00 +/- 9.32
------------------------------------
| eval/              |             |
|    mean action     | -0.46401256 |
|    mean velocity x | 3.38        |
|    mean velocity y | 3.22        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 66          |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 519500      |
------------------------------------
Eval num_timesteps=520000, episode_reward=-76262.43 +/- 20971.37
Episode length: 65.20 +/- 6.68
-----------------------------------
| eval/              |            |
|    mean action     | -0.9191447 |
|    mean velocity x | 5.57       |
|    mean velocity y | 7.65       |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 65.2       |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 520000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.46e+04 |
| time/              |           |
|    fps             | 91        |
|    iterations      | 254       |
|    time_elapsed    | 5659      |
|    total_timesteps | 520192    |
----------------------------------
Eval num_timesteps=520500, episode_reward=-49708.96 +/- 16034.93
Episode length: 57.20 +/- 8.08
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.005565094 |
|    mean velocity x      | 0.995       |
|    mean velocity y      | 1.07        |
|    mean velocity z      | 16.4        |
|    mean_ep_length       | 57.2        |
|    mean_reward          | -4.97e+04   |
| time/                   |             |
|    total_timesteps      | 520500      |
| train/                  |             |
|    approx_kl            | 0.000659878 |
|    clip_fraction        | 0.000293    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.001       |
|    loss                 | 1.24e+08    |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.000955   |
|    std                  | 0.925       |
|    value_loss           | 2.18e+08    |
-----------------------------------------
Eval num_timesteps=521000, episode_reward=-59004.84 +/- 28416.40
Episode length: 64.20 +/- 22.63
----------------------------------
| eval/              |           |
|    mean action     | 0.2173635 |
|    mean velocity x | -0.0456   |
|    mean velocity y | -0.611    |
|    mean velocity z | 18.7      |
|    mean_ep_length  | 64.2      |
|    mean_reward     | -5.9e+04  |
| time/              |           |
|    total_timesteps | 521000    |
----------------------------------
Eval num_timesteps=521500, episode_reward=-73281.78 +/- 37231.99
Episode length: 63.40 +/- 14.79
------------------------------------
| eval/              |             |
|    mean action     | -0.08541537 |
|    mean velocity x | 0.667       |
|    mean velocity y | 0.16        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -7.33e+04   |
| time/              |             |
|    total_timesteps | 521500      |
------------------------------------
Eval num_timesteps=522000, episode_reward=-63704.60 +/- 28473.37
Episode length: 61.60 +/- 9.44
------------------------------------
| eval/              |             |
|    mean action     | -0.19101278 |
|    mean velocity x | 0.982       |
|    mean velocity y | 1.65        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -6.37e+04   |
| time/              |             |
|    total_timesteps | 522000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.8      |
|    ep_rew_mean     | -8.38e+04 |
| time/              |           |
|    fps             | 92        |
|    iterations      | 255       |
|    time_elapsed    | 5666      |
|    total_timesteps | 522240    |
----------------------------------
Eval num_timesteps=522500, episode_reward=-88087.74 +/- 16372.47
Episode length: 63.00 +/- 2.45
----------------------------------------
| eval/                   |            |
|    mean action          | -0.3312134 |
|    mean velocity x      | 1.28       |
|    mean velocity y      | 3.14       |
|    mean velocity z      | 23.4       |
|    mean_ep_length       | 63         |
|    mean_reward          | -8.81e+04  |
| time/                   |            |
|    total_timesteps      | 522500     |
| train/                  |            |
|    approx_kl            | 0.00309865 |
|    clip_fraction        | 0.00483    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.139      |
|    learning_rate        | 0.001      |
|    loss                 | 1.57e+08   |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.00318   |
|    std                  | 0.924      |
|    value_loss           | 2.5e+08    |
----------------------------------------
Eval num_timesteps=523000, episode_reward=-47668.37 +/- 34919.10
Episode length: 65.40 +/- 26.53
------------------------------------
| eval/              |             |
|    mean action     | -0.47337803 |
|    mean velocity x | 3.29        |
|    mean velocity y | 3.49        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -4.77e+04   |
| time/              |             |
|    total_timesteps | 523000      |
------------------------------------
Eval num_timesteps=523500, episode_reward=-64395.55 +/- 27323.81
Episode length: 75.60 +/- 36.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.27406397 |
|    mean velocity x | -1.03      |
|    mean velocity y | -2.13      |
|    mean velocity z | 21         |
|    mean_ep_length  | 75.6       |
|    mean_reward     | -6.44e+04  |
| time/              |            |
|    total_timesteps | 523500     |
-----------------------------------
Eval num_timesteps=524000, episode_reward=-84719.58 +/- 27674.73
Episode length: 81.20 +/- 31.99
-----------------------------------
| eval/              |            |
|    mean action     | 0.52925456 |
|    mean velocity x | -2.16      |
|    mean velocity y | -4.1       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 81.2       |
|    mean_reward     | -8.47e+04  |
| time/              |            |
|    total_timesteps | 524000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -8.73e+04 |
| time/              |           |
|    fps             | 92        |
|    iterations      | 256       |
|    time_elapsed    | 5674      |
|    total_timesteps | 524288    |
----------------------------------
Eval num_timesteps=524500, episode_reward=-88516.81 +/- 71969.17
Episode length: 80.20 +/- 67.22
------------------------------------------
| eval/                   |              |
|    mean action          | -0.044311397 |
|    mean velocity x      | -0.143       |
|    mean velocity y      | -0.0332      |
|    mean velocity z      | 20.6         |
|    mean_ep_length       | 80.2         |
|    mean_reward          | -8.85e+04    |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 0.0025329103 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.147        |
|    learning_rate        | 0.001        |
|    loss                 | 1.13e+08     |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.925        |
|    value_loss           | 2.64e+08     |
------------------------------------------
Eval num_timesteps=525000, episode_reward=-80231.80 +/- 34222.72
Episode length: 59.60 +/- 9.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.05186953 |
|    mean velocity x | 0.0533     |
|    mean velocity y | -0.509     |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -8.02e+04  |
| time/              |            |
|    total_timesteps | 525000     |
-----------------------------------
Eval num_timesteps=525500, episode_reward=-58588.30 +/- 24314.27
Episode length: 76.40 +/- 29.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.40127376 |
|    mean velocity x | -0.304     |
|    mean velocity y | -0.993     |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 76.4       |
|    mean_reward     | -5.86e+04  |
| time/              |            |
|    total_timesteps | 525500     |
-----------------------------------
Eval num_timesteps=526000, episode_reward=-40459.99 +/- 26167.27
Episode length: 48.80 +/- 12.24
------------------------------------
| eval/              |             |
|    mean action     | -0.21976386 |
|    mean velocity x | 2.68        |
|    mean velocity y | 3.16        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -4.05e+04   |
| time/              |             |
|    total_timesteps | 526000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.8      |
|    ep_rew_mean     | -9.09e+04 |
| time/              |           |
|    fps             | 92        |
|    iterations      | 257       |
|    time_elapsed    | 5681      |
|    total_timesteps | 526336    |
----------------------------------
Eval num_timesteps=526500, episode_reward=-44156.49 +/- 31257.13
Episode length: 51.60 +/- 26.32
------------------------------------------
| eval/                   |              |
|    mean action          | -0.056584917 |
|    mean velocity x      | 0.164        |
|    mean velocity y      | 1.03         |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -4.42e+04    |
| time/                   |              |
|    total_timesteps      | 526500       |
| train/                  |              |
|    approx_kl            | 0.0035669492 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.001        |
|    loss                 | 1.56e+08     |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 0.927        |
|    value_loss           | 2.27e+08     |
------------------------------------------
Eval num_timesteps=527000, episode_reward=-26813.58 +/- 33629.92
Episode length: 43.80 +/- 18.35
------------------------------------
| eval/              |             |
|    mean action     | -0.25257632 |
|    mean velocity x | -0.139      |
|    mean velocity y | 0.679       |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 43.8        |
|    mean_reward     | -2.68e+04   |
| time/              |             |
|    total_timesteps | 527000      |
------------------------------------
Eval num_timesteps=527500, episode_reward=-58406.56 +/- 40579.69
Episode length: 48.60 +/- 21.27
-----------------------------------
| eval/              |            |
|    mean action     | 0.14589453 |
|    mean velocity x | 1.01       |
|    mean velocity y | -0.59      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 48.6       |
|    mean_reward     | -5.84e+04  |
| time/              |            |
|    total_timesteps | 527500     |
-----------------------------------
Eval num_timesteps=528000, episode_reward=-41306.31 +/- 50294.20
Episode length: 32.40 +/- 24.71
------------------------------------
| eval/              |             |
|    mean action     | -0.47564435 |
|    mean velocity x | 3.11        |
|    mean velocity y | 4.71        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 32.4        |
|    mean_reward     | -4.13e+04   |
| time/              |             |
|    total_timesteps | 528000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.1      |
|    ep_rew_mean     | -9.15e+04 |
| time/              |           |
|    fps             | 92        |
|    iterations      | 258       |
|    time_elapsed    | 5694      |
|    total_timesteps | 528384    |
----------------------------------
Eval num_timesteps=528500, episode_reward=-71410.77 +/- 29278.69
Episode length: 61.20 +/- 17.74
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.25031286 |
|    mean velocity x      | 1.16        |
|    mean velocity y      | 0.725       |
|    mean velocity z      | 17.6        |
|    mean_ep_length       | 61.2        |
|    mean_reward          | -7.14e+04   |
| time/                   |             |
|    total_timesteps      | 528500      |
| train/                  |             |
|    approx_kl            | 0.002930436 |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.001       |
|    loss                 | 6.81e+07    |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00259    |
|    std                  | 0.928       |
|    value_loss           | 2.18e+08    |
-----------------------------------------
Eval num_timesteps=529000, episode_reward=-69042.91 +/- 11156.66
Episode length: 62.00 +/- 4.65
------------------------------------
| eval/              |             |
|    mean action     | -0.02103064 |
|    mean velocity x | 1.23        |
|    mean velocity y | 1.24        |
|    mean velocity z | 23.1        |
|    mean_ep_length  | 62          |
|    mean_reward     | -6.9e+04    |
| time/              |             |
|    total_timesteps | 529000      |
------------------------------------
Eval num_timesteps=529500, episode_reward=-48291.66 +/- 24097.69
Episode length: 59.80 +/- 23.22
------------------------------------
| eval/              |             |
|    mean action     | -0.02160405 |
|    mean velocity x | 0.0899      |
|    mean velocity y | 0.433       |
|    mean velocity z | 20          |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -4.83e+04   |
| time/              |             |
|    total_timesteps | 529500      |
------------------------------------
Eval num_timesteps=530000, episode_reward=-71067.58 +/- 52383.69
Episode length: 49.40 +/- 19.62
------------------------------------
| eval/              |             |
|    mean action     | -0.09248529 |
|    mean velocity x | -0.159      |
|    mean velocity y | 0.977       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 49.4        |
|    mean_reward     | -7.11e+04   |
| time/              |             |
|    total_timesteps | 530000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.6      |
|    ep_rew_mean     | -9.25e+04 |
| time/              |           |
|    fps             | 93        |
|    iterations      | 259       |
|    time_elapsed    | 5701      |
|    total_timesteps | 530432    |
----------------------------------
Eval num_timesteps=530500, episode_reward=-66464.20 +/- 35638.73
Episode length: 58.40 +/- 27.61
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0041994783 |
|    mean velocity x      | 1.52          |
|    mean velocity y      | 2.16          |
|    mean velocity z      | 20.6          |
|    mean_ep_length       | 58.4          |
|    mean_reward          | -6.65e+04     |
| time/                   |               |
|    total_timesteps      | 530500        |
| train/                  |               |
|    approx_kl            | 0.00031886614 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.03         |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.001         |
|    loss                 | 9.04e+07      |
|    n_updates            | 2590          |
|    policy_gradient_loss | -0.000489     |
|    std                  | 0.928         |
|    value_loss           | 2.89e+08      |
-------------------------------------------
Eval num_timesteps=531000, episode_reward=-69974.56 +/- 49843.89
Episode length: 66.00 +/- 43.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.07719919 |
|    mean velocity x | -0.812     |
|    mean velocity y | -1.03      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 66         |
|    mean_reward     | -7e+04     |
| time/              |            |
|    total_timesteps | 531000     |
-----------------------------------
Eval num_timesteps=531500, episode_reward=-65871.33 +/- 35276.95
Episode length: 60.00 +/- 18.02
-----------------------------------
| eval/              |            |
|    mean action     | 0.20295724 |
|    mean velocity x | -0.401     |
|    mean velocity y | 0.401      |
|    mean velocity z | 16.6       |
|    mean_ep_length  | 60         |
|    mean_reward     | -6.59e+04  |
| time/              |            |
|    total_timesteps | 531500     |
-----------------------------------
Eval num_timesteps=532000, episode_reward=-74589.12 +/- 20155.36
Episode length: 65.40 +/- 8.55
------------------------------------
| eval/              |             |
|    mean action     | -0.18296939 |
|    mean velocity x | 3.84        |
|    mean velocity y | 3.24        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 532000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.8      |
|    ep_rew_mean     | -9.15e+04 |
| time/              |           |
|    fps             | 93        |
|    iterations      | 260       |
|    time_elapsed    | 5708      |
|    total_timesteps | 532480    |
----------------------------------
Eval num_timesteps=532500, episode_reward=-64349.85 +/- 21849.41
Episode length: 57.40 +/- 5.43
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.052747194 |
|    mean velocity x      | 0.341       |
|    mean velocity y      | 0.724       |
|    mean velocity z      | 19.7        |
|    mean_ep_length       | 57.4        |
|    mean_reward          | -6.43e+04   |
| time/                   |             |
|    total_timesteps      | 532500      |
| train/                  |             |
|    approx_kl            | 0.001999413 |
|    clip_fraction        | 0.00215     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.154       |
|    learning_rate        | 0.001       |
|    loss                 | 1.47e+08    |
|    n_updates            | 2600        |
|    policy_gradient_loss | -0.0012     |
|    std                  | 0.927       |
|    value_loss           | 2.31e+08    |
-----------------------------------------
Eval num_timesteps=533000, episode_reward=-95609.94 +/- 21445.59
Episode length: 63.00 +/- 2.00
-----------------------------------
| eval/              |            |
|    mean action     | 0.20718205 |
|    mean velocity x | -0.88      |
|    mean velocity y | -1.97      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 63         |
|    mean_reward     | -9.56e+04  |
| time/              |            |
|    total_timesteps | 533000     |
-----------------------------------
Eval num_timesteps=533500, episode_reward=-78614.97 +/- 20378.44
Episode length: 71.80 +/- 13.70
-----------------------------------
| eval/              |            |
|    mean action     | 0.69230884 |
|    mean velocity x | -2.8       |
|    mean velocity y | -4         |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 71.8       |
|    mean_reward     | -7.86e+04  |
| time/              |            |
|    total_timesteps | 533500     |
-----------------------------------
Eval num_timesteps=534000, episode_reward=-101312.68 +/- 5903.10
Episode length: 68.80 +/- 8.45
-----------------------------------
| eval/              |            |
|    mean action     | -0.5278501 |
|    mean velocity x | 2.94       |
|    mean velocity y | 3.82       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 68.8       |
|    mean_reward     | -1.01e+05  |
| time/              |            |
|    total_timesteps | 534000     |
-----------------------------------
Eval num_timesteps=534500, episode_reward=-64674.97 +/- 24940.02
Episode length: 58.00 +/- 11.68
-----------------------------------
| eval/              |            |
|    mean action     | 0.32108086 |
|    mean velocity x | -0.914     |
|    mean velocity y | -2.61      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 58         |
|    mean_reward     | -6.47e+04  |
| time/              |            |
|    total_timesteps | 534500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.7      |
|    ep_rew_mean     | -9.25e+04 |
| time/              |           |
|    fps             | 93        |
|    iterations      | 261       |
|    time_elapsed    | 5715      |
|    total_timesteps | 534528    |
----------------------------------
Eval num_timesteps=535000, episode_reward=-84004.25 +/- 17207.24
Episode length: 67.00 +/- 5.69
------------------------------------------
| eval/                   |              |
|    mean action          | 0.092695855  |
|    mean velocity x      | 2.38         |
|    mean velocity y      | 0.904        |
|    mean velocity z      | 21.9         |
|    mean_ep_length       | 67           |
|    mean_reward          | -8.4e+04     |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 8.255572e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.144        |
|    learning_rate        | 0.001        |
|    loss                 | 9.18e+07     |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.000423    |
|    std                  | 0.927        |
|    value_loss           | 2.37e+08     |
------------------------------------------
Eval num_timesteps=535500, episode_reward=-69792.17 +/- 39719.84
Episode length: 56.60 +/- 17.23
-----------------------------------
| eval/              |            |
|    mean action     | -0.5951459 |
|    mean velocity x | 1.75       |
|    mean velocity y | 5.05       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -6.98e+04  |
| time/              |            |
|    total_timesteps | 535500     |
-----------------------------------
Eval num_timesteps=536000, episode_reward=-50283.31 +/- 37534.74
Episode length: 52.00 +/- 26.42
------------------------------------
| eval/              |             |
|    mean action     | -0.12275987 |
|    mean velocity x | -0.77       |
|    mean velocity y | 0.606       |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 52          |
|    mean_reward     | -5.03e+04   |
| time/              |             |
|    total_timesteps | 536000      |
------------------------------------
Eval num_timesteps=536500, episode_reward=-44842.73 +/- 39428.25
Episode length: 69.20 +/- 44.37
------------------------------------
| eval/              |             |
|    mean action     | -0.03667527 |
|    mean velocity x | 1.76        |
|    mean velocity y | 3.3         |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 69.2        |
|    mean_reward     | -4.48e+04   |
| time/              |             |
|    total_timesteps | 536500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.3      |
|    ep_rew_mean     | -8.92e+04 |
| time/              |           |
|    fps             | 93        |
|    iterations      | 262       |
|    time_elapsed    | 5722      |
|    total_timesteps | 536576    |
----------------------------------
Eval num_timesteps=537000, episode_reward=-73964.94 +/- 34174.59
Episode length: 59.80 +/- 8.70
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.50421596    |
|    mean velocity x      | -1.12         |
|    mean velocity y      | -3.11         |
|    mean velocity z      | 18.1          |
|    mean_ep_length       | 59.8          |
|    mean_reward          | -7.4e+04      |
| time/                   |               |
|    total_timesteps      | 537000        |
| train/                  |               |
|    approx_kl            | 0.00021051726 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.03         |
|    explained_variance   | 0.122         |
|    learning_rate        | 0.001         |
|    loss                 | 1.5e+08       |
|    n_updates            | 2620          |
|    policy_gradient_loss | -0.00048      |
|    std                  | 0.927         |
|    value_loss           | 2.38e+08      |
-------------------------------------------
Eval num_timesteps=537500, episode_reward=-91203.79 +/- 25260.64
Episode length: 93.40 +/- 43.32
------------------------------------
| eval/              |             |
|    mean action     | 0.097055055 |
|    mean velocity x | -0.44       |
|    mean velocity y | -0.931      |
|    mean velocity z | 17          |
|    mean_ep_length  | 93.4        |
|    mean_reward     | -9.12e+04   |
| time/              |             |
|    total_timesteps | 537500      |
------------------------------------
Eval num_timesteps=538000, episode_reward=-79058.58 +/- 21196.91
Episode length: 67.00 +/- 9.23
------------------------------------
| eval/              |             |
|    mean action     | -0.06392184 |
|    mean velocity x | 0.829       |
|    mean velocity y | 1.28        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 67          |
|    mean_reward     | -7.91e+04   |
| time/              |             |
|    total_timesteps | 538000      |
------------------------------------
Eval num_timesteps=538500, episode_reward=-82027.49 +/- 25316.07
Episode length: 75.80 +/- 31.24
------------------------------------
| eval/              |             |
|    mean action     | -0.14716485 |
|    mean velocity x | 0.844       |
|    mean velocity y | 2.31        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 75.8        |
|    mean_reward     | -8.2e+04    |
| time/              |             |
|    total_timesteps | 538500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.45e+04 |
| time/              |           |
|    fps             | 93        |
|    iterations      | 263       |
|    time_elapsed    | 5730      |
|    total_timesteps | 538624    |
----------------------------------
Eval num_timesteps=539000, episode_reward=-55143.02 +/- 24446.83
Episode length: 57.40 +/- 7.31
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.25836122  |
|    mean velocity x      | -0.105      |
|    mean velocity y      | -1.4        |
|    mean velocity z      | 16.4        |
|    mean_ep_length       | 57.4        |
|    mean_reward          | -5.51e+04   |
| time/                   |             |
|    total_timesteps      | 539000      |
| train/                  |             |
|    approx_kl            | 0.004832896 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+08     |
|    n_updates            | 2630        |
|    policy_gradient_loss | -0.00429    |
|    std                  | 0.924       |
|    value_loss           | 2.12e+08    |
-----------------------------------------
Eval num_timesteps=539500, episode_reward=-92625.19 +/- 17921.22
Episode length: 67.20 +/- 11.20
------------------------------------
| eval/              |             |
|    mean action     | -0.48140985 |
|    mean velocity x | 3.25        |
|    mean velocity y | 3.83        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 67.2        |
|    mean_reward     | -9.26e+04   |
| time/              |             |
|    total_timesteps | 539500      |
------------------------------------
Eval num_timesteps=540000, episode_reward=-92690.08 +/- 12093.00
Episode length: 71.00 +/- 14.60
-------------------------------------
| eval/              |              |
|    mean action     | -0.030133521 |
|    mean velocity x | -0.015       |
|    mean velocity y | -0.103       |
|    mean velocity z | 21.1         |
|    mean_ep_length  | 71           |
|    mean_reward     | -9.27e+04    |
| time/              |              |
|    total_timesteps | 540000       |
-------------------------------------
Eval num_timesteps=540500, episode_reward=-74153.58 +/- 34338.98
Episode length: 70.80 +/- 20.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.11328525 |
|    mean velocity x | -1.58      |
|    mean velocity y | -2.27      |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 70.8       |
|    mean_reward     | -7.42e+04  |
| time/              |            |
|    total_timesteps | 540500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -8.34e+04 |
| time/              |           |
|    fps             | 94        |
|    iterations      | 264       |
|    time_elapsed    | 5737      |
|    total_timesteps | 540672    |
----------------------------------
Eval num_timesteps=541000, episode_reward=-34626.17 +/- 31431.60
Episode length: 45.40 +/- 21.88
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.36706117  |
|    mean velocity x      | -1.49       |
|    mean velocity y      | -2.37       |
|    mean velocity z      | 21          |
|    mean_ep_length       | 45.4        |
|    mean_reward          | -3.46e+04   |
| time/                   |             |
|    total_timesteps      | 541000      |
| train/                  |             |
|    approx_kl            | 0.002251858 |
|    clip_fraction        | 0.00425     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.001       |
|    loss                 | 2.47e+08    |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00171    |
|    std                  | 0.924       |
|    value_loss           | 2.51e+08    |
-----------------------------------------
Eval num_timesteps=541500, episode_reward=-89376.78 +/- 58088.93
Episode length: 83.00 +/- 57.62
------------------------------------
| eval/              |             |
|    mean action     | -0.27262703 |
|    mean velocity x | 0.4         |
|    mean velocity y | 1.86        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 83          |
|    mean_reward     | -8.94e+04   |
| time/              |             |
|    total_timesteps | 541500      |
------------------------------------
Eval num_timesteps=542000, episode_reward=-75937.63 +/- 40905.28
Episode length: 51.40 +/- 20.74
------------------------------------
| eval/              |             |
|    mean action     | -0.33085978 |
|    mean velocity x | 1.12        |
|    mean velocity y | 2.06        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 51.4        |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 542000      |
------------------------------------
Eval num_timesteps=542500, episode_reward=-77901.22 +/- 36561.16
Episode length: 85.60 +/- 28.53
----------------------------------
| eval/              |           |
|    mean action     | 0.6050804 |
|    mean velocity x | -3.47     |
|    mean velocity y | -4.29     |
|    mean velocity z | 16.3      |
|    mean_ep_length  | 85.6      |
|    mean_reward     | -7.79e+04 |
| time/              |           |
|    total_timesteps | 542500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -8.03e+04 |
| time/              |           |
|    fps             | 94        |
|    iterations      | 265       |
|    time_elapsed    | 5744      |
|    total_timesteps | 542720    |
----------------------------------
Eval num_timesteps=543000, episode_reward=-62340.78 +/- 29253.23
Episode length: 58.80 +/- 12.45
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.02736084    |
|    mean velocity x      | -0.55         |
|    mean velocity y      | 0.156         |
|    mean velocity z      | 14.9          |
|    mean_ep_length       | 58.8          |
|    mean_reward          | -6.23e+04     |
| time/                   |               |
|    total_timesteps      | 543000        |
| train/                  |               |
|    approx_kl            | 0.00037524715 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.02         |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+08      |
|    n_updates            | 2650          |
|    policy_gradient_loss | -0.000539     |
|    std                  | 0.924         |
|    value_loss           | 2.02e+08      |
-------------------------------------------
Eval num_timesteps=543500, episode_reward=-71825.96 +/- 39448.62
Episode length: 55.60 +/- 22.53
------------------------------------
| eval/              |             |
|    mean action     | -0.06441282 |
|    mean velocity x | 0.531       |
|    mean velocity y | 0.107       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 55.6        |
|    mean_reward     | -7.18e+04   |
| time/              |             |
|    total_timesteps | 543500      |
------------------------------------
Eval num_timesteps=544000, episode_reward=-58376.61 +/- 36102.79
Episode length: 56.60 +/- 19.83
-----------------------------------
| eval/              |            |
|    mean action     | 0.21850762 |
|    mean velocity x | 0.591      |
|    mean velocity y | -0.166     |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -5.84e+04  |
| time/              |            |
|    total_timesteps | 544000     |
-----------------------------------
Eval num_timesteps=544500, episode_reward=-89554.87 +/- 12161.86
Episode length: 67.80 +/- 2.93
------------------------------------
| eval/              |             |
|    mean action     | -0.20613529 |
|    mean velocity x | 1.03        |
|    mean velocity y | 1.3         |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 67.8        |
|    mean_reward     | -8.96e+04   |
| time/              |             |
|    total_timesteps | 544500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.8     |
|    ep_rew_mean     | -8e+04   |
| time/              |          |
|    fps             | 94       |
|    iterations      | 266      |
|    time_elapsed    | 5752     |
|    total_timesteps | 544768   |
---------------------------------
Eval num_timesteps=545000, episode_reward=-93487.23 +/- 13008.33
Episode length: 83.00 +/- 35.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4746867   |
|    mean velocity x      | 2.27         |
|    mean velocity y      | 3.03         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 83           |
|    mean_reward          | -9.35e+04    |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0016278898 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.146        |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+08      |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.001       |
|    std                  | 0.922        |
|    value_loss           | 1.97e+08     |
------------------------------------------
Eval num_timesteps=545500, episode_reward=-62327.60 +/- 31595.88
Episode length: 57.40 +/- 20.11
-------------------------------------
| eval/              |              |
|    mean action     | -0.025901947 |
|    mean velocity x | -1.03        |
|    mean velocity y | 0.147        |
|    mean velocity z | 19.6         |
|    mean_ep_length  | 57.4         |
|    mean_reward     | -6.23e+04    |
| time/              |              |
|    total_timesteps | 545500       |
-------------------------------------
Eval num_timesteps=546000, episode_reward=-50054.05 +/- 21970.24
Episode length: 55.40 +/- 8.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.33621308 |
|    mean velocity x | -1.94      |
|    mean velocity y | -2.9       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 55.4       |
|    mean_reward     | -5.01e+04  |
| time/              |            |
|    total_timesteps | 546000     |
-----------------------------------
Eval num_timesteps=546500, episode_reward=-68529.07 +/- 30295.69
Episode length: 60.40 +/- 12.48
----------------------------------
| eval/              |           |
|    mean action     | -0.344029 |
|    mean velocity x | 2.44      |
|    mean velocity y | 3.27      |
|    mean velocity z | 20.1      |
|    mean_ep_length  | 60.4      |
|    mean_reward     | -6.85e+04 |
| time/              |           |
|    total_timesteps | 546500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | -8.6e+04 |
| time/              |          |
|    fps             | 94       |
|    iterations      | 267      |
|    time_elapsed    | 5759     |
|    total_timesteps | 546816   |
---------------------------------
Eval num_timesteps=547000, episode_reward=-52731.03 +/- 43770.40
Episode length: 52.80 +/- 17.29
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.14370523 |
|    mean velocity x      | 1.35        |
|    mean velocity y      | 1.39        |
|    mean velocity z      | 20.8        |
|    mean_ep_length       | 52.8        |
|    mean_reward          | -5.27e+04   |
| time/                   |             |
|    total_timesteps      | 547000      |
| train/                  |             |
|    approx_kl            | 0.002356698 |
|    clip_fraction        | 0.0082      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.001       |
|    loss                 | 1.32e+08    |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00173    |
|    std                  | 0.922       |
|    value_loss           | 2.54e+08    |
-----------------------------------------
Eval num_timesteps=547500, episode_reward=-81604.42 +/- 37212.93
Episode length: 57.40 +/- 12.99
----------------------------------
| eval/              |           |
|    mean action     | 0.4935103 |
|    mean velocity x | -2.67     |
|    mean velocity y | -3.19     |
|    mean velocity z | 20.1      |
|    mean_ep_length  | 57.4      |
|    mean_reward     | -8.16e+04 |
| time/              |           |
|    total_timesteps | 547500    |
----------------------------------
Eval num_timesteps=548000, episode_reward=-87480.40 +/- 19521.32
Episode length: 63.40 +/- 5.99
------------------------------------
| eval/              |             |
|    mean action     | -0.09564045 |
|    mean velocity x | -0.395      |
|    mean velocity y | 0.339       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -8.75e+04   |
| time/              |             |
|    total_timesteps | 548000      |
------------------------------------
Eval num_timesteps=548500, episode_reward=-76039.40 +/- 30576.56
Episode length: 61.60 +/- 7.96
----------------------------------
| eval/              |           |
|    mean action     | 0.7011971 |
|    mean velocity x | -1.79     |
|    mean velocity y | -3.23     |
|    mean velocity z | 17        |
|    mean_ep_length  | 61.6      |
|    mean_reward     | -7.6e+04  |
| time/              |           |
|    total_timesteps | 548500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.5      |
|    ep_rew_mean     | -8.65e+04 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 268       |
|    time_elapsed    | 5766      |
|    total_timesteps | 548864    |
----------------------------------
Eval num_timesteps=549000, episode_reward=-105436.06 +/- 40369.54
Episode length: 94.20 +/- 45.28
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.26063904   |
|    mean velocity x      | 0.953         |
|    mean velocity y      | 2.48          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 94.2          |
|    mean_reward          | -1.05e+05     |
| time/                   |               |
|    total_timesteps      | 549000        |
| train/                  |               |
|    approx_kl            | 0.00023861858 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.001         |
|    loss                 | 1.25e+08      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -0.000893     |
|    std                  | 0.922         |
|    value_loss           | 2.32e+08      |
-------------------------------------------
Eval num_timesteps=549500, episode_reward=-39586.67 +/- 36241.23
Episode length: 47.60 +/- 35.79
-----------------------------------
| eval/              |            |
|    mean action     | 0.10354339 |
|    mean velocity x | 0.269      |
|    mean velocity y | -0.315     |
|    mean velocity z | 20         |
|    mean_ep_length  | 47.6       |
|    mean_reward     | -3.96e+04  |
| time/              |            |
|    total_timesteps | 549500     |
-----------------------------------
Eval num_timesteps=550000, episode_reward=-72855.09 +/- 32538.16
Episode length: 68.40 +/- 25.44
--------------------------------------
| eval/              |               |
|    mean action     | -0.0013883514 |
|    mean velocity x | 0.941         |
|    mean velocity y | -0.35         |
|    mean velocity z | 17.1          |
|    mean_ep_length  | 68.4          |
|    mean_reward     | -7.29e+04     |
| time/              |               |
|    total_timesteps | 550000        |
--------------------------------------
Eval num_timesteps=550500, episode_reward=-76967.94 +/- 30856.59
Episode length: 71.80 +/- 28.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.09048012 |
|    mean velocity x | -0.148     |
|    mean velocity y | -0.738     |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 71.8       |
|    mean_reward     | -7.7e+04   |
| time/              |            |
|    total_timesteps | 550500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.8      |
|    ep_rew_mean     | -8.99e+04 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 269       |
|    time_elapsed    | 5773      |
|    total_timesteps | 550912    |
----------------------------------
Eval num_timesteps=551000, episode_reward=-41917.11 +/- 34208.63
Episode length: 51.60 +/- 34.06
------------------------------------------
| eval/                   |              |
|    mean action          | 0.03654702   |
|    mean velocity x      | -0.0831      |
|    mean velocity y      | 0.68         |
|    mean velocity z      | 21.6         |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -4.19e+04    |
| time/                   |              |
|    total_timesteps      | 551000       |
| train/                  |              |
|    approx_kl            | 0.0008398222 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.125        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+08     |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 0.922        |
|    value_loss           | 2.58e+08     |
------------------------------------------
Eval num_timesteps=551500, episode_reward=-47207.09 +/- 26085.46
Episode length: 56.00 +/- 21.38
----------------------------------
| eval/              |           |
|    mean action     | 0.0134954 |
|    mean velocity x | -0.172    |
|    mean velocity y | 0.72      |
|    mean velocity z | 20        |
|    mean_ep_length  | 56        |
|    mean_reward     | -4.72e+04 |
| time/              |           |
|    total_timesteps | 551500    |
----------------------------------
Eval num_timesteps=552000, episode_reward=-38289.50 +/- 31219.10
Episode length: 53.00 +/- 27.17
------------------------------------
| eval/              |             |
|    mean action     | -0.11940152 |
|    mean velocity x | -1.48       |
|    mean velocity y | 0.629       |
|    mean velocity z | 19          |
|    mean_ep_length  | 53          |
|    mean_reward     | -3.83e+04   |
| time/              |             |
|    total_timesteps | 552000      |
------------------------------------
Eval num_timesteps=552500, episode_reward=-91300.98 +/- 27568.22
Episode length: 66.60 +/- 4.27
----------------------------------
| eval/              |           |
|    mean action     | 0.1936312 |
|    mean velocity x | -0.0369   |
|    mean velocity y | -0.107    |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 66.6      |
|    mean_reward     | -9.13e+04 |
| time/              |           |
|    total_timesteps | 552500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.4      |
|    ep_rew_mean     | -9.36e+04 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 270       |
|    time_elapsed    | 5780      |
|    total_timesteps | 552960    |
----------------------------------
Eval num_timesteps=553000, episode_reward=-75966.33 +/- 28293.63
Episode length: 59.20 +/- 9.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.16680695  |
|    mean velocity x      | 0.199        |
|    mean velocity y      | 1.34         |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 59.2         |
|    mean_reward          | -7.6e+04     |
| time/                   |              |
|    total_timesteps      | 553000       |
| train/                  |              |
|    approx_kl            | 0.0030666818 |
|    clip_fraction        | 0.00815      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.135        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+08     |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.00213     |
|    std                  | 0.92         |
|    value_loss           | 2.07e+08     |
------------------------------------------
Eval num_timesteps=553500, episode_reward=-66148.79 +/- 27086.49
Episode length: 65.60 +/- 14.05
------------------------------------
| eval/              |             |
|    mean action     | -0.15346523 |
|    mean velocity x | 2.04        |
|    mean velocity y | 0.553       |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 65.6        |
|    mean_reward     | -6.61e+04   |
| time/              |             |
|    total_timesteps | 553500      |
------------------------------------
Eval num_timesteps=554000, episode_reward=-61934.67 +/- 23485.31
Episode length: 62.60 +/- 17.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.13223554 |
|    mean velocity x | 0.754      |
|    mean velocity y | -0.202     |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -6.19e+04  |
| time/              |            |
|    total_timesteps | 554000     |
-----------------------------------
Eval num_timesteps=554500, episode_reward=-37682.35 +/- 32212.11
Episode length: 50.80 +/- 28.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.24566126 |
|    mean velocity x | -2.02      |
|    mean velocity y | -2.63      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 50.8       |
|    mean_reward     | -3.77e+04  |
| time/              |            |
|    total_timesteps | 554500     |
-----------------------------------
Eval num_timesteps=555000, episode_reward=-63096.64 +/- 14058.05
Episode length: 69.60 +/- 14.62
----------------------------------
| eval/              |           |
|    mean action     | 0.3835869 |
|    mean velocity x | -1.02     |
|    mean velocity y | -2.8      |
|    mean velocity z | 18.2      |
|    mean_ep_length  | 69.6      |
|    mean_reward     | -6.31e+04 |
| time/              |           |
|    total_timesteps | 555000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.5      |
|    ep_rew_mean     | -8.58e+04 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 271       |
|    time_elapsed    | 5787      |
|    total_timesteps | 555008    |
----------------------------------
Eval num_timesteps=555500, episode_reward=-68776.66 +/- 34859.00
Episode length: 68.80 +/- 23.77
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5620241   |
|    mean velocity x      | 2.26         |
|    mean velocity y      | 3.41         |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 68.8         |
|    mean_reward          | -6.88e+04    |
| time/                   |              |
|    total_timesteps      | 555500       |
| train/                  |              |
|    approx_kl            | 0.0024283256 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.151        |
|    learning_rate        | 0.001        |
|    loss                 | 7.71e+07     |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00341     |
|    std                  | 0.919        |
|    value_loss           | 2.19e+08     |
------------------------------------------
Eval num_timesteps=556000, episode_reward=-55407.41 +/- 40853.61
Episode length: 51.20 +/- 22.62
------------------------------------
| eval/              |             |
|    mean action     | -0.26705897 |
|    mean velocity x | 1.05        |
|    mean velocity y | 2.28        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 51.2        |
|    mean_reward     | -5.54e+04   |
| time/              |             |
|    total_timesteps | 556000      |
------------------------------------
Eval num_timesteps=556500, episode_reward=-66220.85 +/- 32070.82
Episode length: 56.00 +/- 7.75
------------------------------------
| eval/              |             |
|    mean action     | -0.40320176 |
|    mean velocity x | 1.58        |
|    mean velocity y | 1.96        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 56          |
|    mean_reward     | -6.62e+04   |
| time/              |             |
|    total_timesteps | 556500      |
------------------------------------
Eval num_timesteps=557000, episode_reward=-52081.96 +/- 38147.79
Episode length: 80.60 +/- 50.45
------------------------------------
| eval/              |             |
|    mean action     | -0.24406432 |
|    mean velocity x | 0.458       |
|    mean velocity y | 2.31        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 80.6        |
|    mean_reward     | -5.21e+04   |
| time/              |             |
|    total_timesteps | 557000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -8.49e+04 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 272       |
|    time_elapsed    | 5795      |
|    total_timesteps | 557056    |
----------------------------------
Eval num_timesteps=557500, episode_reward=-64791.71 +/- 32589.35
Episode length: 57.60 +/- 22.73
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.08414014  |
|    mean velocity x      | 0.107       |
|    mean velocity y      | -1.25       |
|    mean velocity z      | 17.7        |
|    mean_ep_length       | 57.6        |
|    mean_reward          | -6.48e+04   |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.000927725 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.001       |
|    loss                 | 1.88e+08    |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00129    |
|    std                  | 0.919       |
|    value_loss           | 2.38e+08    |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=-70827.93 +/- 12655.82
Episode length: 60.80 +/- 7.78
-------------------------------------
| eval/              |              |
|    mean action     | -0.051531576 |
|    mean velocity x | 1.84         |
|    mean velocity y | 0.324        |
|    mean velocity z | 18.1         |
|    mean_ep_length  | 60.8         |
|    mean_reward     | -7.08e+04    |
| time/              |              |
|    total_timesteps | 558000       |
-------------------------------------
Eval num_timesteps=558500, episode_reward=-56536.04 +/- 46485.54
Episode length: 56.40 +/- 36.38
------------------------------------
| eval/              |             |
|    mean action     | -0.03775686 |
|    mean velocity x | 1.61        |
|    mean velocity y | 0.481       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 56.4        |
|    mean_reward     | -5.65e+04   |
| time/              |             |
|    total_timesteps | 558500      |
------------------------------------
Eval num_timesteps=559000, episode_reward=-82302.36 +/- 41393.00
Episode length: 66.20 +/- 41.60
------------------------------------
| eval/              |             |
|    mean action     | -0.14929587 |
|    mean velocity x | -0.0425     |
|    mean velocity y | 0.473       |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 559000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.4      |
|    ep_rew_mean     | -8.06e+04 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 273       |
|    time_elapsed    | 5802      |
|    total_timesteps | 559104    |
----------------------------------
Eval num_timesteps=559500, episode_reward=-80624.82 +/- 31779.96
Episode length: 60.40 +/- 8.24
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.12975426 |
|    mean velocity x      | 1.17        |
|    mean velocity y      | 0.963       |
|    mean velocity z      | 19.6        |
|    mean_ep_length       | 60.4        |
|    mean_reward          | -8.06e+04   |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.004413393 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.001       |
|    loss                 | 1.74e+08    |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00308    |
|    std                  | 0.922       |
|    value_loss           | 2.38e+08    |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=-60103.76 +/- 25225.21
Episode length: 66.60 +/- 24.91
------------------------------------
| eval/              |             |
|    mean action     | -0.38087246 |
|    mean velocity x | 1.77        |
|    mean velocity y | 2.2         |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 66.6        |
|    mean_reward     | -6.01e+04   |
| time/              |             |
|    total_timesteps | 560000      |
------------------------------------
Eval num_timesteps=560500, episode_reward=-100456.38 +/- 13172.56
Episode length: 75.00 +/- 16.38
-----------------------------------
| eval/              |            |
|    mean action     | 0.11477463 |
|    mean velocity x | -1.54      |
|    mean velocity y | -1.84      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 75         |
|    mean_reward     | -1e+05     |
| time/              |            |
|    total_timesteps | 560500     |
-----------------------------------
Eval num_timesteps=561000, episode_reward=-96349.62 +/- 10878.99
Episode length: 65.40 +/- 3.01
-----------------------------------
| eval/              |            |
|    mean action     | -0.4964599 |
|    mean velocity x | 0.0241     |
|    mean velocity y | 2.84       |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 65.4       |
|    mean_reward     | -9.63e+04  |
| time/              |            |
|    total_timesteps | 561000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.4      |
|    ep_rew_mean     | -8.12e+04 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 274       |
|    time_elapsed    | 5809      |
|    total_timesteps | 561152    |
----------------------------------
Eval num_timesteps=561500, episode_reward=-66038.47 +/- 36731.17
Episode length: 59.40 +/- 26.49
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4582286    |
|    mean velocity x      | 2.93          |
|    mean velocity y      | 4.68          |
|    mean velocity z      | 18.7          |
|    mean_ep_length       | 59.4          |
|    mean_reward          | -6.6e+04      |
| time/                   |               |
|    total_timesteps      | 561500        |
| train/                  |               |
|    approx_kl            | 0.00013685448 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.161         |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+08      |
|    n_updates            | 2740          |
|    policy_gradient_loss | -0.000664     |
|    std                  | 0.922         |
|    value_loss           | 2.3e+08       |
-------------------------------------------
Eval num_timesteps=562000, episode_reward=-58167.37 +/- 32082.65
Episode length: 60.40 +/- 26.51
-----------------------------------
| eval/              |            |
|    mean action     | -0.2058161 |
|    mean velocity x | 1.89       |
|    mean velocity y | 1.24       |
|    mean velocity z | 15.5       |
|    mean_ep_length  | 60.4       |
|    mean_reward     | -5.82e+04  |
| time/              |            |
|    total_timesteps | 562000     |
-----------------------------------
Eval num_timesteps=562500, episode_reward=-65122.68 +/- 40004.82
Episode length: 59.40 +/- 22.26
------------------------------------
| eval/              |             |
|    mean action     | -0.42920426 |
|    mean velocity x | 1.69        |
|    mean velocity y | 3.65        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -6.51e+04   |
| time/              |             |
|    total_timesteps | 562500      |
------------------------------------
Eval num_timesteps=563000, episode_reward=-93573.46 +/- 15452.74
Episode length: 79.80 +/- 21.40
------------------------------------
| eval/              |             |
|    mean action     | 0.011059936 |
|    mean velocity x | -0.614      |
|    mean velocity y | -0.254      |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 79.8        |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 563000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.48e+04 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 275       |
|    time_elapsed    | 5816      |
|    total_timesteps | 563200    |
----------------------------------
Eval num_timesteps=563500, episode_reward=-85006.95 +/- 35020.00
Episode length: 59.60 +/- 8.80
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18049484  |
|    mean velocity x      | 0.206        |
|    mean velocity y      | 1.07         |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 59.6         |
|    mean_reward          | -8.5e+04     |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0031233842 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.001        |
|    loss                 | 1.29e+08     |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 0.922        |
|    value_loss           | 2.18e+08     |
------------------------------------------
Eval num_timesteps=564000, episode_reward=-76939.72 +/- 37854.77
Episode length: 60.80 +/- 23.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.19693604 |
|    mean velocity x | -0.581     |
|    mean velocity y | -1.8       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 60.8       |
|    mean_reward     | -7.69e+04  |
| time/              |            |
|    total_timesteps | 564000     |
-----------------------------------
Eval num_timesteps=564500, episode_reward=-59697.75 +/- 30322.87
Episode length: 53.40 +/- 6.15
------------------------------------
| eval/              |             |
|    mean action     | 0.035537407 |
|    mean velocity x | -0.516      |
|    mean velocity y | -0.274      |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 53.4        |
|    mean_reward     | -5.97e+04   |
| time/              |             |
|    total_timesteps | 564500      |
------------------------------------
Eval num_timesteps=565000, episode_reward=-59944.06 +/- 29356.84
Episode length: 52.20 +/- 16.99
------------------------------------
| eval/              |             |
|    mean action     | -0.02971736 |
|    mean velocity x | 0.628       |
|    mean velocity y | 1.45        |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -5.99e+04   |
| time/              |             |
|    total_timesteps | 565000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.7      |
|    ep_rew_mean     | -8.41e+04 |
| time/              |           |
|    fps             | 97        |
|    iterations      | 276       |
|    time_elapsed    | 5823      |
|    total_timesteps | 565248    |
----------------------------------
Eval num_timesteps=565500, episode_reward=-76008.80 +/- 23902.03
Episode length: 62.40 +/- 9.13
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3123177    |
|    mean velocity x      | 0.851         |
|    mean velocity y      | 3.61          |
|    mean velocity z      | 20.1          |
|    mean_ep_length       | 62.4          |
|    mean_reward          | -7.6e+04      |
| time/                   |               |
|    total_timesteps      | 565500        |
| train/                  |               |
|    approx_kl            | 0.00032524465 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 2760          |
|    policy_gradient_loss | -0.00076      |
|    std                  | 0.923         |
|    value_loss           | 2.54e+08      |
-------------------------------------------
Eval num_timesteps=566000, episode_reward=-85063.66 +/- 16503.33
Episode length: 63.20 +/- 3.87
------------------------------------
| eval/              |             |
|    mean action     | -0.38512328 |
|    mean velocity x | -0.41       |
|    mean velocity y | 2.16        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 63.2        |
|    mean_reward     | -8.51e+04   |
| time/              |             |
|    total_timesteps | 566000      |
------------------------------------
Eval num_timesteps=566500, episode_reward=-70890.18 +/- 22086.28
Episode length: 58.20 +/- 5.15
------------------------------------
| eval/              |             |
|    mean action     | -0.25058663 |
|    mean velocity x | -0.688      |
|    mean velocity y | 0.318       |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 566500      |
------------------------------------
Eval num_timesteps=567000, episode_reward=-49415.87 +/- 14246.27
Episode length: 62.60 +/- 15.07
------------------------------------
| eval/              |             |
|    mean action     | -0.40983465 |
|    mean velocity x | 0.669       |
|    mean velocity y | 2.88        |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 62.6        |
|    mean_reward     | -4.94e+04   |
| time/              |             |
|    total_timesteps | 567000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.8      |
|    ep_rew_mean     | -8.82e+04 |
| time/              |           |
|    fps             | 97        |
|    iterations      | 277       |
|    time_elapsed    | 5831      |
|    total_timesteps | 567296    |
----------------------------------
Eval num_timesteps=567500, episode_reward=-56182.84 +/- 42956.16
Episode length: 58.20 +/- 30.08
------------------------------------------
| eval/                   |              |
|    mean action          | 0.3016706    |
|    mean velocity x      | -0.146       |
|    mean velocity y      | -2.48        |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 58.2         |
|    mean_reward          | -5.62e+04    |
| time/                   |              |
|    total_timesteps      | 567500       |
| train/                  |              |
|    approx_kl            | 0.0012980059 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+08     |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 0.923        |
|    value_loss           | 2.63e+08     |
------------------------------------------
Eval num_timesteps=568000, episode_reward=-42707.01 +/- 32712.46
Episode length: 50.00 +/- 27.69
------------------------------------
| eval/              |             |
|    mean action     | 0.032687407 |
|    mean velocity x | -1.13       |
|    mean velocity y | 0.0538      |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 50          |
|    mean_reward     | -4.27e+04   |
| time/              |             |
|    total_timesteps | 568000      |
------------------------------------
Eval num_timesteps=568500, episode_reward=-72085.01 +/- 35236.10
Episode length: 61.20 +/- 8.63
------------------------------------
| eval/              |             |
|    mean action     | -0.24339557 |
|    mean velocity x | 1.86        |
|    mean velocity y | 2.76        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -7.21e+04   |
| time/              |             |
|    total_timesteps | 568500      |
------------------------------------
Eval num_timesteps=569000, episode_reward=-76590.57 +/- 24909.63
Episode length: 63.60 +/- 3.01
-------------------------------------
| eval/              |              |
|    mean action     | -0.058302764 |
|    mean velocity x | 0.15         |
|    mean velocity y | 0.0493       |
|    mean velocity z | 19.7         |
|    mean_ep_length  | 63.6         |
|    mean_reward     | -7.66e+04    |
| time/              |              |
|    total_timesteps | 569000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.5      |
|    ep_rew_mean     | -8.67e+04 |
| time/              |           |
|    fps             | 97        |
|    iterations      | 278       |
|    time_elapsed    | 5838      |
|    total_timesteps | 569344    |
----------------------------------
Eval num_timesteps=569500, episode_reward=-80745.05 +/- 13742.51
Episode length: 68.20 +/- 7.17
------------------------------------------
| eval/                   |              |
|    mean action          | 0.98413247   |
|    mean velocity x      | -4.67        |
|    mean velocity y      | -5.9         |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 68.2         |
|    mean_reward          | -8.07e+04    |
| time/                   |              |
|    total_timesteps      | 569500       |
| train/                  |              |
|    approx_kl            | 0.0003511123 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.001        |
|    loss                 | 1.26e+08     |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.000548    |
|    std                  | 0.923        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=570000, episode_reward=-50851.32 +/- 35061.11
Episode length: 53.20 +/- 25.10
------------------------------------
| eval/              |             |
|    mean action     | 0.112985365 |
|    mean velocity x | 1.49        |
|    mean velocity y | 1.47        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 53.2        |
|    mean_reward     | -5.09e+04   |
| time/              |             |
|    total_timesteps | 570000      |
------------------------------------
Eval num_timesteps=570500, episode_reward=-66764.30 +/- 37234.89
Episode length: 56.80 +/- 19.57
------------------------------------
| eval/              |             |
|    mean action     | -0.13168766 |
|    mean velocity x | 0.0833      |
|    mean velocity y | 0.209       |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 56.8        |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 570500      |
------------------------------------
Eval num_timesteps=571000, episode_reward=-65141.66 +/- 33117.63
Episode length: 50.80 +/- 21.49
-----------------------------------
| eval/              |            |
|    mean action     | 0.35619542 |
|    mean velocity x | 0.716      |
|    mean velocity y | -1.35      |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 50.8       |
|    mean_reward     | -6.51e+04  |
| time/              |            |
|    total_timesteps | 571000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.9      |
|    ep_rew_mean     | -9.09e+04 |
| time/              |           |
|    fps             | 97        |
|    iterations      | 279       |
|    time_elapsed    | 5845      |
|    total_timesteps | 571392    |
----------------------------------
Eval num_timesteps=571500, episode_reward=-59540.69 +/- 27339.01
Episode length: 71.40 +/- 24.29
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0891352    |
|    mean velocity x      | 1.98          |
|    mean velocity y      | 1.52          |
|    mean velocity z      | 21            |
|    mean_ep_length       | 71.4          |
|    mean_reward          | -5.95e+04     |
| time/                   |               |
|    total_timesteps      | 571500        |
| train/                  |               |
|    approx_kl            | 0.00010274368 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.02         |
|    explained_variance   | 0.137         |
|    learning_rate        | 0.001         |
|    loss                 | 8.8e+07       |
|    n_updates            | 2790          |
|    policy_gradient_loss | -0.000439     |
|    std                  | 0.923         |
|    value_loss           | 2.62e+08      |
-------------------------------------------
Eval num_timesteps=572000, episode_reward=-70142.84 +/- 38594.62
Episode length: 67.80 +/- 39.54
----------------------------------
| eval/              |           |
|    mean action     | 0.5241725 |
|    mean velocity x | -2.93     |
|    mean velocity y | -3.26     |
|    mean velocity z | 18.9      |
|    mean_ep_length  | 67.8      |
|    mean_reward     | -7.01e+04 |
| time/              |           |
|    total_timesteps | 572000    |
----------------------------------
Eval num_timesteps=572500, episode_reward=-63635.25 +/- 33807.06
Episode length: 54.40 +/- 7.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.37533802 |
|    mean velocity x | 0.0721     |
|    mean velocity y | -2.11      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -6.36e+04  |
| time/              |            |
|    total_timesteps | 572500     |
-----------------------------------
Eval num_timesteps=573000, episode_reward=-51602.93 +/- 37966.45
Episode length: 50.60 +/- 15.25
----------------------------------
| eval/              |           |
|    mean action     | 0.2752536 |
|    mean velocity x | -0.521    |
|    mean velocity y | -1.27     |
|    mean velocity z | 17.2      |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -5.16e+04 |
| time/              |           |
|    total_timesteps | 573000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.6      |
|    ep_rew_mean     | -8.91e+04 |
| time/              |           |
|    fps             | 97        |
|    iterations      | 280       |
|    time_elapsed    | 5852      |
|    total_timesteps | 573440    |
----------------------------------
Eval num_timesteps=573500, episode_reward=-65082.35 +/- 55937.40
Episode length: 69.20 +/- 56.62
------------------------------------------
| eval/                   |              |
|    mean action          | 0.14672244   |
|    mean velocity x      | 0.633        |
|    mean velocity y      | 0.615        |
|    mean velocity z      | 20.9         |
|    mean_ep_length       | 69.2         |
|    mean_reward          | -6.51e+04    |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0025551699 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.131        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+08     |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 0.922        |
|    value_loss           | 2.25e+08     |
------------------------------------------
Eval num_timesteps=574000, episode_reward=-71811.28 +/- 40018.17
Episode length: 56.00 +/- 21.94
------------------------------------
| eval/              |             |
|    mean action     | 0.022932984 |
|    mean velocity x | 0.035       |
|    mean velocity y | 0.745       |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 56          |
|    mean_reward     | -7.18e+04   |
| time/              |             |
|    total_timesteps | 574000      |
------------------------------------
Eval num_timesteps=574500, episode_reward=-42558.86 +/- 32483.46
Episode length: 54.60 +/- 29.26
-----------------------------------
| eval/              |            |
|    mean action     | 0.12944447 |
|    mean velocity x | -0.28      |
|    mean velocity y | -1.26      |
|    mean velocity z | 16.5       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -4.26e+04  |
| time/              |            |
|    total_timesteps | 574500     |
-----------------------------------
Eval num_timesteps=575000, episode_reward=-55418.19 +/- 46782.59
Episode length: 54.00 +/- 17.03
-----------------------------------
| eval/              |            |
|    mean action     | -0.1031237 |
|    mean velocity x | 0.994      |
|    mean velocity y | 1.66       |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 54         |
|    mean_reward     | -5.54e+04  |
| time/              |            |
|    total_timesteps | 575000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.9     |
|    ep_rew_mean     | -8.4e+04 |
| time/              |          |
|    fps             | 98       |
|    iterations      | 281      |
|    time_elapsed    | 5859     |
|    total_timesteps | 575488   |
---------------------------------
Eval num_timesteps=575500, episode_reward=-69745.82 +/- 10221.75
Episode length: 60.80 +/- 4.35
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.031429444  |
|    mean velocity x      | 1.12          |
|    mean velocity y      | 0.495         |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 60.8          |
|    mean_reward          | -6.97e+04     |
| time/                   |               |
|    total_timesteps      | 575500        |
| train/                  |               |
|    approx_kl            | 0.00021243395 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.001         |
|    loss                 | 6.5e+07       |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.000538     |
|    std                  | 0.922         |
|    value_loss           | 2.25e+08      |
-------------------------------------------
Eval num_timesteps=576000, episode_reward=-42816.27 +/- 32725.23
Episode length: 61.60 +/- 37.18
------------------------------------
| eval/              |             |
|    mean action     | -0.24827972 |
|    mean velocity x | 0.836       |
|    mean velocity y | 2.45        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -4.28e+04   |
| time/              |             |
|    total_timesteps | 576000      |
------------------------------------
Eval num_timesteps=576500, episode_reward=-71743.39 +/- 54214.84
Episode length: 86.00 +/- 68.61
------------------------------------
| eval/              |             |
|    mean action     | -0.29237866 |
|    mean velocity x | 0.429       |
|    mean velocity y | 1.64        |
|    mean velocity z | 18          |
|    mean_ep_length  | 86          |
|    mean_reward     | -7.17e+04   |
| time/              |             |
|    total_timesteps | 576500      |
------------------------------------
Eval num_timesteps=577000, episode_reward=-73485.25 +/- 33720.76
Episode length: 72.80 +/- 34.54
-----------------------------------
| eval/              |            |
|    mean action     | 0.34643665 |
|    mean velocity x | -1.4       |
|    mean velocity y | -1.06      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 72.8       |
|    mean_reward     | -7.35e+04  |
| time/              |            |
|    total_timesteps | 577000     |
-----------------------------------
Eval num_timesteps=577500, episode_reward=-57301.05 +/- 33441.17
Episode length: 50.40 +/- 17.77
-----------------------------------
| eval/              |            |
|    mean action     | -0.2303498 |
|    mean velocity x | 1.78       |
|    mean velocity y | 3.38       |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 50.4       |
|    mean_reward     | -5.73e+04  |
| time/              |            |
|    total_timesteps | 577500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.4      |
|    ep_rew_mean     | -8.04e+04 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 282       |
|    time_elapsed    | 5867      |
|    total_timesteps | 577536    |
----------------------------------
Eval num_timesteps=578000, episode_reward=-72479.12 +/- 51047.76
Episode length: 69.00 +/- 50.39
------------------------------------------
| eval/                   |              |
|    mean action          | -0.60143936  |
|    mean velocity x      | 2.22         |
|    mean velocity y      | 5.27         |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 69           |
|    mean_reward          | -7.25e+04    |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0012520611 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+08     |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 0.923        |
|    value_loss           | 2.45e+08     |
------------------------------------------
Eval num_timesteps=578500, episode_reward=-64725.34 +/- 33581.89
Episode length: 57.80 +/- 17.47
-----------------------------------
| eval/              |            |
|    mean action     | 0.12643409 |
|    mean velocity x | -1.13      |
|    mean velocity y | -0.931     |
|    mean velocity z | 22         |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -6.47e+04  |
| time/              |            |
|    total_timesteps | 578500     |
-----------------------------------
Eval num_timesteps=579000, episode_reward=-75714.14 +/- 20298.19
Episode length: 61.20 +/- 4.53
-----------------------------------
| eval/              |            |
|    mean action     | -0.4609983 |
|    mean velocity x | 3.13       |
|    mean velocity y | 3.23       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -7.57e+04  |
| time/              |            |
|    total_timesteps | 579000     |
-----------------------------------
Eval num_timesteps=579500, episode_reward=-71496.41 +/- 35379.63
Episode length: 67.80 +/- 24.89
------------------------------------
| eval/              |             |
|    mean action     | -0.16041476 |
|    mean velocity x | 0.101       |
|    mean velocity y | 1.06        |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 67.8        |
|    mean_reward     | -7.15e+04   |
| time/              |             |
|    total_timesteps | 579500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.5      |
|    ep_rew_mean     | -8.08e+04 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 283       |
|    time_elapsed    | 5874      |
|    total_timesteps | 579584    |
----------------------------------
Eval num_timesteps=580000, episode_reward=-43182.48 +/- 34121.19
Episode length: 44.20 +/- 20.01
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.39966497    |
|    mean velocity x      | -0.954        |
|    mean velocity y      | -2.57         |
|    mean velocity z      | 16.4          |
|    mean_ep_length       | 44.2          |
|    mean_reward          | -4.32e+04     |
| time/                   |               |
|    total_timesteps      | 580000        |
| train/                  |               |
|    approx_kl            | 0.00045741643 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+08      |
|    n_updates            | 2830          |
|    policy_gradient_loss | -0.000762     |
|    std                  | 0.923         |
|    value_loss           | 2.14e+08      |
-------------------------------------------
Eval num_timesteps=580500, episode_reward=-81020.96 +/- 20494.06
Episode length: 69.00 +/- 12.18
-----------------------------------
| eval/              |            |
|    mean action     | 0.13673303 |
|    mean velocity x | 0.468      |
|    mean velocity y | -0.939     |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 69         |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 580500     |
-----------------------------------
Eval num_timesteps=581000, episode_reward=-93002.79 +/- 33508.46
Episode length: 76.00 +/- 30.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.21080096 |
|    mean velocity x | 0.01       |
|    mean velocity y | -0.108     |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 76         |
|    mean_reward     | -9.3e+04   |
| time/              |            |
|    total_timesteps | 581000     |
-----------------------------------
Eval num_timesteps=581500, episode_reward=-61342.49 +/- 44041.15
Episode length: 57.20 +/- 27.10
------------------------------------
| eval/              |             |
|    mean action     | -0.57810915 |
|    mean velocity x | 1.18        |
|    mean velocity y | 3.95        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.13e+04   |
| time/              |             |
|    total_timesteps | 581500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.5     |
|    ep_rew_mean     | -8.1e+04 |
| time/              |          |
|    fps             | 98       |
|    iterations      | 284      |
|    time_elapsed    | 5881     |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=582000, episode_reward=-46508.25 +/- 32449.24
Episode length: 47.40 +/- 17.92
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31434503  |
|    mean velocity x      | 3.11         |
|    mean velocity y      | 2.75         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 47.4         |
|    mean_reward          | -4.65e+04    |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0023403591 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+08     |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00292     |
|    std                  | 0.923        |
|    value_loss           | 1.99e+08     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-56794.61 +/- 36631.07
Episode length: 54.40 +/- 17.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.43284777 |
|    mean velocity x | -1.54      |
|    mean velocity y | -2.24      |
|    mean velocity z | 17         |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -5.68e+04  |
| time/              |            |
|    total_timesteps | 582500     |
-----------------------------------
Eval num_timesteps=583000, episode_reward=-106096.70 +/- 6819.63
Episode length: 73.20 +/- 21.41
------------------------------------
| eval/              |             |
|    mean action     | -0.23721865 |
|    mean velocity x | 1.74        |
|    mean velocity y | 1.94        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 583000      |
------------------------------------
Eval num_timesteps=583500, episode_reward=-78826.31 +/- 16906.93
Episode length: 62.40 +/- 6.77
-------------------------------------
| eval/              |              |
|    mean action     | -0.118867986 |
|    mean velocity x | 1.86         |
|    mean velocity y | 1.99         |
|    mean velocity z | 20.9         |
|    mean_ep_length  | 62.4         |
|    mean_reward     | -7.88e+04    |
| time/              |              |
|    total_timesteps | 583500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -8.03e+04 |
| time/              |           |
|    fps             | 99        |
|    iterations      | 285       |
|    time_elapsed    | 5888      |
|    total_timesteps | 583680    |
----------------------------------
Eval num_timesteps=584000, episode_reward=-57217.08 +/- 22401.80
Episode length: 72.40 +/- 12.56
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.07574988   |
|    mean velocity x      | 1.15          |
|    mean velocity y      | 0.967         |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 72.4          |
|    mean_reward          | -5.72e+04     |
| time/                   |               |
|    total_timesteps      | 584000        |
| train/                  |               |
|    approx_kl            | 5.0820207e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.02         |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.001         |
|    loss                 | 7.74e+07      |
|    n_updates            | 2850          |
|    policy_gradient_loss | -0.000286     |
|    std                  | 0.923         |
|    value_loss           | 2.5e+08       |
-------------------------------------------
Eval num_timesteps=584500, episode_reward=-63663.47 +/- 41482.00
Episode length: 54.00 +/- 9.84
------------------------------------
| eval/              |             |
|    mean action     | -0.30519328 |
|    mean velocity x | 1.37        |
|    mean velocity y | 1.8         |
|    mean velocity z | 16.3        |
|    mean_ep_length  | 54          |
|    mean_reward     | -6.37e+04   |
| time/              |             |
|    total_timesteps | 584500      |
------------------------------------
Eval num_timesteps=585000, episode_reward=-64302.75 +/- 36089.19
Episode length: 53.80 +/- 12.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.23479916 |
|    mean velocity x | 0.269      |
|    mean velocity y | -0.466     |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 53.8       |
|    mean_reward     | -6.43e+04  |
| time/              |            |
|    total_timesteps | 585000     |
-----------------------------------
Eval num_timesteps=585500, episode_reward=-45572.35 +/- 40610.60
Episode length: 47.60 +/- 17.82
-----------------------------------
| eval/              |            |
|    mean action     | -0.5094379 |
|    mean velocity x | 1.5        |
|    mean velocity y | 2.59       |
|    mean velocity z | 19         |
|    mean_ep_length  | 47.6       |
|    mean_reward     | -4.56e+04  |
| time/              |            |
|    total_timesteps | 585500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.9      |
|    ep_rew_mean     | -7.76e+04 |
| time/              |           |
|    fps             | 99        |
|    iterations      | 286       |
|    time_elapsed    | 5895      |
|    total_timesteps | 585728    |
----------------------------------
Eval num_timesteps=586000, episode_reward=-60714.36 +/- 40633.83
Episode length: 69.80 +/- 36.73
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42676497   |
|    mean velocity x      | 3.16          |
|    mean velocity y      | 3.88          |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 69.8          |
|    mean_reward          | -6.07e+04     |
| time/                   |               |
|    total_timesteps      | 586000        |
| train/                  |               |
|    approx_kl            | 0.00087940914 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.001         |
|    loss                 | 8.85e+07      |
|    n_updates            | 2860          |
|    policy_gradient_loss | -0.00125      |
|    std                  | 0.922         |
|    value_loss           | 1.93e+08      |
-------------------------------------------
Eval num_timesteps=586500, episode_reward=-35780.84 +/- 35219.16
Episode length: 46.80 +/- 18.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.4414775 |
|    mean velocity x | 2.99       |
|    mean velocity y | 3.22       |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 46.8       |
|    mean_reward     | -3.58e+04  |
| time/              |            |
|    total_timesteps | 586500     |
-----------------------------------
Eval num_timesteps=587000, episode_reward=-98275.94 +/- 17811.69
Episode length: 68.00 +/- 11.52
------------------------------------
| eval/              |             |
|    mean action     | -0.37338403 |
|    mean velocity x | 0.822       |
|    mean velocity y | 2.86        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 68          |
|    mean_reward     | -9.83e+04   |
| time/              |             |
|    total_timesteps | 587000      |
------------------------------------
Eval num_timesteps=587500, episode_reward=-71464.45 +/- 37317.27
Episode length: 60.60 +/- 25.41
-----------------------------------
| eval/              |            |
|    mean action     | 0.02405804 |
|    mean velocity x | -0.16      |
|    mean velocity y | -0.256     |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 60.6       |
|    mean_reward     | -7.15e+04  |
| time/              |            |
|    total_timesteps | 587500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -7.76e+04 |
| time/              |           |
|    fps             | 99        |
|    iterations      | 287       |
|    time_elapsed    | 5902      |
|    total_timesteps | 587776    |
----------------------------------
Eval num_timesteps=588000, episode_reward=-48385.83 +/- 21826.75
Episode length: 63.20 +/- 13.56
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45770147   |
|    mean velocity x      | 4.38          |
|    mean velocity y      | 4.14          |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 63.2          |
|    mean_reward          | -4.84e+04     |
| time/                   |               |
|    total_timesteps      | 588000        |
| train/                  |               |
|    approx_kl            | 0.00028899836 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.001         |
|    loss                 | 7.93e+07      |
|    n_updates            | 2870          |
|    policy_gradient_loss | -0.00117      |
|    std                  | 0.923         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=588500, episode_reward=-81560.16 +/- 28314.63
Episode length: 58.60 +/- 8.78
------------------------------------
| eval/              |             |
|    mean action     | -0.38590947 |
|    mean velocity x | 2.8         |
|    mean velocity y | 4.15        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 588500      |
------------------------------------
Eval num_timesteps=589000, episode_reward=-70584.48 +/- 35573.99
Episode length: 55.20 +/- 22.04
-----------------------------------
| eval/              |            |
|    mean action     | 0.34783038 |
|    mean velocity x | -0.713     |
|    mean velocity y | -2.06      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 55.2       |
|    mean_reward     | -7.06e+04  |
| time/              |            |
|    total_timesteps | 589000     |
-----------------------------------
Eval num_timesteps=589500, episode_reward=-86394.95 +/- 22595.10
Episode length: 61.00 +/- 1.79
------------------------------------
| eval/              |             |
|    mean action     | -0.16560611 |
|    mean velocity x | 1.26        |
|    mean velocity y | 2.05        |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 61          |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 589500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.2      |
|    ep_rew_mean     | -7.74e+04 |
| time/              |           |
|    fps             | 99        |
|    iterations      | 288       |
|    time_elapsed    | 5909      |
|    total_timesteps | 589824    |
----------------------------------
Eval num_timesteps=590000, episode_reward=-87753.36 +/- 21477.81
Episode length: 59.40 +/- 3.77
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15702192  |
|    mean velocity x      | 1.9          |
|    mean velocity y      | 1.36         |
|    mean velocity z      | 18.8         |
|    mean_ep_length       | 59.4         |
|    mean_reward          | -8.78e+04    |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0030519417 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.178        |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+08      |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 0.924        |
|    value_loss           | 2.06e+08     |
------------------------------------------
Eval num_timesteps=590500, episode_reward=-77664.65 +/- 28348.07
Episode length: 59.80 +/- 4.66
------------------------------------
| eval/              |             |
|    mean action     | -0.71401465 |
|    mean velocity x | 3.34        |
|    mean velocity y | 6.52        |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 590500      |
------------------------------------
Eval num_timesteps=591000, episode_reward=-65928.15 +/- 12037.76
Episode length: 73.00 +/- 9.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.10450548 |
|    mean velocity x | 1.61       |
|    mean velocity y | 0.574      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 73         |
|    mean_reward     | -6.59e+04  |
| time/              |            |
|    total_timesteps | 591000     |
-----------------------------------
Eval num_timesteps=591500, episode_reward=-47690.11 +/- 36986.56
Episode length: 53.60 +/- 26.64
-----------------------------------
| eval/              |            |
|    mean action     | 0.76224977 |
|    mean velocity x | -2.47      |
|    mean velocity y | -4.44      |
|    mean velocity z | 15.2       |
|    mean_ep_length  | 53.6       |
|    mean_reward     | -4.77e+04  |
| time/              |            |
|    total_timesteps | 591500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -7.89e+04 |
| time/              |           |
|    fps             | 100       |
|    iterations      | 289       |
|    time_elapsed    | 5916      |
|    total_timesteps | 591872    |
----------------------------------
Eval num_timesteps=592000, episode_reward=-70346.17 +/- 37432.45
Episode length: 78.80 +/- 41.24
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08692984  |
|    mean velocity x      | 0.703        |
|    mean velocity y      | 0.963        |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 78.8         |
|    mean_reward          | -7.03e+04    |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0034812507 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.19         |
|    learning_rate        | 0.001        |
|    loss                 | 9.4e+07      |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00307     |
|    std                  | 0.924        |
|    value_loss           | 2e+08        |
------------------------------------------
Eval num_timesteps=592500, episode_reward=-55323.17 +/- 32657.49
Episode length: 51.00 +/- 23.28
------------------------------------
| eval/              |             |
|    mean action     | -0.86854863 |
|    mean velocity x | 5.22        |
|    mean velocity y | 7.46        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 51          |
|    mean_reward     | -5.53e+04   |
| time/              |             |
|    total_timesteps | 592500      |
------------------------------------
Eval num_timesteps=593000, episode_reward=-62656.89 +/- 29626.50
Episode length: 62.20 +/- 8.18
-----------------------------------
| eval/              |            |
|    mean action     | -0.8315847 |
|    mean velocity x | 3.91       |
|    mean velocity y | 5.4        |
|    mean velocity z | 19         |
|    mean_ep_length  | 62.2       |
|    mean_reward     | -6.27e+04  |
| time/              |            |
|    total_timesteps | 593000     |
-----------------------------------
Eval num_timesteps=593500, episode_reward=-51640.89 +/- 27604.64
Episode length: 59.00 +/- 17.67
------------------------------------
| eval/              |             |
|    mean action     | -0.10482619 |
|    mean velocity x | -1.18       |
|    mean velocity y | 0.84        |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 59          |
|    mean_reward     | -5.16e+04   |
| time/              |             |
|    total_timesteps | 593500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.6      |
|    ep_rew_mean     | -8.13e+04 |
| time/              |           |
|    fps             | 100       |
|    iterations      | 290       |
|    time_elapsed    | 5923      |
|    total_timesteps | 593920    |
----------------------------------
Eval num_timesteps=594000, episode_reward=-65228.29 +/- 41644.53
Episode length: 54.80 +/- 14.44
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2850235    |
|    mean velocity x      | -1.61        |
|    mean velocity y      | -1.88        |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -6.52e+04    |
| time/                   |              |
|    total_timesteps      | 594000       |
| train/                  |              |
|    approx_kl            | 0.0023833225 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.001        |
|    loss                 | 9.67e+07     |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 0.925        |
|    value_loss           | 2.24e+08     |
------------------------------------------
Eval num_timesteps=594500, episode_reward=-71696.80 +/- 28366.09
Episode length: 65.20 +/- 15.54
-----------------------------------
| eval/              |            |
|    mean action     | 0.13466187 |
|    mean velocity x | -0.435     |
|    mean velocity y | -1.01      |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 65.2       |
|    mean_reward     | -7.17e+04  |
| time/              |            |
|    total_timesteps | 594500     |
-----------------------------------
Eval num_timesteps=595000, episode_reward=-74129.88 +/- 37106.99
Episode length: 81.80 +/- 39.62
-----------------------------------
| eval/              |            |
|    mean action     | 0.08757788 |
|    mean velocity x | 0.303      |
|    mean velocity y | 0.302      |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 81.8       |
|    mean_reward     | -7.41e+04  |
| time/              |            |
|    total_timesteps | 595000     |
-----------------------------------
Eval num_timesteps=595500, episode_reward=-87581.88 +/- 41654.90
Episode length: 56.40 +/- 17.40
----------------------------------
| eval/              |           |
|    mean action     | 0.3166734 |
|    mean velocity x | 1.01      |
|    mean velocity y | -0.897    |
|    mean velocity z | 16.4      |
|    mean_ep_length  | 56.4      |
|    mean_reward     | -8.76e+04 |
| time/              |           |
|    total_timesteps | 595500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.2      |
|    ep_rew_mean     | -8.41e+04 |
| time/              |           |
|    fps             | 100       |
|    iterations      | 291       |
|    time_elapsed    | 5930      |
|    total_timesteps | 595968    |
----------------------------------
Eval num_timesteps=596000, episode_reward=-63153.21 +/- 44393.29
Episode length: 52.00 +/- 17.10
------------------------------------------
| eval/                   |              |
|    mean action          | 0.16186756   |
|    mean velocity x      | 0.567        |
|    mean velocity y      | 0.00125      |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 52           |
|    mean_reward          | -6.32e+04    |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0011746417 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.02        |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.001        |
|    loss                 | 1.61e+08     |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.00214     |
|    std                  | 0.925        |
|    value_loss           | 2.68e+08     |
------------------------------------------
Eval num_timesteps=596500, episode_reward=-79577.98 +/- 39152.10
Episode length: 57.80 +/- 10.61
-----------------------------------
| eval/              |            |
|    mean action     | 0.15032464 |
|    mean velocity x | -1.34      |
|    mean velocity y | -1.84      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -7.96e+04  |
| time/              |            |
|    total_timesteps | 596500     |
-----------------------------------
Eval num_timesteps=597000, episode_reward=-42622.70 +/- 37820.90
Episode length: 57.60 +/- 27.30
------------------------------------
| eval/              |             |
|    mean action     | -0.12249714 |
|    mean velocity x | 0.632       |
|    mean velocity y | 0.431       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -4.26e+04   |
| time/              |             |
|    total_timesteps | 597000      |
------------------------------------
Eval num_timesteps=597500, episode_reward=-88154.10 +/- 36570.06
Episode length: 59.00 +/- 8.17
------------------------------------
| eval/              |             |
|    mean action     | -0.04234109 |
|    mean velocity x | 1.64        |
|    mean velocity y | 2.07        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 59          |
|    mean_reward     | -8.82e+04   |
| time/              |             |
|    total_timesteps | 597500      |
------------------------------------
Eval num_timesteps=598000, episode_reward=-75172.65 +/- 36760.92
Episode length: 60.20 +/- 2.71
------------------------------------
| eval/              |             |
|    mean action     | -0.75462085 |
|    mean velocity x | 2.79        |
|    mean velocity y | 5.51        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 60.2        |
|    mean_reward     | -7.52e+04   |
| time/              |             |
|    total_timesteps | 598000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.8      |
|    ep_rew_mean     | -8.42e+04 |
| time/              |           |
|    fps             | 100       |
|    iterations      | 292       |
|    time_elapsed    | 5938      |
|    total_timesteps | 598016    |
----------------------------------
Eval num_timesteps=598500, episode_reward=-65340.57 +/- 36546.85
Episode length: 53.20 +/- 21.98
----------------------------------------
| eval/                   |            |
|    mean action          | 0.45193708 |
|    mean velocity x      | -1.01      |
|    mean velocity y      | -2.66      |
|    mean velocity z      | 14.4       |
|    mean_ep_length       | 53.2       |
|    mean_reward          | -6.53e+04  |
| time/                   |            |
|    total_timesteps      | 598500     |
| train/                  |            |
|    approx_kl            | 0.00196411 |
|    clip_fraction        | 0.00698    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.02      |
|    explained_variance   | 0.185      |
|    learning_rate        | 0.001      |
|    loss                 | 1.04e+08   |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.00234   |
|    std                  | 0.923      |
|    value_loss           | 2.35e+08   |
----------------------------------------
Eval num_timesteps=599000, episode_reward=-57229.03 +/- 33339.65
Episode length: 51.20 +/- 18.44
------------------------------------
| eval/              |             |
|    mean action     | -0.64028513 |
|    mean velocity x | 3.09        |
|    mean velocity y | 4.16        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 51.2        |
|    mean_reward     | -5.72e+04   |
| time/              |             |
|    total_timesteps | 599000      |
------------------------------------
Eval num_timesteps=599500, episode_reward=-73343.74 +/- 40713.98
Episode length: 66.40 +/- 28.10
-----------------------------------
| eval/              |            |
|    mean action     | -0.6514567 |
|    mean velocity x | 2          |
|    mean velocity y | 4.22       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 66.4       |
|    mean_reward     | -7.33e+04  |
| time/              |            |
|    total_timesteps | 599500     |
-----------------------------------
Eval num_timesteps=600000, episode_reward=-70305.45 +/- 26427.49
Episode length: 72.40 +/- 16.01
-------------------------------------
| eval/              |              |
|    mean action     | -0.018517364 |
|    mean velocity x | -0.666       |
|    mean velocity y | 0.144        |
|    mean velocity z | 17.7         |
|    mean_ep_length  | 72.4         |
|    mean_reward     | -7.03e+04    |
| time/              |              |
|    total_timesteps | 600000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.8      |
|    ep_rew_mean     | -7.43e+04 |
| time/              |           |
|    fps             | 100       |
|    iterations      | 293       |
|    time_elapsed    | 5945      |
|    total_timesteps | 600064    |
----------------------------------
Eval num_timesteps=600500, episode_reward=-90826.44 +/- 19399.67
Episode length: 62.20 +/- 4.49
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.4515361  |
|    mean velocity x      | 3.75        |
|    mean velocity y      | 3.82        |
|    mean velocity z      | 17.3        |
|    mean_ep_length       | 62.2        |
|    mean_reward          | -9.08e+04   |
| time/                   |             |
|    total_timesteps      | 600500      |
| train/                  |             |
|    approx_kl            | 0.000404811 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.02       |
|    explained_variance   | 0.187       |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+08     |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.000628   |
|    std                  | 0.923       |
|    value_loss           | 1.84e+08    |
-----------------------------------------
Eval num_timesteps=601000, episode_reward=-82381.30 +/- 12611.82
Episode length: 71.60 +/- 5.31
-----------------------------------
| eval/              |            |
|    mean action     | 0.18382396 |
|    mean velocity x | -1.44      |
|    mean velocity y | -1.19      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 71.6       |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 601000     |
-----------------------------------
Eval num_timesteps=601500, episode_reward=-88564.71 +/- 15342.53
Episode length: 75.40 +/- 17.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.59635293 |
|    mean velocity x | -1.22      |
|    mean velocity y | -2.97      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 75.4       |
|    mean_reward     | -8.86e+04  |
| time/              |            |
|    total_timesteps | 601500     |
-----------------------------------
Eval num_timesteps=602000, episode_reward=-92433.05 +/- 10721.01
Episode length: 66.20 +/- 4.58
-------------------------------------
| eval/              |              |
|    mean action     | -0.025406739 |
|    mean velocity x | -0.193       |
|    mean velocity y | 0.74         |
|    mean velocity z | 20.4         |
|    mean_ep_length  | 66.2         |
|    mean_reward     | -9.24e+04    |
| time/              |              |
|    total_timesteps | 602000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.5      |
|    ep_rew_mean     | -7.73e+04 |
| time/              |           |
|    fps             | 101       |
|    iterations      | 294       |
|    time_elapsed    | 5952      |
|    total_timesteps | 602112    |
----------------------------------
Eval num_timesteps=602500, episode_reward=-80053.93 +/- 55645.37
Episode length: 83.40 +/- 59.23
------------------------------------------
| eval/                   |              |
|    mean action          | -0.42059162  |
|    mean velocity x      | 0.338        |
|    mean velocity y      | 2.26         |
|    mean velocity z      | 18.6         |
|    mean_ep_length       | 83.4         |
|    mean_reward          | -8.01e+04    |
| time/                   |              |
|    total_timesteps      | 602500       |
| train/                  |              |
|    approx_kl            | 0.0007716509 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.001        |
|    loss                 | 1.58e+08     |
|    n_updates            | 2940         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 0.923        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=603000, episode_reward=-81909.07 +/- 34692.54
Episode length: 62.40 +/- 8.69
------------------------------------
| eval/              |             |
|    mean action     | 0.044707503 |
|    mean velocity x | 0.694       |
|    mean velocity y | 1.2         |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 62.4        |
|    mean_reward     | -8.19e+04   |
| time/              |             |
|    total_timesteps | 603000      |
------------------------------------
Eval num_timesteps=603500, episode_reward=-86874.32 +/- 25679.24
Episode length: 70.80 +/- 13.73
------------------------------------
| eval/              |             |
|    mean action     | -0.23103385 |
|    mean velocity x | 0.158       |
|    mean velocity y | 1.07        |
|    mean velocity z | 15.6        |
|    mean_ep_length  | 70.8        |
|    mean_reward     | -8.69e+04   |
| time/              |             |
|    total_timesteps | 603500      |
------------------------------------
Eval num_timesteps=604000, episode_reward=-26914.08 +/- 39387.87
Episode length: 36.40 +/- 18.14
-----------------------------------
| eval/              |            |
|    mean action     | 0.17977579 |
|    mean velocity x | 1.07       |
|    mean velocity y | -0.136     |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 36.4       |
|    mean_reward     | -2.69e+04  |
| time/              |            |
|    total_timesteps | 604000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.5      |
|    ep_rew_mean     | -7.77e+04 |
| time/              |           |
|    fps             | 101       |
|    iterations      | 295       |
|    time_elapsed    | 5959      |
|    total_timesteps | 604160    |
----------------------------------
Eval num_timesteps=604500, episode_reward=-58257.07 +/- 40457.64
Episode length: 50.00 +/- 20.54
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31584704  |
|    mean velocity x      | 1.86         |
|    mean velocity y      | 2.3          |
|    mean velocity z      | 18           |
|    mean_ep_length       | 50           |
|    mean_reward          | -5.83e+04    |
| time/                   |              |
|    total_timesteps      | 604500       |
| train/                  |              |
|    approx_kl            | 0.0011740022 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.001        |
|    loss                 | 6.48e+07     |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.923        |
|    value_loss           | 2.22e+08     |
------------------------------------------
Eval num_timesteps=605000, episode_reward=-55972.86 +/- 29449.81
Episode length: 63.40 +/- 15.42
-----------------------------------
| eval/              |            |
|    mean action     | 0.16168468 |
|    mean velocity x | -0.557     |
|    mean velocity y | -1.26      |
|    mean velocity z | 22.8       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -5.6e+04   |
| time/              |            |
|    total_timesteps | 605000     |
-----------------------------------
Eval num_timesteps=605500, episode_reward=-84309.42 +/- 23495.99
Episode length: 61.40 +/- 3.07
-------------------------------------
| eval/              |              |
|    mean action     | -0.112577945 |
|    mean velocity x | 0.119        |
|    mean velocity y | 1.67         |
|    mean velocity z | 17.2         |
|    mean_ep_length  | 61.4         |
|    mean_reward     | -8.43e+04    |
| time/              |              |
|    total_timesteps | 605500       |
-------------------------------------
Eval num_timesteps=606000, episode_reward=-61074.61 +/- 29006.26
Episode length: 57.60 +/- 17.17
-------------------------------------
| eval/              |              |
|    mean action     | -0.035829958 |
|    mean velocity x | 1.48         |
|    mean velocity y | 1.24         |
|    mean velocity z | 20.4         |
|    mean_ep_length  | 57.6         |
|    mean_reward     | -6.11e+04    |
| time/              |              |
|    total_timesteps | 606000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.6      |
|    ep_rew_mean     | -8.07e+04 |
| time/              |           |
|    fps             | 101       |
|    iterations      | 296       |
|    time_elapsed    | 5967      |
|    total_timesteps | 606208    |
----------------------------------
Eval num_timesteps=606500, episode_reward=-44881.28 +/- 39727.24
Episode length: 46.40 +/- 27.34
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3947085   |
|    mean velocity x      | 1.51         |
|    mean velocity y      | 2.11         |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 46.4         |
|    mean_reward          | -4.49e+04    |
| time/                   |              |
|    total_timesteps      | 606500       |
| train/                  |              |
|    approx_kl            | 0.0020212545 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.001        |
|    loss                 | 6.62e+07     |
|    n_updates            | 2960         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 0.923        |
|    value_loss           | 2.44e+08     |
------------------------------------------
Eval num_timesteps=607000, episode_reward=-71267.64 +/- 23091.32
Episode length: 68.80 +/- 13.60
------------------------------------
| eval/              |             |
|    mean action     | 0.060049243 |
|    mean velocity x | -0.23       |
|    mean velocity y | -0.42       |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 68.8        |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 607000      |
------------------------------------
Eval num_timesteps=607500, episode_reward=-32536.77 +/- 34799.44
Episode length: 41.40 +/- 19.91
------------------------------------
| eval/              |             |
|    mean action     | -0.26151064 |
|    mean velocity x | 2.65        |
|    mean velocity y | 3.13        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 41.4        |
|    mean_reward     | -3.25e+04   |
| time/              |             |
|    total_timesteps | 607500      |
------------------------------------
Eval num_timesteps=608000, episode_reward=-71135.87 +/- 18893.58
Episode length: 62.20 +/- 8.42
----------------------------------
| eval/              |           |
|    mean action     | 0.0851532 |
|    mean velocity x | -2.27     |
|    mean velocity y | -2.72     |
|    mean velocity z | 17.8      |
|    mean_ep_length  | 62.2      |
|    mean_reward     | -7.11e+04 |
| time/              |           |
|    total_timesteps | 608000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.2      |
|    ep_rew_mean     | -8.36e+04 |
| time/              |           |
|    fps             | 101       |
|    iterations      | 297       |
|    time_elapsed    | 5974      |
|    total_timesteps | 608256    |
----------------------------------
Eval num_timesteps=608500, episode_reward=-66503.74 +/- 36552.23
Episode length: 53.60 +/- 23.57
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13427652   |
|    mean velocity x      | -0.18         |
|    mean velocity y      | 1.09          |
|    mean velocity z      | 19.3          |
|    mean_ep_length       | 53.6          |
|    mean_reward          | -6.65e+04     |
| time/                   |               |
|    total_timesteps      | 608500        |
| train/                  |               |
|    approx_kl            | 0.00042689583 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+08      |
|    n_updates            | 2970          |
|    policy_gradient_loss | -0.00132      |
|    std                  | 0.922         |
|    value_loss           | 2.76e+08      |
-------------------------------------------
Eval num_timesteps=609000, episode_reward=-85062.96 +/- 13642.10
Episode length: 80.20 +/- 24.96
-----------------------------------
| eval/              |            |
|    mean action     | -0.6316081 |
|    mean velocity x | 2.42       |
|    mean velocity y | 4.44       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 80.2       |
|    mean_reward     | -8.51e+04  |
| time/              |            |
|    total_timesteps | 609000     |
-----------------------------------
Eval num_timesteps=609500, episode_reward=-76363.85 +/- 38944.72
Episode length: 64.00 +/- 8.53
------------------------------------
| eval/              |             |
|    mean action     | -0.20433852 |
|    mean velocity x | -0.0412     |
|    mean velocity y | 2.36        |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 64          |
|    mean_reward     | -7.64e+04   |
| time/              |             |
|    total_timesteps | 609500      |
------------------------------------
Eval num_timesteps=610000, episode_reward=-81444.96 +/- 35607.48
Episode length: 61.20 +/- 16.63
------------------------------------
| eval/              |             |
|    mean action     | -0.06524424 |
|    mean velocity x | 1.87        |
|    mean velocity y | 0.626       |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -8.14e+04   |
| time/              |             |
|    total_timesteps | 610000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.5      |
|    ep_rew_mean     | -7.86e+04 |
| time/              |           |
|    fps             | 102       |
|    iterations      | 298       |
|    time_elapsed    | 5981      |
|    total_timesteps | 610304    |
----------------------------------
Eval num_timesteps=610500, episode_reward=-95888.76 +/- 19082.14
Episode length: 85.80 +/- 36.14
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6721047   |
|    mean velocity x      | 2.5          |
|    mean velocity y      | 4.51         |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 85.8         |
|    mean_reward          | -9.59e+04    |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0008207803 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.001        |
|    loss                 | 7.9e+07      |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.000797    |
|    std                  | 0.922        |
|    value_loss           | 1.75e+08     |
------------------------------------------
Eval num_timesteps=611000, episode_reward=-64887.82 +/- 34354.06
Episode length: 61.80 +/- 25.10
------------------------------------
| eval/              |             |
|    mean action     | -0.41106012 |
|    mean velocity x | 2.34        |
|    mean velocity y | 2.54        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -6.49e+04   |
| time/              |             |
|    total_timesteps | 611000      |
------------------------------------
Eval num_timesteps=611500, episode_reward=-76109.90 +/- 26900.59
Episode length: 63.00 +/- 6.51
------------------------------------
| eval/              |             |
|    mean action     | -0.08722069 |
|    mean velocity x | 0.516       |
|    mean velocity y | 0.611       |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 63          |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 611500      |
------------------------------------
Eval num_timesteps=612000, episode_reward=-70728.03 +/- 78418.11
Episode length: 73.00 +/- 75.17
------------------------------------
| eval/              |             |
|    mean action     | -0.61799026 |
|    mean velocity x | 3.08        |
|    mean velocity y | 6.01        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 73          |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 612000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.7      |
|    ep_rew_mean     | -7.81e+04 |
| time/              |           |
|    fps             | 102       |
|    iterations      | 299       |
|    time_elapsed    | 5988      |
|    total_timesteps | 612352    |
----------------------------------
Eval num_timesteps=612500, episode_reward=-64019.24 +/- 33552.49
Episode length: 77.00 +/- 37.55
------------------------------------------
| eval/                   |              |
|    mean action          | -0.09688542  |
|    mean velocity x      | -0.422       |
|    mean velocity y      | -0.601       |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 77           |
|    mean_reward          | -6.4e+04     |
| time/                   |              |
|    total_timesteps      | 612500       |
| train/                  |              |
|    approx_kl            | 0.0021510064 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.171        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+08     |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.922        |
|    value_loss           | 2.4e+08      |
------------------------------------------
Eval num_timesteps=613000, episode_reward=-52813.73 +/- 31837.79
Episode length: 58.00 +/- 18.96
------------------------------------
| eval/              |             |
|    mean action     | -0.17010923 |
|    mean velocity x | 1.1         |
|    mean velocity y | 1.91        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 58          |
|    mean_reward     | -5.28e+04   |
| time/              |             |
|    total_timesteps | 613000      |
------------------------------------
Eval num_timesteps=613500, episode_reward=-47247.86 +/- 39907.44
Episode length: 50.00 +/- 22.49
----------------------------------
| eval/              |           |
|    mean action     | 0.0647544 |
|    mean velocity x | 0.275     |
|    mean velocity y | 0.69      |
|    mean velocity z | 18        |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.72e+04 |
| time/              |           |
|    total_timesteps | 613500    |
----------------------------------
Eval num_timesteps=614000, episode_reward=-68636.50 +/- 34869.01
Episode length: 67.40 +/- 36.25
------------------------------------
| eval/              |             |
|    mean action     | -0.29249385 |
|    mean velocity x | 1.56        |
|    mean velocity y | 2.5         |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -6.86e+04   |
| time/              |             |
|    total_timesteps | 614000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.3      |
|    ep_rew_mean     | -7.88e+04 |
| time/              |           |
|    fps             | 102       |
|    iterations      | 300       |
|    time_elapsed    | 5996      |
|    total_timesteps | 614400    |
----------------------------------
Eval num_timesteps=614500, episode_reward=-34644.35 +/- 32497.34
Episode length: 61.40 +/- 39.11
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.38650623 |
|    mean velocity x      | 0.456       |
|    mean velocity y      | 3.19        |
|    mean velocity z      | 17.1        |
|    mean_ep_length       | 61.4        |
|    mean_reward          | -3.46e+04   |
| time/                   |             |
|    total_timesteps      | 614500      |
| train/                  |             |
|    approx_kl            | 0.00398873  |
|    clip_fraction        | 0.00928     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.001       |
|    loss                 | 5.63e+07    |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00394    |
|    std                  | 0.921       |
|    value_loss           | 1.95e+08    |
-----------------------------------------
Eval num_timesteps=615000, episode_reward=-97380.16 +/- 20445.51
Episode length: 72.00 +/- 7.29
-------------------------------------
| eval/              |              |
|    mean action     | -0.012926575 |
|    mean velocity x | -0.719       |
|    mean velocity y | 1.14         |
|    mean velocity z | 16.4         |
|    mean_ep_length  | 72           |
|    mean_reward     | -9.74e+04    |
| time/              |              |
|    total_timesteps | 615000       |
-------------------------------------
Eval num_timesteps=615500, episode_reward=-63606.70 +/- 50326.45
Episode length: 48.00 +/- 22.30
----------------------------------
| eval/              |           |
|    mean action     | 0.510276  |
|    mean velocity x | -1.54     |
|    mean velocity y | -3.27     |
|    mean velocity z | 18.2      |
|    mean_ep_length  | 48        |
|    mean_reward     | -6.36e+04 |
| time/              |           |
|    total_timesteps | 615500    |
----------------------------------
Eval num_timesteps=616000, episode_reward=-93137.74 +/- 29588.92
Episode length: 78.40 +/- 31.67
----------------------------------
| eval/              |           |
|    mean action     | 0.3968439 |
|    mean velocity x | -2.18     |
|    mean velocity y | -2.08     |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 78.4      |
|    mean_reward     | -9.31e+04 |
| time/              |           |
|    total_timesteps | 616000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.1      |
|    ep_rew_mean     | -7.39e+04 |
| time/              |           |
|    fps             | 102       |
|    iterations      | 301       |
|    time_elapsed    | 6003      |
|    total_timesteps | 616448    |
----------------------------------
Eval num_timesteps=616500, episode_reward=-68356.88 +/- 44071.67
Episode length: 76.60 +/- 49.44
------------------------------------------
| eval/                   |              |
|    mean action          | -0.9592837   |
|    mean velocity x      | 3.22         |
|    mean velocity y      | 5.4          |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 76.6         |
|    mean_reward          | -6.84e+04    |
| time/                   |              |
|    total_timesteps      | 616500       |
| train/                  |              |
|    approx_kl            | 0.0006661394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+08     |
|    n_updates            | 3010         |
|    policy_gradient_loss | -0.000698    |
|    std                  | 0.921        |
|    value_loss           | 1.72e+08     |
------------------------------------------
Eval num_timesteps=617000, episode_reward=-94428.51 +/- 51965.29
Episode length: 86.40 +/- 53.93
------------------------------------
| eval/              |             |
|    mean action     | -0.38698408 |
|    mean velocity x | 1.87        |
|    mean velocity y | 2.58        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 86.4        |
|    mean_reward     | -9.44e+04   |
| time/              |             |
|    total_timesteps | 617000      |
------------------------------------
Eval num_timesteps=617500, episode_reward=-56916.24 +/- 18545.76
Episode length: 66.80 +/- 11.77
------------------------------------
| eval/              |             |
|    mean action     | -0.44424382 |
|    mean velocity x | 3.05        |
|    mean velocity y | 3.05        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 617500      |
------------------------------------
Eval num_timesteps=618000, episode_reward=-57146.21 +/- 23263.01
Episode length: 57.80 +/- 9.26
-----------------------------------
| eval/              |            |
|    mean action     | -0.6298923 |
|    mean velocity x | 1.19       |
|    mean velocity y | 3.43       |
|    mean velocity z | 19         |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -5.71e+04  |
| time/              |            |
|    total_timesteps | 618000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.2      |
|    ep_rew_mean     | -7.95e+04 |
| time/              |           |
|    fps             | 102       |
|    iterations      | 302       |
|    time_elapsed    | 6010      |
|    total_timesteps | 618496    |
----------------------------------
Eval num_timesteps=618500, episode_reward=-73455.71 +/- 28206.92
Episode length: 69.60 +/- 14.80
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5346632   |
|    mean velocity x      | 2.96         |
|    mean velocity y      | 3.52         |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 69.6         |
|    mean_reward          | -7.35e+04    |
| time/                   |              |
|    total_timesteps      | 618500       |
| train/                  |              |
|    approx_kl            | 3.962664e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.001        |
|    loss                 | 8.66e+07     |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.00028     |
|    std                  | 0.921        |
|    value_loss           | 2.45e+08     |
------------------------------------------
Eval num_timesteps=619000, episode_reward=-67287.71 +/- 33343.29
Episode length: 65.00 +/- 19.93
------------------------------------
| eval/              |             |
|    mean action     | -0.48767433 |
|    mean velocity x | 1.79        |
|    mean velocity y | 3.27        |
|    mean velocity z | 21          |
|    mean_ep_length  | 65          |
|    mean_reward     | -6.73e+04   |
| time/              |             |
|    total_timesteps | 619000      |
------------------------------------
Eval num_timesteps=619500, episode_reward=-84966.70 +/- 19411.95
Episode length: 68.20 +/- 10.76
-----------------------------------
| eval/              |            |
|    mean action     | -0.2357473 |
|    mean velocity x | 2.35       |
|    mean velocity y | 2.82       |
|    mean velocity z | 19         |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -8.5e+04   |
| time/              |            |
|    total_timesteps | 619500     |
-----------------------------------
Eval num_timesteps=620000, episode_reward=-67908.08 +/- 14410.27
Episode length: 77.80 +/- 25.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.48303258 |
|    mean velocity x | -2.29      |
|    mean velocity y | -3.35      |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 77.8       |
|    mean_reward     | -6.79e+04  |
| time/              |            |
|    total_timesteps | 620000     |
-----------------------------------
Eval num_timesteps=620500, episode_reward=-77469.89 +/- 32805.27
Episode length: 58.20 +/- 6.40
-----------------------------------
| eval/              |            |
|    mean action     | -0.5628951 |
|    mean velocity x | 3.73       |
|    mean velocity y | 4.78       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 58.2       |
|    mean_reward     | -7.75e+04  |
| time/              |            |
|    total_timesteps | 620500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -8.28e+04 |
| time/              |           |
|    fps             | 103       |
|    iterations      | 303       |
|    time_elapsed    | 6018      |
|    total_timesteps | 620544    |
----------------------------------
Eval num_timesteps=621000, episode_reward=-72849.64 +/- 36550.47
Episode length: 68.60 +/- 26.30
------------------------------------------
| eval/                   |              |
|    mean action          | -0.10869067  |
|    mean velocity x      | 2.42         |
|    mean velocity y      | 2.23         |
|    mean velocity z      | 19.8         |
|    mean_ep_length       | 68.6         |
|    mean_reward          | -7.28e+04    |
| time/                   |              |
|    total_timesteps      | 621000       |
| train/                  |              |
|    approx_kl            | 0.0002805831 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+08     |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.000808    |
|    std                  | 0.92         |
|    value_loss           | 2.62e+08     |
------------------------------------------
Eval num_timesteps=621500, episode_reward=-67893.34 +/- 36429.79
Episode length: 57.40 +/- 12.16
------------------------------------
| eval/              |             |
|    mean action     | -0.65118635 |
|    mean velocity x | 3.89        |
|    mean velocity y | 5.39        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 57.4        |
|    mean_reward     | -6.79e+04   |
| time/              |             |
|    total_timesteps | 621500      |
------------------------------------
Eval num_timesteps=622000, episode_reward=-59559.18 +/- 44008.56
Episode length: 54.80 +/- 21.03
-----------------------------------
| eval/              |            |
|    mean action     | -0.6507954 |
|    mean velocity x | 1.78       |
|    mean velocity y | 4.36       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 54.8       |
|    mean_reward     | -5.96e+04  |
| time/              |            |
|    total_timesteps | 622000     |
-----------------------------------
Eval num_timesteps=622500, episode_reward=-66112.35 +/- 38425.47
Episode length: 62.20 +/- 19.11
-----------------------------------
| eval/              |            |
|    mean action     | -0.1196664 |
|    mean velocity x | 1.09       |
|    mean velocity y | 0.479      |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 62.2       |
|    mean_reward     | -6.61e+04  |
| time/              |            |
|    total_timesteps | 622500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.9      |
|    ep_rew_mean     | -8.52e+04 |
| time/              |           |
|    fps             | 103       |
|    iterations      | 304       |
|    time_elapsed    | 6025      |
|    total_timesteps | 622592    |
----------------------------------
Eval num_timesteps=623000, episode_reward=-77405.80 +/- 42044.48
Episode length: 71.20 +/- 40.27
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.75199276   |
|    mean velocity x      | 3.96          |
|    mean velocity y      | 5.12          |
|    mean velocity z      | 17.6          |
|    mean_ep_length       | 71.2          |
|    mean_reward          | -7.74e+04     |
| time/                   |               |
|    total_timesteps      | 623000        |
| train/                  |               |
|    approx_kl            | 0.00060522195 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.001         |
|    loss                 | 2.13e+08      |
|    n_updates            | 3040          |
|    policy_gradient_loss | -0.00111      |
|    std                  | 0.92          |
|    value_loss           | 2.03e+08      |
-------------------------------------------
Eval num_timesteps=623500, episode_reward=-89968.58 +/- 24940.37
Episode length: 62.80 +/- 2.04
-----------------------------------
| eval/              |            |
|    mean action     | 0.28147894 |
|    mean velocity x | -0.172     |
|    mean velocity y | -1.22      |
|    mean velocity z | 16         |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -9e+04     |
| time/              |            |
|    total_timesteps | 623500     |
-----------------------------------
Eval num_timesteps=624000, episode_reward=-78218.95 +/- 32421.39
Episode length: 63.80 +/- 7.39
------------------------------------
| eval/              |             |
|    mean action     | 0.028876098 |
|    mean velocity x | -0.641      |
|    mean velocity y | -0.923      |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 624000      |
------------------------------------
Eval num_timesteps=624500, episode_reward=-84424.17 +/- 11207.98
Episode length: 70.00 +/- 8.60
----------------------------------
| eval/              |           |
|    mean action     | 0.2720647 |
|    mean velocity x | -1.19     |
|    mean velocity y | -2.99     |
|    mean velocity z | 15.3      |
|    mean_ep_length  | 70        |
|    mean_reward     | -8.44e+04 |
| time/              |           |
|    total_timesteps | 624500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | -8.6e+04 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 305      |
|    time_elapsed    | 6032     |
|    total_timesteps | 624640   |
---------------------------------
Eval num_timesteps=625000, episode_reward=-69139.98 +/- 36340.33
Episode length: 70.40 +/- 35.54
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14508171  |
|    mean velocity x      | 0.522        |
|    mean velocity y      | 0.2          |
|    mean velocity z      | 20           |
|    mean_ep_length       | 70.4         |
|    mean_reward          | -6.91e+04    |
| time/                   |              |
|    total_timesteps      | 625000       |
| train/                  |              |
|    approx_kl            | 0.0023436276 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+08     |
|    n_updates            | 3050         |
|    policy_gradient_loss | -0.00239     |
|    std                  | 0.921        |
|    value_loss           | 1.95e+08     |
------------------------------------------
Eval num_timesteps=625500, episode_reward=-61400.28 +/- 28301.93
Episode length: 66.40 +/- 24.34
------------------------------------
| eval/              |             |
|    mean action     | -0.15812747 |
|    mean velocity x | -0.837      |
|    mean velocity y | 0.284       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -6.14e+04   |
| time/              |             |
|    total_timesteps | 625500      |
------------------------------------
Eval num_timesteps=626000, episode_reward=-46451.16 +/- 40272.57
Episode length: 61.20 +/- 27.58
------------------------------------
| eval/              |             |
|    mean action     | -0.07934142 |
|    mean velocity x | 1.21        |
|    mean velocity y | 1.5         |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -4.65e+04   |
| time/              |             |
|    total_timesteps | 626000      |
------------------------------------
Eval num_timesteps=626500, episode_reward=-64195.94 +/- 31414.85
Episode length: 55.20 +/- 13.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.17604397 |
|    mean velocity x | -2.14      |
|    mean velocity y | -3.54      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 55.2       |
|    mean_reward     | -6.42e+04  |
| time/              |            |
|    total_timesteps | 626500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.3      |
|    ep_rew_mean     | -8.53e+04 |
| time/              |           |
|    fps             | 103       |
|    iterations      | 306       |
|    time_elapsed    | 6039      |
|    total_timesteps | 626688    |
----------------------------------
Eval num_timesteps=627000, episode_reward=-102969.75 +/- 28375.59
Episode length: 97.20 +/- 52.30
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2967379    |
|    mean velocity x      | 2.38          |
|    mean velocity y      | 2.95          |
|    mean velocity z      | 17.4          |
|    mean_ep_length       | 97.2          |
|    mean_reward          | -1.03e+05     |
| time/                   |               |
|    total_timesteps      | 627000        |
| train/                  |               |
|    approx_kl            | 0.00044888837 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 1.92e+08      |
|    n_updates            | 3060          |
|    policy_gradient_loss | -0.000684     |
|    std                  | 0.921         |
|    value_loss           | 2.36e+08      |
-------------------------------------------
Eval num_timesteps=627500, episode_reward=-68546.59 +/- 34616.94
Episode length: 75.80 +/- 20.02
------------------------------------
| eval/              |             |
|    mean action     | -0.19476251 |
|    mean velocity x | 1.72        |
|    mean velocity y | 1.99        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 75.8        |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 627500      |
------------------------------------
Eval num_timesteps=628000, episode_reward=-56834.19 +/- 13836.44
Episode length: 63.80 +/- 6.55
----------------------------------
| eval/              |           |
|    mean action     | -0.251794 |
|    mean velocity x | 0.171     |
|    mean velocity y | 1.21      |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 63.8      |
|    mean_reward     | -5.68e+04 |
| time/              |           |
|    total_timesteps | 628000    |
----------------------------------
Eval num_timesteps=628500, episode_reward=-86694.89 +/- 15097.19
Episode length: 72.00 +/- 15.48
------------------------------------
| eval/              |             |
|    mean action     | -0.89427775 |
|    mean velocity x | 3.64        |
|    mean velocity y | 6.62        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 72          |
|    mean_reward     | -8.67e+04   |
| time/              |             |
|    total_timesteps | 628500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.2      |
|    ep_rew_mean     | -8.01e+04 |
| time/              |           |
|    fps             | 103       |
|    iterations      | 307       |
|    time_elapsed    | 6046      |
|    total_timesteps | 628736    |
----------------------------------
Eval num_timesteps=629000, episode_reward=-49214.23 +/- 45229.28
Episode length: 51.60 +/- 36.07
------------------------------------------
| eval/                   |              |
|    mean action          | 0.037063576  |
|    mean velocity x      | -0.987       |
|    mean velocity y      | -1.3         |
|    mean velocity z      | 17.2         |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -4.92e+04    |
| time/                   |              |
|    total_timesteps      | 629000       |
| train/                  |              |
|    approx_kl            | 0.0029934961 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+08     |
|    n_updates            | 3070         |
|    policy_gradient_loss | -0.00304     |
|    std                  | 0.92         |
|    value_loss           | 1.87e+08     |
------------------------------------------
Eval num_timesteps=629500, episode_reward=-77374.17 +/- 28398.16
Episode length: 67.60 +/- 7.66
----------------------------------
| eval/              |           |
|    mean action     | -0.704885 |
|    mean velocity x | 3.17      |
|    mean velocity y | 5.39      |
|    mean velocity z | 21.5      |
|    mean_ep_length  | 67.6      |
|    mean_reward     | -7.74e+04 |
| time/              |           |
|    total_timesteps | 629500    |
----------------------------------
Eval num_timesteps=630000, episode_reward=-54719.80 +/- 37813.27
Episode length: 54.80 +/- 22.94
------------------------------------
| eval/              |             |
|    mean action     | -0.14400919 |
|    mean velocity x | 1.18        |
|    mean velocity y | 0.737       |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -5.47e+04   |
| time/              |             |
|    total_timesteps | 630000      |
------------------------------------
Eval num_timesteps=630500, episode_reward=-61264.50 +/- 46335.39
Episode length: 51.00 +/- 24.19
----------------------------------
| eval/              |           |
|    mean action     | 0.1956024 |
|    mean velocity x | -0.706    |
|    mean velocity y | -2.01     |
|    mean velocity z | 20.1      |
|    mean_ep_length  | 51        |
|    mean_reward     | -6.13e+04 |
| time/              |           |
|    total_timesteps | 630500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.1      |
|    ep_rew_mean     | -8.51e+04 |
| time/              |           |
|    fps             | 104       |
|    iterations      | 308       |
|    time_elapsed    | 6061      |
|    total_timesteps | 630784    |
----------------------------------
Eval num_timesteps=631000, episode_reward=-66305.28 +/- 30096.03
Episode length: 61.80 +/- 13.29
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.11883526    |
|    mean velocity x      | -0.145        |
|    mean velocity y      | -0.405        |
|    mean velocity z      | 19.9          |
|    mean_ep_length       | 61.8          |
|    mean_reward          | -6.63e+04     |
| time/                   |               |
|    total_timesteps      | 631000        |
| train/                  |               |
|    approx_kl            | 0.00014233869 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.153         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+08      |
|    n_updates            | 3080          |
|    policy_gradient_loss | -0.000484     |
|    std                  | 0.92          |
|    value_loss           | 2.61e+08      |
-------------------------------------------
Eval num_timesteps=631500, episode_reward=-74867.54 +/- 40673.23
Episode length: 57.20 +/- 17.37
----------------------------------
| eval/              |           |
|    mean action     | 0.4770036 |
|    mean velocity x | -2.75     |
|    mean velocity y | -3.17     |
|    mean velocity z | 18.7      |
|    mean_ep_length  | 57.2      |
|    mean_reward     | -7.49e+04 |
| time/              |           |
|    total_timesteps | 631500    |
----------------------------------
Eval num_timesteps=632000, episode_reward=-52933.79 +/- 29567.80
Episode length: 56.80 +/- 9.70
------------------------------------
| eval/              |             |
|    mean action     | -0.19311294 |
|    mean velocity x | -0.445      |
|    mean velocity y | 0.142       |
|    mean velocity z | 16.5        |
|    mean_ep_length  | 56.8        |
|    mean_reward     | -5.29e+04   |
| time/              |             |
|    total_timesteps | 632000      |
------------------------------------
Eval num_timesteps=632500, episode_reward=-44761.11 +/- 28204.72
Episode length: 51.40 +/- 12.72
-----------------------------------
| eval/              |            |
|    mean action     | -0.9251356 |
|    mean velocity x | 3.06       |
|    mean velocity y | 5.96       |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 51.4       |
|    mean_reward     | -4.48e+04  |
| time/              |            |
|    total_timesteps | 632500     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.1     |
|    ep_rew_mean     | -9e+04   |
| time/              |          |
|    fps             | 104      |
|    iterations      | 309      |
|    time_elapsed    | 6068     |
|    total_timesteps | 632832   |
---------------------------------
Eval num_timesteps=633000, episode_reward=-47954.08 +/- 40652.06
Episode length: 44.20 +/- 24.12
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25558594   |
|    mean velocity x      | 1.1           |
|    mean velocity y      | 1.73          |
|    mean velocity z      | 20            |
|    mean_ep_length       | 44.2          |
|    mean_reward          | -4.8e+04      |
| time/                   |               |
|    total_timesteps      | 633000        |
| train/                  |               |
|    approx_kl            | 6.2258536e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.001         |
|    loss                 | 8.85e+07      |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.000424     |
|    std                  | 0.92          |
|    value_loss           | 2.21e+08      |
-------------------------------------------
Eval num_timesteps=633500, episode_reward=-51553.16 +/- 30490.91
Episode length: 58.00 +/- 14.48
----------------------------------
| eval/              |           |
|    mean action     | -0.671184 |
|    mean velocity x | 3.27      |
|    mean velocity y | 4.79      |
|    mean velocity z | 16.9      |
|    mean_ep_length  | 58        |
|    mean_reward     | -5.16e+04 |
| time/              |           |
|    total_timesteps | 633500    |
----------------------------------
Eval num_timesteps=634000, episode_reward=-44427.33 +/- 25318.40
Episode length: 51.60 +/- 10.67
----------------------------------
| eval/              |           |
|    mean action     | 0.5753276 |
|    mean velocity x | -2.75     |
|    mean velocity y | -2.9      |
|    mean velocity z | 18        |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.44e+04 |
| time/              |           |
|    total_timesteps | 634000    |
----------------------------------
Eval num_timesteps=634500, episode_reward=-76160.49 +/- 9668.30
Episode length: 77.20 +/- 12.58
----------------------------------
| eval/              |           |
|    mean action     | 0.6592876 |
|    mean velocity x | -3.01     |
|    mean velocity y | -4.29     |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 77.2      |
|    mean_reward     | -7.62e+04 |
| time/              |           |
|    total_timesteps | 634500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.1      |
|    ep_rew_mean     | -9.16e+04 |
| time/              |           |
|    fps             | 104       |
|    iterations      | 310       |
|    time_elapsed    | 6074      |
|    total_timesteps | 634880    |
----------------------------------
Eval num_timesteps=635000, episode_reward=-49902.31 +/- 24230.43
Episode length: 61.20 +/- 16.63
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.33605587   |
|    mean velocity x      | 1.96          |
|    mean velocity y      | 2.22          |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 61.2          |
|    mean_reward          | -4.99e+04     |
| time/                   |               |
|    total_timesteps      | 635000        |
| train/                  |               |
|    approx_kl            | 0.00015991065 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01         |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 3100          |
|    policy_gradient_loss | -0.000719     |
|    std                  | 0.92          |
|    value_loss           | 1.89e+08      |
-------------------------------------------
Eval num_timesteps=635500, episode_reward=-72532.80 +/- 21955.82
Episode length: 64.40 +/- 16.16
-------------------------------------
| eval/              |              |
|    mean action     | -0.077812806 |
|    mean velocity x | -0.242       |
|    mean velocity y | 0.538        |
|    mean velocity z | 20.3         |
|    mean_ep_length  | 64.4         |
|    mean_reward     | -7.25e+04    |
| time/              |              |
|    total_timesteps | 635500       |
-------------------------------------
Eval num_timesteps=636000, episode_reward=-79965.69 +/- 32826.40
Episode length: 64.60 +/- 11.36
------------------------------------
| eval/              |             |
|    mean action     | -0.15394966 |
|    mean velocity x | 0.361       |
|    mean velocity y | 0.188       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 64.6        |
|    mean_reward     | -8e+04      |
| time/              |             |
|    total_timesteps | 636000      |
------------------------------------
Eval num_timesteps=636500, episode_reward=-72997.68 +/- 35726.90
Episode length: 63.00 +/- 21.14
------------------------------------
| eval/              |             |
|    mean action     | -0.24271065 |
|    mean velocity x | 0.953       |
|    mean velocity y | 1.97        |
|    mean velocity z | 15.2        |
|    mean_ep_length  | 63          |
|    mean_reward     | -7.3e+04    |
| time/              |             |
|    total_timesteps | 636500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.2      |
|    ep_rew_mean     | -8.95e+04 |
| time/              |           |
|    fps             | 104       |
|    iterations      | 311       |
|    time_elapsed    | 6082      |
|    total_timesteps | 636928    |
----------------------------------
Eval num_timesteps=637000, episode_reward=-71029.86 +/- 25864.82
Episode length: 68.00 +/- 14.04
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2632655    |
|    mean velocity x      | 0.0055       |
|    mean velocity y      | -0.866       |
|    mean velocity z      | 19           |
|    mean_ep_length       | 68           |
|    mean_reward          | -7.1e+04     |
| time/                   |              |
|    total_timesteps      | 637000       |
| train/                  |              |
|    approx_kl            | 0.0037759286 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.001        |
|    loss                 | 7.4e+07      |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.00403     |
|    std                  | 0.92         |
|    value_loss           | 2.46e+08     |
------------------------------------------
Eval num_timesteps=637500, episode_reward=-58957.69 +/- 25618.77
Episode length: 58.40 +/- 11.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.12175324 |
|    mean velocity x | -0.114     |
|    mean velocity y | 0.479      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 58.4       |
|    mean_reward     | -5.9e+04   |
| time/              |            |
|    total_timesteps | 637500     |
-----------------------------------
Eval num_timesteps=638000, episode_reward=-46263.47 +/- 30184.50
Episode length: 47.80 +/- 14.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.5693952 |
|    mean velocity x | 3.04       |
|    mean velocity y | 4.37       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 47.8       |
|    mean_reward     | -4.63e+04  |
| time/              |            |
|    total_timesteps | 638000     |
-----------------------------------
Eval num_timesteps=638500, episode_reward=-45402.24 +/- 36583.29
Episode length: 59.80 +/- 30.78
------------------------------------
| eval/              |             |
|    mean action     | 0.094895035 |
|    mean velocity x | 0.291       |
|    mean velocity y | -0.00385    |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -4.54e+04   |
| time/              |             |
|    total_timesteps | 638500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.13e+04 |
| time/              |           |
|    fps             | 104       |
|    iterations      | 312       |
|    time_elapsed    | 6089      |
|    total_timesteps | 638976    |
----------------------------------
Eval num_timesteps=639000, episode_reward=-61840.19 +/- 36533.19
Episode length: 54.40 +/- 13.53
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.6179084    |
|    mean velocity x      | 2.24          |
|    mean velocity y      | 4.86          |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 54.4          |
|    mean_reward          | -6.18e+04     |
| time/                   |               |
|    total_timesteps      | 639000        |
| train/                  |               |
|    approx_kl            | 0.00018852242 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.169         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+08      |
|    n_updates            | 3120          |
|    policy_gradient_loss | -0.000577     |
|    std                  | 0.92          |
|    value_loss           | 2.34e+08      |
-------------------------------------------
Eval num_timesteps=639500, episode_reward=-101550.95 +/- 18344.98
Episode length: 65.80 +/- 1.72
------------------------------------
| eval/              |             |
|    mean action     | -0.61014503 |
|    mean velocity x | 2.65        |
|    mean velocity y | 4.2         |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 65.8        |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 639500      |
------------------------------------
Eval num_timesteps=640000, episode_reward=-54541.10 +/- 48326.97
Episode length: 65.80 +/- 58.20
-----------------------------------
| eval/              |            |
|    mean action     | -0.4668423 |
|    mean velocity x | 0.834      |
|    mean velocity y | 2.59       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -5.45e+04  |
| time/              |            |
|    total_timesteps | 640000     |
-----------------------------------
Eval num_timesteps=640500, episode_reward=-80550.81 +/- 18567.97
Episode length: 95.00 +/- 31.09
------------------------------------
| eval/              |             |
|    mean action     | -0.26484355 |
|    mean velocity x | 1.26        |
|    mean velocity y | 1.85        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 95          |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 640500      |
------------------------------------
Eval num_timesteps=641000, episode_reward=-72895.24 +/- 19643.01
Episode length: 76.20 +/- 23.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.44184458 |
|    mean velocity x | -1.8       |
|    mean velocity y | -3.04      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 76.2       |
|    mean_reward     | -7.29e+04  |
| time/              |            |
|    total_timesteps | 641000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.8      |
|    ep_rew_mean     | -8.57e+04 |
| time/              |           |
|    fps             | 105       |
|    iterations      | 313       |
|    time_elapsed    | 6097      |
|    total_timesteps | 641024    |
----------------------------------
Eval num_timesteps=641500, episode_reward=-37556.21 +/- 35672.22
Episode length: 48.20 +/- 31.04
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.47791904 |
|    mean velocity x      | 1.59        |
|    mean velocity y      | 2.97        |
|    mean velocity z      | 22.1        |
|    mean_ep_length       | 48.2        |
|    mean_reward          | -3.76e+04   |
| time/                   |             |
|    total_timesteps      | 641500      |
| train/                  |             |
|    approx_kl            | 0.00239582  |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.001       |
|    loss                 | 1.46e+08    |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00273    |
|    std                  | 0.921       |
|    value_loss           | 2.7e+08     |
-----------------------------------------
Eval num_timesteps=642000, episode_reward=-90845.66 +/- 26683.52
Episode length: 68.20 +/- 4.53
-----------------------------------
| eval/              |            |
|    mean action     | -0.8312032 |
|    mean velocity x | 3.58       |
|    mean velocity y | 5.57       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -9.08e+04  |
| time/              |            |
|    total_timesteps | 642000     |
-----------------------------------
Eval num_timesteps=642500, episode_reward=-45926.19 +/- 30393.10
Episode length: 50.60 +/- 19.19
------------------------------------
| eval/              |             |
|    mean action     | -0.16408227 |
|    mean velocity x | 1.11        |
|    mean velocity y | 0.692       |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 50.6        |
|    mean_reward     | -4.59e+04   |
| time/              |             |
|    total_timesteps | 642500      |
------------------------------------
Eval num_timesteps=643000, episode_reward=-89617.48 +/- 28996.81
Episode length: 64.80 +/- 4.35
-------------------------------------
| eval/              |              |
|    mean action     | -0.100924365 |
|    mean velocity x | -1.14        |
|    mean velocity y | -0.502       |
|    mean velocity z | 16.3         |
|    mean_ep_length  | 64.8         |
|    mean_reward     | -8.96e+04    |
| time/              |              |
|    total_timesteps | 643000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -8.66e+04 |
| time/              |           |
|    fps             | 105       |
|    iterations      | 314       |
|    time_elapsed    | 6104      |
|    total_timesteps | 643072    |
----------------------------------
Eval num_timesteps=643500, episode_reward=-84333.98 +/- 37887.22
Episode length: 62.60 +/- 7.39
------------------------------------------
| eval/                   |              |
|    mean action          | -0.06910344  |
|    mean velocity x      | -0.805       |
|    mean velocity y      | -0.989       |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -8.43e+04    |
| time/                   |              |
|    total_timesteps      | 643500       |
| train/                  |              |
|    approx_kl            | 0.0016522252 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.001        |
|    loss                 | 8.6e+07      |
|    n_updates            | 3140         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.922        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=644000, episode_reward=-72959.60 +/- 43428.87
Episode length: 56.20 +/- 12.09
------------------------------------
| eval/              |             |
|    mean action     | -0.19709222 |
|    mean velocity x | 0.604       |
|    mean velocity y | 1.5         |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 56.2        |
|    mean_reward     | -7.3e+04    |
| time/              |             |
|    total_timesteps | 644000      |
------------------------------------
Eval num_timesteps=644500, episode_reward=-95050.19 +/- 35002.54
Episode length: 98.80 +/- 42.59
------------------------------------
| eval/              |             |
|    mean action     | -0.33441162 |
|    mean velocity x | -0.0818     |
|    mean velocity y | 1.38        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 98.8        |
|    mean_reward     | -9.51e+04   |
| time/              |             |
|    total_timesteps | 644500      |
------------------------------------
Eval num_timesteps=645000, episode_reward=-64634.79 +/- 34593.22
Episode length: 58.20 +/- 22.79
------------------------------------
| eval/              |             |
|    mean action     | -0.30323747 |
|    mean velocity x | 0.132       |
|    mean velocity y | 1.23        |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -6.46e+04   |
| time/              |             |
|    total_timesteps | 645000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82.7      |
|    ep_rew_mean     | -9.32e+04 |
| time/              |           |
|    fps             | 105       |
|    iterations      | 315       |
|    time_elapsed    | 6111      |
|    total_timesteps | 645120    |
----------------------------------
Eval num_timesteps=645500, episode_reward=-59685.69 +/- 44009.50
Episode length: 53.20 +/- 16.17
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.62668836    |
|    mean velocity x      | 1.85           |
|    mean velocity y      | 4.15           |
|    mean velocity z      | 19.2           |
|    mean_ep_length       | 53.2           |
|    mean_reward          | -5.97e+04      |
| time/                   |                |
|    total_timesteps      | 645500         |
| train/                  |                |
|    approx_kl            | 0.000117988006 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.01          |
|    explained_variance   | 0.186          |
|    learning_rate        | 0.001          |
|    loss                 | 1.09e+08       |
|    n_updates            | 3150           |
|    policy_gradient_loss | -0.000439      |
|    std                  | 0.921          |
|    value_loss           | 2.34e+08       |
--------------------------------------------
Eval num_timesteps=646000, episode_reward=-60976.68 +/- 29267.56
Episode length: 55.60 +/- 18.96
------------------------------------
| eval/              |             |
|    mean action     | -0.18705603 |
|    mean velocity x | 0.749       |
|    mean velocity y | 2.32        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 55.6        |
|    mean_reward     | -6.1e+04    |
| time/              |             |
|    total_timesteps | 646000      |
------------------------------------
Eval num_timesteps=646500, episode_reward=-41641.12 +/- 41716.23
Episode length: 50.00 +/- 30.88
------------------------------------
| eval/              |             |
|    mean action     | 0.029550567 |
|    mean velocity x | 0.871       |
|    mean velocity y | -1.02       |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 50          |
|    mean_reward     | -4.16e+04   |
| time/              |             |
|    total_timesteps | 646500      |
------------------------------------
Eval num_timesteps=647000, episode_reward=-70509.70 +/- 58101.60
Episode length: 72.00 +/- 46.81
-----------------------------------
| eval/              |            |
|    mean action     | -0.5948449 |
|    mean velocity x | 4.01       |
|    mean velocity y | 4.45       |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 72         |
|    mean_reward     | -7.05e+04  |
| time/              |            |
|    total_timesteps | 647000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82        |
|    ep_rew_mean     | -9.04e+04 |
| time/              |           |
|    fps             | 105       |
|    iterations      | 316       |
|    time_elapsed    | 6118      |
|    total_timesteps | 647168    |
----------------------------------
Eval num_timesteps=647500, episode_reward=-58070.58 +/- 53198.22
Episode length: 67.20 +/- 53.70
------------------------------------------
| eval/                   |              |
|    mean action          | -0.97533256  |
|    mean velocity x      | 4.79         |
|    mean velocity y      | 6.32         |
|    mean velocity z      | 20.3         |
|    mean_ep_length       | 67.2         |
|    mean_reward          | -5.81e+04    |
| time/                   |              |
|    total_timesteps      | 647500       |
| train/                  |              |
|    approx_kl            | 0.0006144302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.001        |
|    loss                 | 8.92e+07     |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.919        |
|    value_loss           | 2.07e+08     |
------------------------------------------
Eval num_timesteps=648000, episode_reward=-70137.03 +/- 26707.41
Episode length: 63.40 +/- 11.22
------------------------------------
| eval/              |             |
|    mean action     | -0.43055615 |
|    mean velocity x | 2.6         |
|    mean velocity y | 2.97        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -7.01e+04   |
| time/              |             |
|    total_timesteps | 648000      |
------------------------------------
Eval num_timesteps=648500, episode_reward=-84015.03 +/- 19399.94
Episode length: 67.20 +/- 4.87
-----------------------------------
| eval/              |            |
|    mean action     | -1.2362787 |
|    mean velocity x | 5.85       |
|    mean velocity y | 9.29       |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 67.2       |
|    mean_reward     | -8.4e+04   |
| time/              |            |
|    total_timesteps | 648500     |
-----------------------------------
Eval num_timesteps=649000, episode_reward=-57570.32 +/- 33855.51
Episode length: 56.20 +/- 15.43
------------------------------------
| eval/              |             |
|    mean action     | -0.52884704 |
|    mean velocity x | 2.43        |
|    mean velocity y | 4.01        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 56.2        |
|    mean_reward     | -5.76e+04   |
| time/              |             |
|    total_timesteps | 649000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.1      |
|    ep_rew_mean     | -9.54e+04 |
| time/              |           |
|    fps             | 105       |
|    iterations      | 317       |
|    time_elapsed    | 6125      |
|    total_timesteps | 649216    |
----------------------------------
Eval num_timesteps=649500, episode_reward=-88858.12 +/- 18521.17
Episode length: 72.80 +/- 8.40
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5105891   |
|    mean velocity x      | 2.29         |
|    mean velocity y      | 4.29         |
|    mean velocity z      | 20.7         |
|    mean_ep_length       | 72.8         |
|    mean_reward          | -8.89e+04    |
| time/                   |              |
|    total_timesteps      | 649500       |
| train/                  |              |
|    approx_kl            | 0.0010307011 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.172        |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+08     |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.0012      |
|    std                  | 0.919        |
|    value_loss           | 2.3e+08      |
------------------------------------------
Eval num_timesteps=650000, episode_reward=-49422.77 +/- 51960.37
Episode length: 41.60 +/- 21.84
------------------------------------
| eval/              |             |
|    mean action     | -0.63320255 |
|    mean velocity x | 2.04        |
|    mean velocity y | 4.72        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 41.6        |
|    mean_reward     | -4.94e+04   |
| time/              |             |
|    total_timesteps | 650000      |
------------------------------------
Eval num_timesteps=650500, episode_reward=-89013.17 +/- 50717.32
Episode length: 81.60 +/- 56.69
-----------------------------------
| eval/              |            |
|    mean action     | -0.7778035 |
|    mean velocity x | 3.67       |
|    mean velocity y | 5.3        |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 81.6       |
|    mean_reward     | -8.9e+04   |
| time/              |            |
|    total_timesteps | 650500     |
-----------------------------------
Eval num_timesteps=651000, episode_reward=-84457.60 +/- 19156.73
Episode length: 69.40 +/- 8.55
----------------------------------
| eval/              |           |
|    mean action     | -0.274151 |
|    mean velocity x | 2.44      |
|    mean velocity y | 2.01      |
|    mean velocity z | 20.9      |
|    mean_ep_length  | 69.4      |
|    mean_reward     | -8.45e+04 |
| time/              |           |
|    total_timesteps | 651000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.7      |
|    ep_rew_mean     | -9.12e+04 |
| time/              |           |
|    fps             | 106       |
|    iterations      | 318       |
|    time_elapsed    | 6132      |
|    total_timesteps | 651264    |
----------------------------------
Eval num_timesteps=651500, episode_reward=-76831.65 +/- 38915.64
Episode length: 70.20 +/- 24.96
------------------------------------------
| eval/                   |              |
|    mean action          | -0.013584015 |
|    mean velocity x      | -0.289       |
|    mean velocity y      | -1.38        |
|    mean velocity z      | 19.8         |
|    mean_ep_length       | 70.2         |
|    mean_reward          | -7.68e+04    |
| time/                   |              |
|    total_timesteps      | 651500       |
| train/                  |              |
|    approx_kl            | 0.0019610988 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.179        |
|    learning_rate        | 0.001        |
|    loss                 | 1.43e+08     |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.919        |
|    value_loss           | 2.26e+08     |
------------------------------------------
Eval num_timesteps=652000, episode_reward=-82740.69 +/- 24517.90
Episode length: 63.60 +/- 5.68
-----------------------------------
| eval/              |            |
|    mean action     | -0.6677784 |
|    mean velocity x | 2.32       |
|    mean velocity y | 4.52       |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 63.6       |
|    mean_reward     | -8.27e+04  |
| time/              |            |
|    total_timesteps | 652000     |
-----------------------------------
Eval num_timesteps=652500, episode_reward=-69185.99 +/- 32464.94
Episode length: 64.80 +/- 18.83
-----------------------------------
| eval/              |            |
|    mean action     | -0.8707097 |
|    mean velocity x | 2.73       |
|    mean velocity y | 5.14       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 64.8       |
|    mean_reward     | -6.92e+04  |
| time/              |            |
|    total_timesteps | 652500     |
-----------------------------------
Eval num_timesteps=653000, episode_reward=-66319.67 +/- 36855.65
Episode length: 59.80 +/- 13.92
-------------------------------------
| eval/              |              |
|    mean action     | -0.020838898 |
|    mean velocity x | 1.09         |
|    mean velocity y | 1.19         |
|    mean velocity z | 20.7         |
|    mean_ep_length  | 59.8         |
|    mean_reward     | -6.63e+04    |
| time/              |              |
|    total_timesteps | 653000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 88.6      |
|    ep_rew_mean     | -9.75e+04 |
| time/              |           |
|    fps             | 106       |
|    iterations      | 319       |
|    time_elapsed    | 6139      |
|    total_timesteps | 653312    |
----------------------------------
Eval num_timesteps=653500, episode_reward=-71324.21 +/- 46781.22
Episode length: 52.20 +/- 19.76
------------------------------------------
| eval/                   |              |
|    mean action          | -0.65107864  |
|    mean velocity x      | 2.75         |
|    mean velocity y      | 4.24         |
|    mean velocity z      | 19           |
|    mean_ep_length       | 52.2         |
|    mean_reward          | -7.13e+04    |
| time/                   |              |
|    total_timesteps      | 653500       |
| train/                  |              |
|    approx_kl            | 0.0013191437 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.001        |
|    loss                 | 9.13e+07     |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.919        |
|    value_loss           | 2.6e+08      |
------------------------------------------
Eval num_timesteps=654000, episode_reward=-72626.85 +/- 29613.31
Episode length: 80.00 +/- 20.59
-----------------------------------
| eval/              |            |
|    mean action     | -0.1942462 |
|    mean velocity x | 1.21       |
|    mean velocity y | 1.27       |
|    mean velocity z | 15.7       |
|    mean_ep_length  | 80         |
|    mean_reward     | -7.26e+04  |
| time/              |            |
|    total_timesteps | 654000     |
-----------------------------------
Eval num_timesteps=654500, episode_reward=-54496.52 +/- 43788.02
Episode length: 58.20 +/- 16.88
-----------------------------------
| eval/              |            |
|    mean action     | -0.5547482 |
|    mean velocity x | 1.2        |
|    mean velocity y | 3.93       |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 58.2       |
|    mean_reward     | -5.45e+04  |
| time/              |            |
|    total_timesteps | 654500     |
-----------------------------------
Eval num_timesteps=655000, episode_reward=-91823.36 +/- 22424.06
Episode length: 68.80 +/- 9.70
------------------------------------
| eval/              |             |
|    mean action     | -0.74480003 |
|    mean velocity x | 3.46        |
|    mean velocity y | 4.74        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 68.8        |
|    mean_reward     | -9.18e+04   |
| time/              |             |
|    total_timesteps | 655000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.8      |
|    ep_rew_mean     | -9.71e+04 |
| time/              |           |
|    fps             | 106       |
|    iterations      | 320       |
|    time_elapsed    | 6146      |
|    total_timesteps | 655360    |
----------------------------------
Eval num_timesteps=655500, episode_reward=-68921.24 +/- 38985.86
Episode length: 58.20 +/- 23.16
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.18173066    |
|    mean velocity x      | 0.846         |
|    mean velocity y      | 0.104         |
|    mean velocity z      | 17.3          |
|    mean_ep_length       | 58.2          |
|    mean_reward          | -6.89e+04     |
| time/                   |               |
|    total_timesteps      | 655500        |
| train/                  |               |
|    approx_kl            | 0.00087241526 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.145         |
|    learning_rate        | 0.001         |
|    loss                 | 9.32e+07      |
|    n_updates            | 3200          |
|    policy_gradient_loss | -0.00129      |
|    std                  | 0.919         |
|    value_loss           | 2.25e+08      |
-------------------------------------------
Eval num_timesteps=656000, episode_reward=-44584.28 +/- 40229.37
Episode length: 48.20 +/- 17.15
------------------------------------
| eval/              |             |
|    mean action     | -0.27147684 |
|    mean velocity x | 0.132       |
|    mean velocity y | 0.937       |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 48.2        |
|    mean_reward     | -4.46e+04   |
| time/              |             |
|    total_timesteps | 656000      |
------------------------------------
Eval num_timesteps=656500, episode_reward=-50352.85 +/- 37585.52
Episode length: 63.80 +/- 36.59
------------------------------------
| eval/              |             |
|    mean action     | 0.036206905 |
|    mean velocity x | 0.0911      |
|    mean velocity y | -1.03       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -5.04e+04   |
| time/              |             |
|    total_timesteps | 656500      |
------------------------------------
Eval num_timesteps=657000, episode_reward=-77254.69 +/- 22898.99
Episode length: 72.60 +/- 22.22
------------------------------------
| eval/              |             |
|    mean action     | -0.31298262 |
|    mean velocity x | -0.0272     |
|    mean velocity y | 0.589       |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 72.6        |
|    mean_reward     | -7.73e+04   |
| time/              |             |
|    total_timesteps | 657000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82        |
|    ep_rew_mean     | -9.18e+04 |
| time/              |           |
|    fps             | 106       |
|    iterations      | 321       |
|    time_elapsed    | 6153      |
|    total_timesteps | 657408    |
----------------------------------
Eval num_timesteps=657500, episode_reward=-80964.32 +/- 23641.65
Episode length: 72.00 +/- 5.40
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3249269   |
|    mean velocity x      | 1.53         |
|    mean velocity y      | 1.54         |
|    mean velocity z      | 18.3         |
|    mean_ep_length       | 72           |
|    mean_reward          | -8.1e+04     |
| time/                   |              |
|    total_timesteps      | 657500       |
| train/                  |              |
|    approx_kl            | 7.523125e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+08     |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.00035     |
|    std                  | 0.919        |
|    value_loss           | 2.26e+08     |
------------------------------------------
Eval num_timesteps=658000, episode_reward=-78260.06 +/- 26896.75
Episode length: 93.60 +/- 32.53
------------------------------------
| eval/              |             |
|    mean action     | -0.40543962 |
|    mean velocity x | 0.975       |
|    mean velocity y | 2.95        |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 93.6        |
|    mean_reward     | -7.83e+04   |
| time/              |             |
|    total_timesteps | 658000      |
------------------------------------
Eval num_timesteps=658500, episode_reward=-71345.84 +/- 37715.99
Episode length: 54.40 +/- 20.25
------------------------------------
| eval/              |             |
|    mean action     | -0.24651366 |
|    mean velocity x | 1.82        |
|    mean velocity y | 2.88        |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 658500      |
------------------------------------
Eval num_timesteps=659000, episode_reward=-50931.02 +/- 20816.95
Episode length: 56.80 +/- 9.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.050585784 |
|    mean velocity x | -0.0161      |
|    mean velocity y | -1.62        |
|    mean velocity z | 18.2         |
|    mean_ep_length  | 56.8         |
|    mean_reward     | -5.09e+04    |
| time/              |              |
|    total_timesteps | 659000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.3      |
|    ep_rew_mean     | -8.78e+04 |
| time/              |           |
|    fps             | 107       |
|    iterations      | 322       |
|    time_elapsed    | 6160      |
|    total_timesteps | 659456    |
----------------------------------
Eval num_timesteps=659500, episode_reward=-58815.92 +/- 45571.75
Episode length: 60.40 +/- 24.61
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.11838146   |
|    mean velocity x      | 2.36          |
|    mean velocity y      | 0.959         |
|    mean velocity z      | 21.2          |
|    mean_ep_length       | 60.4          |
|    mean_reward          | -5.88e+04     |
| time/                   |               |
|    total_timesteps      | 659500        |
| train/                  |               |
|    approx_kl            | 0.00020253955 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+08      |
|    n_updates            | 3220          |
|    policy_gradient_loss | -0.000611     |
|    std                  | 0.919         |
|    value_loss           | 2.45e+08      |
-------------------------------------------
Eval num_timesteps=660000, episode_reward=-54527.18 +/- 31457.03
Episode length: 55.00 +/- 13.27
-----------------------------------
| eval/              |            |
|    mean action     | 0.21343267 |
|    mean velocity x | -1.65      |
|    mean velocity y | -1.66      |
|    mean velocity z | 16.3       |
|    mean_ep_length  | 55         |
|    mean_reward     | -5.45e+04  |
| time/              |            |
|    total_timesteps | 660000     |
-----------------------------------
Eval num_timesteps=660500, episode_reward=-75250.03 +/- 12105.14
Episode length: 81.00 +/- 12.21
-----------------------------------
| eval/              |            |
|    mean action     | -0.4313499 |
|    mean velocity x | 1.21       |
|    mean velocity y | 2.53       |
|    mean velocity z | 15.7       |
|    mean_ep_length  | 81         |
|    mean_reward     | -7.53e+04  |
| time/              |            |
|    total_timesteps | 660500     |
-----------------------------------
Eval num_timesteps=661000, episode_reward=-71913.17 +/- 29855.36
Episode length: 65.80 +/- 12.69
-----------------------------------
| eval/              |            |
|    mean action     | 0.15874833 |
|    mean velocity x | -1.13      |
|    mean velocity y | -0.887     |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -7.19e+04  |
| time/              |            |
|    total_timesteps | 661000     |
-----------------------------------
Eval num_timesteps=661500, episode_reward=-79041.39 +/- 27381.03
Episode length: 87.80 +/- 33.70
-----------------------------------
| eval/              |            |
|    mean action     | -0.6629912 |
|    mean velocity x | 3.51       |
|    mean velocity y | 4.25       |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 87.8       |
|    mean_reward     | -7.9e+04   |
| time/              |            |
|    total_timesteps | 661500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.9      |
|    ep_rew_mean     | -7.88e+04 |
| time/              |           |
|    fps             | 107       |
|    iterations      | 323       |
|    time_elapsed    | 6168      |
|    total_timesteps | 661504    |
----------------------------------
Eval num_timesteps=662000, episode_reward=-76052.76 +/- 22291.17
Episode length: 60.40 +/- 6.37
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.71919775   |
|    mean velocity x      | 2.57          |
|    mean velocity y      | 4.86          |
|    mean velocity z      | 20.3          |
|    mean_ep_length       | 60.4          |
|    mean_reward          | -7.61e+04     |
| time/                   |               |
|    total_timesteps      | 662000        |
| train/                  |               |
|    approx_kl            | 0.00022445252 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.181         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+08      |
|    n_updates            | 3230          |
|    policy_gradient_loss | -0.000734     |
|    std                  | 0.919         |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=662500, episode_reward=-60922.40 +/- 42133.28
Episode length: 54.20 +/- 21.39
------------------------------------
| eval/              |             |
|    mean action     | -0.18527974 |
|    mean velocity x | 0.866       |
|    mean velocity y | 1.84        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 54.2        |
|    mean_reward     | -6.09e+04   |
| time/              |             |
|    total_timesteps | 662500      |
------------------------------------
Eval num_timesteps=663000, episode_reward=-71984.60 +/- 25527.55
Episode length: 64.80 +/- 6.05
------------------------------------
| eval/              |             |
|    mean action     | -0.31676188 |
|    mean velocity x | 0.325       |
|    mean velocity y | 0.931       |
|    mean velocity z | 19          |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 663000      |
------------------------------------
Eval num_timesteps=663500, episode_reward=-45169.78 +/- 26422.87
Episode length: 51.00 +/- 18.58
-----------------------------------
| eval/              |            |
|    mean action     | 0.06674164 |
|    mean velocity x | 0.853      |
|    mean velocity y | -0.285     |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 51         |
|    mean_reward     | -4.52e+04  |
| time/              |            |
|    total_timesteps | 663500     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.8     |
|    ep_rew_mean     | -7.9e+04 |
| time/              |          |
|    fps             | 107      |
|    iterations      | 324      |
|    time_elapsed    | 6175     |
|    total_timesteps | 663552   |
---------------------------------
Eval num_timesteps=664000, episode_reward=-65099.91 +/- 13271.93
Episode length: 66.20 +/- 9.95
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.21735787   |
|    mean velocity x      | 0.427         |
|    mean velocity y      | 0.652         |
|    mean velocity z      | 20            |
|    mean_ep_length       | 66.2          |
|    mean_reward          | -6.51e+04     |
| time/                   |               |
|    total_timesteps      | 664000        |
| train/                  |               |
|    approx_kl            | 2.6810245e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+08      |
|    n_updates            | 3240          |
|    policy_gradient_loss | -0.000382     |
|    std                  | 0.919         |
|    value_loss           | 2.43e+08      |
-------------------------------------------
Eval num_timesteps=664500, episode_reward=-68186.35 +/- 24533.56
Episode length: 67.40 +/- 22.90
------------------------------------
| eval/              |             |
|    mean action     | -0.29544526 |
|    mean velocity x | 1.07        |
|    mean velocity y | 2           |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -6.82e+04   |
| time/              |             |
|    total_timesteps | 664500      |
------------------------------------
Eval num_timesteps=665000, episode_reward=-89679.87 +/- 14219.59
Episode length: 68.00 +/- 7.27
------------------------------------
| eval/              |             |
|    mean action     | -0.41493773 |
|    mean velocity x | 0.917       |
|    mean velocity y | 1.54        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 68          |
|    mean_reward     | -8.97e+04   |
| time/              |             |
|    total_timesteps | 665000      |
------------------------------------
Eval num_timesteps=665500, episode_reward=-82133.82 +/- 19863.37
Episode length: 68.60 +/- 9.79
-----------------------------------
| eval/              |            |
|    mean action     | -0.7993523 |
|    mean velocity x | 2.56       |
|    mean velocity y | 5.02       |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 68.6       |
|    mean_reward     | -8.21e+04  |
| time/              |            |
|    total_timesteps | 665500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.4      |
|    ep_rew_mean     | -8.68e+04 |
| time/              |           |
|    fps             | 107       |
|    iterations      | 325       |
|    time_elapsed    | 6182      |
|    total_timesteps | 665600    |
----------------------------------
Eval num_timesteps=666000, episode_reward=-68154.09 +/- 29683.14
Episode length: 58.40 +/- 10.69
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.08514093   |
|    mean velocity x      | -1.03         |
|    mean velocity y      | 0.358         |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 58.4          |
|    mean_reward          | -6.82e+04     |
| time/                   |               |
|    total_timesteps      | 666000        |
| train/                  |               |
|    approx_kl            | 0.00022302449 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+08      |
|    n_updates            | 3250          |
|    policy_gradient_loss | -0.000553     |
|    std                  | 0.919         |
|    value_loss           | 2.62e+08      |
-------------------------------------------
Eval num_timesteps=666500, episode_reward=-93045.63 +/- 24220.11
Episode length: 63.80 +/- 3.19
------------------------------------
| eval/              |             |
|    mean action     | -0.01952156 |
|    mean velocity x | 0.00648     |
|    mean velocity y | -0.404      |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -9.3e+04    |
| time/              |             |
|    total_timesteps | 666500      |
------------------------------------
Eval num_timesteps=667000, episode_reward=-94028.93 +/- 16733.28
Episode length: 67.20 +/- 3.87
------------------------------------
| eval/              |             |
|    mean action     | -0.01772783 |
|    mean velocity x | -0.74       |
|    mean velocity y | -0.683      |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 67.2        |
|    mean_reward     | -9.4e+04    |
| time/              |             |
|    total_timesteps | 667000      |
------------------------------------
Eval num_timesteps=667500, episode_reward=-86864.56 +/- 32362.29
Episode length: 60.40 +/- 5.35
-----------------------------------
| eval/              |            |
|    mean action     | 0.12695467 |
|    mean velocity x | -0.0466    |
|    mean velocity y | -0.539     |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 60.4       |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 667500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.8      |
|    ep_rew_mean     | -8.58e+04 |
| time/              |           |
|    fps             | 107       |
|    iterations      | 326       |
|    time_elapsed    | 6190      |
|    total_timesteps | 667648    |
----------------------------------
Eval num_timesteps=668000, episode_reward=-51660.02 +/- 32872.15
Episode length: 59.60 +/- 30.47
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3991562   |
|    mean velocity x      | 2.16         |
|    mean velocity y      | 3.06         |
|    mean velocity z      | 21           |
|    mean_ep_length       | 59.6         |
|    mean_reward          | -5.17e+04    |
| time/                   |              |
|    total_timesteps      | 668000       |
| train/                  |              |
|    approx_kl            | 0.0018738378 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+08     |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.919        |
|    value_loss           | 2.04e+08     |
------------------------------------------
Eval num_timesteps=668500, episode_reward=-72759.90 +/- 35825.32
Episode length: 67.20 +/- 17.47
------------------------------------
| eval/              |             |
|    mean action     | -0.36380443 |
|    mean velocity x | 0.181       |
|    mean velocity y | 1.02        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 67.2        |
|    mean_reward     | -7.28e+04   |
| time/              |             |
|    total_timesteps | 668500      |
------------------------------------
Eval num_timesteps=669000, episode_reward=-95915.10 +/- 15981.07
Episode length: 72.00 +/- 15.23
-----------------------------------
| eval/              |            |
|    mean action     | -0.6761539 |
|    mean velocity x | 2.43       |
|    mean velocity y | 4.23       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 72         |
|    mean_reward     | -9.59e+04  |
| time/              |            |
|    total_timesteps | 669000     |
-----------------------------------
Eval num_timesteps=669500, episode_reward=-119504.93 +/- 28184.87
Episode length: 92.00 +/- 50.09
-------------------------------------
| eval/              |              |
|    mean action     | -0.096613534 |
|    mean velocity x | 0.0872       |
|    mean velocity y | -0.228       |
|    mean velocity z | 20.3         |
|    mean_ep_length  | 92           |
|    mean_reward     | -1.2e+05     |
| time/              |              |
|    total_timesteps | 669500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.3      |
|    ep_rew_mean     | -9.05e+04 |
| time/              |           |
|    fps             | 108       |
|    iterations      | 327       |
|    time_elapsed    | 6197      |
|    total_timesteps | 669696    |
----------------------------------
Eval num_timesteps=670000, episode_reward=-61720.71 +/- 37302.06
Episode length: 77.40 +/- 35.56
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3129816    |
|    mean velocity x      | 0.479         |
|    mean velocity y      | 2.57          |
|    mean velocity z      | 17.1          |
|    mean_ep_length       | 77.4          |
|    mean_reward          | -6.17e+04     |
| time/                   |               |
|    total_timesteps      | 670000        |
| train/                  |               |
|    approx_kl            | 0.00039876034 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.176         |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 3270          |
|    policy_gradient_loss | -0.000798     |
|    std                  | 0.919         |
|    value_loss           | 2.37e+08      |
-------------------------------------------
Eval num_timesteps=670500, episode_reward=-72696.17 +/- 24895.69
Episode length: 63.80 +/- 8.40
------------------------------------
| eval/              |             |
|    mean action     | -0.30838695 |
|    mean velocity x | 0.662       |
|    mean velocity y | 1.1         |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -7.27e+04   |
| time/              |             |
|    total_timesteps | 670500      |
------------------------------------
Eval num_timesteps=671000, episode_reward=-85593.67 +/- 24178.31
Episode length: 65.00 +/- 9.61
------------------------------------
| eval/              |             |
|    mean action     | -0.37534615 |
|    mean velocity x | 1.8         |
|    mean velocity y | 2.39        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 65          |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 671000      |
------------------------------------
Eval num_timesteps=671500, episode_reward=-91134.09 +/- 22017.08
Episode length: 65.40 +/- 5.85
------------------------------------
| eval/              |             |
|    mean action     | -0.64011526 |
|    mean velocity x | 2.06        |
|    mean velocity y | 3.19        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -9.11e+04   |
| time/              |             |
|    total_timesteps | 671500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.1      |
|    ep_rew_mean     | -8.85e+04 |
| time/              |           |
|    fps             | 108       |
|    iterations      | 328       |
|    time_elapsed    | 6204      |
|    total_timesteps | 671744    |
----------------------------------
Eval num_timesteps=672000, episode_reward=-99999.61 +/- 20349.21
Episode length: 105.20 +/- 37.44
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.08261573    |
|    mean velocity x      | -0.525        |
|    mean velocity y      | -0.359        |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 105           |
|    mean_reward          | -1e+05        |
| time/                   |               |
|    total_timesteps      | 672000        |
| train/                  |               |
|    approx_kl            | 0.00081066624 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.001         |
|    loss                 | 9.47e+07      |
|    n_updates            | 3280          |
|    policy_gradient_loss | -0.00126      |
|    std                  | 0.92          |
|    value_loss           | 1.89e+08      |
-------------------------------------------
Eval num_timesteps=672500, episode_reward=-98103.79 +/- 16170.85
Episode length: 64.60 +/- 2.58
-------------------------------------
| eval/              |              |
|    mean action     | -0.058595743 |
|    mean velocity x | -0.651       |
|    mean velocity y | -0.0982      |
|    mean velocity z | 17.3         |
|    mean_ep_length  | 64.6         |
|    mean_reward     | -9.81e+04    |
| time/              |              |
|    total_timesteps | 672500       |
-------------------------------------
Eval num_timesteps=673000, episode_reward=-80355.92 +/- 27439.42
Episode length: 65.20 +/- 10.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.20335114 |
|    mean velocity x | 0.886      |
|    mean velocity y | -1.73      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 65.2       |
|    mean_reward     | -8.04e+04  |
| time/              |            |
|    total_timesteps | 673000     |
-----------------------------------
Eval num_timesteps=673500, episode_reward=-50695.56 +/- 39490.37
Episode length: 48.40 +/- 18.77
-----------------------------------
| eval/              |            |
|    mean action     | -1.1403457 |
|    mean velocity x | 5.44       |
|    mean velocity y | 7.92       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 48.4       |
|    mean_reward     | -5.07e+04  |
| time/              |            |
|    total_timesteps | 673500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80        |
|    ep_rew_mean     | -8.71e+04 |
| time/              |           |
|    fps             | 108       |
|    iterations      | 329       |
|    time_elapsed    | 6211      |
|    total_timesteps | 673792    |
----------------------------------
Eval num_timesteps=674000, episode_reward=-86129.17 +/- 29369.52
Episode length: 68.40 +/- 14.89
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35718724  |
|    mean velocity x      | 0.627        |
|    mean velocity y      | 1.44         |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 68.4         |
|    mean_reward          | -8.61e+04    |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0028618253 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.001        |
|    loss                 | 8.43e+07     |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.92         |
|    value_loss           | 2.51e+08     |
------------------------------------------
Eval num_timesteps=674500, episode_reward=-96317.48 +/- 13007.48
Episode length: 72.60 +/- 10.61
------------------------------------
| eval/              |             |
|    mean action     | -0.30024955 |
|    mean velocity x | -0.0892     |
|    mean velocity y | -0.145      |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 72.6        |
|    mean_reward     | -9.63e+04   |
| time/              |             |
|    total_timesteps | 674500      |
------------------------------------
Eval num_timesteps=675000, episode_reward=-93449.43 +/- 7923.53
Episode length: 69.00 +/- 7.67
------------------------------------
| eval/              |             |
|    mean action     | -0.41075304 |
|    mean velocity x | 1.76        |
|    mean velocity y | 2.51        |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 69          |
|    mean_reward     | -9.34e+04   |
| time/              |             |
|    total_timesteps | 675000      |
------------------------------------
Eval num_timesteps=675500, episode_reward=-84118.85 +/- 39324.88
Episode length: 60.20 +/- 14.34
-----------------------------------
| eval/              |            |
|    mean action     | -0.9240591 |
|    mean velocity x | 3.84       |
|    mean velocity y | 6.83       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 60.2       |
|    mean_reward     | -8.41e+04  |
| time/              |            |
|    total_timesteps | 675500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.5      |
|    ep_rew_mean     | -9.11e+04 |
| time/              |           |
|    fps             | 108       |
|    iterations      | 330       |
|    time_elapsed    | 6218      |
|    total_timesteps | 675840    |
----------------------------------
Eval num_timesteps=676000, episode_reward=-79682.39 +/- 21097.58
Episode length: 66.60 +/- 7.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6709031   |
|    mean velocity x      | 3.65         |
|    mean velocity y      | 4.2          |
|    mean velocity z      | 16.6         |
|    mean_ep_length       | 66.6         |
|    mean_reward          | -7.97e+04    |
| time/                   |              |
|    total_timesteps      | 676000       |
| train/                  |              |
|    approx_kl            | 0.0028251268 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.201        |
|    learning_rate        | 0.001        |
|    loss                 | 6.86e+07     |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 0.919        |
|    value_loss           | 2.04e+08     |
------------------------------------------
Eval num_timesteps=676500, episode_reward=-76734.86 +/- 26037.68
Episode length: 68.80 +/- 23.03
-----------------------------------
| eval/              |            |
|    mean action     | -1.2084849 |
|    mean velocity x | 4.58       |
|    mean velocity y | 7.31       |
|    mean velocity z | 17         |
|    mean_ep_length  | 68.8       |
|    mean_reward     | -7.67e+04  |
| time/              |            |
|    total_timesteps | 676500     |
-----------------------------------
Eval num_timesteps=677000, episode_reward=-72344.47 +/- 23375.41
Episode length: 73.80 +/- 21.37
-----------------------------------
| eval/              |            |
|    mean action     | -0.6176094 |
|    mean velocity x | 2.83       |
|    mean velocity y | 4.16       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 73.8       |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 677000     |
-----------------------------------
Eval num_timesteps=677500, episode_reward=-41096.01 +/- 40190.35
Episode length: 47.60 +/- 22.01
------------------------------------
| eval/              |             |
|    mean action     | -0.42530453 |
|    mean velocity x | 2.49        |
|    mean velocity y | 2.62        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 47.6        |
|    mean_reward     | -4.11e+04   |
| time/              |             |
|    total_timesteps | 677500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.1      |
|    ep_rew_mean     | -9.11e+04 |
| time/              |           |
|    fps             | 108       |
|    iterations      | 331       |
|    time_elapsed    | 6225      |
|    total_timesteps | 677888    |
----------------------------------
Eval num_timesteps=678000, episode_reward=-70528.85 +/- 22346.53
Episode length: 83.20 +/- 26.78
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.66975904   |
|    mean velocity x      | 1.21          |
|    mean velocity y      | 3.79          |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 83.2          |
|    mean_reward          | -7.05e+04     |
| time/                   |               |
|    total_timesteps      | 678000        |
| train/                  |               |
|    approx_kl            | 5.0269882e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+08      |
|    n_updates            | 3310          |
|    policy_gradient_loss | -0.000288     |
|    std                  | 0.919         |
|    value_loss           | 2.29e+08      |
-------------------------------------------
Eval num_timesteps=678500, episode_reward=-87964.49 +/- 9966.32
Episode length: 67.40 +/- 4.32
------------------------------------
| eval/              |             |
|    mean action     | -0.25207403 |
|    mean velocity x | -0.358      |
|    mean velocity y | 0.581       |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -8.8e+04    |
| time/              |             |
|    total_timesteps | 678500      |
------------------------------------
Eval num_timesteps=679000, episode_reward=-58242.31 +/- 41048.74
Episode length: 57.00 +/- 27.89
------------------------------------
| eval/              |             |
|    mean action     | -0.39114177 |
|    mean velocity x | 3           |
|    mean velocity y | 2.29        |
|    mean velocity z | 15.9        |
|    mean_ep_length  | 57          |
|    mean_reward     | -5.82e+04   |
| time/              |             |
|    total_timesteps | 679000      |
------------------------------------
Eval num_timesteps=679500, episode_reward=-99379.19 +/- 26773.88
Episode length: 95.80 +/- 39.18
------------------------------------
| eval/              |             |
|    mean action     | -0.16169047 |
|    mean velocity x | 0.209       |
|    mean velocity y | -0.487      |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 95.8        |
|    mean_reward     | -9.94e+04   |
| time/              |             |
|    total_timesteps | 679500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 87.2      |
|    ep_rew_mean     | -9.25e+04 |
| time/              |           |
|    fps             | 109       |
|    iterations      | 332       |
|    time_elapsed    | 6233      |
|    total_timesteps | 679936    |
----------------------------------
Eval num_timesteps=680000, episode_reward=-85371.28 +/- 23785.76
Episode length: 65.60 +/- 7.42
------------------------------------------
| eval/                   |              |
|    mean action          | -0.13250518  |
|    mean velocity x      | 0.471        |
|    mean velocity y      | -0.376       |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 65.6         |
|    mean_reward          | -8.54e+04    |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 5.827364e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+08     |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.000292    |
|    std                  | 0.919        |
|    value_loss           | 2.26e+08     |
------------------------------------------
Eval num_timesteps=680500, episode_reward=-73522.53 +/- 40258.13
Episode length: 57.00 +/- 19.72
------------------------------------
| eval/              |             |
|    mean action     | -0.29548332 |
|    mean velocity x | 1.12        |
|    mean velocity y | 2.89        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 57          |
|    mean_reward     | -7.35e+04   |
| time/              |             |
|    total_timesteps | 680500      |
------------------------------------
Eval num_timesteps=681000, episode_reward=-72209.29 +/- 38225.50
Episode length: 67.80 +/- 41.03
------------------------------------
| eval/              |             |
|    mean action     | -0.75544226 |
|    mean velocity x | 2.37        |
|    mean velocity y | 5.98        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 67.8        |
|    mean_reward     | -7.22e+04   |
| time/              |             |
|    total_timesteps | 681000      |
------------------------------------
Eval num_timesteps=681500, episode_reward=-98712.83 +/- 16515.10
Episode length: 67.00 +/- 3.29
------------------------------------
| eval/              |             |
|    mean action     | -0.25342476 |
|    mean velocity x | 1.38        |
|    mean velocity y | 1.75        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 67          |
|    mean_reward     | -9.87e+04   |
| time/              |             |
|    total_timesteps | 681500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83        |
|    ep_rew_mean     | -8.96e+04 |
| time/              |           |
|    fps             | 109       |
|    iterations      | 333       |
|    time_elapsed    | 6240      |
|    total_timesteps | 681984    |
----------------------------------
Eval num_timesteps=682000, episode_reward=-85480.38 +/- 19757.44
Episode length: 63.80 +/- 4.87
------------------------------------------
| eval/                   |              |
|    mean action          | -0.54561603  |
|    mean velocity x      | 1.94         |
|    mean velocity y      | 3.85         |
|    mean velocity z      | 20.7         |
|    mean_ep_length       | 63.8         |
|    mean_reward          | -8.55e+04    |
| time/                   |              |
|    total_timesteps      | 682000       |
| train/                  |              |
|    approx_kl            | 0.0018820825 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.165        |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+08     |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 0.918        |
|    value_loss           | 2.4e+08      |
------------------------------------------
Eval num_timesteps=682500, episode_reward=-38158.32 +/- 31720.72
Episode length: 55.60 +/- 17.84
-----------------------------------
| eval/              |            |
|    mean action     | 0.56913215 |
|    mean velocity x | -0.408     |
|    mean velocity y | -2.91      |
|    mean velocity z | 21         |
|    mean_ep_length  | 55.6       |
|    mean_reward     | -3.82e+04  |
| time/              |            |
|    total_timesteps | 682500     |
-----------------------------------
Eval num_timesteps=683000, episode_reward=-43426.45 +/- 28139.88
Episode length: 51.40 +/- 17.23
-------------------------------------
| eval/              |              |
|    mean action     | -0.029874595 |
|    mean velocity x | -0.363       |
|    mean velocity y | -1.26        |
|    mean velocity z | 21.5         |
|    mean_ep_length  | 51.4         |
|    mean_reward     | -4.34e+04    |
| time/              |              |
|    total_timesteps | 683000       |
-------------------------------------
Eval num_timesteps=683500, episode_reward=-81659.27 +/- 16359.28
Episode length: 65.60 +/- 7.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.13204888 |
|    mean velocity x | -0.522     |
|    mean velocity y | -1.93      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 65.6       |
|    mean_reward     | -8.17e+04  |
| time/              |            |
|    total_timesteps | 683500     |
-----------------------------------
Eval num_timesteps=684000, episode_reward=-88315.69 +/- 13134.27
Episode length: 72.80 +/- 5.64
------------------------------------
| eval/              |             |
|    mean action     | -0.43445313 |
|    mean velocity x | 1.41        |
|    mean velocity y | 3.42        |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 72.8        |
|    mean_reward     | -8.83e+04   |
| time/              |             |
|    total_timesteps | 684000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | -9.2e+04 |
| time/              |          |
|    fps             | 109      |
|    iterations      | 334      |
|    time_elapsed    | 6247     |
|    total_timesteps | 684032   |
---------------------------------
Eval num_timesteps=684500, episode_reward=-58828.10 +/- 46185.53
Episode length: 68.00 +/- 33.02
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6128368   |
|    mean velocity x      | 1.69         |
|    mean velocity y      | 5.12         |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 68           |
|    mean_reward          | -5.88e+04    |
| time/                   |              |
|    total_timesteps      | 684500       |
| train/                  |              |
|    approx_kl            | 0.0019826922 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.162        |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+08     |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00254     |
|    std                  | 0.918        |
|    value_loss           | 2.82e+08     |
------------------------------------------
Eval num_timesteps=685000, episode_reward=-83007.74 +/- 20863.52
Episode length: 67.40 +/- 4.22
----------------------------------
| eval/              |           |
|    mean action     | 0.2934663 |
|    mean velocity x | -1.88     |
|    mean velocity y | -1.81     |
|    mean velocity z | 17.1      |
|    mean_ep_length  | 67.4      |
|    mean_reward     | -8.3e+04  |
| time/              |           |
|    total_timesteps | 685000    |
----------------------------------
Eval num_timesteps=685500, episode_reward=-66736.46 +/- 31148.98
Episode length: 68.40 +/- 20.77
-------------------------------------
| eval/              |              |
|    mean action     | -0.075479805 |
|    mean velocity x | -1.66        |
|    mean velocity y | -2.05        |
|    mean velocity z | 20.3         |
|    mean_ep_length  | 68.4         |
|    mean_reward     | -6.67e+04    |
| time/              |              |
|    total_timesteps | 685500       |
-------------------------------------
Eval num_timesteps=686000, episode_reward=-46323.12 +/- 38869.82
Episode length: 46.40 +/- 16.38
------------------------------------
| eval/              |             |
|    mean action     | -0.94231784 |
|    mean velocity x | 3.27        |
|    mean velocity y | 5.62        |
|    mean velocity z | 17          |
|    mean_ep_length  | 46.4        |
|    mean_reward     | -4.63e+04   |
| time/              |             |
|    total_timesteps | 686000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.8      |
|    ep_rew_mean     | -9.14e+04 |
| time/              |           |
|    fps             | 109       |
|    iterations      | 335       |
|    time_elapsed    | 6255      |
|    total_timesteps | 686080    |
----------------------------------
Eval num_timesteps=686500, episode_reward=-56912.65 +/- 33283.13
Episode length: 61.60 +/- 19.48
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32907057   |
|    mean velocity x      | 1.86          |
|    mean velocity y      | 3.15          |
|    mean velocity z      | 17.4          |
|    mean_ep_length       | 61.6          |
|    mean_reward          | -5.69e+04     |
| time/                   |               |
|    total_timesteps      | 686500        |
| train/                  |               |
|    approx_kl            | 0.00010126195 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.001         |
|    loss                 | 8.56e+07      |
|    n_updates            | 3350          |
|    policy_gradient_loss | -0.00059      |
|    std                  | 0.918         |
|    value_loss           | 1.94e+08      |
-------------------------------------------
Eval num_timesteps=687000, episode_reward=-38540.04 +/- 23777.37
Episode length: 53.40 +/- 18.90
-----------------------------------
| eval/              |            |
|    mean action     | 0.06675305 |
|    mean velocity x | 1          |
|    mean velocity y | 0.52       |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 53.4       |
|    mean_reward     | -3.85e+04  |
| time/              |            |
|    total_timesteps | 687000     |
-----------------------------------
Eval num_timesteps=687500, episode_reward=-82464.14 +/- 15294.21
Episode length: 76.00 +/- 9.70
-----------------------------------
| eval/              |            |
|    mean action     | -0.7207921 |
|    mean velocity x | 1.7        |
|    mean velocity y | 3.54       |
|    mean velocity z | 16.5       |
|    mean_ep_length  | 76         |
|    mean_reward     | -8.25e+04  |
| time/              |            |
|    total_timesteps | 687500     |
-----------------------------------
Eval num_timesteps=688000, episode_reward=-58747.41 +/- 39777.37
Episode length: 72.20 +/- 37.28
-----------------------------------
| eval/              |            |
|    mean action     | -0.6584569 |
|    mean velocity x | 2.46       |
|    mean velocity y | 4.47       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 72.2       |
|    mean_reward     | -5.87e+04  |
| time/              |            |
|    total_timesteps | 688000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 80.4      |
|    ep_rew_mean     | -8.88e+04 |
| time/              |           |
|    fps             | 109       |
|    iterations      | 336       |
|    time_elapsed    | 6262      |
|    total_timesteps | 688128    |
----------------------------------
Eval num_timesteps=688500, episode_reward=-82814.93 +/- 19718.03
Episode length: 63.40 +/- 3.77
------------------------------------------
| eval/                   |              |
|    mean action          | -0.20837496  |
|    mean velocity x      | -0.293       |
|    mean velocity y      | -0.199       |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 63.4         |
|    mean_reward          | -8.28e+04    |
| time/                   |              |
|    total_timesteps      | 688500       |
| train/                  |              |
|    approx_kl            | 0.0024633356 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.001        |
|    loss                 | 8.25e+07     |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00213     |
|    std                  | 0.918        |
|    value_loss           | 2.09e+08     |
------------------------------------------
Eval num_timesteps=689000, episode_reward=-48061.99 +/- 19893.66
Episode length: 58.20 +/- 9.62
-----------------------------------
| eval/              |            |
|    mean action     | -0.8199507 |
|    mean velocity x | 1.15       |
|    mean velocity y | 5.16       |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 58.2       |
|    mean_reward     | -4.81e+04  |
| time/              |            |
|    total_timesteps | 689000     |
-----------------------------------
Eval num_timesteps=689500, episode_reward=-84448.89 +/- 35555.95
Episode length: 70.60 +/- 28.72
-------------------------------------
| eval/              |              |
|    mean action     | -0.053101826 |
|    mean velocity x | 0.0631       |
|    mean velocity y | -0.0441      |
|    mean velocity z | 23           |
|    mean_ep_length  | 70.6         |
|    mean_reward     | -8.44e+04    |
| time/              |              |
|    total_timesteps | 689500       |
-------------------------------------
Eval num_timesteps=690000, episode_reward=-85991.68 +/- 17075.10
Episode length: 77.60 +/- 12.56
------------------------------------
| eval/              |             |
|    mean action     | 0.053842917 |
|    mean velocity x | 1.79        |
|    mean velocity y | 0.718       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 77.6        |
|    mean_reward     | -8.6e+04    |
| time/              |             |
|    total_timesteps | 690000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.1      |
|    ep_rew_mean     | -9.21e+04 |
| time/              |           |
|    fps             | 110       |
|    iterations      | 337       |
|    time_elapsed    | 6269      |
|    total_timesteps | 690176    |
----------------------------------
Eval num_timesteps=690500, episode_reward=-74217.34 +/- 38954.39
Episode length: 54.00 +/- 21.74
------------------------------------------
| eval/                   |              |
|    mean action          | -0.58239484  |
|    mean velocity x      | 2.36         |
|    mean velocity y      | 3.7          |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 54           |
|    mean_reward          | -7.42e+04    |
| time/                   |              |
|    total_timesteps      | 690500       |
| train/                  |              |
|    approx_kl            | 0.0016594897 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+08     |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 0.918        |
|    value_loss           | 2.81e+08     |
------------------------------------------
Eval num_timesteps=691000, episode_reward=-39067.90 +/- 41367.09
Episode length: 41.00 +/- 16.86
-------------------------------------
| eval/              |              |
|    mean action     | -0.090234585 |
|    mean velocity x | 1.43         |
|    mean velocity y | 1.07         |
|    mean velocity z | 20.7         |
|    mean_ep_length  | 41           |
|    mean_reward     | -3.91e+04    |
| time/              |              |
|    total_timesteps | 691000       |
-------------------------------------
Eval num_timesteps=691500, episode_reward=-97442.56 +/- 16097.11
Episode length: 69.80 +/- 7.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.05268399 |
|    mean velocity x | -0.601     |
|    mean velocity y | -0.142     |
|    mean velocity z | 15.6       |
|    mean_ep_length  | 69.8       |
|    mean_reward     | -9.74e+04  |
| time/              |            |
|    total_timesteps | 691500     |
-----------------------------------
Eval num_timesteps=692000, episode_reward=-45646.91 +/- 33873.20
Episode length: 47.00 +/- 19.60
-----------------------------------
| eval/              |            |
|    mean action     | 0.17811017 |
|    mean velocity x | -0.769     |
|    mean velocity y | -0.481     |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 47         |
|    mean_reward     | -4.56e+04  |
| time/              |            |
|    total_timesteps | 692000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.7      |
|    ep_rew_mean     | -8.53e+04 |
| time/              |           |
|    fps             | 110       |
|    iterations      | 338       |
|    time_elapsed    | 6276      |
|    total_timesteps | 692224    |
----------------------------------
Eval num_timesteps=692500, episode_reward=-55391.44 +/- 41239.82
Episode length: 52.00 +/- 18.56
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.47227404   |
|    mean velocity x      | 1.26          |
|    mean velocity y      | 1.31          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 52            |
|    mean_reward          | -5.54e+04     |
| time/                   |               |
|    total_timesteps      | 692500        |
| train/                  |               |
|    approx_kl            | 0.00037329353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+08      |
|    n_updates            | 3380          |
|    policy_gradient_loss | -0.000786     |
|    std                  | 0.918         |
|    value_loss           | 2.03e+08      |
-------------------------------------------
Eval num_timesteps=693000, episode_reward=-50782.85 +/- 42797.88
Episode length: 47.00 +/- 15.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.4672326 |
|    mean velocity x | 0.268      |
|    mean velocity y | 3.09       |
|    mean velocity z | 21.5       |
|    mean_ep_length  | 47         |
|    mean_reward     | -5.08e+04  |
| time/              |            |
|    total_timesteps | 693000     |
-----------------------------------
Eval num_timesteps=693500, episode_reward=-48115.84 +/- 44054.07
Episode length: 50.20 +/- 23.96
------------------------------------
| eval/              |             |
|    mean action     | -0.10316259 |
|    mean velocity x | 1.46        |
|    mean velocity y | 0.635       |
|    mean velocity z | 21.8        |
|    mean_ep_length  | 50.2        |
|    mean_reward     | -4.81e+04   |
| time/              |             |
|    total_timesteps | 693500      |
------------------------------------
Eval num_timesteps=694000, episode_reward=-52346.53 +/- 42807.15
Episode length: 44.00 +/- 21.65
------------------------------------
| eval/              |             |
|    mean action     | -0.72115296 |
|    mean velocity x | 0.664       |
|    mean velocity y | 3.24        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 44          |
|    mean_reward     | -5.23e+04   |
| time/              |             |
|    total_timesteps | 694000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.7      |
|    ep_rew_mean     | -8.54e+04 |
| time/              |           |
|    fps             | 110       |
|    iterations      | 339       |
|    time_elapsed    | 6283      |
|    total_timesteps | 694272    |
----------------------------------
Eval num_timesteps=694500, episode_reward=-47303.93 +/- 30737.48
Episode length: 51.20 +/- 18.26
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34055066  |
|    mean velocity x      | 0.471        |
|    mean velocity y      | 2.13         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 51.2         |
|    mean_reward          | -4.73e+04    |
| time/                   |              |
|    total_timesteps      | 694500       |
| train/                  |              |
|    approx_kl            | 7.409943e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.001        |
|    loss                 | 1.67e+08     |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000459    |
|    std                  | 0.918        |
|    value_loss           | 2.77e+08     |
------------------------------------------
Eval num_timesteps=695000, episode_reward=-80595.60 +/- 16266.90
Episode length: 72.20 +/- 13.48
------------------------------------
| eval/              |             |
|    mean action     | -0.57767344 |
|    mean velocity x | 1.89        |
|    mean velocity y | 3.56        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 72.2        |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 695000      |
------------------------------------
Eval num_timesteps=695500, episode_reward=-56912.84 +/- 36194.12
Episode length: 55.00 +/- 20.01
------------------------------------
| eval/              |             |
|    mean action     | -0.40446565 |
|    mean velocity x | 3.1         |
|    mean velocity y | 3.29        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 55          |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 695500      |
------------------------------------
Eval num_timesteps=696000, episode_reward=-54660.49 +/- 46619.96
Episode length: 59.20 +/- 40.94
------------------------------------
| eval/              |             |
|    mean action     | -0.14886941 |
|    mean velocity x | 0.735       |
|    mean velocity y | 1.35        |
|    mean velocity z | 18          |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -5.47e+04   |
| time/              |             |
|    total_timesteps | 696000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.5      |
|    ep_rew_mean     | -8.26e+04 |
| time/              |           |
|    fps             | 110       |
|    iterations      | 340       |
|    time_elapsed    | 6290      |
|    total_timesteps | 696320    |
----------------------------------
Eval num_timesteps=696500, episode_reward=-87686.55 +/- 25770.39
Episode length: 74.80 +/- 30.02
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.17652613   |
|    mean velocity x      | -0.167        |
|    mean velocity y      | 1.19          |
|    mean velocity z      | 20.7          |
|    mean_ep_length       | 74.8          |
|    mean_reward          | -8.77e+04     |
| time/                   |               |
|    total_timesteps      | 696500        |
| train/                  |               |
|    approx_kl            | 0.00040292923 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.194         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+08      |
|    n_updates            | 3400          |
|    policy_gradient_loss | -0.000955     |
|    std                  | 0.918         |
|    value_loss           | 2.1e+08       |
-------------------------------------------
Eval num_timesteps=697000, episode_reward=-87386.21 +/- 14488.97
Episode length: 79.40 +/- 18.28
-----------------------------------
| eval/              |            |
|    mean action     | 0.14051458 |
|    mean velocity x | 0.0474     |
|    mean velocity y | -1.61      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 79.4       |
|    mean_reward     | -8.74e+04  |
| time/              |            |
|    total_timesteps | 697000     |
-----------------------------------
Eval num_timesteps=697500, episode_reward=-77141.63 +/- 19570.79
Episode length: 61.60 +/- 8.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.19535135 |
|    mean velocity x | -0.382     |
|    mean velocity y | -1.41      |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -7.71e+04  |
| time/              |            |
|    total_timesteps | 697500     |
-----------------------------------
Eval num_timesteps=698000, episode_reward=-62837.99 +/- 31909.37
Episode length: 66.20 +/- 23.16
------------------------------------
| eval/              |             |
|    mean action     | -0.60229623 |
|    mean velocity x | 2.5         |
|    mean velocity y | 4.04        |
|    mean velocity z | 16.3        |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 698000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.4      |
|    ep_rew_mean     | -8.39e+04 |
| time/              |           |
|    fps             | 110       |
|    iterations      | 341       |
|    time_elapsed    | 6297      |
|    total_timesteps | 698368    |
----------------------------------
Eval num_timesteps=698500, episode_reward=-69273.11 +/- 35953.38
Episode length: 58.60 +/- 23.42
------------------------------------------
| eval/                   |              |
|    mean action          | -0.61126405  |
|    mean velocity x      | 1.94         |
|    mean velocity y      | 4.17         |
|    mean velocity z      | 17.2         |
|    mean_ep_length       | 58.6         |
|    mean_reward          | -6.93e+04    |
| time/                   |              |
|    total_timesteps      | 698500       |
| train/                  |              |
|    approx_kl            | 0.0002580125 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.001        |
|    loss                 | 7.8e+07      |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 0.918        |
|    value_loss           | 1.91e+08     |
------------------------------------------
Eval num_timesteps=699000, episode_reward=-75805.42 +/- 26346.14
Episode length: 62.20 +/- 7.83
------------------------------------
| eval/              |             |
|    mean action     | -0.18241706 |
|    mean velocity x | 1.26        |
|    mean velocity y | 2.32        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 699000      |
------------------------------------
Eval num_timesteps=699500, episode_reward=-52733.10 +/- 40420.28
Episode length: 46.20 +/- 20.13
-----------------------------------
| eval/              |            |
|    mean action     | -0.5404038 |
|    mean velocity x | -0.53      |
|    mean velocity y | 2.7        |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 46.2       |
|    mean_reward     | -5.27e+04  |
| time/              |            |
|    total_timesteps | 699500     |
-----------------------------------
Eval num_timesteps=700000, episode_reward=-53079.27 +/- 37534.45
Episode length: 58.80 +/- 30.46
----------------------------------
| eval/              |           |
|    mean action     | -0.510184 |
|    mean velocity x | 1.98      |
|    mean velocity y | 4.36      |
|    mean velocity z | 20.1      |
|    mean_ep_length  | 58.8      |
|    mean_reward     | -5.31e+04 |
| time/              |           |
|    total_timesteps | 700000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.4      |
|    ep_rew_mean     | -8.62e+04 |
| time/              |           |
|    fps             | 111       |
|    iterations      | 342       |
|    time_elapsed    | 6304      |
|    total_timesteps | 700416    |
----------------------------------
Eval num_timesteps=700500, episode_reward=-80387.78 +/- 33447.95
Episode length: 58.80 +/- 8.28
------------------------------------------
| eval/                   |              |
|    mean action          | -0.10083768  |
|    mean velocity x      | 0.232        |
|    mean velocity y      | 0.533        |
|    mean velocity z      | 23.6         |
|    mean_ep_length       | 58.8         |
|    mean_reward          | -8.04e+04    |
| time/                   |              |
|    total_timesteps      | 700500       |
| train/                  |              |
|    approx_kl            | 0.0025202357 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.001        |
|    loss                 | 1.33e+08     |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.918        |
|    value_loss           | 2.77e+08     |
------------------------------------------
Eval num_timesteps=701000, episode_reward=-53239.02 +/- 29464.87
Episode length: 50.40 +/- 12.94
-----------------------------------
| eval/              |            |
|    mean action     | -0.3583755 |
|    mean velocity x | -0.157     |
|    mean velocity y | 1.06       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 50.4       |
|    mean_reward     | -5.32e+04  |
| time/              |            |
|    total_timesteps | 701000     |
-----------------------------------
Eval num_timesteps=701500, episode_reward=-78993.13 +/- 64687.89
Episode length: 90.20 +/- 82.81
-------------------------------------
| eval/              |              |
|    mean action     | -0.105375394 |
|    mean velocity x | 1.06         |
|    mean velocity y | 0.907        |
|    mean velocity z | 20.1         |
|    mean_ep_length  | 90.2         |
|    mean_reward     | -7.9e+04     |
| time/              |              |
|    total_timesteps | 701500       |
-------------------------------------
Eval num_timesteps=702000, episode_reward=-105455.40 +/- 8275.99
Episode length: 66.00 +/- 1.67
----------------------------------
| eval/              |           |
|    mean action     | 0.3403646 |
|    mean velocity x | -0.582    |
|    mean velocity y | -1.69     |
|    mean velocity z | 17.1      |
|    mean_ep_length  | 66        |
|    mean_reward     | -1.05e+05 |
| time/              |           |
|    total_timesteps | 702000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.9      |
|    ep_rew_mean     | -8.75e+04 |
| time/              |           |
|    fps             | 111       |
|    iterations      | 343       |
|    time_elapsed    | 6311      |
|    total_timesteps | 702464    |
----------------------------------
Eval num_timesteps=702500, episode_reward=-73414.36 +/- 39949.75
Episode length: 53.80 +/- 14.85
------------------------------------------
| eval/                   |              |
|    mean action          | -0.47573435  |
|    mean velocity x      | 0.376        |
|    mean velocity y      | 2.64         |
|    mean velocity z      | 20.5         |
|    mean_ep_length       | 53.8         |
|    mean_reward          | -7.34e+04    |
| time/                   |              |
|    total_timesteps      | 702500       |
| train/                  |              |
|    approx_kl            | 0.0003184421 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.177        |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+08     |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.000882    |
|    std                  | 0.918        |
|    value_loss           | 2.58e+08     |
------------------------------------------
Eval num_timesteps=703000, episode_reward=-76197.78 +/- 24944.07
Episode length: 57.60 +/- 6.47
------------------------------------
| eval/              |             |
|    mean action     | -0.41654474 |
|    mean velocity x | 1.65        |
|    mean velocity y | 1.95        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -7.62e+04   |
| time/              |             |
|    total_timesteps | 703000      |
------------------------------------
Eval num_timesteps=703500, episode_reward=-86502.75 +/- 22511.51
Episode length: 72.40 +/- 21.92
-----------------------------------
| eval/              |            |
|    mean action     | 0.16288202 |
|    mean velocity x | -1.4       |
|    mean velocity y | -3.01      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 72.4       |
|    mean_reward     | -8.65e+04  |
| time/              |            |
|    total_timesteps | 703500     |
-----------------------------------
Eval num_timesteps=704000, episode_reward=-70330.28 +/- 32670.75
Episode length: 65.80 +/- 16.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.12962747 |
|    mean velocity x | 0.117      |
|    mean velocity y | -1.51      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -7.03e+04  |
| time/              |            |
|    total_timesteps | 704000     |
-----------------------------------
Eval num_timesteps=704500, episode_reward=-78534.80 +/- 22510.54
Episode length: 73.20 +/- 17.66
-------------------------------------
| eval/              |              |
|    mean action     | -0.055002607 |
|    mean velocity x | -1.47        |
|    mean velocity y | -0.181       |
|    mean velocity z | 19.8         |
|    mean_ep_length  | 73.2         |
|    mean_reward     | -7.85e+04    |
| time/              |              |
|    total_timesteps | 704500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.5      |
|    ep_rew_mean     | -9.02e+04 |
| time/              |           |
|    fps             | 111       |
|    iterations      | 344       |
|    time_elapsed    | 6319      |
|    total_timesteps | 704512    |
----------------------------------
Eval num_timesteps=705000, episode_reward=-92731.92 +/- 10297.92
Episode length: 69.80 +/- 11.65
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.62713486   |
|    mean velocity x      | 1.8           |
|    mean velocity y      | 2.75          |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 69.8          |
|    mean_reward          | -9.27e+04     |
| time/                   |               |
|    total_timesteps      | 705000        |
| train/                  |               |
|    approx_kl            | 7.4258656e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.144         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+08      |
|    n_updates            | 3440          |
|    policy_gradient_loss | -0.000456     |
|    std                  | 0.917         |
|    value_loss           | 2.64e+08      |
-------------------------------------------
Eval num_timesteps=705500, episode_reward=-76063.29 +/- 30365.12
Episode length: 73.60 +/- 24.74
------------------------------------
| eval/              |             |
|    mean action     | -0.12602133 |
|    mean velocity x | 0.322       |
|    mean velocity y | 0.117       |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 73.6        |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 705500      |
------------------------------------
Eval num_timesteps=706000, episode_reward=-76388.41 +/- 27902.06
Episode length: 59.40 +/- 7.76
------------------------------------
| eval/              |             |
|    mean action     | -0.30390084 |
|    mean velocity x | 0.974       |
|    mean velocity y | 1.97        |
|    mean velocity z | 22.2        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -7.64e+04   |
| time/              |             |
|    total_timesteps | 706000      |
------------------------------------
Eval num_timesteps=706500, episode_reward=-47697.50 +/- 36726.86
Episode length: 58.20 +/- 29.69
------------------------------------
| eval/              |             |
|    mean action     | -0.40778884 |
|    mean velocity x | -0.338      |
|    mean velocity y | 1.77        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -4.77e+04   |
| time/              |             |
|    total_timesteps | 706500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -9.29e+04 |
| time/              |           |
|    fps             | 111       |
|    iterations      | 345       |
|    time_elapsed    | 6326      |
|    total_timesteps | 706560    |
----------------------------------
Eval num_timesteps=707000, episode_reward=-55328.51 +/- 50310.18
Episode length: 45.80 +/- 21.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.32385907  |
|    mean velocity x      | 1.34         |
|    mean velocity y      | 2.29         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 45.8         |
|    mean_reward          | -5.53e+04    |
| time/                   |              |
|    total_timesteps      | 707000       |
| train/                  |              |
|    approx_kl            | 0.0001431217 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.152        |
|    learning_rate        | 0.001        |
|    loss                 | 8.82e+07     |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.000639    |
|    std                  | 0.917        |
|    value_loss           | 2.45e+08     |
------------------------------------------
Eval num_timesteps=707500, episode_reward=-82082.33 +/- 27162.73
Episode length: 60.40 +/- 6.89
------------------------------------
| eval/              |             |
|    mean action     | 0.010982628 |
|    mean velocity x | 0.147       |
|    mean velocity y | -0.263      |
|    mean velocity z | 19          |
|    mean_ep_length  | 60.4        |
|    mean_reward     | -8.21e+04   |
| time/              |             |
|    total_timesteps | 707500      |
------------------------------------
Eval num_timesteps=708000, episode_reward=-89470.98 +/- 23764.39
Episode length: 66.20 +/- 3.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.28250214 |
|    mean velocity x | -1.26      |
|    mean velocity y | -2.3       |
|    mean velocity z | 19         |
|    mean_ep_length  | 66.2       |
|    mean_reward     | -8.95e+04  |
| time/              |            |
|    total_timesteps | 708000     |
-----------------------------------
Eval num_timesteps=708500, episode_reward=-70158.77 +/- 32354.26
Episode length: 62.60 +/- 16.75
-----------------------------------
| eval/              |            |
|    mean action     | 0.08134482 |
|    mean velocity x | -1.17      |
|    mean velocity y | -0.349     |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -7.02e+04  |
| time/              |            |
|    total_timesteps | 708500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.9      |
|    ep_rew_mean     | -9.32e+04 |
| time/              |           |
|    fps             | 111       |
|    iterations      | 346       |
|    time_elapsed    | 6333      |
|    total_timesteps | 708608    |
----------------------------------
Eval num_timesteps=709000, episode_reward=-65357.95 +/- 33645.86
Episode length: 55.80 +/- 9.99
------------------------------------------
| eval/                   |              |
|    mean action          | 0.21297272   |
|    mean velocity x      | 0.599        |
|    mean velocity y      | -0.535       |
|    mean velocity z      | 19.8         |
|    mean_ep_length       | 55.8         |
|    mean_reward          | -6.54e+04    |
| time/                   |              |
|    total_timesteps      | 709000       |
| train/                  |              |
|    approx_kl            | 0.0006788828 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.001        |
|    loss                 | 1.49e+08     |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.918        |
|    value_loss           | 2.4e+08      |
------------------------------------------
Eval num_timesteps=709500, episode_reward=-66954.14 +/- 50364.10
Episode length: 71.80 +/- 56.98
------------------------------------
| eval/              |             |
|    mean action     | -0.22387001 |
|    mean velocity x | -0.593      |
|    mean velocity y | 0.407       |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 71.8        |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 709500      |
------------------------------------
Eval num_timesteps=710000, episode_reward=-69830.57 +/- 38645.94
Episode length: 81.00 +/- 49.05
------------------------------------
| eval/              |             |
|    mean action     | -0.26278406 |
|    mean velocity x | 1.01        |
|    mean velocity y | 1.39        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 81          |
|    mean_reward     | -6.98e+04   |
| time/              |             |
|    total_timesteps | 710000      |
------------------------------------
Eval num_timesteps=710500, episode_reward=-75886.81 +/- 27917.17
Episode length: 70.20 +/- 20.50
-----------------------------------
| eval/              |            |
|    mean action     | -0.3333131 |
|    mean velocity x | -0.177     |
|    mean velocity y | 2.07       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 70.2       |
|    mean_reward     | -7.59e+04  |
| time/              |            |
|    total_timesteps | 710500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 79.3      |
|    ep_rew_mean     | -9.12e+04 |
| time/              |           |
|    fps             | 112       |
|    iterations      | 347       |
|    time_elapsed    | 6341      |
|    total_timesteps | 710656    |
----------------------------------
Eval num_timesteps=711000, episode_reward=-91963.29 +/- 18012.64
Episode length: 73.40 +/- 10.59
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.26413068 |
|    mean velocity x      | 0.193       |
|    mean velocity y      | 0.147       |
|    mean velocity z      | 18.2        |
|    mean_ep_length       | 73.4        |
|    mean_reward          | -9.2e+04    |
| time/                   |             |
|    total_timesteps      | 711000      |
| train/                  |             |
|    approx_kl            | 6.90833e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.001       |
|    loss                 | 5.45e+07    |
|    n_updates            | 3470        |
|    policy_gradient_loss | -0.000329   |
|    std                  | 0.918       |
|    value_loss           | 1.97e+08    |
-----------------------------------------
Eval num_timesteps=711500, episode_reward=-68695.54 +/- 33584.38
Episode length: 68.40 +/- 36.46
------------------------------------
| eval/              |             |
|    mean action     | -0.06440686 |
|    mean velocity x | -1.95       |
|    mean velocity y | -1.37       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -6.87e+04   |
| time/              |             |
|    total_timesteps | 711500      |
------------------------------------
Eval num_timesteps=712000, episode_reward=-70144.60 +/- 38832.14
Episode length: 58.00 +/- 13.15
-----------------------------------
| eval/              |            |
|    mean action     | -0.9737789 |
|    mean velocity x | 3.55       |
|    mean velocity y | 7.46       |
|    mean velocity z | 16         |
|    mean_ep_length  | 58         |
|    mean_reward     | -7.01e+04  |
| time/              |            |
|    total_timesteps | 712000     |
-----------------------------------
Eval num_timesteps=712500, episode_reward=-44146.76 +/- 47306.19
Episode length: 44.60 +/- 31.53
------------------------------------
| eval/              |             |
|    mean action     | -0.44107348 |
|    mean velocity x | 0.418       |
|    mean velocity y | 1.6         |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 44.6        |
|    mean_reward     | -4.41e+04   |
| time/              |             |
|    total_timesteps | 712500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.8      |
|    ep_rew_mean     | -9.16e+04 |
| time/              |           |
|    fps             | 112       |
|    iterations      | 348       |
|    time_elapsed    | 6348      |
|    total_timesteps | 712704    |
----------------------------------
Eval num_timesteps=713000, episode_reward=-41576.36 +/- 29582.61
Episode length: 46.80 +/- 22.69
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.240158      |
|    mean velocity x      | -1.56         |
|    mean velocity y      | -1.01         |
|    mean velocity z      | 17.9          |
|    mean_ep_length       | 46.8          |
|    mean_reward          | -4.16e+04     |
| time/                   |               |
|    total_timesteps      | 713000        |
| train/                  |               |
|    approx_kl            | 0.00040509104 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.189         |
|    learning_rate        | 0.001         |
|    loss                 | 4.81e+07      |
|    n_updates            | 3480          |
|    policy_gradient_loss | -0.00109      |
|    std                  | 0.917         |
|    value_loss           | 2.03e+08      |
-------------------------------------------
Eval num_timesteps=713500, episode_reward=-67713.64 +/- 34902.39
Episode length: 62.20 +/- 19.10
------------------------------------
| eval/              |             |
|    mean action     | -0.87881154 |
|    mean velocity x | 2.45        |
|    mean velocity y | 4.48        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -6.77e+04   |
| time/              |             |
|    total_timesteps | 713500      |
------------------------------------
Eval num_timesteps=714000, episode_reward=-64027.35 +/- 38630.97
Episode length: 55.20 +/- 25.17
-------------------------------------
| eval/              |              |
|    mean action     | -0.064870104 |
|    mean velocity x | 0.757        |
|    mean velocity y | -0.388       |
|    mean velocity z | 21           |
|    mean_ep_length  | 55.2         |
|    mean_reward     | -6.4e+04     |
| time/              |              |
|    total_timesteps | 714000       |
-------------------------------------
Eval num_timesteps=714500, episode_reward=-78959.22 +/- 10470.37
Episode length: 73.40 +/- 10.78
------------------------------------
| eval/              |             |
|    mean action     | -0.34352443 |
|    mean velocity x | 1.95        |
|    mean velocity y | 2.78        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 73.4        |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 714500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83        |
|    ep_rew_mean     | -9.04e+04 |
| time/              |           |
|    fps             | 112       |
|    iterations      | 349       |
|    time_elapsed    | 6355      |
|    total_timesteps | 714752    |
----------------------------------
Eval num_timesteps=715000, episode_reward=-63811.88 +/- 33648.18
Episode length: 60.20 +/- 16.90
----------------------------------------
| eval/                   |            |
|    mean action          | 0.20092179 |
|    mean velocity x      | -1.05      |
|    mean velocity y      | -1.99      |
|    mean velocity z      | 18.9       |
|    mean_ep_length       | 60.2       |
|    mean_reward          | -6.38e+04  |
| time/                   |            |
|    total_timesteps      | 715000     |
| train/                  |            |
|    approx_kl            | 0.00253049 |
|    clip_fraction        | 0.00874    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4         |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.001      |
|    loss                 | 6.95e+07   |
|    n_updates            | 3490       |
|    policy_gradient_loss | -0.00243   |
|    std                  | 0.917      |
|    value_loss           | 2.24e+08   |
----------------------------------------
Eval num_timesteps=715500, episode_reward=-87732.64 +/- 18697.85
Episode length: 73.80 +/- 19.63
------------------------------------
| eval/              |             |
|    mean action     | -0.88452893 |
|    mean velocity x | 3.63        |
|    mean velocity y | 4.85        |
|    mean velocity z | 16.2        |
|    mean_ep_length  | 73.8        |
|    mean_reward     | -8.77e+04   |
| time/              |             |
|    total_timesteps | 715500      |
------------------------------------
Eval num_timesteps=716000, episode_reward=-70865.72 +/- 20809.26
Episode length: 67.40 +/- 14.07
------------------------------------
| eval/              |             |
|    mean action     | -0.50392616 |
|    mean velocity x | 4.03        |
|    mean velocity y | 3.65        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 716000      |
------------------------------------
Eval num_timesteps=716500, episode_reward=-52570.58 +/- 28612.04
Episode length: 58.80 +/- 17.41
-----------------------------------
| eval/              |            |
|    mean action     | -0.6025933 |
|    mean velocity x | 1.73       |
|    mean velocity y | 2.51       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -5.26e+04  |
| time/              |            |
|    total_timesteps | 716500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 82.4      |
|    ep_rew_mean     | -8.69e+04 |
| time/              |           |
|    fps             | 112       |
|    iterations      | 350       |
|    time_elapsed    | 6362      |
|    total_timesteps | 716800    |
----------------------------------
Eval num_timesteps=717000, episode_reward=-66816.56 +/- 20843.43
Episode length: 77.40 +/- 17.90
------------------------------------------
| eval/                   |              |
|    mean action          | 0.08907084   |
|    mean velocity x      | -2.48        |
|    mean velocity y      | -0.67        |
|    mean velocity z      | 16.6         |
|    mean_ep_length       | 77.4         |
|    mean_reward          | -6.68e+04    |
| time/                   |              |
|    total_timesteps      | 717000       |
| train/                  |              |
|    approx_kl            | 0.0030182505 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.99        |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.001        |
|    loss                 | 8.83e+07     |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.00355     |
|    std                  | 0.916        |
|    value_loss           | 1.77e+08     |
------------------------------------------
Eval num_timesteps=717500, episode_reward=-72534.05 +/- 39227.80
Episode length: 60.00 +/- 13.90
------------------------------------
| eval/              |             |
|    mean action     | -0.25843588 |
|    mean velocity x | 0.83        |
|    mean velocity y | 2.22        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 60          |
|    mean_reward     | -7.25e+04   |
| time/              |             |
|    total_timesteps | 717500      |
------------------------------------
Eval num_timesteps=718000, episode_reward=-48320.90 +/- 37877.28
Episode length: 47.40 +/- 17.95
------------------------------------
| eval/              |             |
|    mean action     | -0.12832554 |
|    mean velocity x | -0.173      |
|    mean velocity y | -0.562      |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 47.4        |
|    mean_reward     | -4.83e+04   |
| time/              |             |
|    total_timesteps | 718000      |
------------------------------------
Eval num_timesteps=718500, episode_reward=-63080.13 +/- 30127.46
Episode length: 64.80 +/- 8.73
----------------------------------
| eval/              |           |
|    mean action     | -0.415705 |
|    mean velocity x | 2.36      |
|    mean velocity y | 2.94      |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 64.8      |
|    mean_reward     | -6.31e+04 |
| time/              |           |
|    total_timesteps | 718500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 84.2      |
|    ep_rew_mean     | -8.97e+04 |
| time/              |           |
|    fps             | 112       |
|    iterations      | 351       |
|    time_elapsed    | 6369      |
|    total_timesteps | 718848    |
----------------------------------
Eval num_timesteps=719000, episode_reward=-99303.33 +/- 18770.95
Episode length: 64.40 +/- 5.54
------------------------------------------
| eval/                   |              |
|    mean action          | -0.04051587  |
|    mean velocity x      | -0.0813      |
|    mean velocity y      | -0.292       |
|    mean velocity z      | 20.3         |
|    mean_ep_length       | 64.4         |
|    mean_reward          | -9.93e+04    |
| time/                   |              |
|    total_timesteps      | 719000       |
| train/                  |              |
|    approx_kl            | 0.0029006624 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.00344     |
|    std                  | 0.918        |
|    value_loss           | 2.59e+08     |
------------------------------------------
Eval num_timesteps=719500, episode_reward=-68794.06 +/- 26946.71
Episode length: 63.00 +/- 13.78
-----------------------------------
| eval/              |            |
|    mean action     | -0.3611443 |
|    mean velocity x | 2.08       |
|    mean velocity y | 3.73       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 63         |
|    mean_reward     | -6.88e+04  |
| time/              |            |
|    total_timesteps | 719500     |
-----------------------------------
Eval num_timesteps=720000, episode_reward=-68546.82 +/- 36552.26
Episode length: 60.40 +/- 24.19
------------------------------------
| eval/              |             |
|    mean action     | -0.59162575 |
|    mean velocity x | 2.79        |
|    mean velocity y | 4.05        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 60.4        |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 720000      |
------------------------------------
Eval num_timesteps=720500, episode_reward=-32199.45 +/- 27010.42
Episode length: 43.00 +/- 19.82
-----------------------------------
| eval/              |            |
|    mean action     | -0.5830214 |
|    mean velocity x | 0.715      |
|    mean velocity y | 3.26       |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 43         |
|    mean_reward     | -3.22e+04  |
| time/              |            |
|    total_timesteps | 720500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 83.2      |
|    ep_rew_mean     | -9.23e+04 |
| time/              |           |
|    fps             | 113       |
|    iterations      | 352       |
|    time_elapsed    | 6376      |
|    total_timesteps | 720896    |
----------------------------------
Eval num_timesteps=721000, episode_reward=-93605.28 +/- 36583.07
Episode length: 81.40 +/- 42.33
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.39011383   |
|    mean velocity x      | 0.768         |
|    mean velocity y      | 0.793         |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 81.4          |
|    mean_reward          | -9.36e+04     |
| time/                   |               |
|    total_timesteps      | 721000        |
| train/                  |               |
|    approx_kl            | 0.00024879418 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4            |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+08      |
|    n_updates            | 3520          |
|    policy_gradient_loss | -0.000535     |
|    std                  | 0.917         |
|    value_loss           | 2.48e+08      |
-------------------------------------------
Eval num_timesteps=721500, episode_reward=-74515.17 +/- 13239.86
Episode length: 69.60 +/- 12.56
------------------------------------
| eval/              |             |
|    mean action     | -0.28468162 |
|    mean velocity x | 0.0619      |
|    mean velocity y | 1.46        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 69.6        |
|    mean_reward     | -7.45e+04   |
| time/              |             |
|    total_timesteps | 721500      |
------------------------------------
Eval num_timesteps=722000, episode_reward=-71965.79 +/- 39575.44
Episode length: 59.20 +/- 19.52
------------------------------------
| eval/              |             |
|    mean action     | -0.35585383 |
|    mean velocity x | 2.74        |
|    mean velocity y | 2.54        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 722000      |
------------------------------------
Eval num_timesteps=722500, episode_reward=-78290.72 +/- 48197.56
Episode length: 81.80 +/- 37.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.28680182 |
|    mean velocity x | 0.961      |
|    mean velocity y | -0.652     |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 81.8       |
|    mean_reward     | -7.83e+04  |
| time/              |            |
|    total_timesteps | 722500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 81.8      |
|    ep_rew_mean     | -9.04e+04 |
| time/              |           |
|    fps             | 113       |
|    iterations      | 353       |
|    time_elapsed    | 6383      |
|    total_timesteps | 722944    |
----------------------------------
Eval num_timesteps=723000, episode_reward=-88300.80 +/- 17283.67
Episode length: 69.00 +/- 4.60
------------------------------------------
| eval/                   |              |
|    mean action          | 0.11075488   |
|    mean velocity x      | -0.655       |
|    mean velocity y      | -0.641       |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 69           |
|    mean_reward          | -8.83e+04    |
| time/                   |              |
|    total_timesteps      | 723000       |
| train/                  |              |
|    approx_kl            | 0.0010850322 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.2          |
|    learning_rate        | 0.001        |
|    loss                 | 8.44e+07     |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.000985    |
|    std                  | 0.917        |
|    value_loss           | 2.19e+08     |
------------------------------------------
Eval num_timesteps=723500, episode_reward=-73169.34 +/- 24881.66
Episode length: 61.60 +/- 7.55
------------------------------------
| eval/              |             |
|    mean action     | -0.14909735 |
|    mean velocity x | -0.333      |
|    mean velocity y | 0.305       |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -7.32e+04   |
| time/              |             |
|    total_timesteps | 723500      |
------------------------------------
Eval num_timesteps=724000, episode_reward=-84202.93 +/- 32577.37
Episode length: 76.20 +/- 36.87
------------------------------------
| eval/              |             |
|    mean action     | -0.51612663 |
|    mean velocity x | 2.88        |
|    mean velocity y | 3.82        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 76.2        |
|    mean_reward     | -8.42e+04   |
| time/              |             |
|    total_timesteps | 724000      |
------------------------------------
Eval num_timesteps=724500, episode_reward=-83615.08 +/- 12160.90
Episode length: 66.60 +/- 8.40
----------------------------------
| eval/              |           |
|    mean action     | 0.387602  |
|    mean velocity x | -1.46     |
|    mean velocity y | -2.41     |
|    mean velocity z | 17.1      |
|    mean_ep_length  | 66.6      |
|    mean_reward     | -8.36e+04 |
| time/              |           |
|    total_timesteps | 724500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.1      |
|    ep_rew_mean     | -8.78e+04 |
| time/              |           |
|    fps             | 113       |
|    iterations      | 354       |
|    time_elapsed    | 6390      |
|    total_timesteps | 724992    |
----------------------------------
Eval num_timesteps=725000, episode_reward=-75768.17 +/- 34768.95
Episode length: 61.60 +/- 10.52
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.02239512 |
|    mean velocity x      | 0.858       |
|    mean velocity y      | -0.247      |
|    mean velocity z      | 22.5        |
|    mean_ep_length       | 61.6        |
|    mean_reward          | -7.58e+04   |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.001685404 |
|    clip_fraction        | 0.00161     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.194       |
|    learning_rate        | 0.001       |
|    loss                 | 8.53e+07    |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00155    |
|    std                  | 0.916       |
|    value_loss           | 2.55e+08    |
-----------------------------------------
Eval num_timesteps=725500, episode_reward=-67043.08 +/- 25785.02
Episode length: 71.60 +/- 10.78
-----------------------------------
| eval/              |            |
|    mean action     | 0.33462837 |
|    mean velocity x | -0.0444    |
|    mean velocity y | -0.626     |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 71.6       |
|    mean_reward     | -6.7e+04   |
| time/              |            |
|    total_timesteps | 725500     |
-----------------------------------
Eval num_timesteps=726000, episode_reward=-54210.78 +/- 38632.00
Episode length: 70.00 +/- 36.29
-----------------------------------
| eval/              |            |
|    mean action     | -0.5641062 |
|    mean velocity x | 2.92       |
|    mean velocity y | 3.17       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 70         |
|    mean_reward     | -5.42e+04  |
| time/              |            |
|    total_timesteps | 726000     |
-----------------------------------
Eval num_timesteps=726500, episode_reward=-67822.13 +/- 21470.51
Episode length: 61.20 +/- 7.25
-------------------------------------
| eval/              |              |
|    mean action     | -0.019628227 |
|    mean velocity x | 0.246        |
|    mean velocity y | 0.272        |
|    mean velocity z | 21.3         |
|    mean_ep_length  | 61.2         |
|    mean_reward     | -6.78e+04    |
| time/              |              |
|    total_timesteps | 726500       |
-------------------------------------
Eval num_timesteps=727000, episode_reward=-70040.75 +/- 38607.10
Episode length: 58.80 +/- 8.93
-----------------------------------
| eval/              |            |
|    mean action     | 0.82216066 |
|    mean velocity x | -4.13      |
|    mean velocity y | -5.46      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -7e+04     |
| time/              |            |
|    total_timesteps | 727000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -8.82e+04 |
| time/              |           |
|    fps             | 113       |
|    iterations      | 355       |
|    time_elapsed    | 6398      |
|    total_timesteps | 727040    |
----------------------------------
Eval num_timesteps=727500, episode_reward=-53384.96 +/- 43816.18
Episode length: 53.20 +/- 16.50
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.10292421   |
|    mean velocity x      | 0.421         |
|    mean velocity y      | 1.08          |
|    mean velocity z      | 20.7          |
|    mean_ep_length       | 53.2          |
|    mean_reward          | -5.34e+04     |
| time/                   |               |
|    total_timesteps      | 727500        |
| train/                  |               |
|    approx_kl            | 0.00018118907 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.99         |
|    explained_variance   | 0.192         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+08      |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.000636     |
|    std                  | 0.916         |
|    value_loss           | 2.28e+08      |
-------------------------------------------
Eval num_timesteps=728000, episode_reward=-81397.10 +/- 41693.85
Episode length: 57.40 +/- 13.76
----------------------------------
| eval/              |           |
|    mean action     | -0.434781 |
|    mean velocity x | 2.59      |
|    mean velocity y | 3.63      |
|    mean velocity z | 14.5      |
|    mean_ep_length  | 57.4      |
|    mean_reward     | -8.14e+04 |
| time/              |           |
|    total_timesteps | 728000    |
----------------------------------
Eval num_timesteps=728500, episode_reward=-66570.20 +/- 38932.85
Episode length: 58.00 +/- 13.86
------------------------------------
| eval/              |             |
|    mean action     | -0.07654257 |
|    mean velocity x | 1.27        |
|    mean velocity y | 0.712       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 58          |
|    mean_reward     | -6.66e+04   |
| time/              |             |
|    total_timesteps | 728500      |
------------------------------------
Eval num_timesteps=729000, episode_reward=-29733.53 +/- 22866.78
Episode length: 42.40 +/- 17.10
------------------------------------
| eval/              |             |
|    mean action     | -0.64595324 |
|    mean velocity x | 2.61        |
|    mean velocity y | 3.43        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 42.4        |
|    mean_reward     | -2.97e+04   |
| time/              |             |
|    total_timesteps | 729000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.1      |
|    ep_rew_mean     | -8.31e+04 |
| time/              |           |
|    fps             | 113       |
|    iterations      | 356       |
|    time_elapsed    | 6405      |
|    total_timesteps | 729088    |
----------------------------------
Eval num_timesteps=729500, episode_reward=-66237.48 +/- 32274.43
Episode length: 55.80 +/- 10.19
------------------------------------------
| eval/                   |              |
|    mean action          | -0.008568985 |
|    mean velocity x      | 0.411        |
|    mean velocity y      | 0.138        |
|    mean velocity z      | 19.5         |
|    mean_ep_length       | 55.8         |
|    mean_reward          | -6.62e+04    |
| time/                   |              |
|    total_timesteps      | 729500       |
| train/                  |              |
|    approx_kl            | 6.497488e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.99        |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.001        |
|    loss                 | 7.79e+07     |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.000338    |
|    std                  | 0.916        |
|    value_loss           | 2.17e+08     |
------------------------------------------
Eval num_timesteps=730000, episode_reward=-62455.97 +/- 39459.43
Episode length: 51.80 +/- 17.74
------------------------------------
| eval/              |             |
|    mean action     | -0.08220012 |
|    mean velocity x | -2.13       |
|    mean velocity y | -0.475      |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 51.8        |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 730000      |
------------------------------------
Eval num_timesteps=730500, episode_reward=-56240.24 +/- 43823.65
Episode length: 56.00 +/- 30.11
-----------------------------------
| eval/              |            |
|    mean action     | -0.7787678 |
|    mean velocity x | 2.69       |
|    mean velocity y | 4.57       |
|    mean velocity z | 16         |
|    mean_ep_length  | 56         |
|    mean_reward     | -5.62e+04  |
| time/              |            |
|    total_timesteps | 730500     |
-----------------------------------
Eval num_timesteps=731000, episode_reward=-66990.02 +/- 32882.53
Episode length: 59.00 +/- 9.36
------------------------------------
| eval/              |             |
|    mean action     | -0.84415853 |
|    mean velocity x | 3.63        |
|    mean velocity y | 5.72        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 59          |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 731000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.9      |
|    ep_rew_mean     | -8.36e+04 |
| time/              |           |
|    fps             | 114       |
|    iterations      | 357       |
|    time_elapsed    | 6412      |
|    total_timesteps | 731136    |
----------------------------------
Eval num_timesteps=731500, episode_reward=-64043.07 +/- 41151.98
Episode length: 53.40 +/- 16.61
------------------------------------------
| eval/                   |              |
|    mean action          | -0.52287465  |
|    mean velocity x      | 0.315        |
|    mean velocity y      | 3.76         |
|    mean velocity z      | 19.8         |
|    mean_ep_length       | 53.4         |
|    mean_reward          | -6.4e+04     |
| time/                   |              |
|    total_timesteps      | 731500       |
| train/                  |              |
|    approx_kl            | 0.0015934898 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.99        |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.915        |
|    value_loss           | 1.98e+08     |
------------------------------------------
Eval num_timesteps=732000, episode_reward=-77475.20 +/- 10223.72
Episode length: 78.20 +/- 12.16
------------------------------------
| eval/              |             |
|    mean action     | -0.24073943 |
|    mean velocity x | 1.95        |
|    mean velocity y | 1.97        |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 78.2        |
|    mean_reward     | -7.75e+04   |
| time/              |             |
|    total_timesteps | 732000      |
------------------------------------
Eval num_timesteps=732500, episode_reward=-74579.18 +/- 33459.98
Episode length: 58.40 +/- 13.25
------------------------------------
| eval/              |             |
|    mean action     | -0.04394024 |
|    mean velocity x | -1.2        |
|    mean velocity y | -1.73       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 732500      |
------------------------------------
Eval num_timesteps=733000, episode_reward=-70369.82 +/- 37427.03
Episode length: 60.00 +/- 13.37
------------------------------------
| eval/              |             |
|    mean action     | -0.64813894 |
|    mean velocity x | 1.76        |
|    mean velocity y | 3.57        |
|    mean velocity z | 19          |
|    mean_ep_length  | 60          |
|    mean_reward     | -7.04e+04   |
| time/              |             |
|    total_timesteps | 733000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.4      |
|    ep_rew_mean     | -8.05e+04 |
| time/              |           |
|    fps             | 114       |
|    iterations      | 358       |
|    time_elapsed    | 6419      |
|    total_timesteps | 733184    |
----------------------------------
Eval num_timesteps=733500, episode_reward=-65866.37 +/- 30190.98
Episode length: 55.40 +/- 10.11
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5555048    |
|    mean velocity x      | 1.19          |
|    mean velocity y      | 3.97          |
|    mean velocity z      | 20.3          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -6.59e+04     |
| time/                   |               |
|    total_timesteps      | 733500        |
| train/                  |               |
|    approx_kl            | 0.00023783703 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.99         |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+08      |
|    n_updates            | 3580          |
|    policy_gradient_loss | -0.000882     |
|    std                  | 0.914         |
|    value_loss           | 2.05e+08      |
-------------------------------------------
Eval num_timesteps=734000, episode_reward=-96759.60 +/- 23349.95
Episode length: 91.40 +/- 45.28
------------------------------------
| eval/              |             |
|    mean action     | -0.08769966 |
|    mean velocity x | 0.426       |
|    mean velocity y | 0.655       |
|    mean velocity z | 16.3        |
|    mean_ep_length  | 91.4        |
|    mean_reward     | -9.68e+04   |
| time/              |             |
|    total_timesteps | 734000      |
------------------------------------
Eval num_timesteps=734500, episode_reward=-67113.62 +/- 41842.92
Episode length: 73.60 +/- 39.55
------------------------------------
| eval/              |             |
|    mean action     | -0.18801281 |
|    mean velocity x | 0.624       |
|    mean velocity y | 1.04        |
|    mean velocity z | 22.2        |
|    mean_ep_length  | 73.6        |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 734500      |
------------------------------------
Eval num_timesteps=735000, episode_reward=-82553.24 +/- 28872.39
Episode length: 67.60 +/- 15.86
------------------------------------
| eval/              |             |
|    mean action     | -0.31386897 |
|    mean velocity x | -0.458      |
|    mean velocity y | 0.715       |
|    mean velocity z | 20          |
|    mean_ep_length  | 67.6        |
|    mean_reward     | -8.26e+04   |
| time/              |             |
|    total_timesteps | 735000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.6     |
|    ep_rew_mean     | -8.2e+04 |
| time/              |          |
|    fps             | 114      |
|    iterations      | 359      |
|    time_elapsed    | 6427     |
|    total_timesteps | 735232   |
---------------------------------
Eval num_timesteps=735500, episode_reward=-75116.89 +/- 40921.46
Episode length: 55.40 +/- 16.40
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.5394501  |
|    mean velocity x      | 1.56        |
|    mean velocity y      | 3.44        |
|    mean velocity z      | 19.3        |
|    mean_ep_length       | 55.4        |
|    mean_reward          | -7.51e+04   |
| time/                   |             |
|    total_timesteps      | 735500      |
| train/                  |             |
|    approx_kl            | 0.000291676 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.192       |
|    learning_rate        | 0.001       |
|    loss                 | 1.41e+08    |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.000997   |
|    std                  | 0.914       |
|    value_loss           | 2.35e+08    |
-----------------------------------------
Eval num_timesteps=736000, episode_reward=-85804.17 +/- 20035.62
Episode length: 69.40 +/- 13.37
-----------------------------------
| eval/              |            |
|    mean action     | -0.5719016 |
|    mean velocity x | 0.727      |
|    mean velocity y | 2.59       |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 69.4       |
|    mean_reward     | -8.58e+04  |
| time/              |            |
|    total_timesteps | 736000     |
-----------------------------------
Eval num_timesteps=736500, episode_reward=-47931.40 +/- 23740.38
Episode length: 54.20 +/- 11.60
------------------------------------
| eval/              |             |
|    mean action     | -0.49096125 |
|    mean velocity x | -0.025      |
|    mean velocity y | 2.21        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 54.2        |
|    mean_reward     | -4.79e+04   |
| time/              |             |
|    total_timesteps | 736500      |
------------------------------------
Eval num_timesteps=737000, episode_reward=-85554.98 +/- 18788.75
Episode length: 73.20 +/- 20.76
------------------------------------
| eval/              |             |
|    mean action     | -0.08866795 |
|    mean velocity x | -0.968      |
|    mean velocity y | 0.0265      |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -8.56e+04   |
| time/              |             |
|    total_timesteps | 737000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.4      |
|    ep_rew_mean     | -8.43e+04 |
| time/              |           |
|    fps             | 114       |
|    iterations      | 360       |
|    time_elapsed    | 6434      |
|    total_timesteps | 737280    |
----------------------------------
Eval num_timesteps=737500, episode_reward=-60036.08 +/- 48796.74
Episode length: 44.40 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean action          | 0.33665884   |
|    mean velocity x      | -1.57        |
|    mean velocity y      | -2.1         |
|    mean velocity z      | 15           |
|    mean_ep_length       | 44.4         |
|    mean_reward          | -6e+04       |
| time/                   |              |
|    total_timesteps      | 737500       |
| train/                  |              |
|    approx_kl            | 7.867109e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.99        |
|    explained_variance   | 0.197        |
|    learning_rate        | 0.001        |
|    loss                 | 8.46e+07     |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.000421    |
|    std                  | 0.914        |
|    value_loss           | 1.89e+08     |
------------------------------------------
Eval num_timesteps=738000, episode_reward=-62053.31 +/- 32022.66
Episode length: 63.20 +/- 15.97
-------------------------------------
| eval/              |              |
|    mean action     | -0.105851926 |
|    mean velocity x | 1.7          |
|    mean velocity y | 0.732        |
|    mean velocity z | 15.8         |
|    mean_ep_length  | 63.2         |
|    mean_reward     | -6.21e+04    |
| time/              |              |
|    total_timesteps | 738000       |
-------------------------------------
Eval num_timesteps=738500, episode_reward=-66586.96 +/- 27965.84
Episode length: 72.20 +/- 29.63
-----------------------------------
| eval/              |            |
|    mean action     | -0.2148751 |
|    mean velocity x | 0.401      |
|    mean velocity y | 0.459      |
|    mean velocity z | 19         |
|    mean_ep_length  | 72.2       |
|    mean_reward     | -6.66e+04  |
| time/              |            |
|    total_timesteps | 738500     |
-----------------------------------
Eval num_timesteps=739000, episode_reward=-67588.05 +/- 38151.87
Episode length: 61.00 +/- 24.82
-------------------------------------
| eval/              |              |
|    mean action     | -0.010217781 |
|    mean velocity x | 0.462        |
|    mean velocity y | -0.133       |
|    mean velocity z | 20.2         |
|    mean_ep_length  | 61           |
|    mean_reward     | -6.76e+04    |
| time/              |              |
|    total_timesteps | 739000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.2      |
|    ep_rew_mean     | -8.24e+04 |
| time/              |           |
|    fps             | 114       |
|    iterations      | 361       |
|    time_elapsed    | 6441      |
|    total_timesteps | 739328    |
----------------------------------
Eval num_timesteps=739500, episode_reward=-48422.60 +/- 24344.64
Episode length: 58.20 +/- 17.71
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.03550303    |
|    mean velocity x      | 0.194         |
|    mean velocity y      | -0.834        |
|    mean velocity z      | 18.9          |
|    mean_ep_length       | 58.2          |
|    mean_reward          | -4.84e+04     |
| time/                   |               |
|    total_timesteps      | 739500        |
| train/                  |               |
|    approx_kl            | 0.00080559996 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.99         |
|    explained_variance   | 0.148         |
|    learning_rate        | 0.001         |
|    loss                 | 8.77e+07      |
|    n_updates            | 3610          |
|    policy_gradient_loss | -0.00107      |
|    std                  | 0.913         |
|    value_loss           | 2.22e+08      |
-------------------------------------------
Eval num_timesteps=740000, episode_reward=-61006.16 +/- 34126.59
Episode length: 56.00 +/- 17.30
------------------------------------
| eval/              |             |
|    mean action     | -0.17008258 |
|    mean velocity x | 0.118       |
|    mean velocity y | 1.74        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 56          |
|    mean_reward     | -6.1e+04    |
| time/              |             |
|    total_timesteps | 740000      |
------------------------------------
Eval num_timesteps=740500, episode_reward=-94210.81 +/- 30715.62
Episode length: 60.40 +/- 6.28
------------------------------------
| eval/              |             |
|    mean action     | -0.11680881 |
|    mean velocity x | 0.353       |
|    mean velocity y | -0.839      |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 60.4        |
|    mean_reward     | -9.42e+04   |
| time/              |             |
|    total_timesteps | 740500      |
------------------------------------
Eval num_timesteps=741000, episode_reward=-77331.30 +/- 21206.73
Episode length: 63.40 +/- 9.09
------------------------------------
| eval/              |             |
|    mean action     | -0.41183776 |
|    mean velocity x | 0.162       |
|    mean velocity y | 1.72        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -7.73e+04   |
| time/              |             |
|    total_timesteps | 741000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -8.07e+04 |
| time/              |           |
|    fps             | 114       |
|    iterations      | 362       |
|    time_elapsed    | 6448      |
|    total_timesteps | 741376    |
----------------------------------
Eval num_timesteps=741500, episode_reward=-66079.38 +/- 22348.06
Episode length: 74.00 +/- 18.48
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4227326   |
|    mean velocity x      | 0.566        |
|    mean velocity y      | 0.99         |
|    mean velocity z      | 20.5         |
|    mean_ep_length       | 74           |
|    mean_reward          | -6.61e+04    |
| time/                   |              |
|    total_timesteps      | 741500       |
| train/                  |              |
|    approx_kl            | 0.0026498009 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.173        |
|    learning_rate        | 0.001        |
|    loss                 | 7.96e+07     |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 0.913        |
|    value_loss           | 2.4e+08      |
------------------------------------------
Eval num_timesteps=742000, episode_reward=-81633.06 +/- 16689.92
Episode length: 66.40 +/- 6.92
------------------------------------
| eval/              |             |
|    mean action     | -0.44394225 |
|    mean velocity x | 2.06        |
|    mean velocity y | 2.11        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 742000      |
------------------------------------
Eval num_timesteps=742500, episode_reward=-50704.63 +/- 26105.67
Episode length: 61.20 +/- 27.32
------------------------------------
| eval/              |             |
|    mean action     | -0.10409859 |
|    mean velocity x | 0.397       |
|    mean velocity y | 0.184       |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -5.07e+04   |
| time/              |             |
|    total_timesteps | 742500      |
------------------------------------
Eval num_timesteps=743000, episode_reward=-60096.80 +/- 47490.34
Episode length: 45.00 +/- 21.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.29919928 |
|    mean velocity x | -1.71      |
|    mean velocity y | -1.81      |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 45         |
|    mean_reward     | -6.01e+04  |
| time/              |            |
|    total_timesteps | 743000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.4      |
|    ep_rew_mean     | -7.32e+04 |
| time/              |           |
|    fps             | 115       |
|    iterations      | 363       |
|    time_elapsed    | 6455      |
|    total_timesteps | 743424    |
----------------------------------
Eval num_timesteps=743500, episode_reward=-60957.56 +/- 46940.30
Episode length: 46.60 +/- 20.79
------------------------------------------
| eval/                   |              |
|    mean action          | -0.17380247  |
|    mean velocity x      | -0.369       |
|    mean velocity y      | 1.44         |
|    mean velocity z      | 16.6         |
|    mean_ep_length       | 46.6         |
|    mean_reward          | -6.1e+04     |
| time/                   |              |
|    total_timesteps      | 743500       |
| train/                  |              |
|    approx_kl            | 0.0011533827 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.001        |
|    loss                 | 8.32e+07     |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.912        |
|    value_loss           | 1.85e+08     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=-67387.95 +/- 27042.57
Episode length: 60.00 +/- 7.95
------------------------------------
| eval/              |             |
|    mean action     | -0.18266693 |
|    mean velocity x | -0.228      |
|    mean velocity y | 0.865       |
|    mean velocity z | 23.5        |
|    mean_ep_length  | 60          |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 744000      |
------------------------------------
Eval num_timesteps=744500, episode_reward=-59665.20 +/- 34333.18
Episode length: 58.40 +/- 19.98
-----------------------------------
| eval/              |            |
|    mean action     | -0.6220757 |
|    mean velocity x | 2.35       |
|    mean velocity y | 3.5        |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 58.4       |
|    mean_reward     | -5.97e+04  |
| time/              |            |
|    total_timesteps | 744500     |
-----------------------------------
Eval num_timesteps=745000, episode_reward=-44744.87 +/- 44875.70
Episode length: 44.80 +/- 16.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.16326149 |
|    mean velocity x | -0.000534  |
|    mean velocity y | -0.947     |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 44.8       |
|    mean_reward     | -4.47e+04  |
| time/              |            |
|    total_timesteps | 745000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.2      |
|    ep_rew_mean     | -7.49e+04 |
| time/              |           |
|    fps             | 115       |
|    iterations      | 364       |
|    time_elapsed    | 6462      |
|    total_timesteps | 745472    |
----------------------------------
Eval num_timesteps=745500, episode_reward=-85587.75 +/- 16663.42
Episode length: 73.00 +/- 19.81
------------------------------------------
| eval/                   |              |
|    mean action          | 0.290009     |
|    mean velocity x      | -2.12        |
|    mean velocity y      | -3.16        |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 73           |
|    mean_reward          | -8.56e+04    |
| time/                   |              |
|    total_timesteps      | 745500       |
| train/                  |              |
|    approx_kl            | 0.0034000492 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+08      |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.00287     |
|    std                  | 0.912        |
|    value_loss           | 2.64e+08     |
------------------------------------------
Eval num_timesteps=746000, episode_reward=-59593.54 +/- 27512.23
Episode length: 52.00 +/- 12.88
-------------------------------------
| eval/              |              |
|    mean action     | -0.008844177 |
|    mean velocity x | 0.795        |
|    mean velocity y | 0.304        |
|    mean velocity z | 20           |
|    mean_ep_length  | 52           |
|    mean_reward     | -5.96e+04    |
| time/              |              |
|    total_timesteps | 746000       |
-------------------------------------
Eval num_timesteps=746500, episode_reward=-40648.31 +/- 31613.89
Episode length: 44.00 +/- 16.12
------------------------------------
| eval/              |             |
|    mean action     | -0.43673298 |
|    mean velocity x | 1.38        |
|    mean velocity y | 2.58        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 44          |
|    mean_reward     | -4.06e+04   |
| time/              |             |
|    total_timesteps | 746500      |
------------------------------------
Eval num_timesteps=747000, episode_reward=-80114.68 +/- 14202.51
Episode length: 61.80 +/- 4.96
------------------------------------
| eval/              |             |
|    mean action     | -0.39383706 |
|    mean velocity x | 0.76        |
|    mean velocity y | 1.68        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 747000      |
------------------------------------
Eval num_timesteps=747500, episode_reward=-104676.89 +/- 47608.40
Episode length: 97.40 +/- 51.26
------------------------------------
| eval/              |             |
|    mean action     | -0.35472718 |
|    mean velocity x | 1.42        |
|    mean velocity y | 1.86        |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 97.4        |
|    mean_reward     | -1.05e+05   |
| time/              |             |
|    total_timesteps | 747500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.6      |
|    ep_rew_mean     | -8.08e+04 |
| time/              |           |
|    fps             | 115       |
|    iterations      | 365       |
|    time_elapsed    | 6470      |
|    total_timesteps | 747520    |
----------------------------------
Eval num_timesteps=748000, episode_reward=-56160.24 +/- 30156.13
Episode length: 57.60 +/- 22.01
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.14853276   |
|    mean velocity x      | -0.0988       |
|    mean velocity y      | 0.787         |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 57.6          |
|    mean_reward          | -5.62e+04     |
| time/                   |               |
|    total_timesteps      | 748000        |
| train/                  |               |
|    approx_kl            | 0.00035286866 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.001         |
|    loss                 | 8.73e+07      |
|    n_updates            | 3650          |
|    policy_gradient_loss | -0.00124      |
|    std                  | 0.912         |
|    value_loss           | 2.35e+08      |
-------------------------------------------
Eval num_timesteps=748500, episode_reward=-54975.40 +/- 51229.14
Episode length: 60.60 +/- 46.64
------------------------------------
| eval/              |             |
|    mean action     | -0.28757787 |
|    mean velocity x | -0.759      |
|    mean velocity y | 0.273       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 60.6        |
|    mean_reward     | -5.5e+04    |
| time/              |             |
|    total_timesteps | 748500      |
------------------------------------
Eval num_timesteps=749000, episode_reward=-61179.94 +/- 25987.98
Episode length: 58.80 +/- 17.23
-----------------------------------
| eval/              |            |
|    mean action     | 0.16109514 |
|    mean velocity x | -0.709     |
|    mean velocity y | -1.18      |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -6.12e+04  |
| time/              |            |
|    total_timesteps | 749000     |
-----------------------------------
Eval num_timesteps=749500, episode_reward=-52367.47 +/- 38219.33
Episode length: 49.80 +/- 22.60
----------------------------------
| eval/              |           |
|    mean action     | 0.1114111 |
|    mean velocity x | -2.28     |
|    mean velocity y | -2.51     |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -5.24e+04 |
| time/              |           |
|    total_timesteps | 749500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.2      |
|    ep_rew_mean     | -8.48e+04 |
| time/              |           |
|    fps             | 115       |
|    iterations      | 366       |
|    time_elapsed    | 6477      |
|    total_timesteps | 749568    |
----------------------------------
Eval num_timesteps=750000, episode_reward=-73705.37 +/- 26255.38
Episode length: 66.40 +/- 15.45
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.17494793   |
|    mean velocity x      | 0.488         |
|    mean velocity y      | 1.24          |
|    mean velocity z      | 16            |
|    mean_ep_length       | 66.4          |
|    mean_reward          | -7.37e+04     |
| time/                   |               |
|    total_timesteps      | 750000        |
| train/                  |               |
|    approx_kl            | 0.00037023463 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.207         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+08      |
|    n_updates            | 3660          |
|    policy_gradient_loss | -0.000849     |
|    std                  | 0.912         |
|    value_loss           | 2.28e+08      |
-------------------------------------------
Eval num_timesteps=750500, episode_reward=-76614.30 +/- 12793.89
Episode length: 64.80 +/- 3.37
------------------------------------
| eval/              |             |
|    mean action     | -0.28156036 |
|    mean velocity x | 0.639       |
|    mean velocity y | 0.935       |
|    mean velocity z | 20          |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.66e+04   |
| time/              |             |
|    total_timesteps | 750500      |
------------------------------------
Eval num_timesteps=751000, episode_reward=-71143.65 +/- 27673.85
Episode length: 57.60 +/- 7.86
------------------------------------
| eval/              |             |
|    mean action     | -0.45628533 |
|    mean velocity x | 2.07        |
|    mean velocity y | 3.37        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -7.11e+04   |
| time/              |             |
|    total_timesteps | 751000      |
------------------------------------
Eval num_timesteps=751500, episode_reward=-36181.26 +/- 27226.15
Episode length: 59.40 +/- 34.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.15340112 |
|    mean velocity x | -0.258     |
|    mean velocity y | -1.99      |
|    mean velocity z | 21.6       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -3.62e+04  |
| time/              |            |
|    total_timesteps | 751500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.4      |
|    ep_rew_mean     | -8.85e+04 |
| time/              |           |
|    fps             | 115       |
|    iterations      | 367       |
|    time_elapsed    | 6484      |
|    total_timesteps | 751616    |
----------------------------------
Eval num_timesteps=752000, episode_reward=-68845.38 +/- 36047.80
Episode length: 57.40 +/- 9.65
------------------------------------------
| eval/                   |              |
|    mean action          | 0.083768874  |
|    mean velocity x      | -0.739       |
|    mean velocity y      | -0.397       |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 57.4         |
|    mean_reward          | -6.88e+04    |
| time/                   |              |
|    total_timesteps      | 752000       |
| train/                  |              |
|    approx_kl            | 0.0003574396 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.181        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+08     |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.000887    |
|    std                  | 0.912        |
|    value_loss           | 2.4e+08      |
------------------------------------------
Eval num_timesteps=752500, episode_reward=-75176.22 +/- 13030.63
Episode length: 62.00 +/- 1.90
-----------------------------------
| eval/              |            |
|    mean action     | 0.29666921 |
|    mean velocity x | -1.14      |
|    mean velocity y | -2.69      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 62         |
|    mean_reward     | -7.52e+04  |
| time/              |            |
|    total_timesteps | 752500     |
-----------------------------------
Eval num_timesteps=753000, episode_reward=-64998.25 +/- 28309.81
Episode length: 55.60 +/- 7.68
-----------------------------------
| eval/              |            |
|    mean action     | 0.19825573 |
|    mean velocity x | -0.504     |
|    mean velocity y | 0.58       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 55.6       |
|    mean_reward     | -6.5e+04   |
| time/              |            |
|    total_timesteps | 753000     |
-----------------------------------
Eval num_timesteps=753500, episode_reward=-57337.98 +/- 46536.75
Episode length: 48.60 +/- 21.48
------------------------------------
| eval/              |             |
|    mean action     | -0.45448267 |
|    mean velocity x | 1.34        |
|    mean velocity y | 2.36        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -5.73e+04   |
| time/              |             |
|    total_timesteps | 753500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.7      |
|    ep_rew_mean     | -8.38e+04 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 368       |
|    time_elapsed    | 6491      |
|    total_timesteps | 753664    |
----------------------------------
Eval num_timesteps=754000, episode_reward=-85142.47 +/- 14074.52
Episode length: 72.20 +/- 7.14
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.08973767    |
|    mean velocity x      | -0.346        |
|    mean velocity y      | -0.486        |
|    mean velocity z      | 22.6          |
|    mean_ep_length       | 72.2          |
|    mean_reward          | -8.51e+04     |
| time/                   |               |
|    total_timesteps      | 754000        |
| train/                  |               |
|    approx_kl            | 0.00067674817 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.19          |
|    learning_rate        | 0.001         |
|    loss                 | 8.23e+07      |
|    n_updates            | 3680          |
|    policy_gradient_loss | -0.00091      |
|    std                  | 0.912         |
|    value_loss           | 2.19e+08      |
-------------------------------------------
Eval num_timesteps=754500, episode_reward=-80333.17 +/- 21440.08
Episode length: 69.80 +/- 6.79
------------------------------------
| eval/              |             |
|    mean action     | -0.90246606 |
|    mean velocity x | 1.7         |
|    mean velocity y | 5.44        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 69.8        |
|    mean_reward     | -8.03e+04   |
| time/              |             |
|    total_timesteps | 754500      |
------------------------------------
Eval num_timesteps=755000, episode_reward=-58414.34 +/- 32914.06
Episode length: 54.60 +/- 19.04
-----------------------------------
| eval/              |            |
|    mean action     | -1.1564506 |
|    mean velocity x | 4.59       |
|    mean velocity y | 8.24       |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -5.84e+04  |
| time/              |            |
|    total_timesteps | 755000     |
-----------------------------------
Eval num_timesteps=755500, episode_reward=-65553.34 +/- 33927.48
Episode length: 55.40 +/- 23.10
------------------------------------
| eval/              |             |
|    mean action     | 0.025298934 |
|    mean velocity x | -0.7        |
|    mean velocity y | -0.476      |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 55.4        |
|    mean_reward     | -6.56e+04   |
| time/              |             |
|    total_timesteps | 755500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.45e+04 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 369       |
|    time_elapsed    | 6499      |
|    total_timesteps | 755712    |
----------------------------------
Eval num_timesteps=756000, episode_reward=-59560.06 +/- 33862.57
Episode length: 53.00 +/- 19.02
------------------------------------------
| eval/                   |              |
|    mean action          | -0.07123244  |
|    mean velocity x      | -1.79        |
|    mean velocity y      | -0.292       |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 53           |
|    mean_reward          | -5.96e+04    |
| time/                   |              |
|    total_timesteps      | 756000       |
| train/                  |              |
|    approx_kl            | 0.0016464905 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.215        |
|    learning_rate        | 0.001        |
|    loss                 | 8.2e+07      |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00269     |
|    std                  | 0.913        |
|    value_loss           | 1.84e+08     |
------------------------------------------
Eval num_timesteps=756500, episode_reward=-31062.24 +/- 30866.87
Episode length: 40.20 +/- 23.10
------------------------------------
| eval/              |             |
|    mean action     | -0.11362586 |
|    mean velocity x | -1.3        |
|    mean velocity y | -0.514      |
|    mean velocity z | 21.9        |
|    mean_ep_length  | 40.2        |
|    mean_reward     | -3.11e+04   |
| time/              |             |
|    total_timesteps | 756500      |
------------------------------------
Eval num_timesteps=757000, episode_reward=-77590.55 +/- 37168.92
Episode length: 62.00 +/- 11.14
-----------------------------------
| eval/              |            |
|    mean action     | -0.3123502 |
|    mean velocity x | 1.03       |
|    mean velocity y | 2.97       |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 62         |
|    mean_reward     | -7.76e+04  |
| time/              |            |
|    total_timesteps | 757000     |
-----------------------------------
Eval num_timesteps=757500, episode_reward=-88466.65 +/- 15770.85
Episode length: 71.40 +/- 9.54
-----------------------------------
| eval/              |            |
|    mean action     | -0.1180112 |
|    mean velocity x | -0.919     |
|    mean velocity y | 0.304      |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 71.4       |
|    mean_reward     | -8.85e+04  |
| time/              |            |
|    total_timesteps | 757500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.6      |
|    ep_rew_mean     | -8.36e+04 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 370       |
|    time_elapsed    | 6506      |
|    total_timesteps | 757760    |
----------------------------------
Eval num_timesteps=758000, episode_reward=-64442.24 +/- 27352.25
Episode length: 69.80 +/- 29.40
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.3796958  |
|    mean velocity x      | 2.14        |
|    mean velocity y      | 3.16        |
|    mean velocity z      | 20.7        |
|    mean_ep_length       | 69.8        |
|    mean_reward          | -6.44e+04   |
| time/                   |             |
|    total_timesteps      | 758000      |
| train/                  |             |
|    approx_kl            | 0.002012184 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.001       |
|    loss                 | 9.51e+07    |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 0.912       |
|    value_loss           | 2.42e+08    |
-----------------------------------------
Eval num_timesteps=758500, episode_reward=-73537.01 +/- 36941.68
Episode length: 53.00 +/- 17.61
-----------------------------------
| eval/              |            |
|    mean action     | 0.04429744 |
|    mean velocity x | -1.13      |
|    mean velocity y | -0.594     |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 53         |
|    mean_reward     | -7.35e+04  |
| time/              |            |
|    total_timesteps | 758500     |
-----------------------------------
Eval num_timesteps=759000, episode_reward=-47281.94 +/- 34173.28
Episode length: 58.80 +/- 32.98
-----------------------------------
| eval/              |            |
|    mean action     | -0.5582703 |
|    mean velocity x | 0.866      |
|    mean velocity y | 3.65       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -4.73e+04  |
| time/              |            |
|    total_timesteps | 759000     |
-----------------------------------
Eval num_timesteps=759500, episode_reward=-46525.65 +/- 40951.80
Episode length: 41.80 +/- 20.14
----------------------------------
| eval/              |           |
|    mean action     | -0.261289 |
|    mean velocity x | 1.2       |
|    mean velocity y | 1         |
|    mean velocity z | 20.8      |
|    mean_ep_length  | 41.8      |
|    mean_reward     | -4.65e+04 |
| time/              |           |
|    total_timesteps | 759500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.7      |
|    ep_rew_mean     | -8.54e+04 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 371       |
|    time_elapsed    | 6513      |
|    total_timesteps | 759808    |
----------------------------------
Eval num_timesteps=760000, episode_reward=-45030.12 +/- 38244.33
Episode length: 49.00 +/- 25.42
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.14777061    |
|    mean velocity x      | 0.636         |
|    mean velocity y      | -0.359        |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 49            |
|    mean_reward          | -4.5e+04      |
| time/                   |               |
|    total_timesteps      | 760000        |
| train/                  |               |
|    approx_kl            | 0.00062766054 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.213         |
|    learning_rate        | 0.001         |
|    loss                 | 1.18e+08      |
|    n_updates            | 3710          |
|    policy_gradient_loss | -0.00093      |
|    std                  | 0.912         |
|    value_loss           | 2.38e+08      |
-------------------------------------------
Eval num_timesteps=760500, episode_reward=-62955.77 +/- 37945.51
Episode length: 58.20 +/- 9.79
------------------------------------
| eval/              |             |
|    mean action     | -0.22052962 |
|    mean velocity x | 1.18        |
|    mean velocity y | 1.33        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -6.3e+04    |
| time/              |             |
|    total_timesteps | 760500      |
------------------------------------
Eval num_timesteps=761000, episode_reward=-50647.67 +/- 35589.10
Episode length: 52.40 +/- 18.27
------------------------------------
| eval/              |             |
|    mean action     | -0.29295325 |
|    mean velocity x | 0.999       |
|    mean velocity y | 1.2         |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 52.4        |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 761000      |
------------------------------------
Eval num_timesteps=761500, episode_reward=-46561.47 +/- 27528.78
Episode length: 50.40 +/- 13.97
------------------------------------
| eval/              |             |
|    mean action     | -0.23048161 |
|    mean velocity x | 0.349       |
|    mean velocity y | 1.02        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 50.4        |
|    mean_reward     | -4.66e+04   |
| time/              |             |
|    total_timesteps | 761500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.7      |
|    ep_rew_mean     | -8.82e+04 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 372       |
|    time_elapsed    | 6520      |
|    total_timesteps | 761856    |
----------------------------------
Eval num_timesteps=762000, episode_reward=-46025.89 +/- 50990.86
Episode length: 38.80 +/- 21.72
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.06790387   |
|    mean velocity x      | -0.691        |
|    mean velocity y      | -0.603        |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 38.8          |
|    mean_reward          | -4.6e+04      |
| time/                   |               |
|    total_timesteps      | 762000        |
| train/                  |               |
|    approx_kl            | 0.00015059032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.185         |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+08      |
|    n_updates            | 3720          |
|    policy_gradient_loss | -0.000665     |
|    std                  | 0.912         |
|    value_loss           | 2.39e+08      |
-------------------------------------------
Eval num_timesteps=762500, episode_reward=-83698.85 +/- 19091.93
Episode length: 72.20 +/- 11.07
-------------------------------------
| eval/              |              |
|    mean action     | -0.044728905 |
|    mean velocity x | -0.931       |
|    mean velocity y | -1.25        |
|    mean velocity z | 20           |
|    mean_ep_length  | 72.2         |
|    mean_reward     | -8.37e+04    |
| time/              |              |
|    total_timesteps | 762500       |
-------------------------------------
Eval num_timesteps=763000, episode_reward=-76867.39 +/- 38459.54
Episode length: 63.60 +/- 26.06
------------------------------------
| eval/              |             |
|    mean action     | 0.049643286 |
|    mean velocity x | -0.183      |
|    mean velocity y | -2.26       |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 63.6        |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 763000      |
------------------------------------
Eval num_timesteps=763500, episode_reward=-76136.97 +/- 27113.38
Episode length: 69.80 +/- 25.07
------------------------------------
| eval/              |             |
|    mean action     | -0.09211118 |
|    mean velocity x | -1.54       |
|    mean velocity y | -0.925      |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 69.8        |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 763500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -8.39e+04 |
| time/              |           |
|    fps             | 116       |
|    iterations      | 373       |
|    time_elapsed    | 6536      |
|    total_timesteps | 763904    |
----------------------------------
Eval num_timesteps=764000, episode_reward=-63554.57 +/- 34690.33
Episode length: 57.60 +/- 9.13
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.20588766    |
|    mean velocity x      | -0.414        |
|    mean velocity y      | -3.04         |
|    mean velocity z      | 19.6          |
|    mean_ep_length       | 57.6          |
|    mean_reward          | -6.36e+04     |
| time/                   |               |
|    total_timesteps      | 764000        |
| train/                  |               |
|    approx_kl            | 0.00015527077 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.186         |
|    learning_rate        | 0.001         |
|    loss                 | 7.47e+07      |
|    n_updates            | 3730          |
|    policy_gradient_loss | -0.000563     |
|    std                  | 0.912         |
|    value_loss           | 2.4e+08       |
-------------------------------------------
Eval num_timesteps=764500, episode_reward=-62872.65 +/- 19238.03
Episode length: 64.00 +/- 14.24
----------------------------------
| eval/              |           |
|    mean action     | 0.2942213 |
|    mean velocity x | -2.92     |
|    mean velocity y | -2.82     |
|    mean velocity z | 17        |
|    mean_ep_length  | 64        |
|    mean_reward     | -6.29e+04 |
| time/              |           |
|    total_timesteps | 764500    |
----------------------------------
Eval num_timesteps=765000, episode_reward=-61888.74 +/- 48537.37
Episode length: 50.40 +/- 18.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.20514789 |
|    mean velocity x | -1.26      |
|    mean velocity y | -1.48      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 50.4       |
|    mean_reward     | -6.19e+04  |
| time/              |            |
|    total_timesteps | 765000     |
-----------------------------------
Eval num_timesteps=765500, episode_reward=-48887.23 +/- 41185.64
Episode length: 44.60 +/- 16.91
------------------------------------
| eval/              |             |
|    mean action     | -0.28648147 |
|    mean velocity x | 1.46        |
|    mean velocity y | 1.49        |
|    mean velocity z | 17          |
|    mean_ep_length  | 44.6        |
|    mean_reward     | -4.89e+04   |
| time/              |             |
|    total_timesteps | 765500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.2      |
|    ep_rew_mean     | -8.02e+04 |
| time/              |           |
|    fps             | 117       |
|    iterations      | 374       |
|    time_elapsed    | 6543      |
|    total_timesteps | 765952    |
----------------------------------
Eval num_timesteps=766000, episode_reward=-71311.70 +/- 31823.29
Episode length: 64.80 +/- 23.28
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49677917  |
|    mean velocity x      | 1.73         |
|    mean velocity y      | 2.37         |
|    mean velocity z      | 20.8         |
|    mean_ep_length       | 64.8         |
|    mean_reward          | -7.13e+04    |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0011345709 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.195        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+08     |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 0.912        |
|    value_loss           | 2.11e+08     |
------------------------------------------
Eval num_timesteps=766500, episode_reward=-57985.48 +/- 12490.21
Episode length: 59.40 +/- 10.48
-----------------------------------
| eval/              |            |
|    mean action     | -0.5364199 |
|    mean velocity x | 1.93       |
|    mean velocity y | 3.76       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -5.8e+04   |
| time/              |            |
|    total_timesteps | 766500     |
-----------------------------------
Eval num_timesteps=767000, episode_reward=-78050.72 +/- 35996.21
Episode length: 57.00 +/- 11.17
------------------------------------
| eval/              |             |
|    mean action     | -0.17624341 |
|    mean velocity x | 0.771       |
|    mean velocity y | 1.96        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 57          |
|    mean_reward     | -7.81e+04   |
| time/              |             |
|    total_timesteps | 767000      |
------------------------------------
Eval num_timesteps=767500, episode_reward=-53979.71 +/- 27883.46
Episode length: 53.80 +/- 7.78
------------------------------------
| eval/              |             |
|    mean action     | -0.40700674 |
|    mean velocity x | 0.954       |
|    mean velocity y | 1.92        |
|    mean velocity z | 21          |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -5.4e+04    |
| time/              |             |
|    total_timesteps | 767500      |
------------------------------------
Eval num_timesteps=768000, episode_reward=-67907.62 +/- 36095.71
Episode length: 56.20 +/- 11.74
-----------------------------------
| eval/              |            |
|    mean action     | 0.13369916 |
|    mean velocity x | -0.582     |
|    mean velocity y | -1.17      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -6.79e+04  |
| time/              |            |
|    total_timesteps | 768000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67        |
|    ep_rew_mean     | -7.94e+04 |
| time/              |           |
|    fps             | 117       |
|    iterations      | 375       |
|    time_elapsed    | 6551      |
|    total_timesteps | 768000    |
----------------------------------
Eval num_timesteps=768500, episode_reward=-42114.21 +/- 27886.22
Episode length: 46.20 +/- 13.47
------------------------------------------
| eval/                   |              |
|    mean action          | -0.19303684  |
|    mean velocity x      | -0.378       |
|    mean velocity y      | 0.162        |
|    mean velocity z      | 22.6         |
|    mean_ep_length       | 46.2         |
|    mean_reward          | -4.21e+04    |
| time/                   |              |
|    total_timesteps      | 768500       |
| train/                  |              |
|    approx_kl            | 0.0014168564 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.199        |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+08     |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 0.913        |
|    value_loss           | 2.82e+08     |
------------------------------------------
Eval num_timesteps=769000, episode_reward=-76886.68 +/- 38882.12
Episode length: 65.80 +/- 36.42
------------------------------------
| eval/              |             |
|    mean action     | -0.18627085 |
|    mean velocity x | 1.48        |
|    mean velocity y | 0.716       |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 65.8        |
|    mean_reward     | -7.69e+04   |
| time/              |             |
|    total_timesteps | 769000      |
------------------------------------
Eval num_timesteps=769500, episode_reward=-75769.28 +/- 11920.82
Episode length: 75.00 +/- 21.06
-----------------------------------
| eval/              |            |
|    mean action     | 0.37824607 |
|    mean velocity x | -0.0568    |
|    mean velocity y | -1.24      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 75         |
|    mean_reward     | -7.58e+04  |
| time/              |            |
|    total_timesteps | 769500     |
-----------------------------------
Eval num_timesteps=770000, episode_reward=-70295.66 +/- 40472.12
Episode length: 58.60 +/- 20.31
-----------------------------------
| eval/              |            |
|    mean action     | 0.14645553 |
|    mean velocity x | -0.251     |
|    mean velocity y | -1.58      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -7.03e+04  |
| time/              |            |
|    total_timesteps | 770000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.2      |
|    ep_rew_mean     | -7.94e+04 |
| time/              |           |
|    fps             | 117       |
|    iterations      | 376       |
|    time_elapsed    | 6558      |
|    total_timesteps | 770048    |
----------------------------------
Eval num_timesteps=770500, episode_reward=-83712.51 +/- 9789.98
Episode length: 63.40 +/- 3.56
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27336007  |
|    mean velocity x      | 1.07         |
|    mean velocity y      | 1.25         |
|    mean velocity z      | 16.3         |
|    mean_ep_length       | 63.4         |
|    mean_reward          | -8.37e+04    |
| time/                   |              |
|    total_timesteps      | 770500       |
| train/                  |              |
|    approx_kl            | 0.0005098209 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.001        |
|    loss                 | 9.18e+07     |
|    n_updates            | 3760         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 0.913        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=771000, episode_reward=-65959.09 +/- 35723.55
Episode length: 65.40 +/- 21.46
------------------------------------
| eval/              |             |
|    mean action     | -0.09547941 |
|    mean velocity x | 0.345       |
|    mean velocity y | -0.324      |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -6.6e+04    |
| time/              |             |
|    total_timesteps | 771000      |
------------------------------------
Eval num_timesteps=771500, episode_reward=-75588.95 +/- 19348.50
Episode length: 73.40 +/- 23.63
-----------------------------------
| eval/              |            |
|    mean action     | -0.9128539 |
|    mean velocity x | 1.54       |
|    mean velocity y | 4.36       |
|    mean velocity z | 16.5       |
|    mean_ep_length  | 73.4       |
|    mean_reward     | -7.56e+04  |
| time/              |            |
|    total_timesteps | 771500     |
-----------------------------------
Eval num_timesteps=772000, episode_reward=-98890.31 +/- 58806.56
Episode length: 90.00 +/- 69.11
------------------------------------
| eval/              |             |
|    mean action     | -0.39509836 |
|    mean velocity x | 2.63        |
|    mean velocity y | 3.17        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 90          |
|    mean_reward     | -9.89e+04   |
| time/              |             |
|    total_timesteps | 772000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69        |
|    ep_rew_mean     | -7.98e+04 |
| time/              |           |
|    fps             | 117       |
|    iterations      | 377       |
|    time_elapsed    | 6566      |
|    total_timesteps | 772096    |
----------------------------------
Eval num_timesteps=772500, episode_reward=-58496.75 +/- 27838.02
Episode length: 58.00 +/- 20.85
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5063104    |
|    mean velocity x      | 1.21          |
|    mean velocity y      | 2.36          |
|    mean velocity z      | 19.3          |
|    mean_ep_length       | 58            |
|    mean_reward          | -5.85e+04     |
| time/                   |               |
|    total_timesteps      | 772500        |
| train/                  |               |
|    approx_kl            | 0.00093254173 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+08      |
|    n_updates            | 3770          |
|    policy_gradient_loss | -0.000925     |
|    std                  | 0.913         |
|    value_loss           | 1.74e+08      |
-------------------------------------------
Eval num_timesteps=773000, episode_reward=-86059.39 +/- 15768.57
Episode length: 60.00 +/- 4.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.57266474 |
|    mean velocity x | -2.2       |
|    mean velocity y | -4.01      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 60         |
|    mean_reward     | -8.61e+04  |
| time/              |            |
|    total_timesteps | 773000     |
-----------------------------------
Eval num_timesteps=773500, episode_reward=-73744.63 +/- 32826.39
Episode length: 63.60 +/- 17.23
------------------------------------
| eval/              |             |
|    mean action     | -0.84800553 |
|    mean velocity x | 2.34        |
|    mean velocity y | 4.75        |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 63.6        |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 773500      |
------------------------------------
Eval num_timesteps=774000, episode_reward=-85680.83 +/- 23568.12
Episode length: 62.00 +/- 5.93
------------------------------------
| eval/              |             |
|    mean action     | -0.31591377 |
|    mean velocity x | 1.93        |
|    mean velocity y | 2.55        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 62          |
|    mean_reward     | -8.57e+04   |
| time/              |             |
|    total_timesteps | 774000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.9      |
|    ep_rew_mean     | -8.19e+04 |
| time/              |           |
|    fps             | 117       |
|    iterations      | 378       |
|    time_elapsed    | 6573      |
|    total_timesteps | 774144    |
----------------------------------
Eval num_timesteps=774500, episode_reward=-59678.77 +/- 35822.40
Episode length: 61.40 +/- 13.84
------------------------------------------
| eval/                   |              |
|    mean action          | 0.17231223   |
|    mean velocity x      | -1.47        |
|    mean velocity y      | -1.61        |
|    mean velocity z      | 21           |
|    mean_ep_length       | 61.4         |
|    mean_reward          | -5.97e+04    |
| time/                   |              |
|    total_timesteps      | 774500       |
| train/                  |              |
|    approx_kl            | 0.0017036092 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.001        |
|    loss                 | 1.74e+08     |
|    n_updates            | 3780         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.913        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=775000, episode_reward=-32877.20 +/- 28814.04
Episode length: 44.00 +/- 16.41
------------------------------------
| eval/              |             |
|    mean action     | -0.16951261 |
|    mean velocity x | 0.993       |
|    mean velocity y | 1.07        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 44          |
|    mean_reward     | -3.29e+04   |
| time/              |             |
|    total_timesteps | 775000      |
------------------------------------
Eval num_timesteps=775500, episode_reward=-51447.97 +/- 40427.23
Episode length: 57.40 +/- 23.47
------------------------------------
| eval/              |             |
|    mean action     | -0.64879197 |
|    mean velocity x | 3.43        |
|    mean velocity y | 4.27        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 57.4        |
|    mean_reward     | -5.14e+04   |
| time/              |             |
|    total_timesteps | 775500      |
------------------------------------
Eval num_timesteps=776000, episode_reward=-57897.26 +/- 39215.90
Episode length: 51.80 +/- 18.18
----------------------------------
| eval/              |           |
|    mean action     | -0.080876 |
|    mean velocity x | -0.493    |
|    mean velocity y | 0.597     |
|    mean velocity z | 19.7      |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -5.79e+04 |
| time/              |           |
|    total_timesteps | 776000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.2     |
|    ep_rew_mean     | -7.9e+04 |
| time/              |          |
|    fps             | 117      |
|    iterations      | 379      |
|    time_elapsed    | 6580     |
|    total_timesteps | 776192   |
---------------------------------
Eval num_timesteps=776500, episode_reward=-55755.24 +/- 29205.42
Episode length: 58.80 +/- 23.71
------------------------------------------
| eval/                   |              |
|    mean action          | -0.02075421  |
|    mean velocity x      | -0.571       |
|    mean velocity y      | -0.118       |
|    mean velocity z      | 18.6         |
|    mean_ep_length       | 58.8         |
|    mean_reward          | -5.58e+04    |
| time/                   |              |
|    total_timesteps      | 776500       |
| train/                  |              |
|    approx_kl            | 0.0043944414 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+08     |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.00362     |
|    std                  | 0.912        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=777000, episode_reward=-49760.57 +/- 35138.11
Episode length: 56.80 +/- 20.77
-----------------------------------
| eval/              |            |
|    mean action     | -0.9827124 |
|    mean velocity x | 2.06       |
|    mean velocity y | 5.32       |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -4.98e+04  |
| time/              |            |
|    total_timesteps | 777000     |
-----------------------------------
Eval num_timesteps=777500, episode_reward=-63249.33 +/- 47999.87
Episode length: 50.00 +/- 18.50
------------------------------------
| eval/              |             |
|    mean action     | -0.21464908 |
|    mean velocity x | 1.51        |
|    mean velocity y | 1.03        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 50          |
|    mean_reward     | -6.32e+04   |
| time/              |             |
|    total_timesteps | 777500      |
------------------------------------
Eval num_timesteps=778000, episode_reward=-71141.56 +/- 34220.45
Episode length: 60.20 +/- 19.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.1737863 |
|    mean velocity x | -0.883     |
|    mean velocity y | 0.206      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 60.2       |
|    mean_reward     | -7.11e+04  |
| time/              |            |
|    total_timesteps | 778000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.2      |
|    ep_rew_mean     | -8.29e+04 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 380       |
|    time_elapsed    | 6587      |
|    total_timesteps | 778240    |
----------------------------------
Eval num_timesteps=778500, episode_reward=-74611.47 +/- 20520.65
Episode length: 69.60 +/- 16.12
------------------------------------------
| eval/                   |              |
|    mean action          | -0.22109994  |
|    mean velocity x      | 0.546        |
|    mean velocity y      | 0.5          |
|    mean velocity z      | 19           |
|    mean_ep_length       | 69.6         |
|    mean_reward          | -7.46e+04    |
| time/                   |              |
|    total_timesteps      | 778500       |
| train/                  |              |
|    approx_kl            | 0.0016771681 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.001        |
|    loss                 | 8.36e+07     |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 0.912        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=779000, episode_reward=-58942.07 +/- 18819.11
Episode length: 65.00 +/- 20.70
-------------------------------------
| eval/              |              |
|    mean action     | -0.018314946 |
|    mean velocity x | -1.06        |
|    mean velocity y | -1.03        |
|    mean velocity z | 20           |
|    mean_ep_length  | 65           |
|    mean_reward     | -5.89e+04    |
| time/              |              |
|    total_timesteps | 779000       |
-------------------------------------
Eval num_timesteps=779500, episode_reward=-73240.09 +/- 41496.77
Episode length: 55.80 +/- 17.77
------------------------------------
| eval/              |             |
|    mean action     | -0.53223366 |
|    mean velocity x | 0.439       |
|    mean velocity y | 3.07        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 55.8        |
|    mean_reward     | -7.32e+04   |
| time/              |             |
|    total_timesteps | 779500      |
------------------------------------
Eval num_timesteps=780000, episode_reward=-83064.05 +/- 24810.16
Episode length: 65.20 +/- 9.11
------------------------------------
| eval/              |             |
|    mean action     | -0.38989156 |
|    mean velocity x | 2.41        |
|    mean velocity y | 1.51        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 65.2        |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 780000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.9      |
|    ep_rew_mean     | -8.31e+04 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 381       |
|    time_elapsed    | 6594      |
|    total_timesteps | 780288    |
----------------------------------
Eval num_timesteps=780500, episode_reward=-66428.69 +/- 9711.54
Episode length: 67.40 +/- 14.73
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0016149064 |
|    mean velocity x      | -0.287        |
|    mean velocity y      | -0.904        |
|    mean velocity z      | 21.2          |
|    mean_ep_length       | 67.4          |
|    mean_reward          | -6.64e+04     |
| time/                   |               |
|    total_timesteps      | 780500        |
| train/                  |               |
|    approx_kl            | 0.002451043   |
|    clip_fraction        | 0.0158        |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.001         |
|    loss                 | 9.99e+07      |
|    n_updates            | 3810          |
|    policy_gradient_loss | -0.00308      |
|    std                  | 0.912         |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=781000, episode_reward=-59140.46 +/- 36389.25
Episode length: 54.40 +/- 17.12
-----------------------------------
| eval/              |            |
|    mean action     | -0.5110613 |
|    mean velocity x | 0.727      |
|    mean velocity y | 3.16       |
|    mean velocity z | 16.6       |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -5.91e+04  |
| time/              |            |
|    total_timesteps | 781000     |
-----------------------------------
Eval num_timesteps=781500, episode_reward=-76011.17 +/- 15834.94
Episode length: 64.40 +/- 8.82
------------------------------------
| eval/              |             |
|    mean action     | -0.39665204 |
|    mean velocity x | 2.32        |
|    mean velocity y | 2.4         |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 64.4        |
|    mean_reward     | -7.6e+04    |
| time/              |             |
|    total_timesteps | 781500      |
------------------------------------
Eval num_timesteps=782000, episode_reward=-82068.74 +/- 19651.45
Episode length: 63.00 +/- 4.86
-----------------------------------
| eval/              |            |
|    mean action     | -0.1839982 |
|    mean velocity x | 0.415      |
|    mean velocity y | 1.68       |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 63         |
|    mean_reward     | -8.21e+04  |
| time/              |            |
|    total_timesteps | 782000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.7      |
|    ep_rew_mean     | -7.69e+04 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 382       |
|    time_elapsed    | 6601      |
|    total_timesteps | 782336    |
----------------------------------
Eval num_timesteps=782500, episode_reward=-53698.77 +/- 42431.44
Episode length: 55.40 +/- 25.59
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.7006142    |
|    mean velocity x      | 1.95          |
|    mean velocity y      | 4.05          |
|    mean velocity z      | 19.3          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -5.37e+04     |
| time/                   |               |
|    total_timesteps      | 782500        |
| train/                  |               |
|    approx_kl            | 0.00063514593 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.223         |
|    learning_rate        | 0.001         |
|    loss                 | 7.37e+07      |
|    n_updates            | 3820          |
|    policy_gradient_loss | -0.00123      |
|    std                  | 0.912         |
|    value_loss           | 1.86e+08      |
-------------------------------------------
Eval num_timesteps=783000, episode_reward=-54101.31 +/- 38845.69
Episode length: 56.80 +/- 33.59
-------------------------------------
| eval/              |              |
|    mean action     | -0.006038895 |
|    mean velocity x | -0.0236      |
|    mean velocity y | 1.21         |
|    mean velocity z | 17.7         |
|    mean_ep_length  | 56.8         |
|    mean_reward     | -5.41e+04    |
| time/              |              |
|    total_timesteps | 783000       |
-------------------------------------
Eval num_timesteps=783500, episode_reward=-57490.36 +/- 38441.49
Episode length: 53.20 +/- 12.14
-----------------------------------
| eval/              |            |
|    mean action     | 0.11572208 |
|    mean velocity x | -1.15      |
|    mean velocity y | -0.0107    |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 53.2       |
|    mean_reward     | -5.75e+04  |
| time/              |            |
|    total_timesteps | 783500     |
-----------------------------------
Eval num_timesteps=784000, episode_reward=-37616.01 +/- 27615.94
Episode length: 48.00 +/- 20.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.20201224 |
|    mean velocity x | -1.21      |
|    mean velocity y | -1.66      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 48         |
|    mean_reward     | -3.76e+04  |
| time/              |            |
|    total_timesteps | 784000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.4      |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 383       |
|    time_elapsed    | 6608      |
|    total_timesteps | 784384    |
----------------------------------
Eval num_timesteps=784500, episode_reward=-104890.52 +/- 26733.08
Episode length: 88.00 +/- 45.03
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14259164  |
|    mean velocity x      | -1.26        |
|    mean velocity y      | -0.0171      |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 88           |
|    mean_reward          | -1.05e+05    |
| time/                   |              |
|    total_timesteps      | 784500       |
| train/                  |              |
|    approx_kl            | 0.0014632674 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.001        |
|    loss                 | 8.25e+07     |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 0.912        |
|    value_loss           | 2.23e+08     |
------------------------------------------
Eval num_timesteps=785000, episode_reward=-74376.46 +/- 36232.67
Episode length: 69.20 +/- 18.19
------------------------------------
| eval/              |             |
|    mean action     | -0.13299662 |
|    mean velocity x | -0.0465     |
|    mean velocity y | -0.5        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 69.2        |
|    mean_reward     | -7.44e+04   |
| time/              |             |
|    total_timesteps | 785000      |
------------------------------------
Eval num_timesteps=785500, episode_reward=-78079.96 +/- 16836.14
Episode length: 64.40 +/- 8.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.23584098 |
|    mean velocity x | -0.938     |
|    mean velocity y | -2         |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 64.4       |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 785500     |
-----------------------------------
Eval num_timesteps=786000, episode_reward=-88537.57 +/- 19906.26
Episode length: 65.00 +/- 5.48
-----------------------------------
| eval/              |            |
|    mean action     | -0.5160767 |
|    mean velocity x | 0.502      |
|    mean velocity y | 2.79       |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 65         |
|    mean_reward     | -8.85e+04  |
| time/              |            |
|    total_timesteps | 786000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.4      |
|    ep_rew_mean     | -8.31e+04 |
| time/              |           |
|    fps             | 118       |
|    iterations      | 384       |
|    time_elapsed    | 6616      |
|    total_timesteps | 786432    |
----------------------------------
Eval num_timesteps=786500, episode_reward=-60723.77 +/- 34688.46
Episode length: 61.20 +/- 33.13
--------------------------------------------
| eval/                   |                |
|    mean action          | 0.100891404    |
|    mean velocity x      | -0.0871        |
|    mean velocity y      | -1.14          |
|    mean velocity z      | 18.4           |
|    mean_ep_length       | 61.2           |
|    mean_reward          | -6.07e+04      |
| time/                   |                |
|    total_timesteps      | 786500         |
| train/                  |                |
|    approx_kl            | 0.000104783045 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.98          |
|    explained_variance   | 0.202          |
|    learning_rate        | 0.001          |
|    loss                 | 1.17e+08       |
|    n_updates            | 3840           |
|    policy_gradient_loss | -0.000228      |
|    std                  | 0.912          |
|    value_loss           | 2.27e+08       |
--------------------------------------------
Eval num_timesteps=787000, episode_reward=-82818.26 +/- 34453.77
Episode length: 58.40 +/- 8.69
------------------------------------
| eval/              |             |
|    mean action     | -0.15197639 |
|    mean velocity x | -0.493      |
|    mean velocity y | 0.415       |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -8.28e+04   |
| time/              |             |
|    total_timesteps | 787000      |
------------------------------------
Eval num_timesteps=787500, episode_reward=-85111.41 +/- 43254.27
Episode length: 56.40 +/- 18.36
------------------------------------
| eval/              |             |
|    mean action     | -0.34517914 |
|    mean velocity x | -0.197      |
|    mean velocity y | 1.08        |
|    mean velocity z | 19          |
|    mean_ep_length  | 56.4        |
|    mean_reward     | -8.51e+04   |
| time/              |             |
|    total_timesteps | 787500      |
------------------------------------
Eval num_timesteps=788000, episode_reward=-74922.97 +/- 6491.87
Episode length: 69.40 +/- 8.01
-----------------------------------
| eval/              |            |
|    mean action     | 0.10154512 |
|    mean velocity x | -1.59      |
|    mean velocity y | -1.52      |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 69.4       |
|    mean_reward     | -7.49e+04  |
| time/              |            |
|    total_timesteps | 788000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.9      |
|    ep_rew_mean     | -7.88e+04 |
| time/              |           |
|    fps             | 119       |
|    iterations      | 385       |
|    time_elapsed    | 6623      |
|    total_timesteps | 788480    |
----------------------------------
Eval num_timesteps=788500, episode_reward=-94111.51 +/- 21452.65
Episode length: 79.80 +/- 26.16
------------------------------------------
| eval/                   |              |
|    mean action          | -0.25228733  |
|    mean velocity x      | -0.662       |
|    mean velocity y      | 1.56         |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 79.8         |
|    mean_reward          | -9.41e+04    |
| time/                   |              |
|    total_timesteps      | 788500       |
| train/                  |              |
|    approx_kl            | 0.0001908193 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.186        |
|    learning_rate        | 0.001        |
|    loss                 | 7.82e+07     |
|    n_updates            | 3850         |
|    policy_gradient_loss | -0.000497    |
|    std                  | 0.911        |
|    value_loss           | 2e+08        |
------------------------------------------
Eval num_timesteps=789000, episode_reward=-61211.70 +/- 41646.07
Episode length: 51.60 +/- 13.84
-------------------------------------
| eval/              |              |
|    mean action     | -0.032785073 |
|    mean velocity x | 0.193        |
|    mean velocity y | -0.177       |
|    mean velocity z | 20.6         |
|    mean_ep_length  | 51.6         |
|    mean_reward     | -6.12e+04    |
| time/              |              |
|    total_timesteps | 789000       |
-------------------------------------
Eval num_timesteps=789500, episode_reward=-56931.60 +/- 30248.33
Episode length: 61.60 +/- 11.83
------------------------------------
| eval/              |             |
|    mean action     | -0.32446986 |
|    mean velocity x | 0.381       |
|    mean velocity y | 1.24        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -5.69e+04   |
| time/              |             |
|    total_timesteps | 789500      |
------------------------------------
Eval num_timesteps=790000, episode_reward=-50624.00 +/- 14455.47
Episode length: 57.80 +/- 12.81
------------------------------------
| eval/              |             |
|    mean action     | 0.023519775 |
|    mean velocity x | 1.06        |
|    mean velocity y | 0.126       |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 57.8        |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 790000      |
------------------------------------
Eval num_timesteps=790500, episode_reward=-72615.91 +/- 24745.64
Episode length: 66.60 +/- 10.33
-------------------------------------
| eval/              |              |
|    mean action     | -0.046134736 |
|    mean velocity x | -0.319       |
|    mean velocity y | -0.457       |
|    mean velocity z | 17.3         |
|    mean_ep_length  | 66.6         |
|    mean_reward     | -7.26e+04    |
| time/              |              |
|    total_timesteps | 790500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -7.78e+04 |
| time/              |           |
|    fps             | 119       |
|    iterations      | 386       |
|    time_elapsed    | 6631      |
|    total_timesteps | 790528    |
----------------------------------
Eval num_timesteps=791000, episode_reward=-74407.92 +/- 41136.91
Episode length: 55.40 +/- 11.43
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3643877    |
|    mean velocity x      | 0.228         |
|    mean velocity y      | 0.821         |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -7.44e+04     |
| time/                   |               |
|    total_timesteps      | 791000        |
| train/                  |               |
|    approx_kl            | 0.00013765018 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.212         |
|    learning_rate        | 0.001         |
|    loss                 | 9.5e+07       |
|    n_updates            | 3860          |
|    policy_gradient_loss | -0.000615     |
|    std                  | 0.911         |
|    value_loss           | 1.87e+08      |
-------------------------------------------
Eval num_timesteps=791500, episode_reward=-47374.80 +/- 31471.11
Episode length: 49.40 +/- 9.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.05487488 |
|    mean velocity x | -1.79      |
|    mean velocity y | -2.18      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 49.4       |
|    mean_reward     | -4.74e+04  |
| time/              |            |
|    total_timesteps | 791500     |
-----------------------------------
Eval num_timesteps=792000, episode_reward=-77815.66 +/- 40968.99
Episode length: 58.60 +/- 20.04
-----------------------------------
| eval/              |            |
|    mean action     | 0.22447434 |
|    mean velocity x | -1.93      |
|    mean velocity y | -3.28      |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -7.78e+04  |
| time/              |            |
|    total_timesteps | 792000     |
-----------------------------------
Eval num_timesteps=792500, episode_reward=-68323.97 +/- 42142.44
Episode length: 54.00 +/- 11.03
-----------------------------------
| eval/              |            |
|    mean action     | -0.5962511 |
|    mean velocity x | 2.41       |
|    mean velocity y | 3.32       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 54         |
|    mean_reward     | -6.83e+04  |
| time/              |            |
|    total_timesteps | 792500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.5      |
|    ep_rew_mean     | -7.27e+04 |
| time/              |           |
|    fps             | 119       |
|    iterations      | 387       |
|    time_elapsed    | 6638      |
|    total_timesteps | 792576    |
----------------------------------
Eval num_timesteps=793000, episode_reward=-64813.48 +/- 33389.71
Episode length: 66.20 +/- 28.45
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.25574967 |
|    mean velocity x      | 2.48        |
|    mean velocity y      | 1.46        |
|    mean velocity z      | 20.2        |
|    mean_ep_length       | 66.2        |
|    mean_reward          | -6.48e+04   |
| time/                   |             |
|    total_timesteps      | 793000      |
| train/                  |             |
|    approx_kl            | 0.002607436 |
|    clip_fraction        | 0.00693     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.98       |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.001       |
|    loss                 | 1.07e+08    |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.911       |
|    value_loss           | 2.1e+08     |
-----------------------------------------
Eval num_timesteps=793500, episode_reward=-72443.39 +/- 38671.35
Episode length: 63.80 +/- 25.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.21347709 |
|    mean velocity x | -2.12      |
|    mean velocity y | -2.73      |
|    mean velocity z | 16.9       |
|    mean_ep_length  | 63.8       |
|    mean_reward     | -7.24e+04  |
| time/              |            |
|    total_timesteps | 793500     |
-----------------------------------
Eval num_timesteps=794000, episode_reward=-35654.82 +/- 25533.15
Episode length: 49.80 +/- 18.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.14158337 |
|    mean velocity x | -0.301     |
|    mean velocity y | -1.56      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 49.8       |
|    mean_reward     | -3.57e+04  |
| time/              |            |
|    total_timesteps | 794000     |
-----------------------------------
Eval num_timesteps=794500, episode_reward=-88008.75 +/- 42644.14
Episode length: 55.20 +/- 17.63
------------------------------------
| eval/              |             |
|    mean action     | -0.41001472 |
|    mean velocity x | 2.21        |
|    mean velocity y | 3.85        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -8.8e+04    |
| time/              |             |
|    total_timesteps | 794500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67        |
|    ep_rew_mean     | -7.26e+04 |
| time/              |           |
|    fps             | 119       |
|    iterations      | 388       |
|    time_elapsed    | 6645      |
|    total_timesteps | 794624    |
----------------------------------
Eval num_timesteps=795000, episode_reward=-93335.00 +/- 15484.29
Episode length: 70.20 +/- 5.67
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.57853115   |
|    mean velocity x      | 0.4           |
|    mean velocity y      | 3.64          |
|    mean velocity z      | 16.7          |
|    mean_ep_length       | 70.2          |
|    mean_reward          | -9.33e+04     |
| time/                   |               |
|    total_timesteps      | 795000        |
| train/                  |               |
|    approx_kl            | 0.00074455887 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.231         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+08      |
|    n_updates            | 3880          |
|    policy_gradient_loss | -0.0015       |
|    std                  | 0.911         |
|    value_loss           | 1.88e+08      |
-------------------------------------------
Eval num_timesteps=795500, episode_reward=-43667.43 +/- 33266.62
Episode length: 51.20 +/- 17.05
------------------------------------
| eval/              |             |
|    mean action     | -0.20184307 |
|    mean velocity x | 0.248       |
|    mean velocity y | -0.724      |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 51.2        |
|    mean_reward     | -4.37e+04   |
| time/              |             |
|    total_timesteps | 795500      |
------------------------------------
Eval num_timesteps=796000, episode_reward=-52995.75 +/- 24536.84
Episode length: 68.00 +/- 26.50
-----------------------------------
| eval/              |            |
|    mean action     | 0.17781904 |
|    mean velocity x | -1.46      |
|    mean velocity y | -1.12      |
|    mean velocity z | 15.1       |
|    mean_ep_length  | 68         |
|    mean_reward     | -5.3e+04   |
| time/              |            |
|    total_timesteps | 796000     |
-----------------------------------
Eval num_timesteps=796500, episode_reward=-87272.30 +/- 14427.13
Episode length: 65.20 +/- 2.14
------------------------------------
| eval/              |             |
|    mean action     | -0.33250517 |
|    mean velocity x | 1.42        |
|    mean velocity y | 2.3         |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 65.2        |
|    mean_reward     | -8.73e+04   |
| time/              |             |
|    total_timesteps | 796500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | -7.3e+04 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 389      |
|    time_elapsed    | 6652     |
|    total_timesteps | 796672   |
---------------------------------
Eval num_timesteps=797000, episode_reward=-84901.73 +/- 17108.52
Episode length: 66.80 +/- 13.50
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5265813    |
|    mean velocity x      | 1.72          |
|    mean velocity y      | 3.86          |
|    mean velocity z      | 20.7          |
|    mean_ep_length       | 66.8          |
|    mean_reward          | -8.49e+04     |
| time/                   |               |
|    total_timesteps      | 797000        |
| train/                  |               |
|    approx_kl            | 0.00068468734 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.236         |
|    learning_rate        | 0.001         |
|    loss                 | 8.96e+07      |
|    n_updates            | 3890          |
|    policy_gradient_loss | -0.00153      |
|    std                  | 0.911         |
|    value_loss           | 2.01e+08      |
-------------------------------------------
Eval num_timesteps=797500, episode_reward=-89584.58 +/- 17869.59
Episode length: 73.60 +/- 15.29
-----------------------------------
| eval/              |            |
|    mean action     | -0.5891591 |
|    mean velocity x | 1.78       |
|    mean velocity y | 3.96       |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 73.6       |
|    mean_reward     | -8.96e+04  |
| time/              |            |
|    total_timesteps | 797500     |
-----------------------------------
Eval num_timesteps=798000, episode_reward=-70061.64 +/- 39444.19
Episode length: 52.80 +/- 18.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.01262117 |
|    mean velocity x | -2.23      |
|    mean velocity y | -1.6       |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 52.8       |
|    mean_reward     | -7.01e+04  |
| time/              |            |
|    total_timesteps | 798000     |
-----------------------------------
Eval num_timesteps=798500, episode_reward=-47243.56 +/- 32802.97
Episode length: 48.40 +/- 21.17
------------------------------------
| eval/              |             |
|    mean action     | -0.08815255 |
|    mean velocity x | -0.859      |
|    mean velocity y | -2.16       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 48.4        |
|    mean_reward     | -4.72e+04   |
| time/              |             |
|    total_timesteps | 798500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -7.92e+04 |
| time/              |           |
|    fps             | 119       |
|    iterations      | 390       |
|    time_elapsed    | 6659      |
|    total_timesteps | 798720    |
----------------------------------
Eval num_timesteps=799000, episode_reward=-85749.82 +/- 24917.74
Episode length: 68.00 +/- 3.74
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.60885984   |
|    mean velocity x      | 1.69          |
|    mean velocity y      | 3.12          |
|    mean velocity z      | 21.8          |
|    mean_ep_length       | 68            |
|    mean_reward          | -8.57e+04     |
| time/                   |               |
|    total_timesteps      | 799000        |
| train/                  |               |
|    approx_kl            | 3.0326162e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.001         |
|    loss                 | 6.04e+07      |
|    n_updates            | 3900          |
|    policy_gradient_loss | -0.000229     |
|    std                  | 0.911         |
|    value_loss           | 2.43e+08      |
-------------------------------------------
Eval num_timesteps=799500, episode_reward=-83865.98 +/- 28231.45
Episode length: 62.00 +/- 4.05
-----------------------------------
| eval/              |            |
|    mean action     | -0.8239893 |
|    mean velocity x | 2.22       |
|    mean velocity y | 4.69       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 62         |
|    mean_reward     | -8.39e+04  |
| time/              |            |
|    total_timesteps | 799500     |
-----------------------------------
Eval num_timesteps=800000, episode_reward=-102796.81 +/- 12907.31
Episode length: 73.20 +/- 11.53
------------------------------------
| eval/              |             |
|    mean action     | -0.32269824 |
|    mean velocity x | 2.81        |
|    mean velocity y | 2.69        |
|    mean velocity z | 15.6        |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -1.03e+05   |
| time/              |             |
|    total_timesteps | 800000      |
------------------------------------
Eval num_timesteps=800500, episode_reward=-52821.45 +/- 38083.85
Episode length: 48.40 +/- 19.06
-----------------------------------
| eval/              |            |
|    mean action     | 0.13619769 |
|    mean velocity x | -1.33      |
|    mean velocity y | -0.475     |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 48.4       |
|    mean_reward     | -5.28e+04  |
| time/              |            |
|    total_timesteps | 800500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.8      |
|    ep_rew_mean     | -8.01e+04 |
| time/              |           |
|    fps             | 120       |
|    iterations      | 391       |
|    time_elapsed    | 6666      |
|    total_timesteps | 800768    |
----------------------------------
Eval num_timesteps=801000, episode_reward=-28693.84 +/- 39245.02
Episode length: 40.60 +/- 20.84
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.062331744 |
|    mean velocity x      | -0.873      |
|    mean velocity y      | -0.578      |
|    mean velocity z      | 22          |
|    mean_ep_length       | 40.6        |
|    mean_reward          | -2.87e+04   |
| time/                   |             |
|    total_timesteps      | 801000      |
| train/                  |             |
|    approx_kl            | 0.004446741 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.001       |
|    loss                 | 1.3e+08     |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.00396    |
|    std                  | 0.91        |
|    value_loss           | 1.93e+08    |
-----------------------------------------
Eval num_timesteps=801500, episode_reward=-76099.38 +/- 20972.71
Episode length: 72.00 +/- 15.96
----------------------------------
| eval/              |           |
|    mean action     | 0.3616181 |
|    mean velocity x | -0.687    |
|    mean velocity y | -1.34     |
|    mean velocity z | 19.8      |
|    mean_ep_length  | 72        |
|    mean_reward     | -7.61e+04 |
| time/              |           |
|    total_timesteps | 801500    |
----------------------------------
Eval num_timesteps=802000, episode_reward=-75917.99 +/- 31067.65
Episode length: 67.20 +/- 14.50
-----------------------------------
| eval/              |            |
|    mean action     | -0.5623314 |
|    mean velocity x | 2.46       |
|    mean velocity y | 3.96       |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 67.2       |
|    mean_reward     | -7.59e+04  |
| time/              |            |
|    total_timesteps | 802000     |
-----------------------------------
Eval num_timesteps=802500, episode_reward=-54952.43 +/- 43653.80
Episode length: 48.80 +/- 17.54
------------------------------------
| eval/              |             |
|    mean action     | -0.19842115 |
|    mean velocity x | 0.338       |
|    mean velocity y | 1.77        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -5.5e+04    |
| time/              |             |
|    total_timesteps | 802500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.1      |
|    ep_rew_mean     | -8.32e+04 |
| time/              |           |
|    fps             | 120       |
|    iterations      | 392       |
|    time_elapsed    | 6673      |
|    total_timesteps | 802816    |
----------------------------------
Eval num_timesteps=803000, episode_reward=-90857.46 +/- 17590.73
Episode length: 67.40 +/- 6.71
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6049976   |
|    mean velocity x      | 3.1          |
|    mean velocity y      | 3.97         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 67.4         |
|    mean_reward          | -9.09e+04    |
| time/                   |              |
|    total_timesteps      | 803000       |
| train/                  |              |
|    approx_kl            | 0.0010796222 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.001        |
|    loss                 | 9.79e+07     |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 0.911        |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=803500, episode_reward=-64737.14 +/- 31862.24
Episode length: 55.80 +/- 9.50
-----------------------------------
| eval/              |            |
|    mean action     | -0.1060087 |
|    mean velocity x | 1.04       |
|    mean velocity y | 0.341      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 55.8       |
|    mean_reward     | -6.47e+04  |
| time/              |            |
|    total_timesteps | 803500     |
-----------------------------------
Eval num_timesteps=804000, episode_reward=-68760.18 +/- 31986.36
Episode length: 57.80 +/- 12.42
-----------------------------------
| eval/              |            |
|    mean action     | 0.20186023 |
|    mean velocity x | -0.516     |
|    mean velocity y | -1.71      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -6.88e+04  |
| time/              |            |
|    total_timesteps | 804000     |
-----------------------------------
Eval num_timesteps=804500, episode_reward=-74781.71 +/- 26073.14
Episode length: 63.00 +/- 5.10
------------------------------------
| eval/              |             |
|    mean action     | -0.38045922 |
|    mean velocity x | 1.99        |
|    mean velocity y | 2.08        |
|    mean velocity z | 18          |
|    mean_ep_length  | 63          |
|    mean_reward     | -7.48e+04   |
| time/              |             |
|    total_timesteps | 804500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.5      |
|    ep_rew_mean     | -8.02e+04 |
| time/              |           |
|    fps             | 120       |
|    iterations      | 393       |
|    time_elapsed    | 6681      |
|    total_timesteps | 804864    |
----------------------------------
Eval num_timesteps=805000, episode_reward=-61416.79 +/- 38682.16
Episode length: 53.40 +/- 11.04
------------------------------------------
| eval/                   |              |
|    mean action          | 0.3259209    |
|    mean velocity x      | -0.36        |
|    mean velocity y      | -1.31        |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 53.4         |
|    mean_reward          | -6.14e+04    |
| time/                   |              |
|    total_timesteps      | 805000       |
| train/                  |              |
|    approx_kl            | 0.0019044307 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.001        |
|    loss                 | 1.14e+08     |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.911        |
|    value_loss           | 1.82e+08     |
------------------------------------------
Eval num_timesteps=805500, episode_reward=-77185.49 +/- 46646.33
Episode length: 54.40 +/- 21.26
------------------------------------
| eval/              |             |
|    mean action     | -0.06992676 |
|    mean velocity x | -0.495      |
|    mean velocity y | 0.806       |
|    mean velocity z | 17          |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 805500      |
------------------------------------
Eval num_timesteps=806000, episode_reward=-58334.19 +/- 29091.05
Episode length: 63.20 +/- 17.22
----------------------------------
| eval/              |           |
|    mean action     | -0.299698 |
|    mean velocity x | 1.25      |
|    mean velocity y | 2.44      |
|    mean velocity z | 20        |
|    mean_ep_length  | 63.2      |
|    mean_reward     | -5.83e+04 |
| time/              |           |
|    total_timesteps | 806000    |
----------------------------------
Eval num_timesteps=806500, episode_reward=-55318.59 +/- 32630.62
Episode length: 52.60 +/- 10.76
------------------------------------
| eval/              |             |
|    mean action     | 0.071597286 |
|    mean velocity x | -0.0874     |
|    mean velocity y | -0.466      |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 52.6        |
|    mean_reward     | -5.53e+04   |
| time/              |             |
|    total_timesteps | 806500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.1      |
|    ep_rew_mean     | -7.72e+04 |
| time/              |           |
|    fps             | 120       |
|    iterations      | 394       |
|    time_elapsed    | 6688      |
|    total_timesteps | 806912    |
----------------------------------
Eval num_timesteps=807000, episode_reward=-73876.78 +/- 14679.41
Episode length: 68.40 +/- 9.85
------------------------------------------
| eval/                   |              |
|    mean action          | 0.17389977   |
|    mean velocity x      | 0.221        |
|    mean velocity y      | -0.273       |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 68.4         |
|    mean_reward          | -7.39e+04    |
| time/                   |              |
|    total_timesteps      | 807000       |
| train/                  |              |
|    approx_kl            | 0.0004279945 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+08     |
|    n_updates            | 3940         |
|    policy_gradient_loss | -0.000842    |
|    std                  | 0.911        |
|    value_loss           | 1.8e+08      |
------------------------------------------
Eval num_timesteps=807500, episode_reward=-96265.83 +/- 34901.82
Episode length: 60.80 +/- 6.94
------------------------------------
| eval/              |             |
|    mean action     | 0.092467315 |
|    mean velocity x | 0.617       |
|    mean velocity y | -0.344      |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -9.63e+04   |
| time/              |             |
|    total_timesteps | 807500      |
------------------------------------
Eval num_timesteps=808000, episode_reward=-60077.84 +/- 34979.90
Episode length: 59.60 +/- 14.39
------------------------------------
| eval/              |             |
|    mean action     | 0.032681197 |
|    mean velocity x | 0.977       |
|    mean velocity y | 1.44        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -6.01e+04   |
| time/              |             |
|    total_timesteps | 808000      |
------------------------------------
Eval num_timesteps=808500, episode_reward=-54349.32 +/- 39741.14
Episode length: 52.20 +/- 19.69
------------------------------------
| eval/              |             |
|    mean action     | -0.35052305 |
|    mean velocity x | 1.44        |
|    mean velocity y | 2.32        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -5.43e+04   |
| time/              |             |
|    total_timesteps | 808500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76        |
|    ep_rew_mean     | -8.65e+04 |
| time/              |           |
|    fps             | 120       |
|    iterations      | 395       |
|    time_elapsed    | 6695      |
|    total_timesteps | 808960    |
----------------------------------
Eval num_timesteps=809000, episode_reward=-42272.54 +/- 41469.61
Episode length: 43.20 +/- 24.74
------------------------------------------
| eval/                   |              |
|    mean action          | 0.13082553   |
|    mean velocity x      | -1.27        |
|    mean velocity y      | -1.86        |
|    mean velocity z      | 22.7         |
|    mean_ep_length       | 43.2         |
|    mean_reward          | -4.23e+04    |
| time/                   |              |
|    total_timesteps      | 809000       |
| train/                  |              |
|    approx_kl            | 0.0014324042 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.209        |
|    learning_rate        | 0.001        |
|    loss                 | 9.01e+07     |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.911        |
|    value_loss           | 2.74e+08     |
------------------------------------------
Eval num_timesteps=809500, episode_reward=-65809.54 +/- 34420.29
Episode length: 78.00 +/- 33.80
-----------------------------------
| eval/              |            |
|    mean action     | -0.5465234 |
|    mean velocity x | 2.18       |
|    mean velocity y | 4          |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 78         |
|    mean_reward     | -6.58e+04  |
| time/              |            |
|    total_timesteps | 809500     |
-----------------------------------
Eval num_timesteps=810000, episode_reward=-73627.23 +/- 37711.56
Episode length: 61.20 +/- 19.57
------------------------------------
| eval/              |             |
|    mean action     | 0.059054323 |
|    mean velocity x | -0.869      |
|    mean velocity y | -1.2        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 810000      |
------------------------------------
Eval num_timesteps=810500, episode_reward=-91758.07 +/- 24265.27
Episode length: 62.80 +/- 5.88
-----------------------------------
| eval/              |            |
|    mean action     | -0.8829872 |
|    mean velocity x | 2.92       |
|    mean velocity y | 5.19       |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -9.18e+04  |
| time/              |            |
|    total_timesteps | 810500     |
-----------------------------------
Eval num_timesteps=811000, episode_reward=-57349.87 +/- 37739.77
Episode length: 55.80 +/- 23.76
------------------------------------
| eval/              |             |
|    mean action     | -0.23948136 |
|    mean velocity x | 0.812       |
|    mean velocity y | 1.83        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 55.8        |
|    mean_reward     | -5.73e+04   |
| time/              |             |
|    total_timesteps | 811000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.52e+04 |
| time/              |           |
|    fps             | 120       |
|    iterations      | 396       |
|    time_elapsed    | 6702      |
|    total_timesteps | 811008    |
----------------------------------
Eval num_timesteps=811500, episode_reward=-53047.34 +/- 44953.69
Episode length: 69.00 +/- 48.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1784562   |
|    mean velocity x      | -0.734       |
|    mean velocity y      | 0.323        |
|    mean velocity z      | 18.3         |
|    mean_ep_length       | 69           |
|    mean_reward          | -5.3e+04     |
| time/                   |              |
|    total_timesteps      | 811500       |
| train/                  |              |
|    approx_kl            | 7.876789e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.001        |
|    loss                 | 1.31e+08     |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.000552    |
|    std                  | 0.911        |
|    value_loss           | 2.03e+08     |
------------------------------------------
Eval num_timesteps=812000, episode_reward=-68786.24 +/- 31035.72
Episode length: 62.20 +/- 14.08
------------------------------------
| eval/              |             |
|    mean action     | -0.26883265 |
|    mean velocity x | 2.24        |
|    mean velocity y | 2.84        |
|    mean velocity z | 22.3        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -6.88e+04   |
| time/              |             |
|    total_timesteps | 812000      |
------------------------------------
Eval num_timesteps=812500, episode_reward=-63699.41 +/- 42652.73
Episode length: 55.40 +/- 21.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.40698183 |
|    mean velocity x | -0.568     |
|    mean velocity y | -2.41      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 55.4       |
|    mean_reward     | -6.37e+04  |
| time/              |            |
|    total_timesteps | 812500     |
-----------------------------------
Eval num_timesteps=813000, episode_reward=-59802.15 +/- 26830.09
Episode length: 59.20 +/- 5.64
------------------------------------
| eval/              |             |
|    mean action     | -0.60580564 |
|    mean velocity x | 2.49        |
|    mean velocity y | 4.91        |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -5.98e+04   |
| time/              |             |
|    total_timesteps | 813000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.3      |
|    ep_rew_mean     | -8.91e+04 |
| time/              |           |
|    fps             | 121       |
|    iterations      | 397       |
|    time_elapsed    | 6710      |
|    total_timesteps | 813056    |
----------------------------------
Eval num_timesteps=813500, episode_reward=-73859.37 +/- 33861.56
Episode length: 62.20 +/- 9.68
------------------------------------------
| eval/                   |              |
|    mean action          | 0.030035622  |
|    mean velocity x      | -0.496       |
|    mean velocity y      | -0.13        |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 62.2         |
|    mean_reward          | -7.39e+04    |
| time/                   |              |
|    total_timesteps      | 813500       |
| train/                  |              |
|    approx_kl            | 0.0026935406 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+08     |
|    n_updates            | 3970         |
|    policy_gradient_loss | -0.00351     |
|    std                  | 0.911        |
|    value_loss           | 2.62e+08     |
------------------------------------------
Eval num_timesteps=814000, episode_reward=-64012.42 +/- 39160.28
Episode length: 53.00 +/- 20.12
------------------------------------
| eval/              |             |
|    mean action     | -0.31178027 |
|    mean velocity x | 1.21        |
|    mean velocity y | 1.51        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 53          |
|    mean_reward     | -6.4e+04    |
| time/              |             |
|    total_timesteps | 814000      |
------------------------------------
Eval num_timesteps=814500, episode_reward=-77695.55 +/- 30221.62
Episode length: 58.80 +/- 8.21
------------------------------------
| eval/              |             |
|    mean action     | -0.25704303 |
|    mean velocity x | 1.72        |
|    mean velocity y | 1.97        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 58.8        |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 814500      |
------------------------------------
Eval num_timesteps=815000, episode_reward=-83703.15 +/- 18895.45
Episode length: 73.20 +/- 18.15
------------------------------------
| eval/              |             |
|    mean action     | -0.43607205 |
|    mean velocity x | 0.807       |
|    mean velocity y | 3.6         |
|    mean velocity z | 19          |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 815000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.3      |
|    ep_rew_mean     | -8.84e+04 |
| time/              |           |
|    fps             | 121       |
|    iterations      | 398       |
|    time_elapsed    | 6717      |
|    total_timesteps | 815104    |
----------------------------------
Eval num_timesteps=815500, episode_reward=-77248.07 +/- 32823.12
Episode length: 60.80 +/- 9.68
------------------------------------------
| eval/                   |              |
|    mean action          | -0.078813344 |
|    mean velocity x      | 0.314        |
|    mean velocity y      | 0.146        |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 60.8         |
|    mean_reward          | -7.72e+04    |
| time/                   |              |
|    total_timesteps      | 815500       |
| train/                  |              |
|    approx_kl            | 0.0030984594 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.001        |
|    loss                 | 8.44e+07     |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.00363     |
|    std                  | 0.912        |
|    value_loss           | 2.5e+08      |
------------------------------------------
Eval num_timesteps=816000, episode_reward=-66625.84 +/- 52833.94
Episode length: 62.20 +/- 47.81
------------------------------------
| eval/              |             |
|    mean action     | -0.14962706 |
|    mean velocity x | 0.219       |
|    mean velocity y | 0.799       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -6.66e+04   |
| time/              |             |
|    total_timesteps | 816000      |
------------------------------------
Eval num_timesteps=816500, episode_reward=-54000.62 +/- 25283.02
Episode length: 56.20 +/- 8.68
------------------------------------
| eval/              |             |
|    mean action     | -0.07346739 |
|    mean velocity x | -0.295      |
|    mean velocity y | -0.65       |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 56.2        |
|    mean_reward     | -5.4e+04    |
| time/              |             |
|    total_timesteps | 816500      |
------------------------------------
Eval num_timesteps=817000, episode_reward=-53993.76 +/- 31485.20
Episode length: 54.40 +/- 21.56
------------------------------------
| eval/              |             |
|    mean action     | -0.11859134 |
|    mean velocity x | 1.52        |
|    mean velocity y | 1.8         |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5.4e+04    |
| time/              |             |
|    total_timesteps | 817000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.7      |
|    ep_rew_mean     | -8.62e+04 |
| time/              |           |
|    fps             | 121       |
|    iterations      | 399       |
|    time_elapsed    | 6724      |
|    total_timesteps | 817152    |
----------------------------------
Eval num_timesteps=817500, episode_reward=-76988.85 +/- 39343.68
Episode length: 53.80 +/- 18.73
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.3804624     |
|    mean velocity x      | 1.45           |
|    mean velocity y      | 3.27           |
|    mean velocity z      | 19.3           |
|    mean_ep_length       | 53.8           |
|    mean_reward          | -7.7e+04       |
| time/                   |                |
|    total_timesteps      | 817500         |
| train/                  |                |
|    approx_kl            | 0.000121663266 |
|    clip_fraction        | 4.88e-05       |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.98          |
|    explained_variance   | 0.209          |
|    learning_rate        | 0.001          |
|    loss                 | 1.55e+08       |
|    n_updates            | 3990           |
|    policy_gradient_loss | -0.000712      |
|    std                  | 0.911          |
|    value_loss           | 2.48e+08       |
--------------------------------------------
Eval num_timesteps=818000, episode_reward=-73601.48 +/- 35395.90
Episode length: 68.80 +/- 34.99
-----------------------------------
| eval/              |            |
|    mean action     | 0.32527706 |
|    mean velocity x | -0.879     |
|    mean velocity y | -1.18      |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 68.8       |
|    mean_reward     | -7.36e+04  |
| time/              |            |
|    total_timesteps | 818000     |
-----------------------------------
Eval num_timesteps=818500, episode_reward=-57197.65 +/- 34085.03
Episode length: 58.20 +/- 10.72
------------------------------------
| eval/              |             |
|    mean action     | -0.13681155 |
|    mean velocity x | 0.173       |
|    mean velocity y | 0.728       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -5.72e+04   |
| time/              |             |
|    total_timesteps | 818500      |
------------------------------------
Eval num_timesteps=819000, episode_reward=-71002.41 +/- 35694.08
Episode length: 64.20 +/- 19.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.29005197 |
|    mean velocity x | -1.84      |
|    mean velocity y | -2.66      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 64.2       |
|    mean_reward     | -7.1e+04   |
| time/              |            |
|    total_timesteps | 819000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.4      |
|    ep_rew_mean     | -8.51e+04 |
| time/              |           |
|    fps             | 121       |
|    iterations      | 400       |
|    time_elapsed    | 6731      |
|    total_timesteps | 819200    |
----------------------------------
Eval num_timesteps=819500, episode_reward=-34735.51 +/- 40729.58
Episode length: 36.40 +/- 24.22
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.4868638     |
|    mean velocity x      | -0.741        |
|    mean velocity y      | -2.03         |
|    mean velocity z      | 15.7          |
|    mean_ep_length       | 36.4          |
|    mean_reward          | -3.47e+04     |
| time/                   |               |
|    total_timesteps      | 819500        |
| train/                  |               |
|    approx_kl            | 0.00021930487 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.98         |
|    explained_variance   | 0.234         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+08      |
|    n_updates            | 4000          |
|    policy_gradient_loss | -0.000607     |
|    std                  | 0.912         |
|    value_loss           | 2.04e+08      |
-------------------------------------------
Eval num_timesteps=820000, episode_reward=-69497.58 +/- 18512.84
Episode length: 64.60 +/- 15.86
-----------------------------------
| eval/              |            |
|    mean action     | 0.25251377 |
|    mean velocity x | -2.5       |
|    mean velocity y | -3.02      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 64.6       |
|    mean_reward     | -6.95e+04  |
| time/              |            |
|    total_timesteps | 820000     |
-----------------------------------
Eval num_timesteps=820500, episode_reward=-70839.48 +/- 30841.01
Episode length: 65.80 +/- 10.94
-----------------------------------
| eval/              |            |
|    mean action     | -0.5109857 |
|    mean velocity x | 1.62       |
|    mean velocity y | 2.07       |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -7.08e+04  |
| time/              |            |
|    total_timesteps | 820500     |
-----------------------------------
Eval num_timesteps=821000, episode_reward=-83818.97 +/- 15668.50
Episode length: 73.40 +/- 8.11
------------------------------------
| eval/              |             |
|    mean action     | -0.14159371 |
|    mean velocity x | -1.4        |
|    mean velocity y | 0.00732     |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 73.4        |
|    mean_reward     | -8.38e+04   |
| time/              |             |
|    total_timesteps | 821000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76        |
|    ep_rew_mean     | -8.74e+04 |
| time/              |           |
|    fps             | 121       |
|    iterations      | 401       |
|    time_elapsed    | 6738      |
|    total_timesteps | 821248    |
----------------------------------
Eval num_timesteps=821500, episode_reward=-48859.37 +/- 44629.30
Episode length: 46.20 +/- 21.72
------------------------------------------
| eval/                   |              |
|    mean action          | 0.4035106    |
|    mean velocity x      | 0.141        |
|    mean velocity y      | -2.27        |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 46.2         |
|    mean_reward          | -4.89e+04    |
| time/                   |              |
|    total_timesteps      | 821500       |
| train/                  |              |
|    approx_kl            | 0.0034341488 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.198        |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+08     |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.911        |
|    value_loss           | 2.27e+08     |
------------------------------------------
Eval num_timesteps=822000, episode_reward=-48999.97 +/- 43390.88
Episode length: 48.20 +/- 13.60
------------------------------------
| eval/              |             |
|    mean action     | -0.15808274 |
|    mean velocity x | -0.114      |
|    mean velocity y | 1.54        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 48.2        |
|    mean_reward     | -4.9e+04    |
| time/              |             |
|    total_timesteps | 822000      |
------------------------------------
Eval num_timesteps=822500, episode_reward=-61042.72 +/- 48502.03
Episode length: 48.00 +/- 21.75
-----------------------------------
| eval/              |            |
|    mean action     | 0.20162725 |
|    mean velocity x | 1.34       |
|    mean velocity y | -1.12      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 48         |
|    mean_reward     | -6.1e+04   |
| time/              |            |
|    total_timesteps | 822500     |
-----------------------------------
Eval num_timesteps=823000, episode_reward=-81364.85 +/- 25474.23
Episode length: 61.80 +/- 9.79
------------------------------------
| eval/              |             |
|    mean action     | -0.14296216 |
|    mean velocity x | 0.418       |
|    mean velocity y | 1.29        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -8.14e+04   |
| time/              |             |
|    total_timesteps | 823000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.16e+04 |
| time/              |           |
|    fps             | 122       |
|    iterations      | 402       |
|    time_elapsed    | 6745      |
|    total_timesteps | 823296    |
----------------------------------
Eval num_timesteps=823500, episode_reward=-79710.50 +/- 40014.57
Episode length: 59.80 +/- 28.39
------------------------------------------
| eval/                   |              |
|    mean action          | 0.19306847   |
|    mean velocity x      | -0.165       |
|    mean velocity y      | -1.1         |
|    mean velocity z      | 17.7         |
|    mean_ep_length       | 59.8         |
|    mean_reward          | -7.97e+04    |
| time/                   |              |
|    total_timesteps      | 823500       |
| train/                  |              |
|    approx_kl            | 8.590374e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.221        |
|    learning_rate        | 0.001        |
|    loss                 | 5.91e+07     |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.000447    |
|    std                  | 0.911        |
|    value_loss           | 1.8e+08      |
------------------------------------------
Eval num_timesteps=824000, episode_reward=-78293.92 +/- 18305.90
Episode length: 59.40 +/- 2.80
-------------------------------------
| eval/              |              |
|    mean action     | -0.040067337 |
|    mean velocity x | -0.555       |
|    mean velocity y | -0.233       |
|    mean velocity z | 17.7         |
|    mean_ep_length  | 59.4         |
|    mean_reward     | -7.83e+04    |
| time/              |              |
|    total_timesteps | 824000       |
-------------------------------------
Eval num_timesteps=824500, episode_reward=-53385.62 +/- 39708.04
Episode length: 47.60 +/- 19.95
------------------------------------
| eval/              |             |
|    mean action     | -0.38445282 |
|    mean velocity x | 0.264       |
|    mean velocity y | 2.16        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 47.6        |
|    mean_reward     | -5.34e+04   |
| time/              |             |
|    total_timesteps | 824500      |
------------------------------------
Eval num_timesteps=825000, episode_reward=-59279.21 +/- 48404.88
Episode length: 47.80 +/- 27.21
-----------------------------------
| eval/              |            |
|    mean action     | -0.5154001 |
|    mean velocity x | 3.27       |
|    mean velocity y | 4.25       |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 47.8       |
|    mean_reward     | -5.93e+04  |
| time/              |            |
|    total_timesteps | 825000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -7.83e+04 |
| time/              |           |
|    fps             | 122       |
|    iterations      | 403       |
|    time_elapsed    | 6752      |
|    total_timesteps | 825344    |
----------------------------------
Eval num_timesteps=825500, episode_reward=-66605.35 +/- 48757.44
Episode length: 52.40 +/- 22.68
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37563178  |
|    mean velocity x      | 0.795        |
|    mean velocity y      | 1.89         |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 52.4         |
|    mean_reward          | -6.66e+04    |
| time/                   |              |
|    total_timesteps      | 825500       |
| train/                  |              |
|    approx_kl            | 0.0025444715 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.233        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+08     |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 0.91         |
|    value_loss           | 1.89e+08     |
------------------------------------------
Eval num_timesteps=826000, episode_reward=-49571.45 +/- 38395.11
Episode length: 72.80 +/- 49.29
------------------------------------
| eval/              |             |
|    mean action     | -0.89156055 |
|    mean velocity x | 3.47        |
|    mean velocity y | 5.87        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 72.8        |
|    mean_reward     | -4.96e+04   |
| time/              |             |
|    total_timesteps | 826000      |
------------------------------------
Eval num_timesteps=826500, episode_reward=-76494.40 +/- 30229.03
Episode length: 64.80 +/- 11.36
------------------------------------
| eval/              |             |
|    mean action     | -0.47065082 |
|    mean velocity x | 1.58        |
|    mean velocity y | 2           |
|    mean velocity z | 17          |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.65e+04   |
| time/              |             |
|    total_timesteps | 826500      |
------------------------------------
Eval num_timesteps=827000, episode_reward=-80928.80 +/- 39745.64
Episode length: 61.20 +/- 16.49
------------------------------------
| eval/              |             |
|    mean action     | -0.65360016 |
|    mean velocity x | 1.52        |
|    mean velocity y | 3.87        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -8.09e+04   |
| time/              |             |
|    total_timesteps | 827000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.7      |
|    ep_rew_mean     | -7.78e+04 |
| time/              |           |
|    fps             | 122       |
|    iterations      | 404       |
|    time_elapsed    | 6759      |
|    total_timesteps | 827392    |
----------------------------------
Eval num_timesteps=827500, episode_reward=-49255.69 +/- 38375.39
Episode length: 46.00 +/- 17.94
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1642648   |
|    mean velocity x      | 0.19         |
|    mean velocity y      | 1.07         |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 46           |
|    mean_reward          | -4.93e+04    |
| time/                   |              |
|    total_timesteps      | 827500       |
| train/                  |              |
|    approx_kl            | 0.0026419263 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.001        |
|    loss                 | 9.42e+07     |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 0.91         |
|    value_loss           | 1.92e+08     |
------------------------------------------
Eval num_timesteps=828000, episode_reward=-33070.44 +/- 32057.02
Episode length: 41.40 +/- 14.18
-----------------------------------
| eval/              |            |
|    mean action     | -0.3257774 |
|    mean velocity x | 1.04       |
|    mean velocity y | 1.47       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 41.4       |
|    mean_reward     | -3.31e+04  |
| time/              |            |
|    total_timesteps | 828000     |
-----------------------------------
Eval num_timesteps=828500, episode_reward=-88902.89 +/- 31228.78
Episode length: 60.80 +/- 7.08
----------------------------------
| eval/              |           |
|    mean action     | 0.1987052 |
|    mean velocity x | -0.912    |
|    mean velocity y | -1.15     |
|    mean velocity z | 19.1      |
|    mean_ep_length  | 60.8      |
|    mean_reward     | -8.89e+04 |
| time/              |           |
|    total_timesteps | 828500    |
----------------------------------
Eval num_timesteps=829000, episode_reward=-72896.30 +/- 32673.38
Episode length: 56.60 +/- 12.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.15191348 |
|    mean velocity x | -1.89      |
|    mean velocity y | -1.72      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -7.29e+04  |
| time/              |            |
|    total_timesteps | 829000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 405      |
|    time_elapsed    | 6766     |
|    total_timesteps | 829440   |
---------------------------------
Eval num_timesteps=829500, episode_reward=-35387.38 +/- 29425.31
Episode length: 49.80 +/- 23.22
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15382265  |
|    mean velocity x      | 0.648        |
|    mean velocity y      | 1.27         |
|    mean velocity z      | 16.1         |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -3.54e+04    |
| time/                   |              |
|    total_timesteps      | 829500       |
| train/                  |              |
|    approx_kl            | 0.0001622602 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+08     |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.000511    |
|    std                  | 0.91         |
|    value_loss           | 2.07e+08     |
------------------------------------------
Eval num_timesteps=830000, episode_reward=-67982.81 +/- 34084.49
Episode length: 66.80 +/- 22.36
----------------------------------
| eval/              |           |
|    mean action     | 0.4579311 |
|    mean velocity x | -1.21     |
|    mean velocity y | -2.94     |
|    mean velocity z | 16.3      |
|    mean_ep_length  | 66.8      |
|    mean_reward     | -6.8e+04  |
| time/              |           |
|    total_timesteps | 830000    |
----------------------------------
Eval num_timesteps=830500, episode_reward=-75844.86 +/- 26689.96
Episode length: 58.20 +/- 5.19
------------------------------------
| eval/              |             |
|    mean action     | -0.31245795 |
|    mean velocity x | 0.647       |
|    mean velocity y | 2.04        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 830500      |
------------------------------------
Eval num_timesteps=831000, episode_reward=-65238.24 +/- 28691.32
Episode length: 55.60 +/- 8.57
------------------------------------
| eval/              |             |
|    mean action     | -0.37914735 |
|    mean velocity x | 0.442       |
|    mean velocity y | 1.98        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 55.6        |
|    mean_reward     | -6.52e+04   |
| time/              |             |
|    total_timesteps | 831000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -7.83e+04 |
| time/              |           |
|    fps             | 122       |
|    iterations      | 406       |
|    time_elapsed    | 6774      |
|    total_timesteps | 831488    |
----------------------------------
Eval num_timesteps=831500, episode_reward=-90329.05 +/- 29658.93
Episode length: 64.40 +/- 7.96
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2233746    |
|    mean velocity x      | 1.44          |
|    mean velocity y      | 1.8           |
|    mean velocity z      | 19.3          |
|    mean_ep_length       | 64.4          |
|    mean_reward          | -9.03e+04     |
| time/                   |               |
|    total_timesteps      | 831500        |
| train/                  |               |
|    approx_kl            | 0.00016078088 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+08      |
|    n_updates            | 4060          |
|    policy_gradient_loss | -0.00066      |
|    std                  | 0.91          |
|    value_loss           | 2.22e+08      |
-------------------------------------------
Eval num_timesteps=832000, episode_reward=-52836.06 +/- 39026.78
Episode length: 55.20 +/- 24.18
------------------------------------
| eval/              |             |
|    mean action     | 0.028069923 |
|    mean velocity x | -1.75       |
|    mean velocity y | -1.2        |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -5.28e+04   |
| time/              |             |
|    total_timesteps | 832000      |
------------------------------------
Eval num_timesteps=832500, episode_reward=-65005.26 +/- 38505.95
Episode length: 54.80 +/- 9.54
------------------------------------
| eval/              |             |
|    mean action     | -0.64028865 |
|    mean velocity x | 1.06        |
|    mean velocity y | 3.45        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 832500      |
------------------------------------
Eval num_timesteps=833000, episode_reward=-47433.59 +/- 39313.18
Episode length: 51.40 +/- 17.00
------------------------------------
| eval/              |             |
|    mean action     | -0.09086229 |
|    mean velocity x | -1.29       |
|    mean velocity y | -0.059      |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 51.4        |
|    mean_reward     | -4.74e+04   |
| time/              |             |
|    total_timesteps | 833000      |
------------------------------------
Eval num_timesteps=833500, episode_reward=-90267.96 +/- 10170.21
Episode length: 73.80 +/- 22.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.01765763 |
|    mean velocity x | 0.254      |
|    mean velocity y | -0.266     |
|    mean velocity z | 21.5       |
|    mean_ep_length  | 73.8       |
|    mean_reward     | -9.03e+04  |
| time/              |            |
|    total_timesteps | 833500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.2      |
|    ep_rew_mean     | -7.88e+04 |
| time/              |           |
|    fps             | 122       |
|    iterations      | 407       |
|    time_elapsed    | 6781      |
|    total_timesteps | 833536    |
----------------------------------
Eval num_timesteps=834000, episode_reward=-61958.38 +/- 39886.87
Episode length: 59.20 +/- 18.58
------------------------------------------
| eval/                   |              |
|    mean action          | 0.17178988   |
|    mean velocity x      | 0.3          |
|    mean velocity y      | -0.783       |
|    mean velocity z      | 16.6         |
|    mean_ep_length       | 59.2         |
|    mean_reward          | -6.2e+04     |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 0.0027817877 |
|    clip_fraction        | 0.00151      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.231        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+08     |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00281     |
|    std                  | 0.911        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=834500, episode_reward=-77118.08 +/- 83905.72
Episode length: 83.20 +/- 84.53
------------------------------------
| eval/              |             |
|    mean action     | -0.31848437 |
|    mean velocity x | 1.01        |
|    mean velocity y | 2.06        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 83.2        |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 834500      |
------------------------------------
Eval num_timesteps=835000, episode_reward=-68753.70 +/- 39576.67
Episode length: 64.20 +/- 27.53
------------------------------------
| eval/              |             |
|    mean action     | -0.18298353 |
|    mean velocity x | 1.04        |
|    mean velocity y | -0.281      |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 64.2        |
|    mean_reward     | -6.88e+04   |
| time/              |             |
|    total_timesteps | 835000      |
------------------------------------
Eval num_timesteps=835500, episode_reward=-91564.55 +/- 7393.65
Episode length: 62.40 +/- 1.02
------------------------------------
| eval/              |             |
|    mean action     | -0.27329737 |
|    mean velocity x | 0.0201      |
|    mean velocity y | 0.225       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 62.4        |
|    mean_reward     | -9.16e+04   |
| time/              |             |
|    total_timesteps | 835500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 123       |
|    iterations      | 408       |
|    time_elapsed    | 6788      |
|    total_timesteps | 835584    |
----------------------------------
Eval num_timesteps=836000, episode_reward=-59603.62 +/- 19797.95
Episode length: 63.00 +/- 18.35
------------------------------------------
| eval/                   |              |
|    mean action          | -0.09324103  |
|    mean velocity x      | -1.59        |
|    mean velocity y      | -1.72        |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 63           |
|    mean_reward          | -5.96e+04    |
| time/                   |              |
|    total_timesteps      | 836000       |
| train/                  |              |
|    approx_kl            | 0.0027988222 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.001        |
|    loss                 | 9.3e+07      |
|    n_updates            | 4080         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.912        |
|    value_loss           | 2.04e+08     |
------------------------------------------
Eval num_timesteps=836500, episode_reward=-63084.23 +/- 31601.24
Episode length: 72.20 +/- 28.10
-----------------------------------
| eval/              |            |
|    mean action     | -0.2479006 |
|    mean velocity x | -0.385     |
|    mean velocity y | 0.854      |
|    mean velocity z | 22.4       |
|    mean_ep_length  | 72.2       |
|    mean_reward     | -6.31e+04  |
| time/              |            |
|    total_timesteps | 836500     |
-----------------------------------
Eval num_timesteps=837000, episode_reward=-64205.22 +/- 16580.94
Episode length: 65.80 +/- 9.54
------------------------------------
| eval/              |             |
|    mean action     | -0.11773391 |
|    mean velocity x | 1.57        |
|    mean velocity y | 0.66        |
|    mean velocity z | 20          |
|    mean_ep_length  | 65.8        |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 837000      |
------------------------------------
Eval num_timesteps=837500, episode_reward=-68519.13 +/- 35312.67
Episode length: 56.40 +/- 19.87
------------------------------------
| eval/              |             |
|    mean action     | -0.25004128 |
|    mean velocity x | 1.17        |
|    mean velocity y | 1.46        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 56.4        |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 837500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.1      |
|    ep_rew_mean     | -7.63e+04 |
| time/              |           |
|    fps             | 123       |
|    iterations      | 409       |
|    time_elapsed    | 6796      |
|    total_timesteps | 837632    |
----------------------------------
Eval num_timesteps=838000, episode_reward=-69903.55 +/- 51578.60
Episode length: 66.40 +/- 44.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.51933277  |
|    mean velocity x      | 1.11         |
|    mean velocity y      | 3.91         |
|    mean velocity z      | 20.8         |
|    mean_ep_length       | 66.4         |
|    mean_reward          | -6.99e+04    |
| time/                   |              |
|    total_timesteps      | 838000       |
| train/                  |              |
|    approx_kl            | 0.0002621187 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.001        |
|    loss                 | 8.47e+07     |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.000423    |
|    std                  | 0.911        |
|    value_loss           | 2.52e+08     |
------------------------------------------
Eval num_timesteps=838500, episode_reward=-60583.21 +/- 37653.01
Episode length: 56.80 +/- 12.61
----------------------------------
| eval/              |           |
|    mean action     | -0.934971 |
|    mean velocity x | 2.45      |
|    mean velocity y | 5.84      |
|    mean velocity z | 19.3      |
|    mean_ep_length  | 56.8      |
|    mean_reward     | -6.06e+04 |
| time/              |           |
|    total_timesteps | 838500    |
----------------------------------
Eval num_timesteps=839000, episode_reward=-76769.82 +/- 30744.14
Episode length: 56.40 +/- 7.89
-------------------------------------
| eval/              |              |
|    mean action     | -0.020430831 |
|    mean velocity x | -0.784       |
|    mean velocity y | -1.13        |
|    mean velocity z | 19.8         |
|    mean_ep_length  | 56.4         |
|    mean_reward     | -7.68e+04    |
| time/              |              |
|    total_timesteps | 839000       |
-------------------------------------
Eval num_timesteps=839500, episode_reward=-38678.71 +/- 36270.14
Episode length: 56.80 +/- 36.93
-----------------------------------
| eval/              |            |
|    mean action     | -0.4973795 |
|    mean velocity x | 1.01       |
|    mean velocity y | 2.33       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -3.87e+04  |
| time/              |            |
|    total_timesteps | 839500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.4      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 123       |
|    iterations      | 410       |
|    time_elapsed    | 6803      |
|    total_timesteps | 839680    |
----------------------------------
Eval num_timesteps=840000, episode_reward=-63316.15 +/- 32296.64
Episode length: 60.20 +/- 10.61
------------------------------------------
| eval/                   |              |
|    mean action          | -0.28271553  |
|    mean velocity x      | 1.45         |
|    mean velocity y      | 2.77         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 60.2         |
|    mean_reward          | -6.33e+04    |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0013643967 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.001        |
|    loss                 | 1.64e+08     |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.91         |
|    value_loss           | 2.36e+08     |
------------------------------------------
Eval num_timesteps=840500, episode_reward=-57444.48 +/- 35777.97
Episode length: 55.80 +/- 19.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.25385273 |
|    mean velocity x | -0.527     |
|    mean velocity y | -1.69      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 55.8       |
|    mean_reward     | -5.74e+04  |
| time/              |            |
|    total_timesteps | 840500     |
-----------------------------------
Eval num_timesteps=841000, episode_reward=-62969.45 +/- 29262.62
Episode length: 61.80 +/- 10.40
-------------------------------------
| eval/              |              |
|    mean action     | 0.0093047945 |
|    mean velocity x | -0.645       |
|    mean velocity y | 0.0392       |
|    mean velocity z | 19           |
|    mean_ep_length  | 61.8         |
|    mean_reward     | -6.3e+04     |
| time/              |              |
|    total_timesteps | 841000       |
-------------------------------------
Eval num_timesteps=841500, episode_reward=-53666.66 +/- 27715.32
Episode length: 54.80 +/- 16.77
------------------------------------
| eval/              |             |
|    mean action     | -0.37848237 |
|    mean velocity x | 0.423       |
|    mean velocity y | 0.916       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -5.37e+04   |
| time/              |             |
|    total_timesteps | 841500      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -8.1e+04 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 411      |
|    time_elapsed    | 6810     |
|    total_timesteps | 841728   |
---------------------------------
Eval num_timesteps=842000, episode_reward=-76544.58 +/- 13401.74
Episode length: 75.40 +/- 14.73
------------------------------------------
| eval/                   |              |
|    mean action          | 0.034980867  |
|    mean velocity x      | -0.864       |
|    mean velocity y      | -0.0172      |
|    mean velocity z      | 20.5         |
|    mean_ep_length       | 75.4         |
|    mean_reward          | -7.65e+04    |
| time/                   |              |
|    total_timesteps      | 842000       |
| train/                  |              |
|    approx_kl            | 0.0012476196 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+08     |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 0.91         |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=842500, episode_reward=-55503.37 +/- 27836.18
Episode length: 54.60 +/- 8.94
------------------------------------
| eval/              |             |
|    mean action     | -0.08746553 |
|    mean velocity x | -0.534      |
|    mean velocity y | 0.655       |
|    mean velocity z | 13.5        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -5.55e+04   |
| time/              |             |
|    total_timesteps | 842500      |
------------------------------------
Eval num_timesteps=843000, episode_reward=-67645.30 +/- 46898.07
Episode length: 71.20 +/- 33.86
------------------------------------
| eval/              |             |
|    mean action     | -0.40544087 |
|    mean velocity x | 0.963       |
|    mean velocity y | 1.66        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 71.2        |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 843000      |
------------------------------------
Eval num_timesteps=843500, episode_reward=-80388.65 +/- 19962.14
Episode length: 66.40 +/- 12.61
----------------------------------
| eval/              |           |
|    mean action     | 0.3525891 |
|    mean velocity x | -0.262    |
|    mean velocity y | -2.79     |
|    mean velocity z | 19.6      |
|    mean_ep_length  | 66.4      |
|    mean_reward     | -8.04e+04 |
| time/              |           |
|    total_timesteps | 843500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -7.9e+04 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 412      |
|    time_elapsed    | 6817     |
|    total_timesteps | 843776   |
---------------------------------
Eval num_timesteps=844000, episode_reward=-68178.63 +/- 42275.41
Episode length: 51.80 +/- 18.97
------------------------------------------
| eval/                   |              |
|    mean action          | -1.1539303   |
|    mean velocity x      | 3.78         |
|    mean velocity y      | 6.91         |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 51.8         |
|    mean_reward          | -6.82e+04    |
| time/                   |              |
|    total_timesteps      | 844000       |
| train/                  |              |
|    approx_kl            | 0.0011014071 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.001        |
|    loss                 | 7.47e+07     |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.001       |
|    std                  | 0.909        |
|    value_loss           | 1.77e+08     |
------------------------------------------
Eval num_timesteps=844500, episode_reward=-86602.89 +/- 22279.43
Episode length: 66.80 +/- 3.71
------------------------------------
| eval/              |             |
|    mean action     | 0.014986648 |
|    mean velocity x | -0.477      |
|    mean velocity y | 0.292       |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 844500      |
------------------------------------
Eval num_timesteps=845000, episode_reward=-75785.89 +/- 19297.50
Episode length: 71.60 +/- 12.04
------------------------------------
| eval/              |             |
|    mean action     | -0.88811296 |
|    mean velocity x | 2.73        |
|    mean velocity y | 5.81        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 71.6        |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 845000      |
------------------------------------
Eval num_timesteps=845500, episode_reward=-58337.64 +/- 43227.29
Episode length: 52.20 +/- 12.12
------------------------------------
| eval/              |             |
|    mean action     | -0.29390258 |
|    mean velocity x | 0.889       |
|    mean velocity y | 2.31        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -5.83e+04   |
| time/              |             |
|    total_timesteps | 845500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -7.81e+04 |
| time/              |           |
|    fps             | 123       |
|    iterations      | 413       |
|    time_elapsed    | 6824      |
|    total_timesteps | 845824    |
----------------------------------
Eval num_timesteps=846000, episode_reward=-77161.77 +/- 15992.69
Episode length: 71.60 +/- 12.75
------------------------------------------
| eval/                   |              |
|    mean action          | 0.04676172   |
|    mean velocity x      | 0.0342       |
|    mean velocity y      | -0.709       |
|    mean velocity z      | 21.8         |
|    mean_ep_length       | 71.6         |
|    mean_reward          | -7.72e+04    |
| time/                   |              |
|    total_timesteps      | 846000       |
| train/                  |              |
|    approx_kl            | 0.0007328408 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.229        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+08     |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.908        |
|    value_loss           | 2.3e+08      |
------------------------------------------
Eval num_timesteps=846500, episode_reward=-91095.67 +/- 15302.88
Episode length: 69.40 +/- 7.34
------------------------------------
| eval/              |             |
|    mean action     | -0.18054385 |
|    mean velocity x | 1.43        |
|    mean velocity y | 2.92        |
|    mean velocity z | 18          |
|    mean_ep_length  | 69.4        |
|    mean_reward     | -9.11e+04   |
| time/              |             |
|    total_timesteps | 846500      |
------------------------------------
Eval num_timesteps=847000, episode_reward=-38418.22 +/- 27498.21
Episode length: 48.00 +/- 11.82
-----------------------------------
| eval/              |            |
|    mean action     | -0.6040413 |
|    mean velocity x | 2.5        |
|    mean velocity y | 3.64       |
|    mean velocity z | 15         |
|    mean_ep_length  | 48         |
|    mean_reward     | -3.84e+04  |
| time/              |            |
|    total_timesteps | 847000     |
-----------------------------------
Eval num_timesteps=847500, episode_reward=-69182.58 +/- 12052.40
Episode length: 59.00 +/- 2.76
------------------------------------
| eval/              |             |
|    mean action     | -0.17242962 |
|    mean velocity x | 0.992       |
|    mean velocity y | 1.46        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 59          |
|    mean_reward     | -6.92e+04   |
| time/              |             |
|    total_timesteps | 847500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.1      |
|    ep_rew_mean     | -7.78e+04 |
| time/              |           |
|    fps             | 124       |
|    iterations      | 414       |
|    time_elapsed    | 6832      |
|    total_timesteps | 847872    |
----------------------------------
Eval num_timesteps=848000, episode_reward=-68117.47 +/- 38747.09
Episode length: 59.20 +/- 15.30
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.29924515   |
|    mean velocity x      | -0.373        |
|    mean velocity y      | 1.12          |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 59.2          |
|    mean_reward          | -6.81e+04     |
| time/                   |               |
|    total_timesteps      | 848000        |
| train/                  |               |
|    approx_kl            | 0.00022598109 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.249         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+08      |
|    n_updates            | 4140          |
|    policy_gradient_loss | -0.000728     |
|    std                  | 0.908         |
|    value_loss           | 2.09e+08      |
-------------------------------------------
Eval num_timesteps=848500, episode_reward=-85646.07 +/- 9730.78
Episode length: 65.40 +/- 4.32
-----------------------------------
| eval/              |            |
|    mean action     | -0.5337466 |
|    mean velocity x | 0.98       |
|    mean velocity y | 3.2        |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 65.4       |
|    mean_reward     | -8.56e+04  |
| time/              |            |
|    total_timesteps | 848500     |
-----------------------------------
Eval num_timesteps=849000, episode_reward=-87098.89 +/- 31745.94
Episode length: 60.80 +/- 7.63
-----------------------------------
| eval/              |            |
|    mean action     | -0.4469778 |
|    mean velocity x | 0.277      |
|    mean velocity y | 2.35       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 60.8       |
|    mean_reward     | -8.71e+04  |
| time/              |            |
|    total_timesteps | 849000     |
-----------------------------------
Eval num_timesteps=849500, episode_reward=-69842.45 +/- 44900.95
Episode length: 54.20 +/- 14.43
------------------------------------
| eval/              |             |
|    mean action     | -0.31818005 |
|    mean velocity x | -1.05       |
|    mean velocity y | 0.141       |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 54.2        |
|    mean_reward     | -6.98e+04   |
| time/              |             |
|    total_timesteps | 849500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.7      |
|    ep_rew_mean     | -7.96e+04 |
| time/              |           |
|    fps             | 124       |
|    iterations      | 415       |
|    time_elapsed    | 6839      |
|    total_timesteps | 849920    |
----------------------------------
Eval num_timesteps=850000, episode_reward=-61079.40 +/- 24310.37
Episode length: 56.00 +/- 15.35
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.81087786   |
|    mean velocity x      | 1.68          |
|    mean velocity y      | 3.99          |
|    mean velocity z      | 15.4          |
|    mean_ep_length       | 56            |
|    mean_reward          | -6.11e+04     |
| time/                   |               |
|    total_timesteps      | 850000        |
| train/                  |               |
|    approx_kl            | 0.00058842317 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.001         |
|    loss                 | 7.08e+07      |
|    n_updates            | 4150          |
|    policy_gradient_loss | -0.00161      |
|    std                  | 0.908         |
|    value_loss           | 1.87e+08      |
-------------------------------------------
Eval num_timesteps=850500, episode_reward=-99329.65 +/- 22351.95
Episode length: 90.80 +/- 38.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.5558798 |
|    mean velocity x | 1.76       |
|    mean velocity y | 2.62       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 90.8       |
|    mean_reward     | -9.93e+04  |
| time/              |            |
|    total_timesteps | 850500     |
-----------------------------------
Eval num_timesteps=851000, episode_reward=-71630.52 +/- 30610.25
Episode length: 56.40 +/- 6.37
----------------------------------
| eval/              |           |
|    mean action     | 0.1416218 |
|    mean velocity x | -2.41     |
|    mean velocity y | -2.08     |
|    mean velocity z | 19.1      |
|    mean_ep_length  | 56.4      |
|    mean_reward     | -7.16e+04 |
| time/              |           |
|    total_timesteps | 851000    |
----------------------------------
Eval num_timesteps=851500, episode_reward=-77935.66 +/- 22020.67
Episode length: 60.40 +/- 6.89
-----------------------------------
| eval/              |            |
|    mean action     | -0.7522042 |
|    mean velocity x | 2.4        |
|    mean velocity y | 4.14       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 60.4       |
|    mean_reward     | -7.79e+04  |
| time/              |            |
|    total_timesteps | 851500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69        |
|    ep_rew_mean     | -7.42e+04 |
| time/              |           |
|    fps             | 124       |
|    iterations      | 416       |
|    time_elapsed    | 6846      |
|    total_timesteps | 851968    |
----------------------------------
Eval num_timesteps=852000, episode_reward=-75663.00 +/- 30733.83
Episode length: 61.80 +/- 10.80
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.15536697 |
|    mean velocity x      | 0.343       |
|    mean velocity y      | 0.015       |
|    mean velocity z      | 17.7        |
|    mean_ep_length       | 61.8        |
|    mean_reward          | -7.57e+04   |
| time/                   |             |
|    total_timesteps      | 852000      |
| train/                  |             |
|    approx_kl            | 0.000679335 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.001       |
|    loss                 | 1.03e+08    |
|    n_updates            | 4160        |
|    policy_gradient_loss | -0.000705   |
|    std                  | 0.907       |
|    value_loss           | 1.82e+08    |
-----------------------------------------
Eval num_timesteps=852500, episode_reward=-59720.94 +/- 30378.35
Episode length: 62.80 +/- 26.84
-----------------------------------
| eval/              |            |
|    mean action     | -0.1388089 |
|    mean velocity x | 0.232      |
|    mean velocity y | 1.28       |
|    mean velocity z | 15.1       |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -5.97e+04  |
| time/              |            |
|    total_timesteps | 852500     |
-----------------------------------
Eval num_timesteps=853000, episode_reward=-74695.45 +/- 17022.18
Episode length: 67.00 +/- 18.21
------------------------------------
| eval/              |             |
|    mean action     | -0.34608093 |
|    mean velocity x | 1.62        |
|    mean velocity y | 2.72        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 67          |
|    mean_reward     | -7.47e+04   |
| time/              |             |
|    total_timesteps | 853000      |
------------------------------------
Eval num_timesteps=853500, episode_reward=-80631.77 +/- 31884.26
Episode length: 60.00 +/- 9.08
-----------------------------------
| eval/              |            |
|    mean action     | -0.3137687 |
|    mean velocity x | 1.47       |
|    mean velocity y | 1.65       |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 60         |
|    mean_reward     | -8.06e+04  |
| time/              |            |
|    total_timesteps | 853500     |
-----------------------------------
Eval num_timesteps=854000, episode_reward=-94698.33 +/- 16711.16
Episode length: 68.40 +/- 11.84
-------------------------------------
| eval/              |              |
|    mean action     | -0.016274566 |
|    mean velocity x | 0.0942       |
|    mean velocity y | 0.301        |
|    mean velocity z | 18.7         |
|    mean_ep_length  | 68.4         |
|    mean_reward     | -9.47e+04    |
| time/              |              |
|    total_timesteps | 854000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.4      |
|    ep_rew_mean     | -7.66e+04 |
| time/              |           |
|    fps             | 124       |
|    iterations      | 417       |
|    time_elapsed    | 6854      |
|    total_timesteps | 854016    |
----------------------------------
Eval num_timesteps=854500, episode_reward=-47475.36 +/- 37930.64
Episode length: 49.60 +/- 23.62
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34838516   |
|    mean velocity x      | 2.57          |
|    mean velocity y      | 2.23          |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 49.6          |
|    mean_reward          | -4.75e+04     |
| time/                   |               |
|    total_timesteps      | 854500        |
| train/                  |               |
|    approx_kl            | 3.7032616e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.227         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+08      |
|    n_updates            | 4170          |
|    policy_gradient_loss | -0.000298     |
|    std                  | 0.907         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=855000, episode_reward=-63336.67 +/- 50673.26
Episode length: 46.20 +/- 20.59
----------------------------------
| eval/              |           |
|    mean action     | 0.5989931 |
|    mean velocity x | -0.684    |
|    mean velocity y | -3.54     |
|    mean velocity z | 19.9      |
|    mean_ep_length  | 46.2      |
|    mean_reward     | -6.33e+04 |
| time/              |           |
|    total_timesteps | 855000    |
----------------------------------
Eval num_timesteps=855500, episode_reward=-64389.13 +/- 25218.58
Episode length: 57.80 +/- 10.42
-------------------------------------
| eval/              |              |
|    mean action     | -0.031126099 |
|    mean velocity x | 0.198        |
|    mean velocity y | -0.226       |
|    mean velocity z | 19.2         |
|    mean_ep_length  | 57.8         |
|    mean_reward     | -6.44e+04    |
| time/              |              |
|    total_timesteps | 855500       |
-------------------------------------
Eval num_timesteps=856000, episode_reward=-50046.88 +/- 26571.22
Episode length: 54.40 +/- 26.43
------------------------------------
| eval/              |             |
|    mean action     | -0.13195224 |
|    mean velocity x | 0.259       |
|    mean velocity y | 1.31        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5e+04      |
| time/              |             |
|    total_timesteps | 856000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.8     |
|    ep_rew_mean     | -7.9e+04 |
| time/              |          |
|    fps             | 124      |
|    iterations      | 418      |
|    time_elapsed    | 6861     |
|    total_timesteps | 856064   |
---------------------------------
Eval num_timesteps=856500, episode_reward=-61638.28 +/- 35202.92
Episode length: 62.20 +/- 11.30
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.17611301    |
|    mean velocity x      | -0.209        |
|    mean velocity y      | -0.601        |
|    mean velocity z      | 21.4          |
|    mean_ep_length       | 62.2          |
|    mean_reward          | -6.16e+04     |
| time/                   |               |
|    total_timesteps      | 856500        |
| train/                  |               |
|    approx_kl            | 0.00020641621 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.191         |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+08      |
|    n_updates            | 4180          |
|    policy_gradient_loss | -0.000409     |
|    std                  | 0.907         |
|    value_loss           | 2.49e+08      |
-------------------------------------------
Eval num_timesteps=857000, episode_reward=-64540.65 +/- 45654.93
Episode length: 65.20 +/- 34.99
-------------------------------------
| eval/              |              |
|    mean action     | -0.007388504 |
|    mean velocity x | -0.223       |
|    mean velocity y | 0.41         |
|    mean velocity z | 17.3         |
|    mean_ep_length  | 65.2         |
|    mean_reward     | -6.45e+04    |
| time/              |              |
|    total_timesteps | 857000       |
-------------------------------------
Eval num_timesteps=857500, episode_reward=-55563.91 +/- 44755.54
Episode length: 46.00 +/- 24.66
-----------------------------------
| eval/              |            |
|    mean action     | -0.5786956 |
|    mean velocity x | 0.487      |
|    mean velocity y | 2.5        |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 46         |
|    mean_reward     | -5.56e+04  |
| time/              |            |
|    total_timesteps | 857500     |
-----------------------------------
Eval num_timesteps=858000, episode_reward=-79392.00 +/- 24998.22
Episode length: 56.60 +/- 7.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.88639915 |
|    mean velocity x | -2.98      |
|    mean velocity y | -5.91      |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -7.94e+04  |
| time/              |            |
|    total_timesteps | 858000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74        |
|    ep_rew_mean     | -8.09e+04 |
| time/              |           |
|    fps             | 124       |
|    iterations      | 419       |
|    time_elapsed    | 6868      |
|    total_timesteps | 858112    |
----------------------------------
Eval num_timesteps=858500, episode_reward=-61987.58 +/- 60820.33
Episode length: 68.60 +/- 61.04
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.040311668  |
|    mean velocity x      | 1.02          |
|    mean velocity y      | 1.96          |
|    mean velocity z      | 15.7          |
|    mean_ep_length       | 68.6          |
|    mean_reward          | -6.2e+04      |
| time/                   |               |
|    total_timesteps      | 858500        |
| train/                  |               |
|    approx_kl            | 0.00074928335 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.001         |
|    loss                 | 6.91e+07      |
|    n_updates            | 4190          |
|    policy_gradient_loss | -0.00156      |
|    std                  | 0.907         |
|    value_loss           | 1.93e+08      |
-------------------------------------------
Eval num_timesteps=859000, episode_reward=-92296.19 +/- 23307.93
Episode length: 64.40 +/- 6.47
-----------------------------------
| eval/              |            |
|    mean action     | -0.6805138 |
|    mean velocity x | 2.56       |
|    mean velocity y | 4.12       |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 64.4       |
|    mean_reward     | -9.23e+04  |
| time/              |            |
|    total_timesteps | 859000     |
-----------------------------------
Eval num_timesteps=859500, episode_reward=-69534.80 +/- 52750.09
Episode length: 48.20 +/- 19.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.14089614 |
|    mean velocity x | -1.31      |
|    mean velocity y | -2.15      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 48.2       |
|    mean_reward     | -6.95e+04  |
| time/              |            |
|    total_timesteps | 859500     |
-----------------------------------
Eval num_timesteps=860000, episode_reward=-82086.40 +/- 11564.79
Episode length: 81.80 +/- 14.33
-----------------------------------
| eval/              |            |
|    mean action     | 0.24442792 |
|    mean velocity x | 0.184      |
|    mean velocity y | -1.02      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 81.8       |
|    mean_reward     | -8.21e+04  |
| time/              |            |
|    total_timesteps | 860000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.9      |
|    ep_rew_mean     | -8.11e+04 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 420       |
|    time_elapsed    | 6875      |
|    total_timesteps | 860160    |
----------------------------------
Eval num_timesteps=860500, episode_reward=-55691.27 +/- 46605.55
Episode length: 46.80 +/- 24.04
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0134732975 |
|    mean velocity x      | -0.927        |
|    mean velocity y      | -0.525        |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 46.8          |
|    mean_reward          | -5.57e+04     |
| time/                   |               |
|    total_timesteps      | 860500        |
| train/                  |               |
|    approx_kl            | 0.00013040885 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.209         |
|    learning_rate        | 0.001         |
|    loss                 | 6.82e+07      |
|    n_updates            | 4200          |
|    policy_gradient_loss | -0.000408     |
|    std                  | 0.907         |
|    value_loss           | 2.1e+08       |
-------------------------------------------
Eval num_timesteps=861000, episode_reward=-73378.71 +/- 24456.95
Episode length: 66.40 +/- 10.33
-------------------------------------
| eval/              |              |
|    mean action     | -0.025223445 |
|    mean velocity x | 0.00719      |
|    mean velocity y | -0.22        |
|    mean velocity z | 19.6         |
|    mean_ep_length  | 66.4         |
|    mean_reward     | -7.34e+04    |
| time/              |              |
|    total_timesteps | 861000       |
-------------------------------------
Eval num_timesteps=861500, episode_reward=-89775.86 +/- 23390.99
Episode length: 84.60 +/- 32.49
-----------------------------------
| eval/              |            |
|    mean action     | 0.02608847 |
|    mean velocity x | -0.535     |
|    mean velocity y | -0.142     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 84.6       |
|    mean_reward     | -8.98e+04  |
| time/              |            |
|    total_timesteps | 861500     |
-----------------------------------
Eval num_timesteps=862000, episode_reward=-63585.58 +/- 34962.99
Episode length: 80.20 +/- 45.17
------------------------------------
| eval/              |             |
|    mean action     | -0.68004185 |
|    mean velocity x | 1.98        |
|    mean velocity y | 3.9         |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 80.2        |
|    mean_reward     | -6.36e+04   |
| time/              |             |
|    total_timesteps | 862000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.4      |
|    ep_rew_mean     | -7.96e+04 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 421       |
|    time_elapsed    | 6883      |
|    total_timesteps | 862208    |
----------------------------------
Eval num_timesteps=862500, episode_reward=-63887.56 +/- 21553.04
Episode length: 62.60 +/- 13.88
------------------------------------------
| eval/                   |              |
|    mean action          | -0.59214514  |
|    mean velocity x      | 2.55         |
|    mean velocity y      | 4.02         |
|    mean velocity z      | 21.1         |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -6.39e+04    |
| time/                   |              |
|    total_timesteps      | 862500       |
| train/                  |              |
|    approx_kl            | 6.781067e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.209        |
|    learning_rate        | 0.001        |
|    loss                 | 9.32e+07     |
|    n_updates            | 4210         |
|    policy_gradient_loss | -0.000581    |
|    std                  | 0.907        |
|    value_loss           | 2.24e+08     |
------------------------------------------
Eval num_timesteps=863000, episode_reward=-56668.77 +/- 35799.49
Episode length: 50.60 +/- 10.38
-------------------------------------
| eval/              |              |
|    mean action     | 0.0057416763 |
|    mean velocity x | 0.907        |
|    mean velocity y | 0.636        |
|    mean velocity z | 22.8         |
|    mean_ep_length  | 50.6         |
|    mean_reward     | -5.67e+04    |
| time/              |              |
|    total_timesteps | 863000       |
-------------------------------------
Eval num_timesteps=863500, episode_reward=-67274.92 +/- 25931.90
Episode length: 56.80 +/- 5.42
-----------------------------------
| eval/              |            |
|    mean action     | 0.18770589 |
|    mean velocity x | -1.87      |
|    mean velocity y | -1.83      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -6.73e+04  |
| time/              |            |
|    total_timesteps | 863500     |
-----------------------------------
Eval num_timesteps=864000, episode_reward=-57883.47 +/- 43316.22
Episode length: 51.40 +/- 15.65
-----------------------------------
| eval/              |            |
|    mean action     | 0.23155445 |
|    mean velocity x | -0.0491    |
|    mean velocity y | -1.73      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 51.4       |
|    mean_reward     | -5.79e+04  |
| time/              |            |
|    total_timesteps | 864000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.5      |
|    ep_rew_mean     | -8.49e+04 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 422       |
|    time_elapsed    | 6890      |
|    total_timesteps | 864256    |
----------------------------------
Eval num_timesteps=864500, episode_reward=-79453.71 +/- 42427.03
Episode length: 52.20 +/- 19.16
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5272021    |
|    mean velocity x      | 2.15          |
|    mean velocity y      | 3.83          |
|    mean velocity z      | 20            |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -7.95e+04     |
| time/                   |               |
|    total_timesteps      | 864500        |
| train/                  |               |
|    approx_kl            | 0.00018969682 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.001         |
|    loss                 | 9.66e+07      |
|    n_updates            | 4220          |
|    policy_gradient_loss | -0.000579     |
|    std                  | 0.907         |
|    value_loss           | 2.88e+08      |
-------------------------------------------
Eval num_timesteps=865000, episode_reward=-90008.53 +/- 19674.34
Episode length: 68.20 +/- 9.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.19267645 |
|    mean velocity x | -0.00179   |
|    mean velocity y | 0.344      |
|    mean velocity z | 15.2       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -9e+04     |
| time/              |            |
|    total_timesteps | 865000     |
-----------------------------------
Eval num_timesteps=865500, episode_reward=-43071.28 +/- 41149.19
Episode length: 42.80 +/- 19.30
-----------------------------------
| eval/              |            |
|    mean action     | 0.20122975 |
|    mean velocity x | -1.45      |
|    mean velocity y | -2.3       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 42.8       |
|    mean_reward     | -4.31e+04  |
| time/              |            |
|    total_timesteps | 865500     |
-----------------------------------
Eval num_timesteps=866000, episode_reward=-64454.95 +/- 29922.27
Episode length: 69.80 +/- 23.47
------------------------------------
| eval/              |             |
|    mean action     | 0.049094573 |
|    mean velocity x | -0.593      |
|    mean velocity y | -0.524      |
|    mean velocity z | 19          |
|    mean_ep_length  | 69.8        |
|    mean_reward     | -6.45e+04   |
| time/              |             |
|    total_timesteps | 866000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.9      |
|    ep_rew_mean     | -8.77e+04 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 423       |
|    time_elapsed    | 6896      |
|    total_timesteps | 866304    |
----------------------------------
Eval num_timesteps=866500, episode_reward=-70462.91 +/- 32436.90
Episode length: 60.40 +/- 7.42
------------------------------------------
| eval/                   |              |
|    mean action          | -0.011152801 |
|    mean velocity x      | -0.524       |
|    mean velocity y      | -0.193       |
|    mean velocity z      | 16.7         |
|    mean_ep_length       | 60.4         |
|    mean_reward          | -7.05e+04    |
| time/                   |              |
|    total_timesteps      | 866500       |
| train/                  |              |
|    approx_kl            | 0.0013246407 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.001        |
|    loss                 | 9.42e+07     |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 0.907        |
|    value_loss           | 1.69e+08     |
------------------------------------------
Eval num_timesteps=867000, episode_reward=-65146.12 +/- 32110.66
Episode length: 58.40 +/- 11.18
-----------------------------------
| eval/              |            |
|    mean action     | -0.3346222 |
|    mean velocity x | 0.2        |
|    mean velocity y | 1.04       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 58.4       |
|    mean_reward     | -6.51e+04  |
| time/              |            |
|    total_timesteps | 867000     |
-----------------------------------
Eval num_timesteps=867500, episode_reward=-45986.74 +/- 53392.26
Episode length: 39.20 +/- 21.41
------------------------------------
| eval/              |             |
|    mean action     | 0.014699204 |
|    mean velocity x | 0.0262      |
|    mean velocity y | -0.51       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 39.2        |
|    mean_reward     | -4.6e+04    |
| time/              |             |
|    total_timesteps | 867500      |
------------------------------------
Eval num_timesteps=868000, episode_reward=-54607.82 +/- 33465.40
Episode length: 54.40 +/- 10.46
------------------------------------
| eval/              |             |
|    mean action     | -0.90171856 |
|    mean velocity x | 3.21        |
|    mean velocity y | 6.23        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -5.46e+04   |
| time/              |             |
|    total_timesteps | 868000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77        |
|    ep_rew_mean     | -8.85e+04 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 424       |
|    time_elapsed    | 6903      |
|    total_timesteps | 868352    |
----------------------------------
Eval num_timesteps=868500, episode_reward=-58363.42 +/- 24930.21
Episode length: 58.20 +/- 8.77
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.42236328   |
|    mean velocity x      | 1.53          |
|    mean velocity y      | 2.77          |
|    mean velocity z      | 21.4          |
|    mean_ep_length       | 58.2          |
|    mean_reward          | -5.84e+04     |
| time/                   |               |
|    total_timesteps      | 868500        |
| train/                  |               |
|    approx_kl            | 0.00012718345 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.223         |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+08       |
|    n_updates            | 4240          |
|    policy_gradient_loss | -0.000736     |
|    std                  | 0.907         |
|    value_loss           | 2.55e+08      |
-------------------------------------------
Eval num_timesteps=869000, episode_reward=-85130.30 +/- 20769.82
Episode length: 61.60 +/- 7.68
------------------------------------
| eval/              |             |
|    mean action     | -0.19666503 |
|    mean velocity x | 0.228       |
|    mean velocity y | 1.04        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -8.51e+04   |
| time/              |             |
|    total_timesteps | 869000      |
------------------------------------
Eval num_timesteps=869500, episode_reward=-68020.22 +/- 30471.00
Episode length: 58.60 +/- 11.53
-----------------------------------
| eval/              |            |
|    mean action     | -0.4617589 |
|    mean velocity x | 1.88       |
|    mean velocity y | 3.09       |
|    mean velocity z | 22.1       |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -6.8e+04   |
| time/              |            |
|    total_timesteps | 869500     |
-----------------------------------
Eval num_timesteps=870000, episode_reward=-64956.55 +/- 35026.07
Episode length: 61.80 +/- 21.61
------------------------------------
| eval/              |             |
|    mean action     | -0.43950093 |
|    mean velocity x | 0.353       |
|    mean velocity y | 2.32        |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 870000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.9      |
|    ep_rew_mean     | -8.61e+04 |
| time/              |           |
|    fps             | 125       |
|    iterations      | 425       |
|    time_elapsed    | 6911      |
|    total_timesteps | 870400    |
----------------------------------
Eval num_timesteps=870500, episode_reward=-80867.23 +/- 32148.51
Episode length: 54.00 +/- 13.75
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31300694  |
|    mean velocity x      | 1.61         |
|    mean velocity y      | 2.44         |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 54           |
|    mean_reward          | -8.09e+04    |
| time/                   |              |
|    total_timesteps      | 870500       |
| train/                  |              |
|    approx_kl            | 0.0021864558 |
|    clip_fraction        | 0.00356      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.217        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+08     |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 0.907        |
|    value_loss           | 2.64e+08     |
------------------------------------------
Eval num_timesteps=871000, episode_reward=-55255.74 +/- 41975.85
Episode length: 49.20 +/- 12.94
-----------------------------------
| eval/              |            |
|    mean action     | 0.08703994 |
|    mean velocity x | -0.371     |
|    mean velocity y | -1.12      |
|    mean velocity z | 21         |
|    mean_ep_length  | 49.2       |
|    mean_reward     | -5.53e+04  |
| time/              |            |
|    total_timesteps | 871000     |
-----------------------------------
Eval num_timesteps=871500, episode_reward=-81963.82 +/- 40986.85
Episode length: 69.40 +/- 40.39
-----------------------------------
| eval/              |            |
|    mean action     | -1.1079677 |
|    mean velocity x | 3.59       |
|    mean velocity y | 7.1        |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 69.4       |
|    mean_reward     | -8.2e+04   |
| time/              |            |
|    total_timesteps | 871500     |
-----------------------------------
Eval num_timesteps=872000, episode_reward=-53162.16 +/- 31157.60
Episode length: 66.00 +/- 30.84
-----------------------------------
| eval/              |            |
|    mean action     | 0.16176586 |
|    mean velocity x | -0.488     |
|    mean velocity y | -2.9       |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 66         |
|    mean_reward     | -5.32e+04  |
| time/              |            |
|    total_timesteps | 872000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.6     |
|    ep_rew_mean     | -8.4e+04 |
| time/              |          |
|    fps             | 126      |
|    iterations      | 426      |
|    time_elapsed    | 6918     |
|    total_timesteps | 872448   |
---------------------------------
Eval num_timesteps=872500, episode_reward=-44037.04 +/- 49315.46
Episode length: 38.00 +/- 20.17
------------------------------------------
| eval/                   |              |
|    mean action          | -0.17046756  |
|    mean velocity x      | 0.0984       |
|    mean velocity y      | 0.798        |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 38           |
|    mean_reward          | -4.4e+04     |
| time/                   |              |
|    total_timesteps      | 872500       |
| train/                  |              |
|    approx_kl            | 0.0016544546 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.232        |
|    learning_rate        | 0.001        |
|    loss                 | 9.44e+07     |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.907        |
|    value_loss           | 2.38e+08     |
------------------------------------------
Eval num_timesteps=873000, episode_reward=-59752.01 +/- 36235.73
Episode length: 76.00 +/- 47.23
------------------------------------
| eval/              |             |
|    mean action     | -0.21635717 |
|    mean velocity x | -0.446      |
|    mean velocity y | 0.584       |
|    mean velocity z | 23.7        |
|    mean_ep_length  | 76          |
|    mean_reward     | -5.98e+04   |
| time/              |             |
|    total_timesteps | 873000      |
------------------------------------
Eval num_timesteps=873500, episode_reward=-67213.50 +/- 33591.89
Episode length: 62.20 +/- 10.57
-----------------------------------
| eval/              |            |
|    mean action     | -0.3098861 |
|    mean velocity x | 1.97       |
|    mean velocity y | 0.97       |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 62.2       |
|    mean_reward     | -6.72e+04  |
| time/              |            |
|    total_timesteps | 873500     |
-----------------------------------
Eval num_timesteps=874000, episode_reward=-74340.99 +/- 27614.21
Episode length: 78.60 +/- 23.51
------------------------------------
| eval/              |             |
|    mean action     | -0.12856992 |
|    mean velocity x | 0.198       |
|    mean velocity y | 1.6         |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 78.6        |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 874000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.7      |
|    ep_rew_mean     | -8.79e+04 |
| time/              |           |
|    fps             | 126       |
|    iterations      | 427       |
|    time_elapsed    | 6925      |
|    total_timesteps | 874496    |
----------------------------------
Eval num_timesteps=874500, episode_reward=-78760.73 +/- 39885.97
Episode length: 59.80 +/- 24.69
------------------------------------------
| eval/                   |              |
|    mean action          | -0.078660876 |
|    mean velocity x      | -0.844       |
|    mean velocity y      | 0.336        |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 59.8         |
|    mean_reward          | -7.88e+04    |
| time/                   |              |
|    total_timesteps      | 874500       |
| train/                  |              |
|    approx_kl            | 0.001025389  |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.235        |
|    learning_rate        | 0.001        |
|    loss                 | 1.34e+08     |
|    n_updates            | 4270         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.907        |
|    value_loss           | 2.53e+08     |
------------------------------------------
Eval num_timesteps=875000, episode_reward=-89227.68 +/- 5508.10
Episode length: 60.60 +/- 2.15
-----------------------------------
| eval/              |            |
|    mean action     | -0.5954414 |
|    mean velocity x | 1.74       |
|    mean velocity y | 4.87       |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 60.6       |
|    mean_reward     | -8.92e+04  |
| time/              |            |
|    total_timesteps | 875000     |
-----------------------------------
Eval num_timesteps=875500, episode_reward=-68530.95 +/- 35723.88
Episode length: 56.00 +/- 10.77
------------------------------------
| eval/              |             |
|    mean action     | -0.50361305 |
|    mean velocity x | 2.1         |
|    mean velocity y | 3.16        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 56          |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 875500      |
------------------------------------
Eval num_timesteps=876000, episode_reward=-74554.47 +/- 15889.89
Episode length: 74.60 +/- 11.53
------------------------------------
| eval/              |             |
|    mean action     | 0.052787114 |
|    mean velocity x | -0.21       |
|    mean velocity y | -0.337      |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 74.6        |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 876000      |
------------------------------------
Eval num_timesteps=876500, episode_reward=-50596.33 +/- 39994.54
Episode length: 43.40 +/- 22.46
------------------------------------
| eval/              |             |
|    mean action     | -0.15176772 |
|    mean velocity x | -0.212      |
|    mean velocity y | -0.68       |
|    mean velocity z | 16.2        |
|    mean_ep_length  | 43.4        |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 876500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69        |
|    ep_rew_mean     | -8.17e+04 |
| time/              |           |
|    fps             | 126       |
|    iterations      | 428       |
|    time_elapsed    | 6933      |
|    total_timesteps | 876544    |
----------------------------------
Eval num_timesteps=877000, episode_reward=-62572.24 +/- 40799.51
Episode length: 50.80 +/- 18.02
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13621494   |
|    mean velocity x      | -0.414        |
|    mean velocity y      | 1.35          |
|    mean velocity z      | 21.7          |
|    mean_ep_length       | 50.8          |
|    mean_reward          | -6.26e+04     |
| time/                   |               |
|    total_timesteps      | 877000        |
| train/                  |               |
|    approx_kl            | 0.00035632588 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.259         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 4280          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 0.907         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=877500, episode_reward=-51405.66 +/- 36856.75
Episode length: 50.80 +/- 22.68
------------------------------------
| eval/              |             |
|    mean action     | -0.27459055 |
|    mean velocity x | 1.1         |
|    mean velocity y | 0.319       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 50.8        |
|    mean_reward     | -5.14e+04   |
| time/              |             |
|    total_timesteps | 877500      |
------------------------------------
Eval num_timesteps=878000, episode_reward=-76446.16 +/- 40315.40
Episode length: 68.80 +/- 39.82
------------------------------------
| eval/              |             |
|    mean action     | 0.016493881 |
|    mean velocity x | -0.642      |
|    mean velocity y | 0.0724      |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 68.8        |
|    mean_reward     | -7.64e+04   |
| time/              |             |
|    total_timesteps | 878000      |
------------------------------------
Eval num_timesteps=878500, episode_reward=-76524.30 +/- 20343.83
Episode length: 60.00 +/- 2.90
------------------------------------
| eval/              |             |
|    mean action     | -0.39183068 |
|    mean velocity x | 0.1         |
|    mean velocity y | 1.51        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 60          |
|    mean_reward     | -7.65e+04   |
| time/              |             |
|    total_timesteps | 878500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.8      |
|    ep_rew_mean     | -8.07e+04 |
| time/              |           |
|    fps             | 126       |
|    iterations      | 429       |
|    time_elapsed    | 6940      |
|    total_timesteps | 878592    |
----------------------------------
Eval num_timesteps=879000, episode_reward=-67118.44 +/- 34225.00
Episode length: 56.80 +/- 17.26
------------------------------------------
| eval/                   |              |
|    mean action          | -0.28567758  |
|    mean velocity x      | 0.0397       |
|    mean velocity y      | 1.54         |
|    mean velocity z      | 20.4         |
|    mean_ep_length       | 56.8         |
|    mean_reward          | -6.71e+04    |
| time/                   |              |
|    total_timesteps      | 879000       |
| train/                  |              |
|    approx_kl            | 0.0014584516 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+08     |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 0.906        |
|    value_loss           | 2.76e+08     |
------------------------------------------
Eval num_timesteps=879500, episode_reward=-69821.56 +/- 29792.40
Episode length: 60.80 +/- 14.11
------------------------------------
| eval/              |             |
|    mean action     | -0.83158946 |
|    mean velocity x | 2.82        |
|    mean velocity y | 5.26        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -6.98e+04   |
| time/              |             |
|    total_timesteps | 879500      |
------------------------------------
Eval num_timesteps=880000, episode_reward=-81503.15 +/- 27348.76
Episode length: 62.80 +/- 3.37
-----------------------------------
| eval/              |            |
|    mean action     | -0.4555412 |
|    mean velocity x | 1.4        |
|    mean velocity y | 2.82       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 880000     |
-----------------------------------
Eval num_timesteps=880500, episode_reward=-44032.67 +/- 31802.70
Episode length: 51.00 +/- 19.62
-----------------------------------
| eval/              |            |
|    mean action     | -0.4594157 |
|    mean velocity x | 1.58       |
|    mean velocity y | 2.27       |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 51         |
|    mean_reward     | -4.4e+04   |
| time/              |            |
|    total_timesteps | 880500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.91e+04 |
| time/              |           |
|    fps             | 126       |
|    iterations      | 430       |
|    time_elapsed    | 6947      |
|    total_timesteps | 880640    |
----------------------------------
Eval num_timesteps=881000, episode_reward=-69789.72 +/- 19600.90
Episode length: 69.40 +/- 6.28
------------------------------------------
| eval/                   |              |
|    mean action          | -0.57204175  |
|    mean velocity x      | 2.71         |
|    mean velocity y      | 4.45         |
|    mean velocity z      | 21.3         |
|    mean_ep_length       | 69.4         |
|    mean_reward          | -6.98e+04    |
| time/                   |              |
|    total_timesteps      | 881000       |
| train/                  |              |
|    approx_kl            | 0.0018852046 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.263        |
|    learning_rate        | 0.001        |
|    loss                 | 8.34e+07     |
|    n_updates            | 4300         |
|    policy_gradient_loss | -0.00225     |
|    std                  | 0.906        |
|    value_loss           | 2.02e+08     |
------------------------------------------
Eval num_timesteps=881500, episode_reward=-46504.60 +/- 32682.94
Episode length: 55.00 +/- 23.09
----------------------------------
| eval/              |           |
|    mean action     | -0.308904 |
|    mean velocity x | 0.663     |
|    mean velocity y | 1.59      |
|    mean velocity z | 20.9      |
|    mean_ep_length  | 55        |
|    mean_reward     | -4.65e+04 |
| time/              |           |
|    total_timesteps | 881500    |
----------------------------------
Eval num_timesteps=882000, episode_reward=-71063.17 +/- 21943.03
Episode length: 66.20 +/- 14.85
------------------------------------
| eval/              |             |
|    mean action     | -0.29266596 |
|    mean velocity x | 1.34        |
|    mean velocity y | 2.84        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -7.11e+04   |
| time/              |             |
|    total_timesteps | 882000      |
------------------------------------
Eval num_timesteps=882500, episode_reward=-66623.97 +/- 40002.68
Episode length: 56.40 +/- 22.10
-----------------------------------
| eval/              |            |
|    mean action     | -0.2926373 |
|    mean velocity x | 1.45       |
|    mean velocity y | 1.24       |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 56.4       |
|    mean_reward     | -6.66e+04  |
| time/              |            |
|    total_timesteps | 882500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.8      |
|    ep_rew_mean     | -8.23e+04 |
| time/              |           |
|    fps             | 126       |
|    iterations      | 431       |
|    time_elapsed    | 6954      |
|    total_timesteps | 882688    |
----------------------------------
Eval num_timesteps=883000, episode_reward=-34462.21 +/- 36212.94
Episode length: 41.80 +/- 19.61
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.084685974  |
|    mean velocity x      | -0.596        |
|    mean velocity y      | 0.339         |
|    mean velocity z      | 18.1          |
|    mean_ep_length       | 41.8          |
|    mean_reward          | -3.45e+04     |
| time/                   |               |
|    total_timesteps      | 883000        |
| train/                  |               |
|    approx_kl            | 0.00017691895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.226         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+08      |
|    n_updates            | 4310          |
|    policy_gradient_loss | -0.000852     |
|    std                  | 0.906         |
|    value_loss           | 2.38e+08      |
-------------------------------------------
Eval num_timesteps=883500, episode_reward=-33536.41 +/- 32854.32
Episode length: 43.80 +/- 18.08
-----------------------------------
| eval/              |            |
|    mean action     | 0.07064388 |
|    mean velocity x | 0.362      |
|    mean velocity y | -0.355     |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 43.8       |
|    mean_reward     | -3.35e+04  |
| time/              |            |
|    total_timesteps | 883500     |
-----------------------------------
Eval num_timesteps=884000, episode_reward=-35248.76 +/- 31386.93
Episode length: 42.60 +/- 13.09
------------------------------------
| eval/              |             |
|    mean action     | -0.15152793 |
|    mean velocity x | -1.01       |
|    mean velocity y | 0.652       |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 42.6        |
|    mean_reward     | -3.52e+04   |
| time/              |             |
|    total_timesteps | 884000      |
------------------------------------
Eval num_timesteps=884500, episode_reward=-60861.51 +/- 35216.67
Episode length: 62.60 +/- 34.86
------------------------------------
| eval/              |             |
|    mean action     | -0.38614503 |
|    mean velocity x | 1.18        |
|    mean velocity y | 2.65        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 62.6        |
|    mean_reward     | -6.09e+04   |
| time/              |             |
|    total_timesteps | 884500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.3      |
|    ep_rew_mean     | -8.25e+04 |
| time/              |           |
|    fps             | 127       |
|    iterations      | 432       |
|    time_elapsed    | 6961      |
|    total_timesteps | 884736    |
----------------------------------
Eval num_timesteps=885000, episode_reward=-73164.82 +/- 24552.28
Episode length: 61.80 +/- 9.24
------------------------------------------
| eval/                   |              |
|    mean action          | -0.23676044  |
|    mean velocity x      | 0.746        |
|    mean velocity y      | 1.35         |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 61.8         |
|    mean_reward          | -7.32e+04    |
| time/                   |              |
|    total_timesteps      | 885000       |
| train/                  |              |
|    approx_kl            | 0.0013740181 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+08      |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.906        |
|    value_loss           | 2.22e+08     |
------------------------------------------
Eval num_timesteps=885500, episode_reward=-67591.27 +/- 33006.28
Episode length: 51.40 +/- 18.25
------------------------------------
| eval/              |             |
|    mean action     | -0.15893173 |
|    mean velocity x | -1.12       |
|    mean velocity y | 0.0108      |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 51.4        |
|    mean_reward     | -6.76e+04   |
| time/              |             |
|    total_timesteps | 885500      |
------------------------------------
Eval num_timesteps=886000, episode_reward=-70907.22 +/- 18417.19
Episode length: 65.00 +/- 6.54
-----------------------------------
| eval/              |            |
|    mean action     | 0.15505372 |
|    mean velocity x | -0.0848    |
|    mean velocity y | -1.56      |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 65         |
|    mean_reward     | -7.09e+04  |
| time/              |            |
|    total_timesteps | 886000     |
-----------------------------------
Eval num_timesteps=886500, episode_reward=-52946.32 +/- 24880.16
Episode length: 56.40 +/- 14.62
------------------------------------
| eval/              |             |
|    mean action     | -0.69023603 |
|    mean velocity x | 1.69        |
|    mean velocity y | 2.69        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 56.4        |
|    mean_reward     | -5.29e+04   |
| time/              |             |
|    total_timesteps | 886500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.5      |
|    ep_rew_mean     | -8.11e+04 |
| time/              |           |
|    fps             | 127       |
|    iterations      | 433       |
|    time_elapsed    | 6968      |
|    total_timesteps | 886784    |
----------------------------------
Eval num_timesteps=887000, episode_reward=-82967.91 +/- 18750.76
Episode length: 64.40 +/- 3.50
------------------------------------------
| eval/                   |              |
|    mean action          | -0.7704661   |
|    mean velocity x      | 2.21         |
|    mean velocity y      | 3.33         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 64.4         |
|    mean_reward          | -8.3e+04     |
| time/                   |              |
|    total_timesteps      | 887000       |
| train/                  |              |
|    approx_kl            | 0.0006539037 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+08     |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 0.906        |
|    value_loss           | 2.13e+08     |
------------------------------------------
Eval num_timesteps=887500, episode_reward=-83896.59 +/- 30719.48
Episode length: 70.80 +/- 28.10
----------------------------------
| eval/              |           |
|    mean action     | 0.4490206 |
|    mean velocity x | -0.718    |
|    mean velocity y | -2.62     |
|    mean velocity z | 18.7      |
|    mean_ep_length  | 70.8      |
|    mean_reward     | -8.39e+04 |
| time/              |           |
|    total_timesteps | 887500    |
----------------------------------
Eval num_timesteps=888000, episode_reward=-73170.78 +/- 15993.73
Episode length: 66.60 +/- 12.53
-----------------------------------
| eval/              |            |
|    mean action     | 0.38259068 |
|    mean velocity x | -2.28      |
|    mean velocity y | -2.88      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 66.6       |
|    mean_reward     | -7.32e+04  |
| time/              |            |
|    total_timesteps | 888000     |
-----------------------------------
Eval num_timesteps=888500, episode_reward=-59669.35 +/- 31612.99
Episode length: 59.20 +/- 17.34
-----------------------------------
| eval/              |            |
|    mean action     | -0.7204538 |
|    mean velocity x | 2.79       |
|    mean velocity y | 4.59       |
|    mean velocity z | 15.3       |
|    mean_ep_length  | 59.2       |
|    mean_reward     | -5.97e+04  |
| time/              |            |
|    total_timesteps | 888500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.1      |
|    ep_rew_mean     | -7.85e+04 |
| time/              |           |
|    fps             | 127       |
|    iterations      | 434       |
|    time_elapsed    | 6976      |
|    total_timesteps | 888832    |
----------------------------------
Eval num_timesteps=889000, episode_reward=-60187.88 +/- 25608.65
Episode length: 63.00 +/- 14.25
------------------------------------------
| eval/                   |              |
|    mean action          | -0.46769872  |
|    mean velocity x      | 0.535        |
|    mean velocity y      | 2.29         |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 63           |
|    mean_reward          | -6.02e+04    |
| time/                   |              |
|    total_timesteps      | 889000       |
| train/                  |              |
|    approx_kl            | 0.0006427089 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.001        |
|    loss                 | 5.19e+07     |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 0.905        |
|    value_loss           | 1.53e+08     |
------------------------------------------
Eval num_timesteps=889500, episode_reward=-91271.76 +/- 15779.22
Episode length: 65.00 +/- 7.18
------------------------------------
| eval/              |             |
|    mean action     | -0.12673068 |
|    mean velocity x | 0.0604      |
|    mean velocity y | -0.935      |
|    mean velocity z | 16.2        |
|    mean_ep_length  | 65          |
|    mean_reward     | -9.13e+04   |
| time/              |             |
|    total_timesteps | 889500      |
------------------------------------
Eval num_timesteps=890000, episode_reward=-64478.21 +/- 18047.48
Episode length: 63.20 +/- 8.38
------------------------------------
| eval/              |             |
|    mean action     | -0.38343677 |
|    mean velocity x | -1.01       |
|    mean velocity y | 0.605       |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 63.2        |
|    mean_reward     | -6.45e+04   |
| time/              |             |
|    total_timesteps | 890000      |
------------------------------------
Eval num_timesteps=890500, episode_reward=-67412.88 +/- 38524.92
Episode length: 53.80 +/- 11.69
------------------------------------
| eval/              |             |
|    mean action     | -0.18570335 |
|    mean velocity x | 1.8         |
|    mean velocity y | 2.04        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 890500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.7      |
|    ep_rew_mean     | -7.51e+04 |
| time/              |           |
|    fps             | 127       |
|    iterations      | 435       |
|    time_elapsed    | 6983      |
|    total_timesteps | 890880    |
----------------------------------
Eval num_timesteps=891000, episode_reward=-49757.60 +/- 33161.81
Episode length: 58.20 +/- 34.57
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24904068  |
|    mean velocity x      | 1.46         |
|    mean velocity y      | 2.19         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 58.2         |
|    mean_reward          | -4.98e+04    |
| time/                   |              |
|    total_timesteps      | 891000       |
| train/                  |              |
|    approx_kl            | 0.0019653633 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.001        |
|    loss                 | 7.93e+07     |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.0033      |
|    std                  | 0.905        |
|    value_loss           | 2.02e+08     |
------------------------------------------
Eval num_timesteps=891500, episode_reward=-77888.87 +/- 4817.28
Episode length: 81.00 +/- 14.13
------------------------------------
| eval/              |             |
|    mean action     | -0.42265785 |
|    mean velocity x | 0.196       |
|    mean velocity y | 1.59        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 81          |
|    mean_reward     | -7.79e+04   |
| time/              |             |
|    total_timesteps | 891500      |
------------------------------------
Eval num_timesteps=892000, episode_reward=-89946.63 +/- 19295.04
Episode length: 62.60 +/- 5.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.29083923 |
|    mean velocity x | 0.192      |
|    mean velocity y | -1.79      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -8.99e+04  |
| time/              |            |
|    total_timesteps | 892000     |
-----------------------------------
Eval num_timesteps=892500, episode_reward=-78147.23 +/- 15009.90
Episode length: 59.60 +/- 2.24
-----------------------------------
| eval/              |            |
|    mean action     | -0.6868601 |
|    mean velocity x | 2.19       |
|    mean velocity y | 3.96       |
|    mean velocity z | 18         |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 892500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -7.75e+04 |
| time/              |           |
|    fps             | 127       |
|    iterations      | 436       |
|    time_elapsed    | 6990      |
|    total_timesteps | 892928    |
----------------------------------
Eval num_timesteps=893000, episode_reward=-77050.30 +/- 23164.50
Episode length: 58.60 +/- 6.22
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25653195   |
|    mean velocity x      | 1.15          |
|    mean velocity y      | 0.864         |
|    mean velocity z      | 20.5          |
|    mean_ep_length       | 58.6          |
|    mean_reward          | -7.71e+04     |
| time/                   |               |
|    total_timesteps      | 893000        |
| train/                  |               |
|    approx_kl            | 0.00039640442 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.001         |
|    loss                 | 9.16e+07      |
|    n_updates            | 4360          |
|    policy_gradient_loss | -0.000711     |
|    std                  | 0.905         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=893500, episode_reward=-54245.98 +/- 37691.70
Episode length: 54.80 +/- 33.61
-----------------------------------
| eval/              |            |
|    mean action     | -0.2742974 |
|    mean velocity x | -0.205     |
|    mean velocity y | 1.19       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 54.8       |
|    mean_reward     | -5.42e+04  |
| time/              |            |
|    total_timesteps | 893500     |
-----------------------------------
Eval num_timesteps=894000, episode_reward=-89318.55 +/- 24535.65
Episode length: 63.20 +/- 4.40
------------------------------------
| eval/              |             |
|    mean action     | -0.41246375 |
|    mean velocity x | 2.54        |
|    mean velocity y | 3.25        |
|    mean velocity z | 14.9        |
|    mean_ep_length  | 63.2        |
|    mean_reward     | -8.93e+04   |
| time/              |             |
|    total_timesteps | 894000      |
------------------------------------
Eval num_timesteps=894500, episode_reward=-58769.93 +/- 39300.61
Episode length: 52.00 +/- 21.54
------------------------------------
| eval/              |             |
|    mean action     | -0.31512696 |
|    mean velocity x | 0.0786      |
|    mean velocity y | 1.5         |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 52          |
|    mean_reward     | -5.88e+04   |
| time/              |             |
|    total_timesteps | 894500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.5      |
|    ep_rew_mean     | -7.58e+04 |
| time/              |           |
|    fps             | 127       |
|    iterations      | 437       |
|    time_elapsed    | 6997      |
|    total_timesteps | 894976    |
----------------------------------
Eval num_timesteps=895000, episode_reward=-71784.21 +/- 37327.60
Episode length: 53.40 +/- 10.61
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.40957367 |
|    mean velocity x      | 1.23        |
|    mean velocity y      | 2.74        |
|    mean velocity z      | 19.5        |
|    mean_ep_length       | 53.4        |
|    mean_reward          | -7.18e+04   |
| time/                   |             |
|    total_timesteps      | 895000      |
| train/                  |             |
|    approx_kl            | 0.001777956 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.239       |
|    learning_rate        | 0.001       |
|    loss                 | 1.35e+08    |
|    n_updates            | 4370        |
|    policy_gradient_loss | -0.00192    |
|    std                  | 0.906       |
|    value_loss           | 1.92e+08    |
-----------------------------------------
Eval num_timesteps=895500, episode_reward=-48636.72 +/- 36309.56
Episode length: 45.00 +/- 27.33
------------------------------------
| eval/              |             |
|    mean action     | -0.43733314 |
|    mean velocity x | 1.97        |
|    mean velocity y | 1.77        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 45          |
|    mean_reward     | -4.86e+04   |
| time/              |             |
|    total_timesteps | 895500      |
------------------------------------
Eval num_timesteps=896000, episode_reward=-51931.39 +/- 32407.43
Episode length: 55.40 +/- 11.38
------------------------------------
| eval/              |             |
|    mean action     | -0.23579344 |
|    mean velocity x | -1.42       |
|    mean velocity y | 0.136       |
|    mean velocity z | 14.4        |
|    mean_ep_length  | 55.4        |
|    mean_reward     | -5.19e+04   |
| time/              |             |
|    total_timesteps | 896000      |
------------------------------------
Eval num_timesteps=896500, episode_reward=-63207.74 +/- 43220.24
Episode length: 51.20 +/- 20.29
-----------------------------------
| eval/              |            |
|    mean action     | -0.6935838 |
|    mean velocity x | 1.81       |
|    mean velocity y | 4.79       |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 51.2       |
|    mean_reward     | -6.32e+04  |
| time/              |            |
|    total_timesteps | 896500     |
-----------------------------------
Eval num_timesteps=897000, episode_reward=-69169.31 +/- 28802.40
Episode length: 64.00 +/- 15.09
------------------------------------
| eval/              |             |
|    mean action     | -0.10954378 |
|    mean velocity x | -0.205      |
|    mean velocity y | 0.137       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 64          |
|    mean_reward     | -6.92e+04   |
| time/              |             |
|    total_timesteps | 897000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.4      |
|    ep_rew_mean     | -7.64e+04 |
| time/              |           |
|    fps             | 128       |
|    iterations      | 438       |
|    time_elapsed    | 7005      |
|    total_timesteps | 897024    |
----------------------------------
Eval num_timesteps=897500, episode_reward=-53439.48 +/- 35314.32
Episode length: 48.00 +/- 23.55
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.34788358   |
|    mean velocity x      | 2.99          |
|    mean velocity y      | 2             |
|    mean velocity z      | 20.1          |
|    mean_ep_length       | 48            |
|    mean_reward          | -5.34e+04     |
| time/                   |               |
|    total_timesteps      | 897500        |
| train/                  |               |
|    approx_kl            | 0.00026057518 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.269         |
|    learning_rate        | 0.001         |
|    loss                 | 5.55e+07      |
|    n_updates            | 4380          |
|    policy_gradient_loss | -0.000466     |
|    std                  | 0.906         |
|    value_loss           | 1.63e+08      |
-------------------------------------------
Eval num_timesteps=898000, episode_reward=-50466.33 +/- 23572.22
Episode length: 76.60 +/- 32.55
------------------------------------
| eval/              |             |
|    mean action     | -0.25450265 |
|    mean velocity x | 1.54        |
|    mean velocity y | 2.31        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 76.6        |
|    mean_reward     | -5.05e+04   |
| time/              |             |
|    total_timesteps | 898000      |
------------------------------------
Eval num_timesteps=898500, episode_reward=-50792.62 +/- 23440.56
Episode length: 54.80 +/- 8.45
-----------------------------------
| eval/              |            |
|    mean action     | 0.16483703 |
|    mean velocity x | 0.11       |
|    mean velocity y | -1.17      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 54.8       |
|    mean_reward     | -5.08e+04  |
| time/              |            |
|    total_timesteps | 898500     |
-----------------------------------
Eval num_timesteps=899000, episode_reward=-75602.18 +/- 41846.46
Episode length: 55.40 +/- 23.04
-----------------------------------
| eval/              |            |
|    mean action     | 0.30457082 |
|    mean velocity x | -1.57      |
|    mean velocity y | -1.62      |
|    mean velocity z | 18         |
|    mean_ep_length  | 55.4       |
|    mean_reward     | -7.56e+04  |
| time/              |            |
|    total_timesteps | 899000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.32e+04 |
| time/              |           |
|    fps             | 128       |
|    iterations      | 439       |
|    time_elapsed    | 7012      |
|    total_timesteps | 899072    |
----------------------------------
Eval num_timesteps=899500, episode_reward=-52116.76 +/- 30237.77
Episode length: 55.40 +/- 9.99
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37095532   |
|    mean velocity x      | 1.22          |
|    mean velocity y      | 2.09          |
|    mean velocity z      | 16.6          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -5.21e+04     |
| time/                   |               |
|    total_timesteps      | 899500        |
| train/                  |               |
|    approx_kl            | 0.00023314278 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.001         |
|    loss                 | 8.18e+07      |
|    n_updates            | 4390          |
|    policy_gradient_loss | -0.000465     |
|    std                  | 0.906         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=900000, episode_reward=-47354.21 +/- 42397.75
Episode length: 48.40 +/- 23.10
-----------------------------------
| eval/              |            |
|    mean action     | 0.05051342 |
|    mean velocity x | -1.87      |
|    mean velocity y | -1.62      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 48.4       |
|    mean_reward     | -4.74e+04  |
| time/              |            |
|    total_timesteps | 900000     |
-----------------------------------
Eval num_timesteps=900500, episode_reward=-75409.25 +/- 15464.67
Episode length: 68.20 +/- 9.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.09138422 |
|    mean velocity x | -1.49      |
|    mean velocity y | -2.12      |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -7.54e+04  |
| time/              |            |
|    total_timesteps | 900500     |
-----------------------------------
Eval num_timesteps=901000, episode_reward=-67555.65 +/- 36785.22
Episode length: 59.00 +/- 17.85
-----------------------------------
| eval/              |            |
|    mean action     | -0.4790127 |
|    mean velocity x | 1.01       |
|    mean velocity y | 3.02       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 59         |
|    mean_reward     | -6.76e+04  |
| time/              |            |
|    total_timesteps | 901000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70        |
|    ep_rew_mean     | -7.44e+04 |
| time/              |           |
|    fps             | 128       |
|    iterations      | 440       |
|    time_elapsed    | 7019      |
|    total_timesteps | 901120    |
----------------------------------
Eval num_timesteps=901500, episode_reward=-65989.37 +/- 26181.35
Episode length: 63.00 +/- 14.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.8650168   |
|    mean velocity x      | 4.25         |
|    mean velocity y      | 6.02         |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 63           |
|    mean_reward          | -6.6e+04     |
| time/                   |              |
|    total_timesteps      | 901500       |
| train/                  |              |
|    approx_kl            | 0.0006029756 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.237        |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+08     |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.906        |
|    value_loss           | 2.22e+08     |
------------------------------------------
Eval num_timesteps=902000, episode_reward=-63173.24 +/- 42031.69
Episode length: 55.20 +/- 15.92
------------------------------------
| eval/              |             |
|    mean action     | -0.08030154 |
|    mean velocity x | -0.0531     |
|    mean velocity y | -0.146      |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -6.32e+04   |
| time/              |             |
|    total_timesteps | 902000      |
------------------------------------
Eval num_timesteps=902500, episode_reward=-85048.07 +/- 18547.31
Episode length: 68.00 +/- 10.02
-----------------------------------
| eval/              |            |
|    mean action     | -0.8301649 |
|    mean velocity x | 1.66       |
|    mean velocity y | 5.17       |
|    mean velocity z | 18         |
|    mean_ep_length  | 68         |
|    mean_reward     | -8.5e+04   |
| time/              |            |
|    total_timesteps | 902500     |
-----------------------------------
Eval num_timesteps=903000, episode_reward=-89255.54 +/- 7263.07
Episode length: 68.20 +/- 6.65
-----------------------------------
| eval/              |            |
|    mean action     | -0.7831214 |
|    mean velocity x | 2.48       |
|    mean velocity y | 4.92       |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -8.93e+04  |
| time/              |            |
|    total_timesteps | 903000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.7      |
|    ep_rew_mean     | -7.72e+04 |
| time/              |           |
|    fps             | 128       |
|    iterations      | 441       |
|    time_elapsed    | 7026      |
|    total_timesteps | 903168    |
----------------------------------
Eval num_timesteps=903500, episode_reward=-79482.42 +/- 35198.65
Episode length: 56.60 +/- 15.64
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2220755    |
|    mean velocity x      | -0.981       |
|    mean velocity y      | -1.17        |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 56.6         |
|    mean_reward          | -7.95e+04    |
| time/                   |              |
|    total_timesteps      | 903500       |
| train/                  |              |
|    approx_kl            | 0.0015698571 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.001        |
|    loss                 | 1.39e+08     |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 0.907        |
|    value_loss           | 2.18e+08     |
------------------------------------------
Eval num_timesteps=904000, episode_reward=-51152.36 +/- 33151.04
Episode length: 53.60 +/- 16.43
------------------------------------
| eval/              |             |
|    mean action     | -0.39344513 |
|    mean velocity x | 0.89        |
|    mean velocity y | 2.94        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -5.12e+04   |
| time/              |             |
|    total_timesteps | 904000      |
------------------------------------
Eval num_timesteps=904500, episode_reward=-62838.73 +/- 50062.59
Episode length: 70.20 +/- 44.43
-----------------------------------
| eval/              |            |
|    mean action     | -0.6741416 |
|    mean velocity x | 1.56       |
|    mean velocity y | 3.47       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 70.2       |
|    mean_reward     | -6.28e+04  |
| time/              |            |
|    total_timesteps | 904500     |
-----------------------------------
Eval num_timesteps=905000, episode_reward=-58538.78 +/- 29439.61
Episode length: 66.80 +/- 31.17
------------------------------------
| eval/              |             |
|    mean action     | -0.65855336 |
|    mean velocity x | 1.88        |
|    mean velocity y | 3.87        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -5.85e+04   |
| time/              |             |
|    total_timesteps | 905000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.3      |
|    ep_rew_mean     | -8.14e+04 |
| time/              |           |
|    fps             | 128       |
|    iterations      | 442       |
|    time_elapsed    | 7033      |
|    total_timesteps | 905216    |
----------------------------------
Eval num_timesteps=905500, episode_reward=-59854.10 +/- 48711.68
Episode length: 43.80 +/- 24.45
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.11907451    |
|    mean velocity x      | -1.31         |
|    mean velocity y      | -2.23         |
|    mean velocity z      | 22.1          |
|    mean_ep_length       | 43.8          |
|    mean_reward          | -5.99e+04     |
| time/                   |               |
|    total_timesteps      | 905500        |
| train/                  |               |
|    approx_kl            | 0.00028112027 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+08      |
|    n_updates            | 4420          |
|    policy_gradient_loss | -0.00105      |
|    std                  | 0.907         |
|    value_loss           | 2.19e+08      |
-------------------------------------------
Eval num_timesteps=906000, episode_reward=-77882.18 +/- 5707.42
Episode length: 72.40 +/- 23.90
----------------------------------
| eval/              |           |
|    mean action     | 0.3333507 |
|    mean velocity x | -2.37     |
|    mean velocity y | -2.45     |
|    mean velocity z | 20.4      |
|    mean_ep_length  | 72.4      |
|    mean_reward     | -7.79e+04 |
| time/              |           |
|    total_timesteps | 906000    |
----------------------------------
Eval num_timesteps=906500, episode_reward=-70651.88 +/- 37223.66
Episode length: 59.40 +/- 21.29
-----------------------------------
| eval/              |            |
|    mean action     | -0.2582725 |
|    mean velocity x | 1          |
|    mean velocity y | 1.17       |
|    mean velocity z | 17         |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -7.07e+04  |
| time/              |            |
|    total_timesteps | 906500     |
-----------------------------------
Eval num_timesteps=907000, episode_reward=-70931.17 +/- 23601.15
Episode length: 59.40 +/- 10.38
------------------------------------
| eval/              |             |
|    mean action     | -0.41669556 |
|    mean velocity x | 1.29        |
|    mean velocity y | 1.91        |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -7.09e+04   |
| time/              |             |
|    total_timesteps | 907000      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.1     |
|    ep_rew_mean     | -8.5e+04 |
| time/              |          |
|    fps             | 128      |
|    iterations      | 443      |
|    time_elapsed    | 7040     |
|    total_timesteps | 907264   |
---------------------------------
Eval num_timesteps=907500, episode_reward=-97812.39 +/- 13140.97
Episode length: 65.00 +/- 5.55
------------------------------------------
| eval/                   |              |
|    mean action          | -0.25647607  |
|    mean velocity x      | 0.958        |
|    mean velocity y      | 1.84         |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 65           |
|    mean_reward          | -9.78e+04    |
| time/                   |              |
|    total_timesteps      | 907500       |
| train/                  |              |
|    approx_kl            | 0.0010427347 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.001        |
|    loss                 | 1.59e+08     |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 0.907        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=908000, episode_reward=-78433.32 +/- 38164.90
Episode length: 56.20 +/- 15.92
-----------------------------------
| eval/              |            |
|    mean action     | -0.4713465 |
|    mean velocity x | 0.85       |
|    mean velocity y | 2.15       |
|    mean velocity z | 22.2       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -7.84e+04  |
| time/              |            |
|    total_timesteps | 908000     |
-----------------------------------
Eval num_timesteps=908500, episode_reward=-75652.78 +/- 30029.57
Episode length: 76.80 +/- 26.55
----------------------------------
| eval/              |           |
|    mean action     | 0.2995573 |
|    mean velocity x | -0.375    |
|    mean velocity y | -0.65     |
|    mean velocity z | 14.7      |
|    mean_ep_length  | 76.8      |
|    mean_reward     | -7.57e+04 |
| time/              |           |
|    total_timesteps | 908500    |
----------------------------------
Eval num_timesteps=909000, episode_reward=-54708.76 +/- 23928.11
Episode length: 55.80 +/- 12.62
------------------------------------
| eval/              |             |
|    mean action     | 0.119356915 |
|    mean velocity x | -0.347      |
|    mean velocity y | -0.575      |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 55.8        |
|    mean_reward     | -5.47e+04   |
| time/              |             |
|    total_timesteps | 909000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.3      |
|    ep_rew_mean     | -8.29e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 444       |
|    time_elapsed    | 7048      |
|    total_timesteps | 909312    |
----------------------------------
Eval num_timesteps=909500, episode_reward=-80615.33 +/- 40323.24
Episode length: 53.00 +/- 19.52
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.7064184    |
|    mean velocity x      | 2.61          |
|    mean velocity y      | 4.01          |
|    mean velocity z      | 17.9          |
|    mean_ep_length       | 53            |
|    mean_reward          | -8.06e+04     |
| time/                   |               |
|    total_timesteps      | 909500        |
| train/                  |               |
|    approx_kl            | 2.3240864e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.001         |
|    loss                 | 1.21e+08      |
|    n_updates            | 4440          |
|    policy_gradient_loss | -0.000285     |
|    std                  | 0.907         |
|    value_loss           | 1.91e+08      |
-------------------------------------------
Eval num_timesteps=910000, episode_reward=-86925.97 +/- 12190.77
Episode length: 63.60 +/- 4.67
-----------------------------------
| eval/              |            |
|    mean action     | -0.5504833 |
|    mean velocity x | 3.43       |
|    mean velocity y | 3.65       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 63.6       |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 910000     |
-----------------------------------
Eval num_timesteps=910500, episode_reward=-39803.47 +/- 33809.70
Episode length: 47.80 +/- 16.56
-----------------------------------
| eval/              |            |
|    mean action     | -0.4070678 |
|    mean velocity x | 1.9        |
|    mean velocity y | 1.66       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 47.8       |
|    mean_reward     | -3.98e+04  |
| time/              |            |
|    total_timesteps | 910500     |
-----------------------------------
Eval num_timesteps=911000, episode_reward=-52288.05 +/- 34717.59
Episode length: 53.20 +/- 14.74
------------------------------------
| eval/              |             |
|    mean action     | -0.44423175 |
|    mean velocity x | 0.892       |
|    mean velocity y | 1.39        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 53.2        |
|    mean_reward     | -5.23e+04   |
| time/              |             |
|    total_timesteps | 911000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.8      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 445       |
|    time_elapsed    | 7055      |
|    total_timesteps | 911360    |
----------------------------------
Eval num_timesteps=911500, episode_reward=-78828.42 +/- 14482.17
Episode length: 67.20 +/- 6.24
------------------------------------------
| eval/                   |              |
|    mean action          | 0.19098975   |
|    mean velocity x      | -2.21        |
|    mean velocity y      | -1.74        |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 67.2         |
|    mean_reward          | -7.88e+04    |
| time/                   |              |
|    total_timesteps      | 911500       |
| train/                  |              |
|    approx_kl            | 0.0016742598 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.201        |
|    learning_rate        | 0.001        |
|    loss                 | 1.85e+08     |
|    n_updates            | 4450         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 0.907        |
|    value_loss           | 2.42e+08     |
------------------------------------------
Eval num_timesteps=912000, episode_reward=-75071.99 +/- 43208.98
Episode length: 52.60 +/- 19.65
------------------------------------
| eval/              |             |
|    mean action     | -0.43495783 |
|    mean velocity x | 1.46        |
|    mean velocity y | 2.06        |
|    mean velocity z | 20          |
|    mean_ep_length  | 52.6        |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 912000      |
------------------------------------
Eval num_timesteps=912500, episode_reward=-52296.15 +/- 28904.27
Episode length: 50.60 +/- 17.78
-----------------------------------
| eval/              |            |
|    mean action     | 0.10214867 |
|    mean velocity x | -0.000601  |
|    mean velocity y | 0.631      |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 50.6       |
|    mean_reward     | -5.23e+04  |
| time/              |            |
|    total_timesteps | 912500     |
-----------------------------------
Eval num_timesteps=913000, episode_reward=-47065.65 +/- 41916.40
Episode length: 51.00 +/- 21.73
-----------------------------------
| eval/              |            |
|    mean action     | -0.2124097 |
|    mean velocity x | -1.53      |
|    mean velocity y | -0.145     |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 51         |
|    mean_reward     | -4.71e+04  |
| time/              |            |
|    total_timesteps | 913000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.2      |
|    ep_rew_mean     | -8.18e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 446       |
|    time_elapsed    | 7062      |
|    total_timesteps | 913408    |
----------------------------------
Eval num_timesteps=913500, episode_reward=-65785.62 +/- 35919.84
Episode length: 59.40 +/- 23.80
------------------------------------------
| eval/                   |              |
|    mean action          | -0.28991583  |
|    mean velocity x      | 0.304        |
|    mean velocity y      | 1.34         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 59.4         |
|    mean_reward          | -6.58e+04    |
| time/                   |              |
|    total_timesteps      | 913500       |
| train/                  |              |
|    approx_kl            | 0.0018449974 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+08     |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00202     |
|    std                  | 0.907        |
|    value_loss           | 2.23e+08     |
------------------------------------------
Eval num_timesteps=914000, episode_reward=-49704.35 +/- 37961.67
Episode length: 48.60 +/- 14.96
------------------------------------
| eval/              |             |
|    mean action     | -0.11831534 |
|    mean velocity x | 0.833       |
|    mean velocity y | 1.81        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -4.97e+04   |
| time/              |             |
|    total_timesteps | 914000      |
------------------------------------
Eval num_timesteps=914500, episode_reward=-82423.71 +/- 34334.79
Episode length: 62.80 +/- 8.03
------------------------------------
| eval/              |             |
|    mean action     | -0.30420905 |
|    mean velocity x | 1.48        |
|    mean velocity y | 1.32        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 62.8        |
|    mean_reward     | -8.24e+04   |
| time/              |             |
|    total_timesteps | 914500      |
------------------------------------
Eval num_timesteps=915000, episode_reward=-48892.05 +/- 36172.84
Episode length: 48.20 +/- 15.17
----------------------------------
| eval/              |           |
|    mean action     | 0.1712377 |
|    mean velocity x | 0.412     |
|    mean velocity y | -1.83     |
|    mean velocity z | 16.5      |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.89e+04 |
| time/              |           |
|    total_timesteps | 915000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.5      |
|    ep_rew_mean     | -7.84e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 447       |
|    time_elapsed    | 7069      |
|    total_timesteps | 915456    |
----------------------------------
Eval num_timesteps=915500, episode_reward=-85454.60 +/- 15285.19
Episode length: 70.40 +/- 14.21
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24542545   |
|    mean velocity x      | 0.431         |
|    mean velocity y      | 0.576         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 70.4          |
|    mean_reward          | -8.55e+04     |
| time/                   |               |
|    total_timesteps      | 915500        |
| train/                  |               |
|    approx_kl            | 0.00032026958 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.001         |
|    loss                 | 8.95e+07      |
|    n_updates            | 4470          |
|    policy_gradient_loss | -0.000939     |
|    std                  | 0.907         |
|    value_loss           | 1.7e+08       |
-------------------------------------------
Eval num_timesteps=916000, episode_reward=-82219.86 +/- 17201.97
Episode length: 78.60 +/- 34.17
------------------------------------
| eval/              |             |
|    mean action     | -0.18670624 |
|    mean velocity x | 1.26        |
|    mean velocity y | 1.45        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 78.6        |
|    mean_reward     | -8.22e+04   |
| time/              |             |
|    total_timesteps | 916000      |
------------------------------------
Eval num_timesteps=916500, episode_reward=-93351.05 +/- 21466.03
Episode length: 71.40 +/- 15.40
----------------------------------
| eval/              |           |
|    mean action     | -0.250806 |
|    mean velocity x | 0.92      |
|    mean velocity y | 0.918     |
|    mean velocity z | 18.6      |
|    mean_ep_length  | 71.4      |
|    mean_reward     | -9.34e+04 |
| time/              |           |
|    total_timesteps | 916500    |
----------------------------------
Eval num_timesteps=917000, episode_reward=-99508.91 +/- 50666.67
Episode length: 93.60 +/- 53.54
------------------------------------
| eval/              |             |
|    mean action     | -0.41045555 |
|    mean velocity x | 2.3         |
|    mean velocity y | 3.03        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 93.6        |
|    mean_reward     | -9.95e+04   |
| time/              |             |
|    total_timesteps | 917000      |
------------------------------------
Eval num_timesteps=917500, episode_reward=-52956.59 +/- 36609.08
Episode length: 49.60 +/- 17.42
------------------------------------
| eval/              |             |
|    mean action     | -0.14507408 |
|    mean velocity x | 0.366       |
|    mean velocity y | 0.592       |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 49.6        |
|    mean_reward     | -5.3e+04    |
| time/              |             |
|    total_timesteps | 917500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 448       |
|    time_elapsed    | 7077      |
|    total_timesteps | 917504    |
----------------------------------
Eval num_timesteps=918000, episode_reward=-54361.14 +/- 38619.67
Episode length: 58.00 +/- 32.69
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.043046642  |
|    mean velocity x      | -0.194        |
|    mean velocity y      | -0.0609       |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 58            |
|    mean_reward          | -5.44e+04     |
| time/                   |               |
|    total_timesteps      | 918000        |
| train/                  |               |
|    approx_kl            | 0.00016286466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.258         |
|    learning_rate        | 0.001         |
|    loss                 | 8.74e+07      |
|    n_updates            | 4480          |
|    policy_gradient_loss | -0.000871     |
|    std                  | 0.908         |
|    value_loss           | 2.16e+08      |
-------------------------------------------
Eval num_timesteps=918500, episode_reward=-64106.53 +/- 50369.07
Episode length: 50.40 +/- 18.90
------------------------------------
| eval/              |             |
|    mean action     | -0.54869384 |
|    mean velocity x | 3.58        |
|    mean velocity y | 3.62        |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 50.4        |
|    mean_reward     | -6.41e+04   |
| time/              |             |
|    total_timesteps | 918500      |
------------------------------------
Eval num_timesteps=919000, episode_reward=-46364.56 +/- 38836.90
Episode length: 43.00 +/- 23.66
------------------------------------
| eval/              |             |
|    mean action     | 0.117125474 |
|    mean velocity x | -1          |
|    mean velocity y | -1.12       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 43          |
|    mean_reward     | -4.64e+04   |
| time/              |             |
|    total_timesteps | 919000      |
------------------------------------
Eval num_timesteps=919500, episode_reward=-74447.10 +/- 28444.05
Episode length: 57.80 +/- 9.47
------------------------------------
| eval/              |             |
|    mean action     | -0.41142038 |
|    mean velocity x | 0.308       |
|    mean velocity y | 1.82        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 57.8        |
|    mean_reward     | -7.44e+04   |
| time/              |             |
|    total_timesteps | 919500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70        |
|    ep_rew_mean     | -7.52e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 449       |
|    time_elapsed    | 7084      |
|    total_timesteps | 919552    |
----------------------------------
Eval num_timesteps=920000, episode_reward=-58949.26 +/- 24330.50
Episode length: 67.00 +/- 19.42
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5295733    |
|    mean velocity x      | 2.83          |
|    mean velocity y      | 3.84          |
|    mean velocity z      | 17.7          |
|    mean_ep_length       | 67            |
|    mean_reward          | -5.89e+04     |
| time/                   |               |
|    total_timesteps      | 920000        |
| train/                  |               |
|    approx_kl            | 0.00020470985 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.001         |
|    loss                 | 6.74e+07      |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000624     |
|    std                  | 0.908         |
|    value_loss           | 2.24e+08      |
-------------------------------------------
Eval num_timesteps=920500, episode_reward=-73062.19 +/- 28782.98
Episode length: 69.60 +/- 22.12
-----------------------------------
| eval/              |            |
|    mean action     | 0.73110765 |
|    mean velocity x | -1.68      |
|    mean velocity y | -3.54      |
|    mean velocity z | 17         |
|    mean_ep_length  | 69.6       |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 920500     |
-----------------------------------
Eval num_timesteps=921000, episode_reward=-74553.83 +/- 22758.10
Episode length: 62.80 +/- 9.66
------------------------------------
| eval/              |             |
|    mean action     | -0.25882798 |
|    mean velocity x | -1.68       |
|    mean velocity y | 0.859       |
|    mean velocity z | 16.8        |
|    mean_ep_length  | 62.8        |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 921000      |
------------------------------------
Eval num_timesteps=921500, episode_reward=-58469.47 +/- 37528.10
Episode length: 50.80 +/- 17.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.6173773 |
|    mean velocity x | 0.951      |
|    mean velocity y | 3.18       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 50.8       |
|    mean_reward     | -5.85e+04  |
| time/              |            |
|    total_timesteps | 921500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.2      |
|    ep_rew_mean     | -7.29e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 450       |
|    time_elapsed    | 7103      |
|    total_timesteps | 921600    |
----------------------------------
Eval num_timesteps=922000, episode_reward=-72381.97 +/- 29334.96
Episode length: 57.20 +/- 9.70
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.57497275   |
|    mean velocity x      | 0.816         |
|    mean velocity y      | 3.24          |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 57.2          |
|    mean_reward          | -7.24e+04     |
| time/                   |               |
|    total_timesteps      | 922000        |
| train/                  |               |
|    approx_kl            | 1.5541184e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.255         |
|    learning_rate        | 0.001         |
|    loss                 | 6.4e+07       |
|    n_updates            | 4500          |
|    policy_gradient_loss | -0.000158     |
|    std                  | 0.908         |
|    value_loss           | 1.69e+08      |
-------------------------------------------
Eval num_timesteps=922500, episode_reward=-62604.66 +/- 34060.70
Episode length: 55.00 +/- 16.53
-----------------------------------
| eval/              |            |
|    mean action     | -0.4708031 |
|    mean velocity x | 3          |
|    mean velocity y | 3.49       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 55         |
|    mean_reward     | -6.26e+04  |
| time/              |            |
|    total_timesteps | 922500     |
-----------------------------------
Eval num_timesteps=923000, episode_reward=-73826.70 +/- 24949.39
Episode length: 62.20 +/- 13.47
----------------------------------
| eval/              |           |
|    mean action     | 0.6066409 |
|    mean velocity x | -1.26     |
|    mean velocity y | -3.21     |
|    mean velocity z | 18        |
|    mean_ep_length  | 62.2      |
|    mean_reward     | -7.38e+04 |
| time/              |           |
|    total_timesteps | 923000    |
----------------------------------
Eval num_timesteps=923500, episode_reward=-55786.37 +/- 38845.42
Episode length: 49.00 +/- 15.87
------------------------------------
| eval/              |             |
|    mean action     | -0.33154327 |
|    mean velocity x | 1.4         |
|    mean velocity y | 2.66        |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 49          |
|    mean_reward     | -5.58e+04   |
| time/              |             |
|    total_timesteps | 923500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.4      |
|    ep_rew_mean     | -7.82e+04 |
| time/              |           |
|    fps             | 129       |
|    iterations      | 451       |
|    time_elapsed    | 7110      |
|    total_timesteps | 923648    |
----------------------------------
Eval num_timesteps=924000, episode_reward=-39592.66 +/- 29725.55
Episode length: 43.40 +/- 18.42
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.22742975   |
|    mean velocity x      | 1.49          |
|    mean velocity y      | 1.75          |
|    mean velocity z      | 18            |
|    mean_ep_length       | 43.4          |
|    mean_reward          | -3.96e+04     |
| time/                   |               |
|    total_timesteps      | 924000        |
| train/                  |               |
|    approx_kl            | 0.00022360028 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.001         |
|    loss                 | 1.56e+08      |
|    n_updates            | 4510          |
|    policy_gradient_loss | -0.000469     |
|    std                  | 0.908         |
|    value_loss           | 2.23e+08      |
-------------------------------------------
Eval num_timesteps=924500, episode_reward=-70084.20 +/- 40396.26
Episode length: 57.00 +/- 14.76
------------------------------------
| eval/              |             |
|    mean action     | -0.07284524 |
|    mean velocity x | 0.651       |
|    mean velocity y | 0.591       |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 57          |
|    mean_reward     | -7.01e+04   |
| time/              |             |
|    total_timesteps | 924500      |
------------------------------------
Eval num_timesteps=925000, episode_reward=-72155.28 +/- 36948.22
Episode length: 55.40 +/- 13.35
------------------------------------
| eval/              |             |
|    mean action     | -0.24146326 |
|    mean velocity x | 2.23        |
|    mean velocity y | 1.72        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 55.4        |
|    mean_reward     | -7.22e+04   |
| time/              |             |
|    total_timesteps | 925000      |
------------------------------------
Eval num_timesteps=925500, episode_reward=-42264.14 +/- 27169.86
Episode length: 45.60 +/- 12.52
------------------------------------
| eval/              |             |
|    mean action     | -0.26108587 |
|    mean velocity x | 0.722       |
|    mean velocity y | 3.1         |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 45.6        |
|    mean_reward     | -4.23e+04   |
| time/              |             |
|    total_timesteps | 925500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -7.43e+04 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 452       |
|    time_elapsed    | 7117      |
|    total_timesteps | 925696    |
----------------------------------
Eval num_timesteps=926000, episode_reward=-97931.30 +/- 11351.53
Episode length: 66.80 +/- 6.40
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.015311454   |
|    mean velocity x      | -0.822        |
|    mean velocity y      | -1.12         |
|    mean velocity z      | 21.7          |
|    mean_ep_length       | 66.8          |
|    mean_reward          | -9.79e+04     |
| time/                   |               |
|    total_timesteps      | 926000        |
| train/                  |               |
|    approx_kl            | 0.00031014282 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.221         |
|    learning_rate        | 0.001         |
|    loss                 | 1.32e+08      |
|    n_updates            | 4520          |
|    policy_gradient_loss | -0.000554     |
|    std                  | 0.908         |
|    value_loss           | 2.23e+08      |
-------------------------------------------
Eval num_timesteps=926500, episode_reward=-70510.52 +/- 18516.31
Episode length: 74.40 +/- 14.57
------------------------------------
| eval/              |             |
|    mean action     | -0.49819967 |
|    mean velocity x | 1.76        |
|    mean velocity y | 2.36        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 74.4        |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 926500      |
------------------------------------
Eval num_timesteps=927000, episode_reward=-52731.73 +/- 40120.61
Episode length: 55.00 +/- 26.21
------------------------------------
| eval/              |             |
|    mean action     | -0.40941077 |
|    mean velocity x | 1.6         |
|    mean velocity y | 3.33        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 55          |
|    mean_reward     | -5.27e+04   |
| time/              |             |
|    total_timesteps | 927000      |
------------------------------------
Eval num_timesteps=927500, episode_reward=-66436.84 +/- 37943.33
Episode length: 52.20 +/- 13.89
-----------------------------------
| eval/              |            |
|    mean action     | 0.20215371 |
|    mean velocity x | -0.306     |
|    mean velocity y | -1.11      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 52.2       |
|    mean_reward     | -6.64e+04  |
| time/              |            |
|    total_timesteps | 927500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -7.85e+04 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 453       |
|    time_elapsed    | 7124      |
|    total_timesteps | 927744    |
----------------------------------
Eval num_timesteps=928000, episode_reward=-39835.69 +/- 28411.05
Episode length: 55.20 +/- 26.93
------------------------------------------
| eval/                   |              |
|    mean action          | -0.41178465  |
|    mean velocity x      | 1.65         |
|    mean velocity y      | 2.59         |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 55.2         |
|    mean_reward          | -3.98e+04    |
| time/                   |              |
|    total_timesteps      | 928000       |
| train/                  |              |
|    approx_kl            | 0.0019228553 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.206        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+08     |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 0.908        |
|    value_loss           | 2.75e+08     |
------------------------------------------
Eval num_timesteps=928500, episode_reward=-61642.57 +/- 25870.17
Episode length: 59.40 +/- 11.13
------------------------------------
| eval/              |             |
|    mean action     | -0.31063583 |
|    mean velocity x | 1.93        |
|    mean velocity y | 2.18        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -6.16e+04   |
| time/              |             |
|    total_timesteps | 928500      |
------------------------------------
Eval num_timesteps=929000, episode_reward=-49643.90 +/- 41691.35
Episode length: 54.80 +/- 29.77
------------------------------------
| eval/              |             |
|    mean action     | -0.58956224 |
|    mean velocity x | 1.77        |
|    mean velocity y | 4.39        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -4.96e+04   |
| time/              |             |
|    total_timesteps | 929000      |
------------------------------------
Eval num_timesteps=929500, episode_reward=-45673.02 +/- 34086.44
Episode length: 49.00 +/- 17.42
-----------------------------------
| eval/              |            |
|    mean action     | -0.5167782 |
|    mean velocity x | 1.39       |
|    mean velocity y | 4.34       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 49         |
|    mean_reward     | -4.57e+04  |
| time/              |            |
|    total_timesteps | 929500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.4      |
|    ep_rew_mean     | -7.82e+04 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 454       |
|    time_elapsed    | 7131      |
|    total_timesteps | 929792    |
----------------------------------
Eval num_timesteps=930000, episode_reward=-76281.36 +/- 14380.22
Episode length: 67.60 +/- 11.29
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.106379956  |
|    mean velocity x      | -0.972        |
|    mean velocity y      | -0.497        |
|    mean velocity z      | 17.9          |
|    mean_ep_length       | 67.6          |
|    mean_reward          | -7.63e+04     |
| time/                   |               |
|    total_timesteps      | 930000        |
| train/                  |               |
|    approx_kl            | 0.00081724644 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.001         |
|    loss                 | 9.53e+07      |
|    n_updates            | 4540          |
|    policy_gradient_loss | -0.00084      |
|    std                  | 0.908         |
|    value_loss           | 1.94e+08      |
-------------------------------------------
Eval num_timesteps=930500, episode_reward=-65535.67 +/- 37801.08
Episode length: 56.00 +/- 15.56
------------------------------------
| eval/              |             |
|    mean action     | 0.101447664 |
|    mean velocity x | -1.96       |
|    mean velocity y | -0.839      |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 56          |
|    mean_reward     | -6.55e+04   |
| time/              |             |
|    total_timesteps | 930500      |
------------------------------------
Eval num_timesteps=931000, episode_reward=-61788.48 +/- 40084.60
Episode length: 54.00 +/- 13.21
------------------------------------
| eval/              |             |
|    mean action     | -0.16356158 |
|    mean velocity x | 1.11        |
|    mean velocity y | 0.164       |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 54          |
|    mean_reward     | -6.18e+04   |
| time/              |             |
|    total_timesteps | 931000      |
------------------------------------
Eval num_timesteps=931500, episode_reward=-62126.16 +/- 41351.83
Episode length: 52.20 +/- 16.85
------------------------------------
| eval/              |             |
|    mean action     | -0.17714277 |
|    mean velocity x | 2.53        |
|    mean velocity y | 1.1         |
|    mean velocity z | 17          |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -6.21e+04   |
| time/              |             |
|    total_timesteps | 931500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.4      |
|    ep_rew_mean     | -8.02e+04 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 455       |
|    time_elapsed    | 7138      |
|    total_timesteps | 931840    |
----------------------------------
Eval num_timesteps=932000, episode_reward=-91914.20 +/- 19769.41
Episode length: 65.00 +/- 5.37
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35147437  |
|    mean velocity x      | 0.347        |
|    mean velocity y      | 1.63         |
|    mean velocity z      | 21.7         |
|    mean_ep_length       | 65           |
|    mean_reward          | -9.19e+04    |
| time/                   |              |
|    total_timesteps      | 932000       |
| train/                  |              |
|    approx_kl            | 0.0005612708 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.001        |
|    loss                 | 9.05e+07     |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.000501    |
|    std                  | 0.908        |
|    value_loss           | 2.13e+08     |
------------------------------------------
Eval num_timesteps=932500, episode_reward=-45839.09 +/- 36911.93
Episode length: 52.60 +/- 27.24
-------------------------------------
| eval/              |              |
|    mean action     | -0.060159333 |
|    mean velocity x | 0.14         |
|    mean velocity y | -0.209       |
|    mean velocity z | 17           |
|    mean_ep_length  | 52.6         |
|    mean_reward     | -4.58e+04    |
| time/              |              |
|    total_timesteps | 932500       |
-------------------------------------
Eval num_timesteps=933000, episode_reward=-76492.72 +/- 36502.46
Episode length: 52.40 +/- 16.35
-----------------------------------
| eval/              |            |
|    mean action     | -0.7735784 |
|    mean velocity x | 3.36       |
|    mean velocity y | 5.61       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 52.4       |
|    mean_reward     | -7.65e+04  |
| time/              |            |
|    total_timesteps | 933000     |
-----------------------------------
Eval num_timesteps=933500, episode_reward=-77557.17 +/- 10302.91
Episode length: 83.20 +/- 19.21
-------------------------------------
| eval/              |              |
|    mean action     | -0.004430542 |
|    mean velocity x | 1.12         |
|    mean velocity y | 0.838        |
|    mean velocity z | 17.7         |
|    mean_ep_length  | 83.2         |
|    mean_reward     | -7.76e+04    |
| time/              |              |
|    total_timesteps | 933500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.6      |
|    ep_rew_mean     | -7.83e+04 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 456       |
|    time_elapsed    | 7145      |
|    total_timesteps | 933888    |
----------------------------------
Eval num_timesteps=934000, episode_reward=-44771.53 +/- 35169.23
Episode length: 46.40 +/- 19.09
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.05582088   |
|    mean velocity x      | 1.07          |
|    mean velocity y      | 0.47          |
|    mean velocity z      | 18.4          |
|    mean_ep_length       | 46.4          |
|    mean_reward          | -4.48e+04     |
| time/                   |               |
|    total_timesteps      | 934000        |
| train/                  |               |
|    approx_kl            | 0.00032953976 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.263         |
|    learning_rate        | 0.001         |
|    loss                 | 7.95e+07      |
|    n_updates            | 4560          |
|    policy_gradient_loss | -0.000469     |
|    std                  | 0.909         |
|    value_loss           | 1.86e+08      |
-------------------------------------------
Eval num_timesteps=934500, episode_reward=-61760.83 +/- 28357.55
Episode length: 59.80 +/- 18.41
----------------------------------
| eval/              |           |
|    mean action     | -0.319654 |
|    mean velocity x | 0.0277    |
|    mean velocity y | 0.0569    |
|    mean velocity z | 21.5      |
|    mean_ep_length  | 59.8      |
|    mean_reward     | -6.18e+04 |
| time/              |           |
|    total_timesteps | 934500    |
----------------------------------
Eval num_timesteps=935000, episode_reward=-64118.72 +/- 33896.21
Episode length: 53.80 +/- 19.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.30887294 |
|    mean velocity x | -1.05      |
|    mean velocity y | -3.65      |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 53.8       |
|    mean_reward     | -6.41e+04  |
| time/              |            |
|    total_timesteps | 935000     |
-----------------------------------
Eval num_timesteps=935500, episode_reward=-81219.51 +/- 16025.95
Episode length: 63.60 +/- 3.61
-----------------------------------
| eval/              |            |
|    mean action     | -1.1171887 |
|    mean velocity x | 4.39       |
|    mean velocity y | 7.13       |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 63.6       |
|    mean_reward     | -8.12e+04  |
| time/              |            |
|    total_timesteps | 935500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.5      |
|    ep_rew_mean     | -8.44e+04 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 457       |
|    time_elapsed    | 7152      |
|    total_timesteps | 935936    |
----------------------------------
Eval num_timesteps=936000, episode_reward=-37193.09 +/- 29471.02
Episode length: 53.40 +/- 25.49
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29135618  |
|    mean velocity x      | 0.537        |
|    mean velocity y      | 1.17         |
|    mean velocity z      | 17.1         |
|    mean_ep_length       | 53.4         |
|    mean_reward          | -3.72e+04    |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0019525157 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+08      |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.909        |
|    value_loss           | 2.33e+08     |
------------------------------------------
Eval num_timesteps=936500, episode_reward=-50986.94 +/- 37484.65
Episode length: 45.40 +/- 21.21
-----------------------------------
| eval/              |            |
|    mean action     | -0.5357701 |
|    mean velocity x | 1.2        |
|    mean velocity y | 3.88       |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 45.4       |
|    mean_reward     | -5.1e+04   |
| time/              |            |
|    total_timesteps | 936500     |
-----------------------------------
Eval num_timesteps=937000, episode_reward=-65016.17 +/- 41072.24
Episode length: 51.80 +/- 15.97
------------------------------------
| eval/              |             |
|    mean action     | -0.06292762 |
|    mean velocity x | -0.817      |
|    mean velocity y | -1.35       |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 51.8        |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 937000      |
------------------------------------
Eval num_timesteps=937500, episode_reward=-89258.68 +/- 13467.10
Episode length: 66.40 +/- 6.02
-----------------------------------
| eval/              |            |
|    mean action     | -0.3265736 |
|    mean velocity x | -0.636     |
|    mean velocity y | 0.491      |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 66.4       |
|    mean_reward     | -8.93e+04  |
| time/              |            |
|    total_timesteps | 937500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.2      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 458       |
|    time_elapsed    | 7159      |
|    total_timesteps | 937984    |
----------------------------------
Eval num_timesteps=938000, episode_reward=-79969.88 +/- 24009.60
Episode length: 59.80 +/- 5.71
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35495907  |
|    mean velocity x      | 1.03         |
|    mean velocity y      | 1.33         |
|    mean velocity z      | 20.3         |
|    mean_ep_length       | 59.8         |
|    mean_reward          | -8e+04       |
| time/                   |              |
|    total_timesteps      | 938000       |
| train/                  |              |
|    approx_kl            | 0.0009039765 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 9.96e+07     |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.000816    |
|    std                  | 0.909        |
|    value_loss           | 2.04e+08     |
------------------------------------------
Eval num_timesteps=938500, episode_reward=-49680.29 +/- 32679.73
Episode length: 53.60 +/- 23.40
------------------------------------
| eval/              |             |
|    mean action     | -0.45771635 |
|    mean velocity x | -0.108      |
|    mean velocity y | 1.76        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -4.97e+04   |
| time/              |             |
|    total_timesteps | 938500      |
------------------------------------
Eval num_timesteps=939000, episode_reward=-85778.43 +/- 23415.49
Episode length: 60.20 +/- 7.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.15134084 |
|    mean velocity x | -0.705     |
|    mean velocity y | -0.505     |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 60.2       |
|    mean_reward     | -8.58e+04  |
| time/              |            |
|    total_timesteps | 939000     |
-----------------------------------
Eval num_timesteps=939500, episode_reward=-83494.13 +/- 15113.98
Episode length: 79.20 +/- 26.27
-----------------------------------
| eval/              |            |
|    mean action     | 0.16182104 |
|    mean velocity x | -0.863     |
|    mean velocity y | -1.64      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 79.2       |
|    mean_reward     | -8.35e+04  |
| time/              |            |
|    total_timesteps | 939500     |
-----------------------------------
Eval num_timesteps=940000, episode_reward=-62109.57 +/- 48708.99
Episode length: 62.40 +/- 35.66
-----------------------------------
| eval/              |            |
|    mean action     | -0.1635788 |
|    mean velocity x | 1.01       |
|    mean velocity y | 0.789      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -6.21e+04  |
| time/              |            |
|    total_timesteps | 940000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.1      |
|    ep_rew_mean     | -8.66e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 459       |
|    time_elapsed    | 7167      |
|    total_timesteps | 940032    |
----------------------------------
Eval num_timesteps=940500, episode_reward=-45752.02 +/- 28483.57
Episode length: 52.60 +/- 23.57
------------------------------------------
| eval/                   |              |
|    mean action          | 0.01075795   |
|    mean velocity x      | -0.818       |
|    mean velocity y      | -0.257       |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 52.6         |
|    mean_reward          | -4.58e+04    |
| time/                   |              |
|    total_timesteps      | 940500       |
| train/                  |              |
|    approx_kl            | 0.0001122744 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.208        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+08      |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.000198    |
|    std                  | 0.909        |
|    value_loss           | 2.6e+08      |
------------------------------------------
Eval num_timesteps=941000, episode_reward=-63626.75 +/- 30273.99
Episode length: 70.40 +/- 25.05
------------------------------------
| eval/              |             |
|    mean action     | -0.38779297 |
|    mean velocity x | 0.666       |
|    mean velocity y | 1.62        |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 70.4        |
|    mean_reward     | -6.36e+04   |
| time/              |             |
|    total_timesteps | 941000      |
------------------------------------
Eval num_timesteps=941500, episode_reward=-59789.77 +/- 23865.83
Episode length: 58.20 +/- 11.96
----------------------------------
| eval/              |           |
|    mean action     | -0.584162 |
|    mean velocity x | 1.89      |
|    mean velocity y | 3.61      |
|    mean velocity z | 18        |
|    mean_ep_length  | 58.2      |
|    mean_reward     | -5.98e+04 |
| time/              |           |
|    total_timesteps | 941500    |
----------------------------------
Eval num_timesteps=942000, episode_reward=-82821.55 +/- 14034.45
Episode length: 78.00 +/- 17.19
-----------------------------------
| eval/              |            |
|    mean action     | -0.7250887 |
|    mean velocity x | 2.88       |
|    mean velocity y | 4.03       |
|    mean velocity z | 15.9       |
|    mean_ep_length  | 78         |
|    mean_reward     | -8.28e+04  |
| time/              |            |
|    total_timesteps | 942000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.1      |
|    ep_rew_mean     | -8.75e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 460       |
|    time_elapsed    | 7174      |
|    total_timesteps | 942080    |
----------------------------------
Eval num_timesteps=942500, episode_reward=-85462.08 +/- 53898.40
Episode length: 73.00 +/- 51.98
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.14964664    |
|    mean velocity x      | 0.334          |
|    mean velocity y      | -0.108         |
|    mean velocity z      | 16             |
|    mean_ep_length       | 73             |
|    mean_reward          | -8.55e+04      |
| time/                   |                |
|    total_timesteps      | 942500         |
| train/                  |                |
|    approx_kl            | 0.000117657706 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.97          |
|    explained_variance   | 0.262          |
|    learning_rate        | 0.001          |
|    loss                 | 8.69e+07       |
|    n_updates            | 4600           |
|    policy_gradient_loss | -0.00068       |
|    std                  | 0.909          |
|    value_loss           | 1.59e+08       |
--------------------------------------------
Eval num_timesteps=943000, episode_reward=-69937.91 +/- 40705.63
Episode length: 57.60 +/- 8.11
------------------------------------
| eval/              |             |
|    mean action     | -0.07773626 |
|    mean velocity x | 1.45        |
|    mean velocity y | 1.35        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -6.99e+04   |
| time/              |             |
|    total_timesteps | 943000      |
------------------------------------
Eval num_timesteps=943500, episode_reward=-56243.79 +/- 31346.54
Episode length: 55.40 +/- 12.47
------------------------------------
| eval/              |             |
|    mean action     | -0.49979907 |
|    mean velocity x | 2.1         |
|    mean velocity y | 2.54        |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 55.4        |
|    mean_reward     | -5.62e+04   |
| time/              |             |
|    total_timesteps | 943500      |
------------------------------------
Eval num_timesteps=944000, episode_reward=-29990.31 +/- 38094.08
Episode length: 38.40 +/- 17.73
------------------------------------
| eval/              |             |
|    mean action     | -0.59027416 |
|    mean velocity x | 1.83        |
|    mean velocity y | 3.58        |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 38.4        |
|    mean_reward     | -3e+04      |
| time/              |             |
|    total_timesteps | 944000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.6      |
|    ep_rew_mean     | -7.87e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 461       |
|    time_elapsed    | 7182      |
|    total_timesteps | 944128    |
----------------------------------
Eval num_timesteps=944500, episode_reward=-70086.72 +/- 20293.17
Episode length: 64.60 +/- 9.73
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.046058547  |
|    mean velocity x      | 0.499         |
|    mean velocity y      | -0.384        |
|    mean velocity z      | 20.3          |
|    mean_ep_length       | 64.6          |
|    mean_reward          | -7.01e+04     |
| time/                   |               |
|    total_timesteps      | 944500        |
| train/                  |               |
|    approx_kl            | 0.00023616484 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+08      |
|    n_updates            | 4610          |
|    policy_gradient_loss | -0.000782     |
|    std                  | 0.909         |
|    value_loss           | 2.02e+08      |
-------------------------------------------
Eval num_timesteps=945000, episode_reward=-100713.95 +/- 4792.74
Episode length: 64.40 +/- 3.83
----------------------------------
| eval/              |           |
|    mean action     | 0.7007007 |
|    mean velocity x | -1.38     |
|    mean velocity y | -4.37     |
|    mean velocity z | 19        |
|    mean_ep_length  | 64.4      |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 945000    |
----------------------------------
Eval num_timesteps=945500, episode_reward=-48046.74 +/- 26572.53
Episode length: 55.80 +/- 9.74
-----------------------------------
| eval/              |            |
|    mean action     | -0.5859849 |
|    mean velocity x | 2.38       |
|    mean velocity y | 3.16       |
|    mean velocity z | 17         |
|    mean_ep_length  | 55.8       |
|    mean_reward     | -4.8e+04   |
| time/              |            |
|    total_timesteps | 945500     |
-----------------------------------
Eval num_timesteps=946000, episode_reward=-91003.69 +/- 15407.76
Episode length: 66.20 +/- 10.57
------------------------------------
| eval/              |             |
|    mean action     | -0.09956227 |
|    mean velocity x | -0.0915     |
|    mean velocity y | -0.439      |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 66.2        |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 946000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.5      |
|    ep_rew_mean     | -7.68e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 462       |
|    time_elapsed    | 7189      |
|    total_timesteps | 946176    |
----------------------------------
Eval num_timesteps=946500, episode_reward=-66067.14 +/- 26189.79
Episode length: 67.80 +/- 22.70
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.28317186    |
|    mean velocity x      | -1.32         |
|    mean velocity y      | -2.07         |
|    mean velocity z      | 21.1          |
|    mean_ep_length       | 67.8          |
|    mean_reward          | -6.61e+04     |
| time/                   |               |
|    total_timesteps      | 946500        |
| train/                  |               |
|    approx_kl            | 0.00035106155 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.001         |
|    loss                 | 1.18e+08      |
|    n_updates            | 4620          |
|    policy_gradient_loss | -0.00108      |
|    std                  | 0.908         |
|    value_loss           | 2.16e+08      |
-------------------------------------------
Eval num_timesteps=947000, episode_reward=-73882.51 +/- 37595.00
Episode length: 53.80 +/- 21.14
------------------------------------
| eval/              |             |
|    mean action     | -0.26266664 |
|    mean velocity x | 2.6         |
|    mean velocity y | 2.42        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -7.39e+04   |
| time/              |             |
|    total_timesteps | 947000      |
------------------------------------
Eval num_timesteps=947500, episode_reward=-46070.93 +/- 35643.66
Episode length: 48.00 +/- 18.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.13327368 |
|    mean velocity x | 0.293      |
|    mean velocity y | -1.68      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 48         |
|    mean_reward     | -4.61e+04  |
| time/              |            |
|    total_timesteps | 947500     |
-----------------------------------
Eval num_timesteps=948000, episode_reward=-63865.09 +/- 38854.21
Episode length: 51.60 +/- 15.68
-------------------------------------
| eval/              |              |
|    mean action     | -0.065148674 |
|    mean velocity x | 0.00802      |
|    mean velocity y | -0.442       |
|    mean velocity z | 20.1         |
|    mean_ep_length  | 51.6         |
|    mean_reward     | -6.39e+04    |
| time/              |              |
|    total_timesteps | 948000       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -7.65e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 463       |
|    time_elapsed    | 7196      |
|    total_timesteps | 948224    |
----------------------------------
Eval num_timesteps=948500, episode_reward=-66388.66 +/- 33769.30
Episode length: 69.20 +/- 29.93
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.7410917     |
|    mean velocity x      | -3.54         |
|    mean velocity y      | -5.11         |
|    mean velocity z      | 15.9          |
|    mean_ep_length       | 69.2          |
|    mean_reward          | -6.64e+04     |
| time/                   |               |
|    total_timesteps      | 948500        |
| train/                  |               |
|    approx_kl            | 0.00019810747 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.235         |
|    learning_rate        | 0.001         |
|    loss                 | 8.62e+07      |
|    n_updates            | 4630          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 0.908         |
|    value_loss           | 2.08e+08      |
-------------------------------------------
Eval num_timesteps=949000, episode_reward=-68065.10 +/- 30822.99
Episode length: 57.60 +/- 19.02
------------------------------------
| eval/              |             |
|    mean action     | -0.07437912 |
|    mean velocity x | -0.392      |
|    mean velocity y | -0.00404    |
|    mean velocity z | 22          |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 949000      |
------------------------------------
Eval num_timesteps=949500, episode_reward=-63798.78 +/- 38595.68
Episode length: 51.00 +/- 16.35
------------------------------------
| eval/              |             |
|    mean action     | 0.040180773 |
|    mean velocity x | -0.0283     |
|    mean velocity y | -0.605      |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 51          |
|    mean_reward     | -6.38e+04   |
| time/              |             |
|    total_timesteps | 949500      |
------------------------------------
Eval num_timesteps=950000, episode_reward=-47587.67 +/- 33156.13
Episode length: 48.60 +/- 15.07
-----------------------------------
| eval/              |            |
|    mean action     | 0.31787238 |
|    mean velocity x | 0.608      |
|    mean velocity y | -1.72      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 48.6       |
|    mean_reward     | -4.76e+04  |
| time/              |            |
|    total_timesteps | 950000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.9      |
|    ep_rew_mean     | -7.95e+04 |
| time/              |           |
|    fps             | 131       |
|    iterations      | 464       |
|    time_elapsed    | 7203      |
|    total_timesteps | 950272    |
----------------------------------
Eval num_timesteps=950500, episode_reward=-54518.12 +/- 31830.94
Episode length: 56.60 +/- 13.46
------------------------------------------
| eval/                   |              |
|    mean action          | -0.6347728   |
|    mean velocity x      | 1.95         |
|    mean velocity y      | 2.65         |
|    mean velocity z      | 18           |
|    mean_ep_length       | 56.6         |
|    mean_reward          | -5.45e+04    |
| time/                   |              |
|    total_timesteps      | 950500       |
| train/                  |              |
|    approx_kl            | 0.0024637505 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.232        |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+08     |
|    n_updates            | 4640         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 0.908        |
|    value_loss           | 2.23e+08     |
------------------------------------------
Eval num_timesteps=951000, episode_reward=-44134.41 +/- 28022.86
Episode length: 51.20 +/- 21.03
-----------------------------------
| eval/              |            |
|    mean action     | 0.14923242 |
|    mean velocity x | -0.262     |
|    mean velocity y | -0.314     |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 51.2       |
|    mean_reward     | -4.41e+04  |
| time/              |            |
|    total_timesteps | 951000     |
-----------------------------------
Eval num_timesteps=951500, episode_reward=-52956.08 +/- 35099.27
Episode length: 55.00 +/- 6.90
------------------------------------
| eval/              |             |
|    mean action     | -0.44457862 |
|    mean velocity x | 0.708       |
|    mean velocity y | 1.55        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 55          |
|    mean_reward     | -5.3e+04    |
| time/              |             |
|    total_timesteps | 951500      |
------------------------------------
Eval num_timesteps=952000, episode_reward=-81276.09 +/- 17049.24
Episode length: 60.00 +/- 2.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.50650764 |
|    mean velocity x | -2.36      |
|    mean velocity y | -3.33      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 60         |
|    mean_reward     | -8.13e+04  |
| time/              |            |
|    total_timesteps | 952000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.3      |
|    ep_rew_mean     | -8.18e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 465       |
|    time_elapsed    | 7210      |
|    total_timesteps | 952320    |
----------------------------------
Eval num_timesteps=952500, episode_reward=-68922.58 +/- 42566.94
Episode length: 49.00 +/- 21.06
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18120277  |
|    mean velocity x      | 1.11         |
|    mean velocity y      | 0.797        |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 49           |
|    mean_reward          | -6.89e+04    |
| time/                   |              |
|    total_timesteps      | 952500       |
| train/                  |              |
|    approx_kl            | 0.0006817595 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.001        |
|    loss                 | 1.22e+08     |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.908        |
|    value_loss           | 2.29e+08     |
------------------------------------------
Eval num_timesteps=953000, episode_reward=-79269.12 +/- 16503.58
Episode length: 68.40 +/- 13.94
-----------------------------------
| eval/              |            |
|    mean action     | -0.5816868 |
|    mean velocity x | 1.5        |
|    mean velocity y | 4.79       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 68.4       |
|    mean_reward     | -7.93e+04  |
| time/              |            |
|    total_timesteps | 953000     |
-----------------------------------
Eval num_timesteps=953500, episode_reward=-75350.13 +/- 11668.49
Episode length: 69.80 +/- 8.82
-------------------------------------
| eval/              |              |
|    mean action     | -0.010267303 |
|    mean velocity x | 0.872        |
|    mean velocity y | 0.586        |
|    mean velocity z | 18.1         |
|    mean_ep_length  | 69.8         |
|    mean_reward     | -7.54e+04    |
| time/              |              |
|    total_timesteps | 953500       |
-------------------------------------
Eval num_timesteps=954000, episode_reward=-74554.77 +/- 37826.61
Episode length: 66.40 +/- 30.34
------------------------------------
| eval/              |             |
|    mean action     | -0.50293714 |
|    mean velocity x | 1.15        |
|    mean velocity y | 2.33        |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -7.46e+04   |
| time/              |             |
|    total_timesteps | 954000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.7      |
|    ep_rew_mean     | -8.26e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 466       |
|    time_elapsed    | 7217      |
|    total_timesteps | 954368    |
----------------------------------
Eval num_timesteps=954500, episode_reward=-87470.82 +/- 29924.91
Episode length: 60.40 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean action          | -0.21336612  |
|    mean velocity x      | -0.868       |
|    mean velocity y      | 0.0462       |
|    mean velocity z      | 20.9         |
|    mean_ep_length       | 60.4         |
|    mean_reward          | -8.75e+04    |
| time/                   |              |
|    total_timesteps      | 954500       |
| train/                  |              |
|    approx_kl            | 6.411283e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.233        |
|    learning_rate        | 0.001        |
|    loss                 | 1.25e+08     |
|    n_updates            | 4660         |
|    policy_gradient_loss | -9.23e-06    |
|    std                  | 0.907        |
|    value_loss           | 2.26e+08     |
------------------------------------------
Eval num_timesteps=955000, episode_reward=-67523.78 +/- 10598.70
Episode length: 61.20 +/- 5.38
------------------------------------
| eval/              |             |
|    mean action     | -0.28945792 |
|    mean velocity x | 2.52        |
|    mean velocity y | 2.1         |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -6.75e+04   |
| time/              |             |
|    total_timesteps | 955000      |
------------------------------------
Eval num_timesteps=955500, episode_reward=-81006.94 +/- 33419.71
Episode length: 63.00 +/- 12.26
-----------------------------------
| eval/              |            |
|    mean action     | 0.11678925 |
|    mean velocity x | -1.68      |
|    mean velocity y | -1.83      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 63         |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 955500     |
-----------------------------------
Eval num_timesteps=956000, episode_reward=-46435.36 +/- 35234.50
Episode length: 50.20 +/- 16.29
-----------------------------------
| eval/              |            |
|    mean action     | -0.4269196 |
|    mean velocity x | 0.433      |
|    mean velocity y | 1.39       |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 50.2       |
|    mean_reward     | -4.64e+04  |
| time/              |            |
|    total_timesteps | 956000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -8.15e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 467       |
|    time_elapsed    | 7224      |
|    total_timesteps | 956416    |
----------------------------------
Eval num_timesteps=956500, episode_reward=-75611.39 +/- 7789.17
Episode length: 65.60 +/- 4.08
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31800076  |
|    mean velocity x      | 1.74         |
|    mean velocity y      | 1.78         |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 65.6         |
|    mean_reward          | -7.56e+04    |
| time/                   |              |
|    total_timesteps      | 956500       |
| train/                  |              |
|    approx_kl            | 0.0024406717 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+08     |
|    n_updates            | 4670         |
|    policy_gradient_loss | -0.00276     |
|    std                  | 0.908        |
|    value_loss           | 2.18e+08     |
------------------------------------------
Eval num_timesteps=957000, episode_reward=-94916.78 +/- 12568.01
Episode length: 62.20 +/- 2.04
------------------------------------
| eval/              |             |
|    mean action     | -0.21219534 |
|    mean velocity x | 0.164       |
|    mean velocity y | 1.73        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 62.2        |
|    mean_reward     | -9.49e+04   |
| time/              |             |
|    total_timesteps | 957000      |
------------------------------------
Eval num_timesteps=957500, episode_reward=-53426.75 +/- 42474.62
Episode length: 46.60 +/- 16.68
------------------------------------
| eval/              |             |
|    mean action     | 0.012336792 |
|    mean velocity x | -0.9        |
|    mean velocity y | -0.847      |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 46.6        |
|    mean_reward     | -5.34e+04   |
| time/              |             |
|    total_timesteps | 957500      |
------------------------------------
Eval num_timesteps=958000, episode_reward=-73763.98 +/- 14238.20
Episode length: 64.80 +/- 6.27
------------------------------------
| eval/              |             |
|    mean action     | -0.20161906 |
|    mean velocity x | 1.04        |
|    mean velocity y | 0.665       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.38e+04   |
| time/              |             |
|    total_timesteps | 958000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 468       |
|    time_elapsed    | 7232      |
|    total_timesteps | 958464    |
----------------------------------
Eval num_timesteps=958500, episode_reward=-66233.18 +/- 33383.05
Episode length: 66.00 +/- 28.24
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44462615   |
|    mean velocity x      | 0.308         |
|    mean velocity y      | 3.15          |
|    mean velocity z      | 18.6          |
|    mean_ep_length       | 66            |
|    mean_reward          | -6.62e+04     |
| time/                   |               |
|    total_timesteps      | 958500        |
| train/                  |               |
|    approx_kl            | 0.00026101986 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.236         |
|    learning_rate        | 0.001         |
|    loss                 | 7.41e+07      |
|    n_updates            | 4680          |
|    policy_gradient_loss | -0.000611     |
|    std                  | 0.908         |
|    value_loss           | 2.36e+08      |
-------------------------------------------
Eval num_timesteps=959000, episode_reward=-81451.98 +/- 11280.87
Episode length: 63.00 +/- 5.48
-----------------------------------
| eval/              |            |
|    mean action     | 0.76204395 |
|    mean velocity x | -1.7       |
|    mean velocity y | -5.2       |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 63         |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 959000     |
-----------------------------------
Eval num_timesteps=959500, episode_reward=-56881.45 +/- 19772.63
Episode length: 69.20 +/- 14.95
----------------------------------
| eval/              |           |
|    mean action     | 0.1654563 |
|    mean velocity x | -0.117    |
|    mean velocity y | -1.59     |
|    mean velocity z | 18.1      |
|    mean_ep_length  | 69.2      |
|    mean_reward     | -5.69e+04 |
| time/              |           |
|    total_timesteps | 959500    |
----------------------------------
Eval num_timesteps=960000, episode_reward=-74802.11 +/- 22369.15
Episode length: 59.60 +/- 6.50
-----------------------------------
| eval/              |            |
|    mean action     | 0.04887942 |
|    mean velocity x | 0.267      |
|    mean velocity y | 0.945      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -7.48e+04  |
| time/              |            |
|    total_timesteps | 960000     |
-----------------------------------
Eval num_timesteps=960500, episode_reward=-66437.02 +/- 34342.94
Episode length: 57.60 +/- 24.50
-----------------------------------
| eval/              |            |
|    mean action     | -0.4117778 |
|    mean velocity x | 2.5        |
|    mean velocity y | 2.98       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -6.64e+04  |
| time/              |            |
|    total_timesteps | 960500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.8      |
|    ep_rew_mean     | -7.62e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 469       |
|    time_elapsed    | 7239      |
|    total_timesteps | 960512    |
----------------------------------
Eval num_timesteps=961000, episode_reward=-53514.62 +/- 30488.79
Episode length: 53.40 +/- 12.08
------------------------------------------
| eval/                   |              |
|    mean action          | 0.046744164  |
|    mean velocity x      | -0.817       |
|    mean velocity y      | -1.51        |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 53.4         |
|    mean_reward          | -5.35e+04    |
| time/                   |              |
|    total_timesteps      | 961000       |
| train/                  |              |
|    approx_kl            | 0.0011435711 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.001        |
|    loss                 | 7.74e+07     |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00196     |
|    std                  | 0.908        |
|    value_loss           | 1.97e+08     |
------------------------------------------
Eval num_timesteps=961500, episode_reward=-102161.61 +/- 35342.75
Episode length: 83.80 +/- 39.31
------------------------------------
| eval/              |             |
|    mean action     | -0.16610985 |
|    mean velocity x | -0.475      |
|    mean velocity y | 1.36        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 83.8        |
|    mean_reward     | -1.02e+05   |
| time/              |             |
|    total_timesteps | 961500      |
------------------------------------
Eval num_timesteps=962000, episode_reward=-90726.42 +/- 15137.82
Episode length: 72.80 +/- 15.54
-----------------------------------
| eval/              |            |
|    mean action     | 0.25483406 |
|    mean velocity x | -1.92      |
|    mean velocity y | -1.57      |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 72.8       |
|    mean_reward     | -9.07e+04  |
| time/              |            |
|    total_timesteps | 962000     |
-----------------------------------
Eval num_timesteps=962500, episode_reward=-89530.78 +/- 24507.38
Episode length: 91.80 +/- 36.64
------------------------------------
| eval/              |             |
|    mean action     | -0.59610283 |
|    mean velocity x | 1.95        |
|    mean velocity y | 4.08        |
|    mean velocity z | 20          |
|    mean_ep_length  | 91.8        |
|    mean_reward     | -8.95e+04   |
| time/              |             |
|    total_timesteps | 962500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.1      |
|    ep_rew_mean     | -7.18e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 470       |
|    time_elapsed    | 7247      |
|    total_timesteps | 962560    |
----------------------------------
Eval num_timesteps=963000, episode_reward=-76372.78 +/- 28641.90
Episode length: 71.60 +/- 21.52
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.27106896 |
|    mean velocity x      | 0.825       |
|    mean velocity y      | 2.18        |
|    mean velocity z      | 17.7        |
|    mean_ep_length       | 71.6        |
|    mean_reward          | -7.64e+04   |
| time/                   |             |
|    total_timesteps      | 963000      |
| train/                  |             |
|    approx_kl            | 8.91246e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.001       |
|    loss                 | 9.6e+07     |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.000374   |
|    std                  | 0.907       |
|    value_loss           | 1.56e+08    |
-----------------------------------------
Eval num_timesteps=963500, episode_reward=-61441.48 +/- 31186.64
Episode length: 51.40 +/- 12.40
----------------------------------
| eval/              |           |
|    mean action     | 0.3409877 |
|    mean velocity x | -1.08     |
|    mean velocity y | -1.98     |
|    mean velocity z | 20.1      |
|    mean_ep_length  | 51.4      |
|    mean_reward     | -6.14e+04 |
| time/              |           |
|    total_timesteps | 963500    |
----------------------------------
Eval num_timesteps=964000, episode_reward=-40987.91 +/- 23449.12
Episode length: 53.40 +/- 10.71
-----------------------------------
| eval/              |            |
|    mean action     | -0.3246143 |
|    mean velocity x | 2.1        |
|    mean velocity y | 1.91       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 53.4       |
|    mean_reward     | -4.1e+04   |
| time/              |            |
|    total_timesteps | 964000     |
-----------------------------------
Eval num_timesteps=964500, episode_reward=-64313.47 +/- 42256.64
Episode length: 50.80 +/- 17.68
------------------------------------
| eval/              |             |
|    mean action     | 0.048145685 |
|    mean velocity x | -1.41       |
|    mean velocity y | -1.05       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 50.8        |
|    mean_reward     | -6.43e+04   |
| time/              |             |
|    total_timesteps | 964500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.6      |
|    ep_rew_mean     | -7.07e+04 |
| time/              |           |
|    fps             | 132       |
|    iterations      | 471       |
|    time_elapsed    | 7254      |
|    total_timesteps | 964608    |
----------------------------------
Eval num_timesteps=965000, episode_reward=-58412.19 +/- 21285.62
Episode length: 56.20 +/- 7.08
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.40224594    |
|    mean velocity x      | -1.9          |
|    mean velocity y      | -3.05         |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 56.2          |
|    mean_reward          | -5.84e+04     |
| time/                   |               |
|    total_timesteps      | 965000        |
| train/                  |               |
|    approx_kl            | 0.00026590834 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.001         |
|    loss                 | 7.87e+07      |
|    n_updates            | 4710          |
|    policy_gradient_loss | -0.000762     |
|    std                  | 0.907         |
|    value_loss           | 2.06e+08      |
-------------------------------------------
Eval num_timesteps=965500, episode_reward=-76477.54 +/- 33402.44
Episode length: 61.20 +/- 9.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.37308666 |
|    mean velocity x | 0.807      |
|    mean velocity y | -0.535     |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -7.65e+04  |
| time/              |            |
|    total_timesteps | 965500     |
-----------------------------------
Eval num_timesteps=966000, episode_reward=-84639.88 +/- 10818.19
Episode length: 63.20 +/- 3.06
-----------------------------------
| eval/              |            |
|    mean action     | 0.06588834 |
|    mean velocity x | -1.39      |
|    mean velocity y | -2.27      |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -8.46e+04  |
| time/              |            |
|    total_timesteps | 966000     |
-----------------------------------
Eval num_timesteps=966500, episode_reward=-75995.35 +/- 22555.18
Episode length: 62.80 +/- 3.71
-----------------------------------
| eval/              |            |
|    mean action     | -0.6252983 |
|    mean velocity x | 2.54       |
|    mean velocity y | 4.56       |
|    mean velocity z | 16.6       |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -7.6e+04   |
| time/              |            |
|    total_timesteps | 966500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.1      |
|    ep_rew_mean     | -7.23e+04 |
| time/              |           |
|    fps             | 133       |
|    iterations      | 472       |
|    time_elapsed    | 7261      |
|    total_timesteps | 966656    |
----------------------------------
Eval num_timesteps=967000, episode_reward=-56888.16 +/- 36832.08
Episode length: 52.80 +/- 16.68
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.08543187   |
|    mean velocity x      | -0.112        |
|    mean velocity y      | 1.56          |
|    mean velocity z      | 21.1          |
|    mean_ep_length       | 52.8          |
|    mean_reward          | -5.69e+04     |
| time/                   |               |
|    total_timesteps      | 967000        |
| train/                  |               |
|    approx_kl            | 2.0779058e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 7.76e+07      |
|    n_updates            | 4720          |
|    policy_gradient_loss | -0.000276     |
|    std                  | 0.907         |
|    value_loss           | 2e+08         |
-------------------------------------------
Eval num_timesteps=967500, episode_reward=-78160.62 +/- 30024.24
Episode length: 56.60 +/- 7.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.37950522 |
|    mean velocity x | -1.51      |
|    mean velocity y | -2.49      |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -7.82e+04  |
| time/              |            |
|    total_timesteps | 967500     |
-----------------------------------
Eval num_timesteps=968000, episode_reward=-95440.26 +/- 17149.34
Episode length: 61.60 +/- 1.85
------------------------------------
| eval/              |             |
|    mean action     | -0.32055393 |
|    mean velocity x | 0.856       |
|    mean velocity y | 2.29        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -9.54e+04   |
| time/              |             |
|    total_timesteps | 968000      |
------------------------------------
Eval num_timesteps=968500, episode_reward=-65797.66 +/- 45465.39
Episode length: 50.20 +/- 18.79
-------------------------------------
| eval/              |              |
|    mean action     | -0.036043428 |
|    mean velocity x | 1.06         |
|    mean velocity y | 1.45         |
|    mean velocity z | 17.1         |
|    mean_ep_length  | 50.2         |
|    mean_reward     | -6.58e+04    |
| time/              |              |
|    total_timesteps | 968500       |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.7      |
|    ep_rew_mean     | -7.48e+04 |
| time/              |           |
|    fps             | 133       |
|    iterations      | 473       |
|    time_elapsed    | 7269      |
|    total_timesteps | 968704    |
----------------------------------
Eval num_timesteps=969000, episode_reward=-97625.83 +/- 14863.37
Episode length: 82.40 +/- 24.83
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5026006   |
|    mean velocity x      | 1.18         |
|    mean velocity y      | 3.21         |
|    mean velocity z      | 22.5         |
|    mean_ep_length       | 82.4         |
|    mean_reward          | -9.76e+04    |
| time/                   |              |
|    total_timesteps      | 969000       |
| train/                  |              |
|    approx_kl            | 0.0002217147 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.001        |
|    loss                 | 9.32e+07     |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.000575    |
|    std                  | 0.907        |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=969500, episode_reward=-55289.28 +/- 34306.22
Episode length: 53.00 +/- 14.68
------------------------------------
| eval/              |             |
|    mean action     | -0.31194946 |
|    mean velocity x | 2.5         |
|    mean velocity y | 2.25        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 53          |
|    mean_reward     | -5.53e+04   |
| time/              |             |
|    total_timesteps | 969500      |
------------------------------------
Eval num_timesteps=970000, episode_reward=-45115.03 +/- 41759.13
Episode length: 47.00 +/- 19.48
-------------------------------------
| eval/              |              |
|    mean action     | -0.026910705 |
|    mean velocity x | -1.71        |
|    mean velocity y | -0.676       |
|    mean velocity z | 20.4         |
|    mean_ep_length  | 47           |
|    mean_reward     | -4.51e+04    |
| time/              |              |
|    total_timesteps | 970000       |
-------------------------------------
Eval num_timesteps=970500, episode_reward=-56813.16 +/- 23651.74
Episode length: 59.20 +/- 8.68
------------------------------------
| eval/              |             |
|    mean action     | -0.79336387 |
|    mean velocity x | 3.46        |
|    mean velocity y | 5.41        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -5.68e+04   |
| time/              |             |
|    total_timesteps | 970500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.82e+04 |
| time/              |           |
|    fps             | 133       |
|    iterations      | 474       |
|    time_elapsed    | 7276      |
|    total_timesteps | 970752    |
----------------------------------
Eval num_timesteps=971000, episode_reward=-76441.43 +/- 38792.08
Episode length: 52.80 +/- 21.48
------------------------------------------
| eval/                   |              |
|    mean action          | -0.346757    |
|    mean velocity x      | 1.06         |
|    mean velocity y      | 1.88         |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 52.8         |
|    mean_reward          | -7.64e+04    |
| time/                   |              |
|    total_timesteps      | 971000       |
| train/                  |              |
|    approx_kl            | 0.0030823096 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+08     |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 0.908        |
|    value_loss           | 2.17e+08     |
------------------------------------------
Eval num_timesteps=971500, episode_reward=-68214.37 +/- 37634.47
Episode length: 66.80 +/- 28.72
------------------------------------
| eval/              |             |
|    mean action     | -0.09081224 |
|    mean velocity x | -0.562      |
|    mean velocity y | 0.371       |
|    mean velocity z | 16.6        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -6.82e+04   |
| time/              |             |
|    total_timesteps | 971500      |
------------------------------------
Eval num_timesteps=972000, episode_reward=-69452.00 +/- 39094.24
Episode length: 60.00 +/- 27.62
------------------------------------
| eval/              |             |
|    mean action     | -0.67867017 |
|    mean velocity x | 0.0155      |
|    mean velocity y | 2.93        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 60          |
|    mean_reward     | -6.95e+04   |
| time/              |             |
|    total_timesteps | 972000      |
------------------------------------
Eval num_timesteps=972500, episode_reward=-40898.17 +/- 28297.64
Episode length: 57.40 +/- 24.23
-----------------------------------
| eval/              |            |
|    mean action     | -0.4511342 |
|    mean velocity x | 0.942      |
|    mean velocity y | 2.1        |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 57.4       |
|    mean_reward     | -4.09e+04  |
| time/              |            |
|    total_timesteps | 972500     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.6      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 133       |
|    iterations      | 475       |
|    time_elapsed    | 7283      |
|    total_timesteps | 972800    |
----------------------------------
Eval num_timesteps=973000, episode_reward=-55668.93 +/- 35607.97
Episode length: 48.80 +/- 17.27
------------------------------------------
| eval/                   |              |
|    mean action          | 0.29640734   |
|    mean velocity x      | 0.637        |
|    mean velocity y      | -1.04        |
|    mean velocity z      | 21.2         |
|    mean_ep_length       | 48.8         |
|    mean_reward          | -5.57e+04    |
| time/                   |              |
|    total_timesteps      | 973000       |
| train/                  |              |
|    approx_kl            | 0.0009134597 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.001        |
|    loss                 | 7.77e+07     |
|    n_updates            | 4750         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 0.908        |
|    value_loss           | 1.9e+08      |
------------------------------------------
Eval num_timesteps=973500, episode_reward=-89910.77 +/- 15032.58
Episode length: 75.60 +/- 16.57
-------------------------------------
| eval/              |              |
|    mean action     | -0.035859633 |
|    mean velocity x | 1.69         |
|    mean velocity y | 0.418        |
|    mean velocity z | 18.1         |
|    mean_ep_length  | 75.6         |
|    mean_reward     | -8.99e+04    |
| time/              |              |
|    total_timesteps | 973500       |
-------------------------------------
Eval num_timesteps=974000, episode_reward=-88549.12 +/- 15275.66
Episode length: 81.60 +/- 16.93
-------------------------------------
| eval/              |              |
|    mean action     | -0.009506622 |
|    mean velocity x | -0.8         |
|    mean velocity y | -0.811       |
|    mean velocity z | 19.2         |
|    mean_ep_length  | 81.6         |
|    mean_reward     | -8.85e+04    |
| time/              |              |
|    total_timesteps | 974000       |
-------------------------------------
Eval num_timesteps=974500, episode_reward=-68473.13 +/- 19933.89
Episode length: 57.20 +/- 7.41
------------------------------------
| eval/              |             |
|    mean action     | 0.041013367 |
|    mean velocity x | -0.378      |
|    mean velocity y | -0.332      |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 974500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.5      |
|    ep_rew_mean     | -7.84e+04 |
| time/              |           |
|    fps             | 133       |
|    iterations      | 476       |
|    time_elapsed    | 7290      |
|    total_timesteps | 974848    |
----------------------------------
Eval num_timesteps=975000, episode_reward=-52856.60 +/- 24662.67
Episode length: 61.00 +/- 16.71
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.6103834  |
|    mean velocity x      | 1.56        |
|    mean velocity y      | 3.08        |
|    mean velocity z      | 20.9        |
|    mean_ep_length       | 61          |
|    mean_reward          | -5.29e+04   |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 6.33373e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+08    |
|    n_updates            | 4760        |
|    policy_gradient_loss | -0.000217   |
|    std                  | 0.908       |
|    value_loss           | 2.25e+08    |
-----------------------------------------
Eval num_timesteps=975500, episode_reward=-53863.11 +/- 32658.31
Episode length: 67.60 +/- 30.00
------------------------------------
| eval/              |             |
|    mean action     | -0.41977364 |
|    mean velocity x | 1.21        |
|    mean velocity y | 2.34        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 67.6        |
|    mean_reward     | -5.39e+04   |
| time/              |             |
|    total_timesteps | 975500      |
------------------------------------
Eval num_timesteps=976000, episode_reward=-62829.31 +/- 33803.16
Episode length: 49.80 +/- 15.71
------------------------------------
| eval/              |             |
|    mean action     | -0.59072065 |
|    mean velocity x | 2.32        |
|    mean velocity y | 4.12        |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 49.8        |
|    mean_reward     | -6.28e+04   |
| time/              |             |
|    total_timesteps | 976000      |
------------------------------------
Eval num_timesteps=976500, episode_reward=-75737.62 +/- 20391.02
Episode length: 67.40 +/- 4.22
------------------------------------
| eval/              |             |
|    mean action     | -0.39079607 |
|    mean velocity x | 1.9         |
|    mean velocity y | 2.39        |
|    mean velocity z | 21.8        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -7.57e+04   |
| time/              |             |
|    total_timesteps | 976500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.2      |
|    ep_rew_mean     | -7.92e+04 |
| time/              |           |
|    fps             | 133       |
|    iterations      | 477       |
|    time_elapsed    | 7298      |
|    total_timesteps | 976896    |
----------------------------------
Eval num_timesteps=977000, episode_reward=-94480.95 +/- 16240.28
Episode length: 70.80 +/- 7.93
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.5332329    |
|    mean velocity x      | 0.0901        |
|    mean velocity y      | 2.82          |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 70.8          |
|    mean_reward          | -9.45e+04     |
| time/                   |               |
|    total_timesteps      | 977000        |
| train/                  |               |
|    approx_kl            | 0.00022918239 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.249         |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+08      |
|    n_updates            | 4770          |
|    policy_gradient_loss | -0.00063      |
|    std                  | 0.908         |
|    value_loss           | 2.38e+08      |
-------------------------------------------
Eval num_timesteps=977500, episode_reward=-39801.59 +/- 29797.59
Episode length: 47.80 +/- 19.53
-------------------------------------
| eval/              |              |
|    mean action     | -0.091434844 |
|    mean velocity x | 0.989        |
|    mean velocity y | 1.13         |
|    mean velocity z | 19.1         |
|    mean_ep_length  | 47.8         |
|    mean_reward     | -3.98e+04    |
| time/              |              |
|    total_timesteps | 977500       |
-------------------------------------
Eval num_timesteps=978000, episode_reward=-53472.00 +/- 25196.56
Episode length: 56.00 +/- 11.78
----------------------------------
| eval/              |           |
|    mean action     | 0.3837354 |
|    mean velocity x | 0.578     |
|    mean velocity y | -1.69     |
|    mean velocity z | 18        |
|    mean_ep_length  | 56        |
|    mean_reward     | -5.35e+04 |
| time/              |           |
|    total_timesteps | 978000    |
----------------------------------
Eval num_timesteps=978500, episode_reward=-67967.56 +/- 29878.00
Episode length: 70.20 +/- 17.23
------------------------------------
| eval/              |             |
|    mean action     | -0.15137552 |
|    mean velocity x | 1.48        |
|    mean velocity y | 1.2         |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 70.2        |
|    mean_reward     | -6.8e+04    |
| time/              |             |
|    total_timesteps | 978500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.8      |
|    ep_rew_mean     | -7.98e+04 |
| time/              |           |
|    fps             | 134       |
|    iterations      | 478       |
|    time_elapsed    | 7305      |
|    total_timesteps | 978944    |
----------------------------------
Eval num_timesteps=979000, episode_reward=-59149.43 +/- 40558.58
Episode length: 54.00 +/- 18.35
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.44445622    |
|    mean velocity x      | -0.926        |
|    mean velocity y      | -2.43         |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 54            |
|    mean_reward          | -5.91e+04     |
| time/                   |               |
|    total_timesteps      | 979000        |
| train/                  |               |
|    approx_kl            | 0.00031722648 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.223         |
|    learning_rate        | 0.001         |
|    loss                 | 7.75e+07      |
|    n_updates            | 4780          |
|    policy_gradient_loss | -0.000954     |
|    std                  | 0.908         |
|    value_loss           | 2.33e+08      |
-------------------------------------------
Eval num_timesteps=979500, episode_reward=-77921.70 +/- 35993.65
Episode length: 58.20 +/- 9.13
----------------------------------
| eval/              |           |
|    mean action     | 0.5902218 |
|    mean velocity x | -2.12     |
|    mean velocity y | -4.37     |
|    mean velocity z | 20        |
|    mean_ep_length  | 58.2      |
|    mean_reward     | -7.79e+04 |
| time/              |           |
|    total_timesteps | 979500    |
----------------------------------
Eval num_timesteps=980000, episode_reward=-71075.68 +/- 42840.05
Episode length: 51.20 +/- 14.96
------------------------------------
| eval/              |             |
|    mean action     | -0.22800894 |
|    mean velocity x | -0.747      |
|    mean velocity y | 0.736       |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 51.2        |
|    mean_reward     | -7.11e+04   |
| time/              |             |
|    total_timesteps | 980000      |
------------------------------------
Eval num_timesteps=980500, episode_reward=-83136.83 +/- 18628.27
Episode length: 61.60 +/- 3.38
------------------------------------
| eval/              |             |
|    mean action     | -0.48064998 |
|    mean velocity x | 1.99        |
|    mean velocity y | 1.94        |
|    mean velocity z | 16.4        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -8.31e+04   |
| time/              |             |
|    total_timesteps | 980500      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -8.51e+04 |
| time/              |           |
|    fps             | 134       |
|    iterations      | 479       |
|    time_elapsed    | 7312      |
|    total_timesteps | 980992    |
----------------------------------
Eval num_timesteps=981000, episode_reward=-42822.54 +/- 38657.37
Episode length: 36.60 +/- 24.18
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.07983227   |
|    mean velocity x      | -0.72         |
|    mean velocity y      | -0.0344       |
|    mean velocity z      | 19.5          |
|    mean_ep_length       | 36.6          |
|    mean_reward          | -4.28e+04     |
| time/                   |               |
|    total_timesteps      | 981000        |
| train/                  |               |
|    approx_kl            | 0.00069435267 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.001         |
|    loss                 | 9.5e+07       |
|    n_updates            | 4790          |
|    policy_gradient_loss | -0.000763     |
|    std                  | 0.908         |
|    value_loss           | 2.25e+08      |
-------------------------------------------
Eval num_timesteps=981500, episode_reward=-64495.77 +/- 40050.72
Episode length: 65.60 +/- 46.58
-----------------------------------
| eval/              |            |
|    mean action     | 0.07314049 |
|    mean velocity x | 0.303      |
|    mean velocity y | -0.447     |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 65.6       |
|    mean_reward     | -6.45e+04  |
| time/              |            |
|    total_timesteps | 981500     |
-----------------------------------
Eval num_timesteps=982000, episode_reward=-58721.46 +/- 22507.88
Episode length: 56.20 +/- 6.62
------------------------------------
| eval/              |             |
|    mean action     | -0.44348803 |
|    mean velocity x | 0.806       |
|    mean velocity y | 2.4         |
|    mean velocity z | 16.6        |
|    mean_ep_length  | 56.2        |
|    mean_reward     | -5.87e+04   |
| time/              |             |
|    total_timesteps | 982000      |
------------------------------------
Eval num_timesteps=982500, episode_reward=-61171.91 +/- 42706.09
Episode length: 57.00 +/- 24.92
-----------------------------------
| eval/              |            |
|    mean action     | 0.30655116 |
|    mean velocity x | -0.879     |
|    mean velocity y | -2.5       |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 57         |
|    mean_reward     | -6.12e+04  |
| time/              |            |
|    total_timesteps | 982500     |
-----------------------------------
Eval num_timesteps=983000, episode_reward=-73451.07 +/- 20381.24
Episode length: 62.40 +/- 8.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.12875676 |
|    mean velocity x | -1.95      |
|    mean velocity y | -1.49      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -7.35e+04  |
| time/              |            |
|    total_timesteps | 983000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -8.2e+04 |
| time/              |          |
|    fps             | 134      |
|    iterations      | 480      |
|    time_elapsed    | 7320     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983500, episode_reward=-74802.93 +/- 55333.28
Episode length: 84.40 +/- 62.74
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.040702775  |
|    mean velocity x      | 0.475         |
|    mean velocity y      | 0.849         |
|    mean velocity z      | 21.5          |
|    mean_ep_length       | 84.4          |
|    mean_reward          | -7.48e+04     |
| time/                   |               |
|    total_timesteps      | 983500        |
| train/                  |               |
|    approx_kl            | 1.1190132e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.229         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+08      |
|    n_updates            | 4800          |
|    policy_gradient_loss | -0.000197     |
|    std                  | 0.908         |
|    value_loss           | 2.15e+08      |
-------------------------------------------
Eval num_timesteps=984000, episode_reward=-77009.26 +/- 10042.56
Episode length: 74.20 +/- 18.31
-----------------------------------
| eval/              |            |
|    mean action     | -0.9348767 |
|    mean velocity x | 2.89       |
|    mean velocity y | 5.69       |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 74.2       |
|    mean_reward     | -7.7e+04   |
| time/              |            |
|    total_timesteps | 984000     |
-----------------------------------
Eval num_timesteps=984500, episode_reward=-79365.92 +/- 19909.27
Episode length: 61.60 +/- 5.31
-----------------------------------
| eval/              |            |
|    mean action     | -0.5824949 |
|    mean velocity x | 1.79       |
|    mean velocity y | 4          |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -7.94e+04  |
| time/              |            |
|    total_timesteps | 984500     |
-----------------------------------
Eval num_timesteps=985000, episode_reward=-47825.20 +/- 29721.30
Episode length: 48.20 +/- 15.74
------------------------------------
| eval/              |             |
|    mean action     | -0.07035344 |
|    mean velocity x | 0.951       |
|    mean velocity y | -0.0306     |
|    mean velocity z | 21.8        |
|    mean_ep_length  | 48.2        |
|    mean_reward     | -4.78e+04   |
| time/              |             |
|    total_timesteps | 985000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.4      |
|    ep_rew_mean     | -8.52e+04 |
| time/              |           |
|    fps             | 134       |
|    iterations      | 481       |
|    time_elapsed    | 7327      |
|    total_timesteps | 985088    |
----------------------------------
Eval num_timesteps=985500, episode_reward=-56009.82 +/- 46014.32
Episode length: 43.80 +/- 24.60
------------------------------------------
| eval/                   |              |
|    mean action          | 0.1161254    |
|    mean velocity x      | 0.922        |
|    mean velocity y      | 0.196        |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 43.8         |
|    mean_reward          | -5.6e+04     |
| time/                   |              |
|    total_timesteps      | 985500       |
| train/                  |              |
|    approx_kl            | 0.0001270909 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.23         |
|    learning_rate        | 0.001        |
|    loss                 | 7.56e+07     |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.000393    |
|    std                  | 0.908        |
|    value_loss           | 2.07e+08     |
------------------------------------------
Eval num_timesteps=986000, episode_reward=-80169.66 +/- 37961.46
Episode length: 88.60 +/- 55.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.11651112 |
|    mean velocity x | 0.962      |
|    mean velocity y | -0.195     |
|    mean velocity z | 19         |
|    mean_ep_length  | 88.6       |
|    mean_reward     | -8.02e+04  |
| time/              |            |
|    total_timesteps | 986000     |
-----------------------------------
Eval num_timesteps=986500, episode_reward=-51337.28 +/- 38228.65
Episode length: 58.00 +/- 25.70
-----------------------------------
| eval/              |            |
|    mean action     | 0.41169024 |
|    mean velocity x | -1.76      |
|    mean velocity y | -3.25      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 58         |
|    mean_reward     | -5.13e+04  |
| time/              |            |
|    total_timesteps | 986500     |
-----------------------------------
Eval num_timesteps=987000, episode_reward=-63359.75 +/- 26148.18
Episode length: 61.20 +/- 9.70
------------------------------------
| eval/              |             |
|    mean action     | -0.12057964 |
|    mean velocity x | 2.89        |
|    mean velocity y | 2.08        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -6.34e+04   |
| time/              |             |
|    total_timesteps | 987000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.1      |
|    ep_rew_mean     | -8.13e+04 |
| time/              |           |
|    fps             | 134       |
|    iterations      | 482       |
|    time_elapsed    | 7334      |
|    total_timesteps | 987136    |
----------------------------------
Eval num_timesteps=987500, episode_reward=-63773.85 +/- 27081.11
Episode length: 60.80 +/- 15.04
------------------------------------------
| eval/                   |              |
|    mean action          | -0.8753996   |
|    mean velocity x      | 3.3          |
|    mean velocity y      | 6.07         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 60.8         |
|    mean_reward          | -6.38e+04    |
| time/                   |              |
|    total_timesteps      | 987500       |
| train/                  |              |
|    approx_kl            | 5.506474e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+08     |
|    n_updates            | 4820         |
|    policy_gradient_loss | -0.000251    |
|    std                  | 0.908        |
|    value_loss           | 1.75e+08     |
------------------------------------------
Eval num_timesteps=988000, episode_reward=-97294.56 +/- 21247.61
Episode length: 65.40 +/- 7.28
-----------------------------------
| eval/              |            |
|    mean action     | 0.43986845 |
|    mean velocity x | -1.7       |
|    mean velocity y | -2.37      |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 65.4       |
|    mean_reward     | -9.73e+04  |
| time/              |            |
|    total_timesteps | 988000     |
-----------------------------------
Eval num_timesteps=988500, episode_reward=-68843.27 +/- 17181.91
Episode length: 69.20 +/- 12.02
-----------------------------------
| eval/              |            |
|    mean action     | 0.05677791 |
|    mean velocity x | 1.63       |
|    mean velocity y | 0.343      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 69.2       |
|    mean_reward     | -6.88e+04  |
| time/              |            |
|    total_timesteps | 988500     |
-----------------------------------
Eval num_timesteps=989000, episode_reward=-47596.26 +/- 37351.91
Episode length: 46.60 +/- 22.89
------------------------------------
| eval/              |             |
|    mean action     | -0.22016473 |
|    mean velocity x | 0.984       |
|    mean velocity y | 0.534       |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 46.6        |
|    mean_reward     | -4.76e+04   |
| time/              |             |
|    total_timesteps | 989000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.3      |
|    ep_rew_mean     | -7.99e+04 |
| time/              |           |
|    fps             | 134       |
|    iterations      | 483       |
|    time_elapsed    | 7341      |
|    total_timesteps | 989184    |
----------------------------------
Eval num_timesteps=989500, episode_reward=-57413.78 +/- 46195.89
Episode length: 51.00 +/- 18.60
------------------------------------------
| eval/                   |              |
|    mean action          | -0.30206957  |
|    mean velocity x      | 1.15         |
|    mean velocity y      | 2.05         |
|    mean velocity z      | 20           |
|    mean_ep_length       | 51           |
|    mean_reward          | -5.74e+04    |
| time/                   |              |
|    total_timesteps      | 989500       |
| train/                  |              |
|    approx_kl            | 9.091271e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+08     |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.000429    |
|    std                  | 0.908        |
|    value_loss           | 1.86e+08     |
------------------------------------------
Eval num_timesteps=990000, episode_reward=-65036.28 +/- 17799.92
Episode length: 61.20 +/- 8.47
-----------------------------------
| eval/              |            |
|    mean action     | 0.20096807 |
|    mean velocity x | -1.34      |
|    mean velocity y | -1.7       |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -6.5e+04   |
| time/              |            |
|    total_timesteps | 990000     |
-----------------------------------
Eval num_timesteps=990500, episode_reward=-42136.09 +/- 36491.97
Episode length: 45.60 +/- 17.92
-----------------------------------
| eval/              |            |
|    mean action     | -0.4540694 |
|    mean velocity x | 1.89       |
|    mean velocity y | 2.52       |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 45.6       |
|    mean_reward     | -4.21e+04  |
| time/              |            |
|    total_timesteps | 990500     |
-----------------------------------
Eval num_timesteps=991000, episode_reward=-81039.79 +/- 37657.75
Episode length: 89.80 +/- 34.01
---------------------------------
| eval/              |          |
|    mean action     | 0.732781 |
|    mean velocity x | -1.66    |
|    mean velocity y | -2.49    |
|    mean velocity z | 17.9     |
|    mean_ep_length  | 89.8     |
|    mean_reward     | -8.1e+04 |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.6      |
|    ep_rew_mean     | -8.07e+04 |
| time/              |           |
|    fps             | 134       |
|    iterations      | 484       |
|    time_elapsed    | 7348      |
|    total_timesteps | 991232    |
----------------------------------
Eval num_timesteps=991500, episode_reward=-61663.89 +/- 19660.11
Episode length: 62.20 +/- 17.06
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5108097   |
|    mean velocity x      | 1.86         |
|    mean velocity y      | 4.39         |
|    mean velocity z      | 18           |
|    mean_ep_length       | 62.2         |
|    mean_reward          | -6.17e+04    |
| time/                   |              |
|    total_timesteps      | 991500       |
| train/                  |              |
|    approx_kl            | 8.263922e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.001        |
|    loss                 | 8.65e+07     |
|    n_updates            | 4840         |
|    policy_gradient_loss | -0.000494    |
|    std                  | 0.908        |
|    value_loss           | 1.98e+08     |
------------------------------------------
Eval num_timesteps=992000, episode_reward=-64890.29 +/- 32781.30
Episode length: 68.20 +/- 34.28
-----------------------------------
| eval/              |            |
|    mean action     | -0.7167898 |
|    mean velocity x | 2.15       |
|    mean velocity y | 4.07       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -6.49e+04  |
| time/              |            |
|    total_timesteps | 992000     |
-----------------------------------
Eval num_timesteps=992500, episode_reward=-73045.16 +/- 29558.92
Episode length: 59.40 +/- 9.79
-------------------------------------
| eval/              |              |
|    mean action     | -0.019269416 |
|    mean velocity x | 2.62         |
|    mean velocity y | 1.95         |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 59.4         |
|    mean_reward     | -7.3e+04     |
| time/              |              |
|    total_timesteps | 992500       |
-------------------------------------
Eval num_timesteps=993000, episode_reward=-49504.74 +/- 41830.65
Episode length: 41.20 +/- 23.46
-----------------------------------
| eval/              |            |
|    mean action     | 0.13613205 |
|    mean velocity x | -0.911     |
|    mean velocity y | -0.897     |
|    mean velocity z | 16.7       |
|    mean_ep_length  | 41.2       |
|    mean_reward     | -4.95e+04  |
| time/              |            |
|    total_timesteps | 993000     |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.3     |
|    ep_rew_mean     | -7.8e+04 |
| time/              |          |
|    fps             | 135      |
|    iterations      | 485      |
|    time_elapsed    | 7355     |
|    total_timesteps | 993280   |
---------------------------------
Eval num_timesteps=993500, episode_reward=-55634.49 +/- 27524.27
Episode length: 65.20 +/- 24.19
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19396693   |
|    mean velocity x      | 0.621         |
|    mean velocity y      | 0.924         |
|    mean velocity z      | 19.3          |
|    mean_ep_length       | 65.2          |
|    mean_reward          | -5.56e+04     |
| time/                   |               |
|    total_timesteps      | 993500        |
| train/                  |               |
|    approx_kl            | 0.00011141971 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.97         |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 6.89e+07      |
|    n_updates            | 4850          |
|    policy_gradient_loss | -0.000583     |
|    std                  | 0.908         |
|    value_loss           | 1.73e+08      |
-------------------------------------------
Eval num_timesteps=994000, episode_reward=-55790.98 +/- 38204.27
Episode length: 52.40 +/- 22.90
------------------------------------
| eval/              |             |
|    mean action     | -0.25199816 |
|    mean velocity x | 0.642       |
|    mean velocity y | 1.34        |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 52.4        |
|    mean_reward     | -5.58e+04   |
| time/              |             |
|    total_timesteps | 994000      |
------------------------------------
Eval num_timesteps=994500, episode_reward=-69497.56 +/- 40500.92
Episode length: 56.00 +/- 26.93
------------------------------------
| eval/              |             |
|    mean action     | -0.21674669 |
|    mean velocity x | 1.18        |
|    mean velocity y | 1.8         |
|    mean velocity z | 22.2        |
|    mean_ep_length  | 56          |
|    mean_reward     | -6.95e+04   |
| time/              |             |
|    total_timesteps | 994500      |
------------------------------------
Eval num_timesteps=995000, episode_reward=-44289.82 +/- 34298.72
Episode length: 50.00 +/- 19.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.08849153 |
|    mean velocity x | -0.981     |
|    mean velocity y | -0.87      |
|    mean velocity z | 16.4       |
|    mean_ep_length  | 50         |
|    mean_reward     | -4.43e+04  |
| time/              |            |
|    total_timesteps | 995000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.5      |
|    ep_rew_mean     | -7.54e+04 |
| time/              |           |
|    fps             | 135       |
|    iterations      | 486       |
|    time_elapsed    | 7363      |
|    total_timesteps | 995328    |
----------------------------------
Eval num_timesteps=995500, episode_reward=-82984.94 +/- 12224.90
Episode length: 60.40 +/- 1.74
------------------------------------------
| eval/                   |              |
|    mean action          | 0.29099062   |
|    mean velocity x      | -1.41        |
|    mean velocity y      | -1.98        |
|    mean velocity z      | 16.2         |
|    mean_ep_length       | 60.4         |
|    mean_reward          | -8.3e+04     |
| time/                   |              |
|    total_timesteps      | 995500       |
| train/                  |              |
|    approx_kl            | 9.898309e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.001        |
|    loss                 | 1.41e+08     |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.000339    |
|    std                  | 0.908        |
|    value_loss           | 2.11e+08     |
------------------------------------------
Eval num_timesteps=996000, episode_reward=-77160.10 +/- 19470.06
Episode length: 67.00 +/- 15.07
------------------------------------
| eval/              |             |
|    mean action     | 0.061314538 |
|    mean velocity x | -0.756      |
|    mean velocity y | -0.0676     |
|    mean velocity z | 19          |
|    mean_ep_length  | 67          |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 996000      |
------------------------------------
Eval num_timesteps=996500, episode_reward=-54757.58 +/- 28440.57
Episode length: 56.20 +/- 24.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.24230465 |
|    mean velocity x | -0.489     |
|    mean velocity y | -1.75      |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -5.48e+04  |
| time/              |            |
|    total_timesteps | 996500     |
-----------------------------------
Eval num_timesteps=997000, episode_reward=-44250.81 +/- 36255.99
Episode length: 52.60 +/- 20.64
-----------------------------------
| eval/              |            |
|    mean action     | 0.18352722 |
|    mean velocity x | 0.597      |
|    mean velocity y | -0.758     |
|    mean velocity z | 18         |
|    mean_ep_length  | 52.6       |
|    mean_reward     | -4.43e+04  |
| time/              |            |
|    total_timesteps | 997000     |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.8      |
|    ep_rew_mean     | -7.39e+04 |
| time/              |           |
|    fps             | 135       |
|    iterations      | 487       |
|    time_elapsed    | 7370      |
|    total_timesteps | 997376    |
----------------------------------
Eval num_timesteps=997500, episode_reward=-96170.71 +/- 17470.77
Episode length: 64.00 +/- 3.58
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27535152  |
|    mean velocity x      | 0.401        |
|    mean velocity y      | 1.57         |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 64           |
|    mean_reward          | -9.62e+04    |
| time/                   |              |
|    total_timesteps      | 997500       |
| train/                  |              |
|    approx_kl            | 2.844425e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.001        |
|    loss                 | 6.19e+07     |
|    n_updates            | 4870         |
|    policy_gradient_loss | -0.000252    |
|    std                  | 0.908        |
|    value_loss           | 1.93e+08     |
------------------------------------------
Eval num_timesteps=998000, episode_reward=-35431.77 +/- 24753.24
Episode length: 48.40 +/- 14.19
------------------------------------
| eval/              |             |
|    mean action     | -0.25679713 |
|    mean velocity x | -0.893      |
|    mean velocity y | 0.234       |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 48.4        |
|    mean_reward     | -3.54e+04   |
| time/              |             |
|    total_timesteps | 998000      |
------------------------------------
Eval num_timesteps=998500, episode_reward=-92587.31 +/- 12894.83
Episode length: 69.20 +/- 12.92
-----------------------------------
| eval/              |            |
|    mean action     | 0.05322299 |
|    mean velocity x | -0.794     |
|    mean velocity y | 0.99       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 69.2       |
|    mean_reward     | -9.26e+04  |
| time/              |            |
|    total_timesteps | 998500     |
-----------------------------------
Eval num_timesteps=999000, episode_reward=-58980.89 +/- 37644.75
Episode length: 52.80 +/- 17.76
------------------------------------
| eval/              |             |
|    mean action     | 0.062437072 |
|    mean velocity x | -2.22       |
|    mean velocity y | -1.89       |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 52.8        |
|    mean_reward     | -5.9e+04    |
| time/              |             |
|    total_timesteps | 999000      |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.5      |
|    ep_rew_mean     | -7.55e+04 |
| time/              |           |
|    fps             | 135       |
|    iterations      | 488       |
|    time_elapsed    | 7377      |
|    total_timesteps | 999424    |
----------------------------------
Eval num_timesteps=999500, episode_reward=-85593.46 +/- 11829.92
Episode length: 61.00 +/- 0.63
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14787659  |
|    mean velocity x      | 1.89         |
|    mean velocity y      | 1.86         |
|    mean velocity z      | 18           |
|    mean_ep_length       | 61           |
|    mean_reward          | -8.56e+04    |
| time/                   |              |
|    total_timesteps      | 999500       |
| train/                  |              |
|    approx_kl            | 0.0006638544 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.231        |
|    learning_rate        | 0.001        |
|    loss                 | 1.65e+08     |
|    n_updates            | 4880         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.907        |
|    value_loss           | 2.31e+08     |
------------------------------------------
Eval num_timesteps=1000000, episode_reward=-94762.87 +/- 18154.09
Episode length: 65.40 +/- 6.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.10287094 |
|    mean velocity x | 1.11       |
|    mean velocity y | 0.59       |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 65.4       |
|    mean_reward     | -9.48e+04  |
| time/              |            |
|    total_timesteps | 1000000    |
-----------------------------------
Eval num_timesteps=1000500, episode_reward=-78875.64 +/- 31034.53
Episode length: 57.40 +/- 9.05
-----------------------------------
| eval/              |            |
|    mean action     | 0.12820734 |
|    mean velocity x | -1.02      |
|    mean velocity y | -1.81      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 57.4       |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 1000500    |
-----------------------------------
Eval num_timesteps=1001000, episode_reward=-49274.67 +/- 30089.40
Episode length: 49.40 +/- 14.75
------------------------------------
| eval/              |             |
|    mean action     | -0.04030082 |
|    mean velocity x | 0.338       |
|    mean velocity y | 0.48        |
|    mean velocity z | 21.3        |
|    mean_ep_length  | 49.4        |
|    mean_reward     | -4.93e+04   |
| time/              |             |
|    total_timesteps | 1001000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.8      |
|    ep_rew_mean     | -7.58e+04 |
| time/              |           |
|    fps             | 135       |
|    iterations      | 489       |
|    time_elapsed    | 7384      |
|    total_timesteps | 1001472   |
----------------------------------
Eval num_timesteps=1001500, episode_reward=-70205.51 +/- 27138.33
Episode length: 60.60 +/- 10.84
------------------------------------------
| eval/                   |              |
|    mean action          | 0.13937643   |
|    mean velocity x      | 1.18         |
|    mean velocity y      | -0.96        |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 60.6         |
|    mean_reward          | -7.02e+04    |
| time/                   |              |
|    total_timesteps      | 1001500      |
| train/                  |              |
|    approx_kl            | 9.611415e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000319    |
|    std                  | 0.907        |
|    value_loss           | 2.29e+08     |
------------------------------------------
Eval num_timesteps=1002000, episode_reward=-88810.88 +/- 16447.83
Episode length: 63.00 +/- 3.03
-------------------------------------
| eval/              |              |
|    mean action     | -0.044281997 |
|    mean velocity x | -0.114       |
|    mean velocity y | 0.476        |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 63           |
|    mean_reward     | -8.88e+04    |
| time/              |              |
|    total_timesteps | 1002000      |
-------------------------------------
Eval num_timesteps=1002500, episode_reward=-91273.55 +/- 19081.73
Episode length: 68.40 +/- 15.04
-----------------------------------
| eval/              |            |
|    mean action     | 0.16573697 |
|    mean velocity x | -0.45      |
|    mean velocity y | -1.25      |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 68.4       |
|    mean_reward     | -9.13e+04  |
| time/              |            |
|    total_timesteps | 1002500    |
-----------------------------------
Eval num_timesteps=1003000, episode_reward=-58445.49 +/- 31376.33
Episode length: 54.60 +/- 22.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.23544696 |
|    mean velocity x | -0.489     |
|    mean velocity y | -1.88      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -5.84e+04  |
| time/              |            |
|    total_timesteps | 1003000    |
-----------------------------------
Eval num_timesteps=1003500, episode_reward=-73635.32 +/- 40670.38
Episode length: 55.60 +/- 19.77
------------------------------------
| eval/              |             |
|    mean action     | 0.096308134 |
|    mean velocity x | 0.127       |
|    mean velocity y | -1.03       |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 55.6        |
|    mean_reward     | -7.36e+04   |
| time/              |             |
|    total_timesteps | 1003500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.2      |
|    ep_rew_mean     | -7.68e+04 |
| time/              |           |
|    fps             | 135       |
|    iterations      | 490       |
|    time_elapsed    | 7392      |
|    total_timesteps | 1003520   |
----------------------------------
Eval num_timesteps=1004000, episode_reward=-53525.06 +/- 51406.95
Episode length: 45.40 +/- 18.38
------------------------------------------
| eval/                   |              |
|    mean action          | 0.30483326   |
|    mean velocity x      | -0.439       |
|    mean velocity y      | -1.46        |
|    mean velocity z      | 18           |
|    mean_ep_length       | 45.4         |
|    mean_reward          | -5.35e+04    |
| time/                   |              |
|    total_timesteps      | 1004000      |
| train/                  |              |
|    approx_kl            | 0.0007703508 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.249        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+08     |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.907        |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=1004500, episode_reward=-84545.48 +/- 20471.22
Episode length: 63.20 +/- 6.49
----------------------------------
| eval/              |           |
|    mean action     | 0.0988328 |
|    mean velocity x | -0.269    |
|    mean velocity y | -0.203    |
|    mean velocity z | 18        |
|    mean_ep_length  | 63.2      |
|    mean_reward     | -8.45e+04 |
| time/              |           |
|    total_timesteps | 1004500   |
----------------------------------
Eval num_timesteps=1005000, episode_reward=-70445.79 +/- 29841.12
Episode length: 60.20 +/- 8.98
------------------------------------
| eval/              |             |
|    mean action     | 0.009272461 |
|    mean velocity x | 0.9         |
|    mean velocity y | 0.358       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 60.2        |
|    mean_reward     | -7.04e+04   |
| time/              |             |
|    total_timesteps | 1005000     |
------------------------------------
Eval num_timesteps=1005500, episode_reward=-51024.93 +/- 40684.22
Episode length: 46.40 +/- 20.34
------------------------------------
| eval/              |             |
|    mean action     | -0.34465253 |
|    mean velocity x | 0.183       |
|    mean velocity y | 1.45        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 46.4        |
|    mean_reward     | -5.1e+04    |
| time/              |             |
|    total_timesteps | 1005500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.9      |
|    ep_rew_mean     | -7.38e+04 |
| time/              |           |
|    fps             | 135       |
|    iterations      | 491       |
|    time_elapsed    | 7399      |
|    total_timesteps | 1005568   |
----------------------------------
Eval num_timesteps=1006000, episode_reward=-67007.18 +/- 31988.31
Episode length: 59.40 +/- 14.92
------------------------------------------
| eval/                   |              |
|    mean action          | -0.31896746  |
|    mean velocity x      | 1.78         |
|    mean velocity y      | 3.08         |
|    mean velocity z      | 16.7         |
|    mean_ep_length       | 59.4         |
|    mean_reward          | -6.7e+04     |
| time/                   |              |
|    total_timesteps      | 1006000      |
| train/                  |              |
|    approx_kl            | 3.808597e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.279        |
|    learning_rate        | 0.001        |
|    loss                 | 1.44e+08     |
|    n_updates            | 4910         |
|    policy_gradient_loss | -0.000268    |
|    std                  | 0.907        |
|    value_loss           | 1.84e+08     |
------------------------------------------
Eval num_timesteps=1006500, episode_reward=-66988.99 +/- 31663.43
Episode length: 59.80 +/- 5.81
------------------------------------
| eval/              |             |
|    mean action     | 0.017699365 |
|    mean velocity x | -0.708      |
|    mean velocity y | -0.163      |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -6.7e+04    |
| time/              |             |
|    total_timesteps | 1006500     |
------------------------------------
Eval num_timesteps=1007000, episode_reward=-53121.96 +/- 26632.56
Episode length: 56.00 +/- 16.43
------------------------------------
| eval/              |             |
|    mean action     | 0.006892197 |
|    mean velocity x | 0.673       |
|    mean velocity y | 0.454       |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 56          |
|    mean_reward     | -5.31e+04   |
| time/              |             |
|    total_timesteps | 1007000     |
------------------------------------
Eval num_timesteps=1007500, episode_reward=-78196.08 +/- 43836.05
Episode length: 80.20 +/- 54.17
------------------------------------
| eval/              |             |
|    mean action     | -0.47851795 |
|    mean velocity x | 0.122       |
|    mean velocity y | 2.33        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 80.2        |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 1007500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.7      |
|    ep_rew_mean     | -7.53e+04 |
| time/              |           |
|    fps             | 136       |
|    iterations      | 492       |
|    time_elapsed    | 7406      |
|    total_timesteps | 1007616   |
----------------------------------
Eval num_timesteps=1008000, episode_reward=-85293.18 +/- 22042.49
Episode length: 60.80 +/- 7.17
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.5310444     |
|    mean velocity x      | -0.55         |
|    mean velocity y      | -2.63         |
|    mean velocity z      | 21            |
|    mean_ep_length       | 60.8          |
|    mean_reward          | -8.53e+04     |
| time/                   |               |
|    total_timesteps      | 1008000       |
| train/                  |               |
|    approx_kl            | 1.0250311e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 8.94e+07      |
|    n_updates            | 4920          |
|    policy_gradient_loss | -0.000135     |
|    std                  | 0.907         |
|    value_loss           | 2.18e+08      |
-------------------------------------------
Eval num_timesteps=1008500, episode_reward=-41363.19 +/- 17302.10
Episode length: 48.20 +/- 9.83
------------------------------------
| eval/              |             |
|    mean action     | 0.026310135 |
|    mean velocity x | 0.658       |
|    mean velocity y | 0.698       |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 48.2        |
|    mean_reward     | -4.14e+04   |
| time/              |             |
|    total_timesteps | 1008500     |
------------------------------------
Eval num_timesteps=1009000, episode_reward=-65401.20 +/- 33677.50
Episode length: 56.40 +/- 12.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.39065322 |
|    mean velocity x | -0.816     |
|    mean velocity y | -2.53      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 56.4       |
|    mean_reward     | -6.54e+04  |
| time/              |            |
|    total_timesteps | 1009000    |
-----------------------------------
Eval num_timesteps=1009500, episode_reward=-84630.97 +/- 11298.84
Episode length: 69.20 +/- 5.64
----------------------------------
| eval/              |           |
|    mean action     | 0.0802491 |
|    mean velocity x | -0.0763   |
|    mean velocity y | -0.274    |
|    mean velocity z | 22.5      |
|    mean_ep_length  | 69.2      |
|    mean_reward     | -8.46e+04 |
| time/              |           |
|    total_timesteps | 1009500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.5      |
|    ep_rew_mean     | -8.24e+04 |
| time/              |           |
|    fps             | 136       |
|    iterations      | 493       |
|    time_elapsed    | 7413      |
|    total_timesteps | 1009664   |
----------------------------------
Eval num_timesteps=1010000, episode_reward=-71478.29 +/- 28535.40
Episode length: 57.60 +/- 7.71
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5936135   |
|    mean velocity x      | 3.01         |
|    mean velocity y      | 4.14         |
|    mean velocity z      | 15.8         |
|    mean_ep_length       | 57.6         |
|    mean_reward          | -7.15e+04    |
| time/                   |              |
|    total_timesteps      | 1010000      |
| train/                  |              |
|    approx_kl            | 0.0012726447 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+08     |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.907        |
|    value_loss           | 2.45e+08     |
------------------------------------------
Eval num_timesteps=1010500, episode_reward=-47107.63 +/- 34423.39
Episode length: 58.60 +/- 29.10
------------------------------------
| eval/              |             |
|    mean action     | 0.030297726 |
|    mean velocity x | -0.00317    |
|    mean velocity y | -0.457      |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -4.71e+04   |
| time/              |             |
|    total_timesteps | 1010500     |
------------------------------------
Eval num_timesteps=1011000, episode_reward=-57342.31 +/- 36210.79
Episode length: 48.00 +/- 19.17
-----------------------------------
| eval/              |            |
|    mean action     | -0.5636749 |
|    mean velocity x | 2.74       |
|    mean velocity y | 4.28       |
|    mean velocity z | 19         |
|    mean_ep_length  | 48         |
|    mean_reward     | -5.73e+04  |
| time/              |            |
|    total_timesteps | 1011000    |
-----------------------------------
Eval num_timesteps=1011500, episode_reward=-74955.55 +/- 30447.06
Episode length: 70.20 +/- 21.48
------------------------------------
| eval/              |             |
|    mean action     | -0.04104715 |
|    mean velocity x | 0.831       |
|    mean velocity y | 0.381       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 70.2        |
|    mean_reward     | -7.5e+04    |
| time/              |             |
|    total_timesteps | 1011500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.3      |
|    ep_rew_mean     | -8.09e+04 |
| time/              |           |
|    fps             | 136       |
|    iterations      | 494       |
|    time_elapsed    | 7421      |
|    total_timesteps | 1011712   |
----------------------------------
Eval num_timesteps=1012000, episode_reward=-43484.66 +/- 47997.76
Episode length: 43.80 +/- 20.17
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.30225605   |
|    mean velocity x      | -0.0318       |
|    mean velocity y      | 1.04          |
|    mean velocity z      | 19.9          |
|    mean_ep_length       | 43.8          |
|    mean_reward          | -4.35e+04     |
| time/                   |               |
|    total_timesteps      | 1012000       |
| train/                  |               |
|    approx_kl            | 0.00040107386 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.255         |
|    learning_rate        | 0.001         |
|    loss                 | 9.66e+07      |
|    n_updates            | 4940          |
|    policy_gradient_loss | -0.00134      |
|    std                  | 0.906         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=1012500, episode_reward=-65210.01 +/- 37419.36
Episode length: 53.00 +/- 12.74
-----------------------------------
| eval/              |            |
|    mean action     | 0.26896727 |
|    mean velocity x | 1.27       |
|    mean velocity y | -0.44      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 53         |
|    mean_reward     | -6.52e+04  |
| time/              |            |
|    total_timesteps | 1012500    |
-----------------------------------
Eval num_timesteps=1013000, episode_reward=-50890.75 +/- 33298.19
Episode length: 54.60 +/- 18.77
------------------------------------
| eval/              |             |
|    mean action     | -0.26525104 |
|    mean velocity x | -0.369      |
|    mean velocity y | 0.813       |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -5.09e+04   |
| time/              |             |
|    total_timesteps | 1013000     |
------------------------------------
Eval num_timesteps=1013500, episode_reward=-72415.36 +/- 31094.90
Episode length: 78.20 +/- 28.08
----------------------------------
| eval/              |           |
|    mean action     | 0.2852861 |
|    mean velocity x | -2.12     |
|    mean velocity y | -1.78     |
|    mean velocity z | 15.5      |
|    mean_ep_length  | 78.2      |
|    mean_reward     | -7.24e+04 |
| time/              |           |
|    total_timesteps | 1013500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.4      |
|    ep_rew_mean     | -8.04e+04 |
| time/              |           |
|    fps             | 136       |
|    iterations      | 495       |
|    time_elapsed    | 7428      |
|    total_timesteps | 1013760   |
----------------------------------
Eval num_timesteps=1014000, episode_reward=-81323.43 +/- 39554.53
Episode length: 61.80 +/- 10.87
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14905787  |
|    mean velocity x      | 1.16         |
|    mean velocity y      | 1.98         |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 61.8         |
|    mean_reward          | -8.13e+04    |
| time/                   |              |
|    total_timesteps      | 1014000      |
| train/                  |              |
|    approx_kl            | 0.0005305598 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+08      |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.907        |
|    value_loss           | 2.3e+08      |
------------------------------------------
Eval num_timesteps=1014500, episode_reward=-59254.42 +/- 25856.53
Episode length: 56.00 +/- 8.32
-----------------------------------
| eval/              |            |
|    mean action     | 0.56981957 |
|    mean velocity x | -0.000214  |
|    mean velocity y | -2.19      |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 56         |
|    mean_reward     | -5.93e+04  |
| time/              |            |
|    total_timesteps | 1014500    |
-----------------------------------
Eval num_timesteps=1015000, episode_reward=-72800.78 +/- 38341.55
Episode length: 59.00 +/- 8.34
-------------------------------------
| eval/              |              |
|    mean action     | 0.0061715697 |
|    mean velocity x | -0.979       |
|    mean velocity y | 0.139        |
|    mean velocity z | 19.7         |
|    mean_ep_length  | 59           |
|    mean_reward     | -7.28e+04    |
| time/              |              |
|    total_timesteps | 1015000      |
-------------------------------------
Eval num_timesteps=1015500, episode_reward=-69964.79 +/- 25327.21
Episode length: 58.00 +/- 4.38
------------------------------------
| eval/              |             |
|    mean action     | -0.18996036 |
|    mean velocity x | 1.92        |
|    mean velocity y | 1.79        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 58          |
|    mean_reward     | -7e+04      |
| time/              |             |
|    total_timesteps | 1015500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.8      |
|    ep_rew_mean     | -8.05e+04 |
| time/              |           |
|    fps             | 136       |
|    iterations      | 496       |
|    time_elapsed    | 7435      |
|    total_timesteps | 1015808   |
----------------------------------
Eval num_timesteps=1016000, episode_reward=-68272.82 +/- 39987.91
Episode length: 57.40 +/- 19.33
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4632238    |
|    mean velocity x      | -0.265        |
|    mean velocity y      | 1.34          |
|    mean velocity z      | 19.6          |
|    mean_ep_length       | 57.4          |
|    mean_reward          | -6.83e+04     |
| time/                   |               |
|    total_timesteps      | 1016000       |
| train/                  |               |
|    approx_kl            | 0.00019871813 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.249         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+08       |
|    n_updates            | 4960          |
|    policy_gradient_loss | -0.000683     |
|    std                  | 0.907         |
|    value_loss           | 2.35e+08      |
-------------------------------------------
Eval num_timesteps=1016500, episode_reward=-79955.29 +/- 19941.15
Episode length: 68.40 +/- 11.31
-----------------------------------
| eval/              |            |
|    mean action     | 0.15387519 |
|    mean velocity x | 0.391      |
|    mean velocity y | 0.254      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 68.4       |
|    mean_reward     | -8e+04     |
| time/              |            |
|    total_timesteps | 1016500    |
-----------------------------------
Eval num_timesteps=1017000, episode_reward=-70031.00 +/- 39242.06
Episode length: 55.40 +/- 11.46
-----------------------------------
| eval/              |            |
|    mean action     | 0.50868315 |
|    mean velocity x | -1.54      |
|    mean velocity y | -2.33      |
|    mean velocity z | 20         |
|    mean_ep_length  | 55.4       |
|    mean_reward     | -7e+04     |
| time/              |            |
|    total_timesteps | 1017000    |
-----------------------------------
Eval num_timesteps=1017500, episode_reward=-80611.06 +/- 23150.85
Episode length: 59.20 +/- 6.21
------------------------------------
| eval/              |             |
|    mean action     | -0.53714085 |
|    mean velocity x | 0.863       |
|    mean velocity y | 2.75        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 59.2        |
|    mean_reward     | -8.06e+04   |
| time/              |             |
|    total_timesteps | 1017500     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -8.2e+04 |
| time/              |          |
|    fps             | 136      |
|    iterations      | 497      |
|    time_elapsed    | 7442     |
|    total_timesteps | 1017856  |
---------------------------------
Eval num_timesteps=1018000, episode_reward=-55651.34 +/- 33644.23
Episode length: 53.80 +/- 13.75
------------------------------------------
| eval/                   |              |
|    mean action          | 0.14684913   |
|    mean velocity x      | -0.322       |
|    mean velocity y      | -0.422       |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 53.8         |
|    mean_reward          | -5.57e+04    |
| time/                   |              |
|    total_timesteps      | 1018000      |
| train/                  |              |
|    approx_kl            | 6.877177e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.241        |
|    learning_rate        | 0.001        |
|    loss                 | 1.51e+08     |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.000122    |
|    std                  | 0.907        |
|    value_loss           | 2.34e+08     |
------------------------------------------
Eval num_timesteps=1018500, episode_reward=-57115.02 +/- 35459.10
Episode length: 58.40 +/- 18.73
------------------------------------
| eval/              |             |
|    mean action     | -0.34600526 |
|    mean velocity x | 0.933       |
|    mean velocity y | 2.05        |
|    mean velocity z | 22          |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -5.71e+04   |
| time/              |             |
|    total_timesteps | 1018500     |
------------------------------------
Eval num_timesteps=1019000, episode_reward=-74776.04 +/- 38789.97
Episode length: 72.60 +/- 34.21
-------------------------------------
| eval/              |              |
|    mean action     | -0.019534035 |
|    mean velocity x | -0.545       |
|    mean velocity y | -0.613       |
|    mean velocity z | 19.2         |
|    mean_ep_length  | 72.6         |
|    mean_reward     | -7.48e+04    |
| time/              |              |
|    total_timesteps | 1019000      |
-------------------------------------
Eval num_timesteps=1019500, episode_reward=-78977.81 +/- 42607.62
Episode length: 57.40 +/- 22.04
------------------------------------
| eval/              |             |
|    mean action     | -0.37686867 |
|    mean velocity x | 1.61        |
|    mean velocity y | 3.1         |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 57.4        |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 1019500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.8      |
|    ep_rew_mean     | -8.39e+04 |
| time/              |           |
|    fps             | 136       |
|    iterations      | 498       |
|    time_elapsed    | 7449      |
|    total_timesteps | 1019904   |
----------------------------------
Eval num_timesteps=1020000, episode_reward=-93104.67 +/- 18259.17
Episode length: 69.80 +/- 6.43
------------------------------------------
| eval/                   |              |
|    mean action          | 0.23232137   |
|    mean velocity x      | -2.37        |
|    mean velocity y      | -0.502       |
|    mean velocity z      | 18.1         |
|    mean_ep_length       | 69.8         |
|    mean_reward          | -9.31e+04    |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 0.0005816692 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+08     |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.906        |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=1020500, episode_reward=-62994.86 +/- 33785.91
Episode length: 58.20 +/- 20.80
-------------------------------------
| eval/              |              |
|    mean action     | -0.084078565 |
|    mean velocity x | 0.175        |
|    mean velocity y | 1.12         |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 58.2         |
|    mean_reward     | -6.3e+04     |
| time/              |              |
|    total_timesteps | 1020500      |
-------------------------------------
Eval num_timesteps=1021000, episode_reward=-94079.56 +/- 18587.38
Episode length: 61.60 +/- 1.85
-----------------------------------
| eval/              |            |
|    mean action     | -0.3093809 |
|    mean velocity x | 1.86       |
|    mean velocity y | 2.66       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -9.41e+04  |
| time/              |            |
|    total_timesteps | 1021000    |
-----------------------------------
Eval num_timesteps=1021500, episode_reward=-56289.86 +/- 36126.82
Episode length: 50.60 +/- 17.08
------------------------------------
| eval/              |             |
|    mean action     | -0.23481971 |
|    mean velocity x | 1.01        |
|    mean velocity y | 0.878       |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 50.6        |
|    mean_reward     | -5.63e+04   |
| time/              |             |
|    total_timesteps | 1021500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.9      |
|    ep_rew_mean     | -8.75e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 499       |
|    time_elapsed    | 7457      |
|    total_timesteps | 1021952   |
----------------------------------
Eval num_timesteps=1022000, episode_reward=-53647.79 +/- 35556.28
Episode length: 50.00 +/- 20.20
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19977619   |
|    mean velocity x      | 1.03          |
|    mean velocity y      | 2.2           |
|    mean velocity z      | 20.9          |
|    mean_ep_length       | 50            |
|    mean_reward          | -5.36e+04     |
| time/                   |               |
|    total_timesteps      | 1022000       |
| train/                  |               |
|    approx_kl            | 0.00087410724 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.26          |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+08      |
|    n_updates            | 4990          |
|    policy_gradient_loss | -0.000926     |
|    std                  | 0.906         |
|    value_loss           | 2.27e+08      |
-------------------------------------------
Eval num_timesteps=1022500, episode_reward=-69678.23 +/- 32350.74
Episode length: 70.00 +/- 20.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.13651922 |
|    mean velocity x | -0.335     |
|    mean velocity y | -1.06      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 70         |
|    mean_reward     | -6.97e+04  |
| time/              |            |
|    total_timesteps | 1022500    |
-----------------------------------
Eval num_timesteps=1023000, episode_reward=-70400.26 +/- 27884.58
Episode length: 69.60 +/- 21.04
-----------------------------------
| eval/              |            |
|    mean action     | -0.3154652 |
|    mean velocity x | 1.85       |
|    mean velocity y | 3.33       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 69.6       |
|    mean_reward     | -7.04e+04  |
| time/              |            |
|    total_timesteps | 1023000    |
-----------------------------------
Eval num_timesteps=1023500, episode_reward=-48076.22 +/- 45872.25
Episode length: 68.80 +/- 53.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.29022792 |
|    mean velocity x | 0.243      |
|    mean velocity y | -1.32      |
|    mean velocity z | 16         |
|    mean_ep_length  | 68.8       |
|    mean_reward     | -4.81e+04  |
| time/              |            |
|    total_timesteps | 1023500    |
-----------------------------------
Eval num_timesteps=1024000, episode_reward=-71718.98 +/- 31464.27
Episode length: 72.80 +/- 32.68
-----------------------------------
| eval/              |            |
|    mean action     | -0.1495732 |
|    mean velocity x | 0.317      |
|    mean velocity y | 0.947      |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 72.8       |
|    mean_reward     | -7.17e+04  |
| time/              |            |
|    total_timesteps | 1024000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.5      |
|    ep_rew_mean     | -8.21e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 500       |
|    time_elapsed    | 7464      |
|    total_timesteps | 1024000   |
----------------------------------
Eval num_timesteps=1024500, episode_reward=-51571.65 +/- 40739.49
Episode length: 48.20 +/- 23.76
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.041354615  |
|    mean velocity x      | -0.873        |
|    mean velocity y      | -0.417        |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 48.2          |
|    mean_reward          | -5.16e+04     |
| time/                   |               |
|    total_timesteps      | 1024500       |
| train/                  |               |
|    approx_kl            | 0.00015779806 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+08      |
|    n_updates            | 5000          |
|    policy_gradient_loss | -0.000626     |
|    std                  | 0.906         |
|    value_loss           | 1.69e+08      |
-------------------------------------------
Eval num_timesteps=1025000, episode_reward=-49326.98 +/- 36998.44
Episode length: 49.00 +/- 21.48
-----------------------------------
| eval/              |            |
|    mean action     | 0.32482067 |
|    mean velocity x | 0.722      |
|    mean velocity y | -0.834     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 49         |
|    mean_reward     | -4.93e+04  |
| time/              |            |
|    total_timesteps | 1025000    |
-----------------------------------
Eval num_timesteps=1025500, episode_reward=-68574.39 +/- 35293.64
Episode length: 59.00 +/- 17.81
-------------------------------------
| eval/              |              |
|    mean action     | -0.024875306 |
|    mean velocity x | 0.835        |
|    mean velocity y | -0.585       |
|    mean velocity z | 19.9         |
|    mean_ep_length  | 59           |
|    mean_reward     | -6.86e+04    |
| time/              |              |
|    total_timesteps | 1025500      |
-------------------------------------
Eval num_timesteps=1026000, episode_reward=-65197.79 +/- 18140.81
Episode length: 58.00 +/- 7.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.09932991 |
|    mean velocity x | -0.328     |
|    mean velocity y | -0.854     |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 58         |
|    mean_reward     | -6.52e+04  |
| time/              |            |
|    total_timesteps | 1026000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.2      |
|    ep_rew_mean     | -7.81e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 501       |
|    time_elapsed    | 7471      |
|    total_timesteps | 1026048   |
----------------------------------
Eval num_timesteps=1026500, episode_reward=-63683.15 +/- 36494.88
Episode length: 62.00 +/- 34.01
------------------------------------------
| eval/                   |              |
|    mean action          | 0.4415816    |
|    mean velocity x      | -2.66        |
|    mean velocity y      | -2.8         |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 62           |
|    mean_reward          | -6.37e+04    |
| time/                   |              |
|    total_timesteps      | 1026500      |
| train/                  |              |
|    approx_kl            | 9.422732e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.001        |
|    loss                 | 1.2e+08      |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.000395    |
|    std                  | 0.906        |
|    value_loss           | 1.82e+08     |
------------------------------------------
Eval num_timesteps=1027000, episode_reward=-72592.41 +/- 6398.51
Episode length: 66.80 +/- 6.58
------------------------------------
| eval/              |             |
|    mean action     | -0.10238924 |
|    mean velocity x | 0.0818      |
|    mean velocity y | -0.738      |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -7.26e+04   |
| time/              |             |
|    total_timesteps | 1027000     |
------------------------------------
Eval num_timesteps=1027500, episode_reward=-59858.33 +/- 32392.42
Episode length: 53.40 +/- 21.93
--------------------------------------
| eval/              |               |
|    mean action     | -0.0132142715 |
|    mean velocity x | 0.583         |
|    mean velocity y | 1.26          |
|    mean velocity z | 20.3          |
|    mean_ep_length  | 53.4          |
|    mean_reward     | -5.99e+04     |
| time/              |               |
|    total_timesteps | 1027500       |
--------------------------------------
Eval num_timesteps=1028000, episode_reward=-61502.89 +/- 50026.83
Episode length: 48.60 +/- 17.81
-----------------------------------
| eval/              |            |
|    mean action     | 0.21792713 |
|    mean velocity x | -0.665     |
|    mean velocity y | -0.7       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 48.6       |
|    mean_reward     | -6.15e+04  |
| time/              |            |
|    total_timesteps | 1028000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.5      |
|    ep_rew_mean     | -8.03e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 502       |
|    time_elapsed    | 7478      |
|    total_timesteps | 1028096   |
----------------------------------
Eval num_timesteps=1028500, episode_reward=-71991.28 +/- 37524.36
Episode length: 57.20 +/- 20.14
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.447205     |
|    mean velocity x      | 2.24          |
|    mean velocity y      | 3.2           |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 57.2          |
|    mean_reward          | -7.2e+04      |
| time/                   |               |
|    total_timesteps      | 1028500       |
| train/                  |               |
|    approx_kl            | 2.8058887e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.236         |
|    learning_rate        | 0.001         |
|    loss                 | 5.05e+07      |
|    n_updates            | 5020          |
|    policy_gradient_loss | -0.000308     |
|    std                  | 0.906         |
|    value_loss           | 2.24e+08      |
-------------------------------------------
Eval num_timesteps=1029000, episode_reward=-82370.11 +/- 15042.64
Episode length: 61.80 +/- 1.94
------------------------------------
| eval/              |             |
|    mean action     | -0.48516777 |
|    mean velocity x | 1.47        |
|    mean velocity y | 2.17        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -8.24e+04   |
| time/              |             |
|    total_timesteps | 1029000     |
------------------------------------
Eval num_timesteps=1029500, episode_reward=-41974.02 +/- 24049.91
Episode length: 54.60 +/- 16.70
------------------------------------
| eval/              |             |
|    mean action     | -0.18520817 |
|    mean velocity x | 0.531       |
|    mean velocity y | 0.203       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -4.2e+04    |
| time/              |             |
|    total_timesteps | 1029500     |
------------------------------------
Eval num_timesteps=1030000, episode_reward=-55612.46 +/- 23875.87
Episode length: 54.80 +/- 7.03
-------------------------------------
| eval/              |              |
|    mean action     | -0.025653305 |
|    mean velocity x | 1.43         |
|    mean velocity y | 0.122        |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 54.8         |
|    mean_reward     | -5.56e+04    |
| time/              |              |
|    total_timesteps | 1030000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73        |
|    ep_rew_mean     | -7.77e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 503       |
|    time_elapsed    | 7486      |
|    total_timesteps | 1030144   |
----------------------------------
Eval num_timesteps=1030500, episode_reward=-49191.99 +/- 27273.73
Episode length: 54.80 +/- 20.35
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2780264   |
|    mean velocity x      | 2.4          |
|    mean velocity y      | 3.53         |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -4.92e+04    |
| time/                   |              |
|    total_timesteps      | 1030500      |
| train/                  |              |
|    approx_kl            | 7.866975e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.248        |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+08     |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.000411    |
|    std                  | 0.906        |
|    value_loss           | 2.05e+08     |
------------------------------------------
Eval num_timesteps=1031000, episode_reward=-50208.79 +/- 39108.39
Episode length: 50.40 +/- 16.78
------------------------------------
| eval/              |             |
|    mean action     | -0.14232852 |
|    mean velocity x | 0.85        |
|    mean velocity y | 2           |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 50.4        |
|    mean_reward     | -5.02e+04   |
| time/              |             |
|    total_timesteps | 1031000     |
------------------------------------
Eval num_timesteps=1031500, episode_reward=-58040.69 +/- 38029.10
Episode length: 57.00 +/- 27.15
------------------------------------
| eval/              |             |
|    mean action     | -0.36547205 |
|    mean velocity x | 2.08        |
|    mean velocity y | 1.46        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 57          |
|    mean_reward     | -5.8e+04    |
| time/              |             |
|    total_timesteps | 1031500     |
------------------------------------
Eval num_timesteps=1032000, episode_reward=-36909.16 +/- 25378.96
Episode length: 52.80 +/- 23.45
-----------------------------------
| eval/              |            |
|    mean action     | 0.16279471 |
|    mean velocity x | 2          |
|    mean velocity y | 1.03       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 52.8       |
|    mean_reward     | -3.69e+04  |
| time/              |            |
|    total_timesteps | 1032000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.6      |
|    ep_rew_mean     | -8.47e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 504       |
|    time_elapsed    | 7493      |
|    total_timesteps | 1032192   |
----------------------------------
Eval num_timesteps=1032500, episode_reward=-54038.94 +/- 48279.65
Episode length: 48.60 +/- 14.64
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.2600399     |
|    mean velocity x      | -1.6          |
|    mean velocity y      | -2.15         |
|    mean velocity z      | 20.9          |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -5.4e+04      |
| time/                   |               |
|    total_timesteps      | 1032500       |
| train/                  |               |
|    approx_kl            | 3.1370728e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.235         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+08      |
|    n_updates            | 5040          |
|    policy_gradient_loss | -0.000285     |
|    std                  | 0.906         |
|    value_loss           | 2.47e+08      |
-------------------------------------------
Eval num_timesteps=1033000, episode_reward=-59685.88 +/- 43311.11
Episode length: 54.60 +/- 20.77
----------------------------------
| eval/              |           |
|    mean action     | 0.1461163 |
|    mean velocity x | -0.118    |
|    mean velocity y | 0.0798    |
|    mean velocity z | 17.5      |
|    mean_ep_length  | 54.6      |
|    mean_reward     | -5.97e+04 |
| time/              |           |
|    total_timesteps | 1033000   |
----------------------------------
Eval num_timesteps=1033500, episode_reward=-73055.98 +/- 39563.32
Episode length: 58.40 +/- 27.36
-----------------------------------
| eval/              |            |
|    mean action     | 0.28014618 |
|    mean velocity x | -2.22      |
|    mean velocity y | -3.08      |
|    mean velocity z | 22.7       |
|    mean_ep_length  | 58.4       |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 1033500    |
-----------------------------------
Eval num_timesteps=1034000, episode_reward=-50938.50 +/- 45884.32
Episode length: 45.00 +/- 17.55
-----------------------------------
| eval/              |            |
|    mean action     | -0.3265613 |
|    mean velocity x | 1.26       |
|    mean velocity y | 3.01       |
|    mean velocity z | 21.8       |
|    mean_ep_length  | 45         |
|    mean_reward     | -5.09e+04  |
| time/              |            |
|    total_timesteps | 1034000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.2      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 137       |
|    iterations      | 505       |
|    time_elapsed    | 7500      |
|    total_timesteps | 1034240   |
----------------------------------
Eval num_timesteps=1034500, episode_reward=-40741.91 +/- 27327.20
Episode length: 46.00 +/- 15.56
------------------------------------------
| eval/                   |              |
|    mean action          | -0.18104553  |
|    mean velocity x      | -1.28        |
|    mean velocity y      | 0.87         |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 46           |
|    mean_reward          | -4.07e+04    |
| time/                   |              |
|    total_timesteps      | 1034500      |
| train/                  |              |
|    approx_kl            | 0.0017754687 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.235        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+08     |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 0.906        |
|    value_loss           | 2.61e+08     |
------------------------------------------
Eval num_timesteps=1035000, episode_reward=-75662.14 +/- 33842.32
Episode length: 52.80 +/- 16.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.16020276 |
|    mean velocity x | 1.42       |
|    mean velocity y | -0.975     |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 52.8       |
|    mean_reward     | -7.57e+04  |
| time/              |            |
|    total_timesteps | 1035000    |
-----------------------------------
Eval num_timesteps=1035500, episode_reward=-71542.95 +/- 19890.80
Episode length: 67.40 +/- 9.60
-------------------------------------
| eval/              |              |
|    mean action     | -0.057191208 |
|    mean velocity x | -0.0576      |
|    mean velocity y | 0.418        |
|    mean velocity z | 18.4         |
|    mean_ep_length  | 67.4         |
|    mean_reward     | -7.15e+04    |
| time/              |              |
|    total_timesteps | 1035500      |
-------------------------------------
Eval num_timesteps=1036000, episode_reward=-44180.84 +/- 29053.40
Episode length: 46.60 +/- 12.37
------------------------------------
| eval/              |             |
|    mean action     | -0.11060785 |
|    mean velocity x | 2.18        |
|    mean velocity y | 1.13        |
|    mean velocity z | 17          |
|    mean_ep_length  | 46.6        |
|    mean_reward     | -4.42e+04   |
| time/              |             |
|    total_timesteps | 1036000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.9      |
|    ep_rew_mean     | -7.99e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 506       |
|    time_elapsed    | 7507      |
|    total_timesteps | 1036288   |
----------------------------------
Eval num_timesteps=1036500, episode_reward=-81462.57 +/- 10991.02
Episode length: 83.00 +/- 19.14
------------------------------------------
| eval/                   |              |
|    mean action          | 0.30240014   |
|    mean velocity x      | -2.53        |
|    mean velocity y      | -1.86        |
|    mean velocity z      | 21.3         |
|    mean_ep_length       | 83           |
|    mean_reward          | -8.15e+04    |
| time/                   |              |
|    total_timesteps      | 1036500      |
| train/                  |              |
|    approx_kl            | 0.0011117205 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.001        |
|    loss                 | 7.64e+07     |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.906        |
|    value_loss           | 1.67e+08     |
------------------------------------------
Eval num_timesteps=1037000, episode_reward=-91049.84 +/- 10756.45
Episode length: 62.60 +/- 3.26
-----------------------------------
| eval/              |            |
|    mean action     | 0.17950064 |
|    mean velocity x | -2.15      |
|    mean velocity y | -1.56      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -9.1e+04   |
| time/              |            |
|    total_timesteps | 1037000    |
-----------------------------------
Eval num_timesteps=1037500, episode_reward=-40372.47 +/- 31151.28
Episode length: 53.60 +/- 9.29
------------------------------------
| eval/              |             |
|    mean action     | -0.06350319 |
|    mean velocity x | -1.93       |
|    mean velocity y | -0.789      |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -4.04e+04   |
| time/              |             |
|    total_timesteps | 1037500     |
------------------------------------
Eval num_timesteps=1038000, episode_reward=-66483.75 +/- 17772.50
Episode length: 63.00 +/- 12.17
-----------------------------------
| eval/              |            |
|    mean action     | 0.54761744 |
|    mean velocity x | -1.74      |
|    mean velocity y | -3.03      |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 63         |
|    mean_reward     | -6.65e+04  |
| time/              |            |
|    total_timesteps | 1038000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -7.89e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 507       |
|    time_elapsed    | 7514      |
|    total_timesteps | 1038336   |
----------------------------------
Eval num_timesteps=1038500, episode_reward=-71335.74 +/- 31055.04
Episode length: 67.80 +/- 18.08
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40639424   |
|    mean velocity x      | 2.07          |
|    mean velocity y      | 4.12          |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 67.8          |
|    mean_reward          | -7.13e+04     |
| time/                   |               |
|    total_timesteps      | 1038500       |
| train/                  |               |
|    approx_kl            | 0.00039252697 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.001         |
|    loss                 | 9.12e+07      |
|    n_updates            | 5070          |
|    policy_gradient_loss | -0.000963     |
|    std                  | 0.906         |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=1039000, episode_reward=-52516.70 +/- 36021.09
Episode length: 48.00 +/- 21.36
-----------------------------------
| eval/              |            |
|    mean action     | 0.52657986 |
|    mean velocity x | 0.146      |
|    mean velocity y | -3.2       |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 48         |
|    mean_reward     | -5.25e+04  |
| time/              |            |
|    total_timesteps | 1039000    |
-----------------------------------
Eval num_timesteps=1039500, episode_reward=-88707.22 +/- 22644.64
Episode length: 59.80 +/- 3.12
------------------------------------
| eval/              |             |
|    mean action     | -0.25140375 |
|    mean velocity x | 2.74        |
|    mean velocity y | 2.41        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -8.87e+04   |
| time/              |             |
|    total_timesteps | 1039500     |
------------------------------------
Eval num_timesteps=1040000, episode_reward=-20578.43 +/- 16662.09
Episode length: 35.60 +/- 12.85
------------------------------------
| eval/              |             |
|    mean action     | -0.09644125 |
|    mean velocity x | 1.94        |
|    mean velocity y | 1.02        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 35.6        |
|    mean_reward     | -2.06e+04   |
| time/              |             |
|    total_timesteps | 1040000     |
------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.3      |
|    ep_rew_mean     | -7.98e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 508       |
|    time_elapsed    | 7521      |
|    total_timesteps | 1040384   |
----------------------------------
Eval num_timesteps=1040500, episode_reward=-95604.76 +/- 28856.51
Episode length: 75.80 +/- 34.11
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.69327796   |
|    mean velocity x      | 2.18          |
|    mean velocity y      | 4.28          |
|    mean velocity z      | 18.1          |
|    mean_ep_length       | 75.8          |
|    mean_reward          | -9.56e+04     |
| time/                   |               |
|    total_timesteps      | 1040500       |
| train/                  |               |
|    approx_kl            | 0.00024144215 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.001         |
|    loss                 | 1.48e+08      |
|    n_updates            | 5080          |
|    policy_gradient_loss | -0.00073      |
|    std                  | 0.905         |
|    value_loss           | 2.2e+08       |
-------------------------------------------
Eval num_timesteps=1041000, episode_reward=-60557.94 +/- 30317.60
Episode length: 59.60 +/- 16.26
------------------------------------
| eval/              |             |
|    mean action     | -0.60980064 |
|    mean velocity x | 2.08        |
|    mean velocity y | 3.49        |
|    mean velocity z | 17.1        |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -6.06e+04   |
| time/              |             |
|    total_timesteps | 1041000     |
------------------------------------
Eval num_timesteps=1041500, episode_reward=-49311.68 +/- 44014.95
Episode length: 51.20 +/- 15.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.37347063 |
|    mean velocity x | -0.249     |
|    mean velocity y | -1.51      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 51.2       |
|    mean_reward     | -4.93e+04  |
| time/              |            |
|    total_timesteps | 1041500    |
-----------------------------------
Eval num_timesteps=1042000, episode_reward=-89119.89 +/- 12350.51
Episode length: 81.40 +/- 36.51
-----------------------------------
| eval/              |            |
|    mean action     | 0.17281596 |
|    mean velocity x | 0.441      |
|    mean velocity y | -1.04      |
|    mean velocity z | 22         |
|    mean_ep_length  | 81.4       |
|    mean_reward     | -8.91e+04  |
| time/              |            |
|    total_timesteps | 1042000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.2      |
|    ep_rew_mean     | -8.24e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 509       |
|    time_elapsed    | 7528      |
|    total_timesteps | 1042432   |
----------------------------------
Eval num_timesteps=1042500, episode_reward=-58345.70 +/- 42047.95
Episode length: 70.60 +/- 40.43
------------------------------------------
| eval/                   |              |
|    mean action          | -0.5264858   |
|    mean velocity x      | 0.948        |
|    mean velocity y      | 2.07         |
|    mean velocity z      | 20.5         |
|    mean_ep_length       | 70.6         |
|    mean_reward          | -5.83e+04    |
| time/                   |              |
|    total_timesteps      | 1042500      |
| train/                  |              |
|    approx_kl            | 8.925368e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.246        |
|    learning_rate        | 0.001        |
|    loss                 | 1.81e+08     |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.000374    |
|    std                  | 0.905        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=1043000, episode_reward=-63636.81 +/- 33651.68
Episode length: 52.80 +/- 13.56
----------------------------------
| eval/              |           |
|    mean action     | 0.3309286 |
|    mean velocity x | -1.19     |
|    mean velocity y | -2.13     |
|    mean velocity z | 16.5      |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -6.36e+04 |
| time/              |           |
|    total_timesteps | 1043000   |
----------------------------------
Eval num_timesteps=1043500, episode_reward=-76714.16 +/- 40183.79
Episode length: 53.60 +/- 20.46
-----------------------------------
| eval/              |            |
|    mean action     | -0.5079654 |
|    mean velocity x | 0.884      |
|    mean velocity y | 1.38       |
|    mean velocity z | 16         |
|    mean_ep_length  | 53.6       |
|    mean_reward     | -7.67e+04  |
| time/              |            |
|    total_timesteps | 1043500    |
-----------------------------------
Eval num_timesteps=1044000, episode_reward=-77033.39 +/- 23360.95
Episode length: 58.80 +/- 5.81
-----------------------------------
| eval/              |            |
|    mean action     | 0.24661234 |
|    mean velocity x | -1.53      |
|    mean velocity y | -1.6       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 58.8       |
|    mean_reward     | -7.7e+04   |
| time/              |            |
|    total_timesteps | 1044000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.7      |
|    ep_rew_mean     | -7.58e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 510       |
|    time_elapsed    | 7536      |
|    total_timesteps | 1044480   |
----------------------------------
Eval num_timesteps=1044500, episode_reward=-85829.65 +/- 11320.62
Episode length: 68.20 +/- 13.01
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.18385366    |
|    mean velocity x      | -0.218        |
|    mean velocity y      | -1.16         |
|    mean velocity z      | 18.9          |
|    mean_ep_length       | 68.2          |
|    mean_reward          | -8.58e+04     |
| time/                   |               |
|    total_timesteps      | 1044500       |
| train/                  |               |
|    approx_kl            | 6.8354624e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.264         |
|    learning_rate        | 0.001         |
|    loss                 | 6.04e+07      |
|    n_updates            | 5100          |
|    policy_gradient_loss | -0.000471     |
|    std                  | 0.905         |
|    value_loss           | 1.93e+08      |
-------------------------------------------
Eval num_timesteps=1045000, episode_reward=-58114.16 +/- 36741.97
Episode length: 52.60 +/- 18.45
------------------------------------
| eval/              |             |
|    mean action     | -0.36135408 |
|    mean velocity x | -0.243      |
|    mean velocity y | 0.905       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 52.6        |
|    mean_reward     | -5.81e+04   |
| time/              |             |
|    total_timesteps | 1045000     |
------------------------------------
Eval num_timesteps=1045500, episode_reward=-67149.24 +/- 29424.96
Episode length: 73.40 +/- 27.24
------------------------------------
| eval/              |             |
|    mean action     | -0.21954657 |
|    mean velocity x | 1.02        |
|    mean velocity y | 1.67        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 73.4        |
|    mean_reward     | -6.71e+04   |
| time/              |             |
|    total_timesteps | 1045500     |
------------------------------------
Eval num_timesteps=1046000, episode_reward=-41020.95 +/- 30175.14
Episode length: 57.20 +/- 33.14
------------------------------------
| eval/              |             |
|    mean action     | -0.36785656 |
|    mean velocity x | 0.594       |
|    mean velocity y | 1.78        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -4.1e+04    |
| time/              |             |
|    total_timesteps | 1046000     |
------------------------------------
Eval num_timesteps=1046500, episode_reward=-67319.02 +/- 19394.35
Episode length: 56.40 +/- 8.31
------------------------------------
| eval/              |             |
|    mean action     | -0.14272203 |
|    mean velocity x | 0.534       |
|    mean velocity y | 0.834       |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 56.4        |
|    mean_reward     | -6.73e+04   |
| time/              |             |
|    total_timesteps | 1046500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.3      |
|    ep_rew_mean     | -7.53e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 511       |
|    time_elapsed    | 7543      |
|    total_timesteps | 1046528   |
----------------------------------
Eval num_timesteps=1047000, episode_reward=-77848.48 +/- 20907.13
Episode length: 68.80 +/- 16.02
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.48411146    |
|    mean velocity x      | -2.92         |
|    mean velocity y      | -2.69         |
|    mean velocity z      | 19            |
|    mean_ep_length       | 68.8          |
|    mean_reward          | -7.78e+04     |
| time/                   |               |
|    total_timesteps      | 1047000       |
| train/                  |               |
|    approx_kl            | 0.00039355975 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.001         |
|    loss                 | 6.82e+07      |
|    n_updates            | 5110          |
|    policy_gradient_loss | -0.000654     |
|    std                  | 0.905         |
|    value_loss           | 2.06e+08      |
-------------------------------------------
Eval num_timesteps=1047500, episode_reward=-65361.18 +/- 26732.98
Episode length: 72.20 +/- 23.81
-----------------------------------
| eval/              |            |
|    mean action     | -0.5792583 |
|    mean velocity x | 2.89       |
|    mean velocity y | 4.07       |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 72.2       |
|    mean_reward     | -6.54e+04  |
| time/              |            |
|    total_timesteps | 1047500    |
-----------------------------------
Eval num_timesteps=1048000, episode_reward=-55888.98 +/- 34677.80
Episode length: 54.80 +/- 16.08
----------------------------------
| eval/              |           |
|    mean action     | 0.67026   |
|    mean velocity x | -1.46     |
|    mean velocity y | -2.83     |
|    mean velocity z | 18.4      |
|    mean_ep_length  | 54.8      |
|    mean_reward     | -5.59e+04 |
| time/              |           |
|    total_timesteps | 1048000   |
----------------------------------
Eval num_timesteps=1048500, episode_reward=-50346.16 +/- 27511.95
Episode length: 51.80 +/- 18.43
------------------------------------
| eval/              |             |
|    mean action     | -0.17467469 |
|    mean velocity x | 1.48        |
|    mean velocity y | 1.23        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 51.8        |
|    mean_reward     | -5.03e+04   |
| time/              |             |
|    total_timesteps | 1048500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.7      |
|    ep_rew_mean     | -7.27e+04 |
| time/              |           |
|    fps             | 138       |
|    iterations      | 512       |
|    time_elapsed    | 7551      |
|    total_timesteps | 1048576   |
----------------------------------
Eval num_timesteps=1049000, episode_reward=-45913.65 +/- 24428.86
Episode length: 57.00 +/- 12.41
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.05245404    |
|    mean velocity x      | 0.72          |
|    mean velocity y      | 0.236         |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 57            |
|    mean_reward          | -4.59e+04     |
| time/                   |               |
|    total_timesteps      | 1049000       |
| train/                  |               |
|    approx_kl            | 0.00025383802 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+08      |
|    n_updates            | 5120          |
|    policy_gradient_loss | -0.000799     |
|    std                  | 0.905         |
|    value_loss           | 2.1e+08       |
-------------------------------------------
Eval num_timesteps=1049500, episode_reward=-66303.00 +/- 28725.50
Episode length: 74.60 +/- 32.33
----------------------------------
| eval/              |           |
|    mean action     | 0.0996022 |
|    mean velocity x | -0.155    |
|    mean velocity y | 0.556     |
|    mean velocity z | 19.5      |
|    mean_ep_length  | 74.6      |
|    mean_reward     | -6.63e+04 |
| time/              |           |
|    total_timesteps | 1049500   |
----------------------------------
Eval num_timesteps=1050000, episode_reward=-77410.97 +/- 28134.94
Episode length: 63.00 +/- 6.36
-----------------------------------
| eval/              |            |
|    mean action     | 0.13386329 |
|    mean velocity x | -1.01      |
|    mean velocity y | -1.4       |
|    mean velocity z | 24.1       |
|    mean_ep_length  | 63         |
|    mean_reward     | -7.74e+04  |
| time/              |            |
|    total_timesteps | 1050000    |
-----------------------------------
Eval num_timesteps=1050500, episode_reward=-54175.05 +/- 35996.02
Episode length: 54.60 +/- 20.56
------------------------------------
| eval/              |             |
|    mean action     | -0.15320104 |
|    mean velocity x | 0.265       |
|    mean velocity y | 0.562       |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -5.42e+04   |
| time/              |             |
|    total_timesteps | 1050500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.1      |
|    ep_rew_mean     | -8.17e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 513       |
|    time_elapsed    | 7558      |
|    total_timesteps | 1050624   |
----------------------------------
Eval num_timesteps=1051000, episode_reward=-64459.20 +/- 15457.15
Episode length: 60.60 +/- 4.50
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.027164033  |
|    mean velocity x      | -0.994        |
|    mean velocity y      | -0.597        |
|    mean velocity z      | 20.6          |
|    mean_ep_length       | 60.6          |
|    mean_reward          | -6.45e+04     |
| time/                   |               |
|    total_timesteps      | 1051000       |
| train/                  |               |
|    approx_kl            | 0.00018790356 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+08      |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000507     |
|    std                  | 0.905         |
|    value_loss           | 2.21e+08      |
-------------------------------------------
Eval num_timesteps=1051500, episode_reward=-76334.14 +/- 32320.84
Episode length: 57.20 +/- 7.52
--------------------------------------
| eval/              |               |
|    mean action     | -0.0058728983 |
|    mean velocity x | -0.136        |
|    mean velocity y | 0.716         |
|    mean velocity z | 19.7          |
|    mean_ep_length  | 57.2          |
|    mean_reward     | -7.63e+04     |
| time/              |               |
|    total_timesteps | 1051500       |
--------------------------------------
Eval num_timesteps=1052000, episode_reward=-59397.42 +/- 47365.54
Episode length: 67.00 +/- 45.49
-----------------------------------
| eval/              |            |
|    mean action     | 0.23972598 |
|    mean velocity x | -2         |
|    mean velocity y | -2.91      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 67         |
|    mean_reward     | -5.94e+04  |
| time/              |            |
|    total_timesteps | 1052000    |
-----------------------------------
Eval num_timesteps=1052500, episode_reward=-70745.52 +/- 28121.44
Episode length: 64.80 +/- 17.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.53472817 |
|    mean velocity x | -0.658     |
|    mean velocity y | -2.89      |
|    mean velocity z | 20.9       |
|    mean_ep_length  | 64.8       |
|    mean_reward     | -7.07e+04  |
| time/              |            |
|    total_timesteps | 1052500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.7      |
|    ep_rew_mean     | -8.68e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 514       |
|    time_elapsed    | 7565      |
|    total_timesteps | 1052672   |
----------------------------------
Eval num_timesteps=1053000, episode_reward=-89475.46 +/- 26305.16
Episode length: 63.00 +/- 7.67
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.57293755   |
|    mean velocity x      | 2.73          |
|    mean velocity y      | 4.1           |
|    mean velocity z      | 19            |
|    mean_ep_length       | 63            |
|    mean_reward          | -8.95e+04     |
| time/                   |               |
|    total_timesteps      | 1053000       |
| train/                  |               |
|    approx_kl            | 2.9461662e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.001         |
|    loss                 | 8.71e+07      |
|    n_updates            | 5140          |
|    policy_gradient_loss | -0.000247     |
|    std                  | 0.906         |
|    value_loss           | 2.31e+08      |
-------------------------------------------
Eval num_timesteps=1053500, episode_reward=-90716.06 +/- 21870.69
Episode length: 60.60 +/- 2.73
------------------------------------
| eval/              |             |
|    mean action     | -0.35445997 |
|    mean velocity x | 1.14        |
|    mean velocity y | 2.22        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 60.6        |
|    mean_reward     | -9.07e+04   |
| time/              |             |
|    total_timesteps | 1053500     |
------------------------------------
Eval num_timesteps=1054000, episode_reward=-29132.78 +/- 31402.93
Episode length: 35.80 +/- 19.32
-----------------------------------
| eval/              |            |
|    mean action     | 0.25942683 |
|    mean velocity x | -0.0945    |
|    mean velocity y | -0.463     |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 35.8       |
|    mean_reward     | -2.91e+04  |
| time/              |            |
|    total_timesteps | 1054000    |
-----------------------------------
Eval num_timesteps=1054500, episode_reward=-22978.08 +/- 26560.00
Episode length: 41.80 +/- 18.54
-------------------------------------
| eval/              |              |
|    mean action     | -0.018309647 |
|    mean velocity x | 1.27         |
|    mean velocity y | 0.961        |
|    mean velocity z | 19.3         |
|    mean_ep_length  | 41.8         |
|    mean_reward     | -2.3e+04     |
| time/              |              |
|    total_timesteps | 1054500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.8      |
|    ep_rew_mean     | -8.84e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 515       |
|    time_elapsed    | 7572      |
|    total_timesteps | 1054720   |
----------------------------------
Eval num_timesteps=1055000, episode_reward=-53627.02 +/- 45099.68
Episode length: 44.80 +/- 20.80
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0077839014 |
|    mean velocity x      | 0.764         |
|    mean velocity y      | 0.167         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 44.8          |
|    mean_reward          | -5.36e+04     |
| time/                   |               |
|    total_timesteps      | 1055000       |
| train/                  |               |
|    approx_kl            | 0.0007853907  |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.264         |
|    learning_rate        | 0.001         |
|    loss                 | 7.23e+07      |
|    n_updates            | 5150          |
|    policy_gradient_loss | -0.00089      |
|    std                  | 0.906         |
|    value_loss           | 1.99e+08      |
-------------------------------------------
Eval num_timesteps=1055500, episode_reward=-62947.18 +/- 26614.81
Episode length: 70.40 +/- 23.74
------------------------------------
| eval/              |             |
|    mean action     | -0.47229916 |
|    mean velocity x | 3.01        |
|    mean velocity y | 3.19        |
|    mean velocity z | 20          |
|    mean_ep_length  | 70.4        |
|    mean_reward     | -6.29e+04   |
| time/              |             |
|    total_timesteps | 1055500     |
------------------------------------
Eval num_timesteps=1056000, episode_reward=-16883.95 +/- 18248.94
Episode length: 36.00 +/- 14.31
-----------------------------------
| eval/              |            |
|    mean action     | 0.22208378 |
|    mean velocity x | -1.55      |
|    mean velocity y | -2.41      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 36         |
|    mean_reward     | -1.69e+04  |
| time/              |            |
|    total_timesteps | 1056000    |
-----------------------------------
New best mean reward!
Eval num_timesteps=1056500, episode_reward=-51442.37 +/- 42556.65
Episode length: 43.00 +/- 28.46
------------------------------------
| eval/              |             |
|    mean action     | -0.06786496 |
|    mean velocity x | 1.58        |
|    mean velocity y | 2.45        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 43          |
|    mean_reward     | -5.14e+04   |
| time/              |             |
|    total_timesteps | 1056500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73        |
|    ep_rew_mean     | -8.52e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 516       |
|    time_elapsed    | 7579      |
|    total_timesteps | 1056768   |
----------------------------------
Eval num_timesteps=1057000, episode_reward=-72935.05 +/- 35652.55
Episode length: 61.40 +/- 20.29
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.8660714    |
|    mean velocity x      | 3.16          |
|    mean velocity y      | 5.6           |
|    mean velocity z      | 20.7          |
|    mean_ep_length       | 61.4          |
|    mean_reward          | -7.29e+04     |
| time/                   |               |
|    total_timesteps      | 1057000       |
| train/                  |               |
|    approx_kl            | 0.00016189303 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.27          |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 5160          |
|    policy_gradient_loss | -0.000146     |
|    std                  | 0.905         |
|    value_loss           | 2.1e+08       |
-------------------------------------------
Eval num_timesteps=1057500, episode_reward=-94385.61 +/- 12151.65
Episode length: 62.20 +/- 4.35
-----------------------------------
| eval/              |            |
|    mean action     | 0.37878984 |
|    mean velocity x | -0.979     |
|    mean velocity y | -2.44      |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 62.2       |
|    mean_reward     | -9.44e+04  |
| time/              |            |
|    total_timesteps | 1057500    |
-----------------------------------
Eval num_timesteps=1058000, episode_reward=-36404.34 +/- 28693.10
Episode length: 44.20 +/- 18.41
-----------------------------------
| eval/              |            |
|    mean action     | 0.13736923 |
|    mean velocity x | -1.85      |
|    mean velocity y | -2.31      |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 44.2       |
|    mean_reward     | -3.64e+04  |
| time/              |            |
|    total_timesteps | 1058000    |
-----------------------------------
Eval num_timesteps=1058500, episode_reward=-100948.21 +/- 6572.14
Episode length: 73.80 +/- 19.27
----------------------------------
| eval/              |           |
|    mean action     | 0.6742308 |
|    mean velocity x | -3.88     |
|    mean velocity y | -4.41     |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 73.8      |
|    mean_reward     | -1.01e+05 |
| time/              |           |
|    total_timesteps | 1058500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.2      |
|    ep_rew_mean     | -7.88e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 517       |
|    time_elapsed    | 7586      |
|    total_timesteps | 1058816   |
----------------------------------
Eval num_timesteps=1059000, episode_reward=-68168.54 +/- 34309.41
Episode length: 58.40 +/- 8.26
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.20158072    |
|    mean velocity x      | -0.501        |
|    mean velocity y      | -1.12         |
|    mean velocity z      | 18.9          |
|    mean_ep_length       | 58.4          |
|    mean_reward          | -6.82e+04     |
| time/                   |               |
|    total_timesteps      | 1059000       |
| train/                  |               |
|    approx_kl            | 0.00056369125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.96         |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.001         |
|    loss                 | 9.25e+07      |
|    n_updates            | 5170          |
|    policy_gradient_loss | -0.00144      |
|    std                  | 0.905         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=1059500, episode_reward=-64851.50 +/- 38621.92
Episode length: 52.60 +/- 13.08
-------------------------------------
| eval/              |              |
|    mean action     | -0.012957229 |
|    mean velocity x | -0.569       |
|    mean velocity y | -0.544       |
|    mean velocity z | 17.7         |
|    mean_ep_length  | 52.6         |
|    mean_reward     | -6.49e+04    |
| time/              |              |
|    total_timesteps | 1059500      |
-------------------------------------
Eval num_timesteps=1060000, episode_reward=-64688.24 +/- 30696.69
Episode length: 73.00 +/- 28.89
-----------------------------------
| eval/              |            |
|    mean action     | 0.20044746 |
|    mean velocity x | 0.688      |
|    mean velocity y | -0.226     |
|    mean velocity z | 16.3       |
|    mean_ep_length  | 73         |
|    mean_reward     | -6.47e+04  |
| time/              |            |
|    total_timesteps | 1060000    |
-----------------------------------
Eval num_timesteps=1060500, episode_reward=-86085.86 +/- 22854.93
Episode length: 63.20 +/- 6.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.31109637 |
|    mean velocity x | -0.78      |
|    mean velocity y | -1.19      |
|    mean velocity z | 17         |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -8.61e+04  |
| time/              |            |
|    total_timesteps | 1060500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -7.3e+04 |
| time/              |          |
|    fps             | 139      |
|    iterations      | 518      |
|    time_elapsed    | 7594     |
|    total_timesteps | 1060864  |
---------------------------------
Eval num_timesteps=1061000, episode_reward=-66092.81 +/- 34510.61
Episode length: 56.80 +/- 11.92
------------------------------------------
| eval/                   |              |
|    mean action          | 0.57926905   |
|    mean velocity x      | -1.79        |
|    mean velocity y      | -3.43        |
|    mean velocity z      | 17.2         |
|    mean_ep_length       | 56.8         |
|    mean_reward          | -6.61e+04    |
| time/                   |              |
|    total_timesteps      | 1061000      |
| train/                  |              |
|    approx_kl            | 0.0016165053 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.001        |
|    loss                 | 8.7e+07      |
|    n_updates            | 5180         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.904        |
|    value_loss           | 1.48e+08     |
------------------------------------------
Eval num_timesteps=1061500, episode_reward=-64446.86 +/- 31158.72
Episode length: 55.00 +/- 10.49
------------------------------------
| eval/              |             |
|    mean action     | -0.12320882 |
|    mean velocity x | -0.769      |
|    mean velocity y | 0.147       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 55          |
|    mean_reward     | -6.44e+04   |
| time/              |             |
|    total_timesteps | 1061500     |
------------------------------------
Eval num_timesteps=1062000, episode_reward=-40047.47 +/- 43156.49
Episode length: 39.20 +/- 19.39
-------------------------------------
| eval/              |              |
|    mean action     | -0.067731716 |
|    mean velocity x | -0.809       |
|    mean velocity y | -0.945       |
|    mean velocity z | 22           |
|    mean_ep_length  | 39.2         |
|    mean_reward     | -4e+04       |
| time/              |              |
|    total_timesteps | 1062000      |
-------------------------------------
Eval num_timesteps=1062500, episode_reward=-51356.51 +/- 39063.32
Episode length: 50.80 +/- 11.57
-----------------------------------
| eval/              |            |
|    mean action     | -0.3531698 |
|    mean velocity x | 3.1        |
|    mean velocity y | 3.29       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 50.8       |
|    mean_reward     | -5.14e+04  |
| time/              |            |
|    total_timesteps | 1062500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -7.36e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 519       |
|    time_elapsed    | 7601      |
|    total_timesteps | 1062912   |
----------------------------------
Eval num_timesteps=1063000, episode_reward=-102649.86 +/- 67690.92
Episode length: 106.40 +/- 65.66
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.27394542  |
|    mean velocity x      | -0.586      |
|    mean velocity y      | -0.816      |
|    mean velocity z      | 17.6        |
|    mean_ep_length       | 106         |
|    mean_reward          | -1.03e+05   |
| time/                   |             |
|    total_timesteps      | 1063000     |
| train/                  |             |
|    approx_kl            | 0.001137001 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+08    |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.000876   |
|    std                  | 0.904       |
|    value_loss           | 2.08e+08    |
-----------------------------------------
Eval num_timesteps=1063500, episode_reward=-83613.72 +/- 11312.69
Episode length: 67.60 +/- 7.86
----------------------------------
| eval/              |           |
|    mean action     | 0.4019257 |
|    mean velocity x | -1.8      |
|    mean velocity y | -1.97     |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 67.6      |
|    mean_reward     | -8.36e+04 |
| time/              |           |
|    total_timesteps | 1063500   |
----------------------------------
Eval num_timesteps=1064000, episode_reward=-64150.55 +/- 23250.34
Episode length: 62.00 +/- 17.02
------------------------------------
| eval/              |             |
|    mean action     | 0.032547135 |
|    mean velocity x | -0.132      |
|    mean velocity y | -0.633      |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 62          |
|    mean_reward     | -6.42e+04   |
| time/              |             |
|    total_timesteps | 1064000     |
------------------------------------
Eval num_timesteps=1064500, episode_reward=-86150.27 +/- 14963.18
Episode length: 73.00 +/- 10.14
------------------------------------
| eval/              |             |
|    mean action     | -0.07532184 |
|    mean velocity x | -1.2        |
|    mean velocity y | -0.259      |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 73          |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 1064500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70        |
|    ep_rew_mean     | -7.37e+04 |
| time/              |           |
|    fps             | 139       |
|    iterations      | 520       |
|    time_elapsed    | 7608      |
|    total_timesteps | 1064960   |
----------------------------------
Eval num_timesteps=1065000, episode_reward=-73898.60 +/- 16794.81
Episode length: 74.20 +/- 9.26
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.22313894    |
|    mean velocity x      | -0.814        |
|    mean velocity y      | -1.03         |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 74.2          |
|    mean_reward          | -7.39e+04     |
| time/                   |               |
|    total_timesteps      | 1065000       |
| train/                  |               |
|    approx_kl            | 0.00013182237 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.001         |
|    loss                 | 8.65e+07      |
|    n_updates            | 5200          |
|    policy_gradient_loss | -0.000406     |
|    std                  | 0.904         |
|    value_loss           | 2.06e+08      |
-------------------------------------------
Eval num_timesteps=1065500, episode_reward=-72554.81 +/- 27518.56
Episode length: 63.40 +/- 12.09
-----------------------------------
| eval/              |            |
|    mean action     | -0.1441422 |
|    mean velocity x | 0.513      |
|    mean velocity y | 0.668      |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -7.26e+04  |
| time/              |            |
|    total_timesteps | 1065500    |
-----------------------------------
Eval num_timesteps=1066000, episode_reward=-70508.88 +/- 37036.28
Episode length: 57.80 +/- 21.51
------------------------------------
| eval/              |             |
|    mean action     | -0.49637008 |
|    mean velocity x | 2.09        |
|    mean velocity y | 3.09        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 57.8        |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 1066000     |
------------------------------------
Eval num_timesteps=1066500, episode_reward=-59845.16 +/- 26157.47
Episode length: 58.60 +/- 11.71
------------------------------------
| eval/              |             |
|    mean action     | -0.31504166 |
|    mean velocity x | 1.86        |
|    mean velocity y | 2.49        |
|    mean velocity z | 16.6        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -5.98e+04   |
| time/              |             |
|    total_timesteps | 1066500     |
------------------------------------
Eval num_timesteps=1067000, episode_reward=-57337.25 +/- 36889.21
Episode length: 49.00 +/- 17.74
----------------------------------
| eval/              |           |
|    mean action     | 0.2133145 |
|    mean velocity x | -0.511    |
|    mean velocity y | -1.07     |
|    mean velocity z | 19.4      |
|    mean_ep_length  | 49        |
|    mean_reward     | -5.73e+04 |
| time/              |           |
|    total_timesteps | 1067000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.9      |
|    ep_rew_mean     | -7.67e+04 |
| time/              |           |
|    fps             | 140       |
|    iterations      | 521       |
|    time_elapsed    | 7616      |
|    total_timesteps | 1067008   |
----------------------------------
Eval num_timesteps=1067500, episode_reward=-54379.70 +/- 40872.52
Episode length: 54.40 +/- 26.08
------------------------------------------
| eval/                   |              |
|    mean action          | -0.38802224  |
|    mean velocity x      | 0.79         |
|    mean velocity y      | 2.53         |
|    mean velocity z      | 20.7         |
|    mean_ep_length       | 54.4         |
|    mean_reward          | -5.44e+04    |
| time/                   |              |
|    total_timesteps      | 1067500      |
| train/                  |              |
|    approx_kl            | 0.0014246807 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+08     |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.904        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=1068000, episode_reward=-42293.39 +/- 35435.61
Episode length: 50.20 +/- 18.45
-------------------------------------
| eval/              |              |
|    mean action     | -0.017864438 |
|    mean velocity x | 0.588        |
|    mean velocity y | 0.382        |
|    mean velocity z | 20           |
|    mean_ep_length  | 50.2         |
|    mean_reward     | -4.23e+04    |
| time/              |              |
|    total_timesteps | 1068000      |
-------------------------------------
Eval num_timesteps=1068500, episode_reward=-62679.48 +/- 30271.20
Episode length: 68.00 +/- 29.62
-----------------------------------
| eval/              |            |
|    mean action     | -0.0899118 |
|    mean velocity x | 0.757      |
|    mean velocity y | 1.9        |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 68         |
|    mean_reward     | -6.27e+04  |
| time/              |            |
|    total_timesteps | 1068500    |
-----------------------------------
Eval num_timesteps=1069000, episode_reward=-65088.16 +/- 33368.74
Episode length: 59.60 +/- 25.57
------------------------------------
| eval/              |             |
|    mean action     | -0.31952327 |
|    mean velocity x | 1.68        |
|    mean velocity y | 2.31        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -6.51e+04   |
| time/              |             |
|    total_timesteps | 1069000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.6      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 140       |
|    iterations      | 522       |
|    time_elapsed    | 7623      |
|    total_timesteps | 1069056   |
----------------------------------
Eval num_timesteps=1069500, episode_reward=-64977.20 +/- 27903.25
Episode length: 59.40 +/- 7.63
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.058213767   |
|    mean velocity x      | -1.84         |
|    mean velocity y      | -1.4          |
|    mean velocity z      | 18.4          |
|    mean_ep_length       | 59.4          |
|    mean_reward          | -6.5e+04      |
| time/                   |               |
|    total_timesteps      | 1069500       |
| train/                  |               |
|    approx_kl            | 0.00015282363 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.001         |
|    loss                 | 4.93e+07      |
|    n_updates            | 5220          |
|    policy_gradient_loss | -0.000613     |
|    std                  | 0.904         |
|    value_loss           | 1.92e+08      |
-------------------------------------------
Eval num_timesteps=1070000, episode_reward=-67964.17 +/- 15460.77
Episode length: 70.40 +/- 17.75
-----------------------------------
| eval/              |            |
|    mean action     | 0.23517163 |
|    mean velocity x | -2.36      |
|    mean velocity y | -1.29      |
|    mean velocity z | 16.5       |
|    mean_ep_length  | 70.4       |
|    mean_reward     | -6.8e+04   |
| time/              |            |
|    total_timesteps | 1070000    |
-----------------------------------
Eval num_timesteps=1070500, episode_reward=-62111.57 +/- 13279.98
Episode length: 65.00 +/- 10.68
-----------------------------------
| eval/              |            |
|    mean action     | 0.53282905 |
|    mean velocity x | -2.87      |
|    mean velocity y | -4.64      |
|    mean velocity z | 16.6       |
|    mean_ep_length  | 65         |
|    mean_reward     | -6.21e+04  |
| time/              |            |
|    total_timesteps | 1070500    |
-----------------------------------
Eval num_timesteps=1071000, episode_reward=-72398.68 +/- 13395.35
Episode length: 74.40 +/- 22.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.35801077 |
|    mean velocity x | 0.55       |
|    mean velocity y | -1.73      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 74.4       |
|    mean_reward     | -7.24e+04  |
| time/              |            |
|    total_timesteps | 1071000    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.1     |
|    ep_rew_mean     | -8e+04   |
| time/              |          |
|    fps             | 140      |
|    iterations      | 523      |
|    time_elapsed    | 7630     |
|    total_timesteps | 1071104  |
---------------------------------
Eval num_timesteps=1071500, episode_reward=-20794.29 +/- 17138.14
Episode length: 36.80 +/- 15.04
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.19123161    |
|    mean velocity x      | -0.705        |
|    mean velocity y      | -0.733        |
|    mean velocity z      | 18            |
|    mean_ep_length       | 36.8          |
|    mean_reward          | -2.08e+04     |
| time/                   |               |
|    total_timesteps      | 1071500       |
| train/                  |               |
|    approx_kl            | 0.00031700585 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.3           |
|    learning_rate        | 0.001         |
|    loss                 | 9.17e+07      |
|    n_updates            | 5230          |
|    policy_gradient_loss | -0.00062      |
|    std                  | 0.904         |
|    value_loss           | 1.49e+08      |
-------------------------------------------
Eval num_timesteps=1072000, episode_reward=-74024.26 +/- 29701.49
Episode length: 61.00 +/- 10.45
-----------------------------------
| eval/              |            |
|    mean action     | 0.03362549 |
|    mean velocity x | -0.193     |
|    mean velocity y | -0.0108    |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 61         |
|    mean_reward     | -7.4e+04   |
| time/              |            |
|    total_timesteps | 1072000    |
-----------------------------------
Eval num_timesteps=1072500, episode_reward=-67682.49 +/- 32302.03
Episode length: 67.00 +/- 25.27
----------------------------------
| eval/              |           |
|    mean action     | 0.7296974 |
|    mean velocity x | -2.35     |
|    mean velocity y | -5.14     |
|    mean velocity z | 18.3      |
|    mean_ep_length  | 67        |
|    mean_reward     | -6.77e+04 |
| time/              |           |
|    total_timesteps | 1072500   |
----------------------------------
Eval num_timesteps=1073000, episode_reward=-68520.71 +/- 32663.54
Episode length: 57.60 +/- 7.03
-----------------------------------
| eval/              |            |
|    mean action     | 0.47141308 |
|    mean velocity x | -1.23      |
|    mean velocity y | -2.42      |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -6.85e+04  |
| time/              |            |
|    total_timesteps | 1073000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.8      |
|    ep_rew_mean     | -7.96e+04 |
| time/              |           |
|    fps             | 140       |
|    iterations      | 524       |
|    time_elapsed    | 7637      |
|    total_timesteps | 1073152   |
----------------------------------
Eval num_timesteps=1073500, episode_reward=-70131.15 +/- 18576.19
Episode length: 61.40 +/- 8.89
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.11422511   |
|    mean velocity x      | -0.622        |
|    mean velocity y      | 0.511         |
|    mean velocity z      | 19.5          |
|    mean_ep_length       | 61.4          |
|    mean_reward          | -7.01e+04     |
| time/                   |               |
|    total_timesteps      | 1073500       |
| train/                  |               |
|    approx_kl            | 0.00024188755 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.001         |
|    loss                 | 8.12e+07      |
|    n_updates            | 5240          |
|    policy_gradient_loss | -0.000617     |
|    std                  | 0.904         |
|    value_loss           | 1.85e+08      |
-------------------------------------------
Eval num_timesteps=1074000, episode_reward=-74294.29 +/- 37724.90
Episode length: 54.00 +/- 16.66
------------------------------------
| eval/              |             |
|    mean action     | -0.07348576 |
|    mean velocity x | -0.428      |
|    mean velocity y | 0.567       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 54          |
|    mean_reward     | -7.43e+04   |
| time/              |             |
|    total_timesteps | 1074000     |
------------------------------------
Eval num_timesteps=1074500, episode_reward=-63577.00 +/- 45625.60
Episode length: 49.00 +/- 16.61
-------------------------------------
| eval/              |              |
|    mean action     | 0.0058839414 |
|    mean velocity x | -1.9         |
|    mean velocity y | -0.551       |
|    mean velocity z | 19.1         |
|    mean_ep_length  | 49           |
|    mean_reward     | -6.36e+04    |
| time/              |              |
|    total_timesteps | 1074500      |
-------------------------------------
Eval num_timesteps=1075000, episode_reward=-65577.10 +/- 22197.57
Episode length: 59.60 +/- 6.71
------------------------------------
| eval/              |             |
|    mean action     | 0.058042016 |
|    mean velocity x | 0.832       |
|    mean velocity y | -0.373      |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -6.56e+04   |
| time/              |             |
|    total_timesteps | 1075000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74        |
|    ep_rew_mean     | -7.79e+04 |
| time/              |           |
|    fps             | 140       |
|    iterations      | 525       |
|    time_elapsed    | 7644      |
|    total_timesteps | 1075200   |
----------------------------------
Eval num_timesteps=1075500, episode_reward=-78711.90 +/- 30761.37
Episode length: 70.20 +/- 18.13
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.44129813    |
|    mean velocity x      | -0.19         |
|    mean velocity y      | -2.76         |
|    mean velocity z      | 19.5          |
|    mean_ep_length       | 70.2          |
|    mean_reward          | -7.87e+04     |
| time/                   |               |
|    total_timesteps      | 1075500       |
| train/                  |               |
|    approx_kl            | 0.00038435933 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.277         |
|    learning_rate        | 0.001         |
|    loss                 | 1.09e+08      |
|    n_updates            | 5250          |
|    policy_gradient_loss | -0.00056      |
|    std                  | 0.904         |
|    value_loss           | 1.89e+08      |
-------------------------------------------
Eval num_timesteps=1076000, episode_reward=-50378.31 +/- 47155.76
Episode length: 40.80 +/- 22.27
------------------------------------
| eval/              |             |
|    mean action     | 0.046011016 |
|    mean velocity x | -1.02       |
|    mean velocity y | 0.647       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 40.8        |
|    mean_reward     | -5.04e+04   |
| time/              |             |
|    total_timesteps | 1076000     |
------------------------------------
Eval num_timesteps=1076500, episode_reward=-66828.17 +/- 25694.74
Episode length: 63.60 +/- 13.75
-------------------------------------
| eval/              |              |
|    mean action     | -0.016935028 |
|    mean velocity x | 0.936        |
|    mean velocity y | 1.32         |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 63.6         |
|    mean_reward     | -6.68e+04    |
| time/              |              |
|    total_timesteps | 1076500      |
-------------------------------------
Eval num_timesteps=1077000, episode_reward=-89983.88 +/- 14289.03
Episode length: 68.80 +/- 9.91
------------------------------------
| eval/              |             |
|    mean action     | -0.35770094 |
|    mean velocity x | 2.33        |
|    mean velocity y | 3.84        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 68.8        |
|    mean_reward     | -9e+04      |
| time/              |             |
|    total_timesteps | 1077000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.9      |
|    ep_rew_mean     | -7.63e+04 |
| time/              |           |
|    fps             | 140       |
|    iterations      | 526       |
|    time_elapsed    | 7652      |
|    total_timesteps | 1077248   |
----------------------------------
Eval num_timesteps=1077500, episode_reward=-95362.24 +/- 20836.50
Episode length: 68.00 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.23867534    |
|    mean velocity x      | -1.27         |
|    mean velocity y      | -1.46         |
|    mean velocity z      | 20.5          |
|    mean_ep_length       | 68            |
|    mean_reward          | -9.54e+04     |
| time/                   |               |
|    total_timesteps      | 1077500       |
| train/                  |               |
|    approx_kl            | 0.00048555937 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 5260          |
|    policy_gradient_loss | -0.000868     |
|    std                  | 0.904         |
|    value_loss           | 2.04e+08      |
-------------------------------------------
Eval num_timesteps=1078000, episode_reward=-90195.26 +/- 14259.03
Episode length: 69.80 +/- 14.66
------------------------------------
| eval/              |             |
|    mean action     | -0.37522683 |
|    mean velocity x | 0.775       |
|    mean velocity y | 2.44        |
|    mean velocity z | 21.8        |
|    mean_ep_length  | 69.8        |
|    mean_reward     | -9.02e+04   |
| time/              |             |
|    total_timesteps | 1078000     |
------------------------------------
Eval num_timesteps=1078500, episode_reward=-73414.87 +/- 27043.49
Episode length: 63.20 +/- 12.83
-----------------------------------
| eval/              |            |
|    mean action     | 0.07808894 |
|    mean velocity x | 1.08       |
|    mean velocity y | -0.132     |
|    mean velocity z | 22.5       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -7.34e+04  |
| time/              |            |
|    total_timesteps | 1078500    |
-----------------------------------
Eval num_timesteps=1079000, episode_reward=-93912.76 +/- 10397.67
Episode length: 78.20 +/- 21.25
------------------------------------
| eval/              |             |
|    mean action     | -0.10978326 |
|    mean velocity x | -0.234      |
|    mean velocity y | 1.15        |
|    mean velocity z | 19          |
|    mean_ep_length  | 78.2        |
|    mean_reward     | -9.39e+04   |
| time/              |             |
|    total_timesteps | 1079000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71        |
|    ep_rew_mean     | -8.13e+04 |
| time/              |           |
|    fps             | 140       |
|    iterations      | 527       |
|    time_elapsed    | 7659      |
|    total_timesteps | 1079296   |
----------------------------------
Eval num_timesteps=1079500, episode_reward=-49235.95 +/- 43904.01
Episode length: 43.00 +/- 21.60
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.00030892945 |
|    mean velocity x      | -0.616        |
|    mean velocity y      | -0.701        |
|    mean velocity z      | 16.4          |
|    mean_ep_length       | 43            |
|    mean_reward          | -4.92e+04     |
| time/                   |               |
|    total_timesteps      | 1079500       |
| train/                  |               |
|    approx_kl            | 0.002291561   |
|    clip_fraction        | 0.00752       |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.277         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+08      |
|    n_updates            | 5270          |
|    policy_gradient_loss | -0.00211      |
|    std                  | 0.904         |
|    value_loss           | 2.33e+08      |
-------------------------------------------
Eval num_timesteps=1080000, episode_reward=-69212.98 +/- 35954.40
Episode length: 55.00 +/- 10.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.12005733 |
|    mean velocity x | -0.0974    |
|    mean velocity y | -0.138     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 55         |
|    mean_reward     | -6.92e+04  |
| time/              |            |
|    total_timesteps | 1080000    |
-----------------------------------
Eval num_timesteps=1080500, episode_reward=-90068.65 +/- 23542.11
Episode length: 61.00 +/- 3.35
-----------------------------------
| eval/              |            |
|    mean action     | -0.3203501 |
|    mean velocity x | 1.08       |
|    mean velocity y | 0.896      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 61         |
|    mean_reward     | -9.01e+04  |
| time/              |            |
|    total_timesteps | 1080500    |
-----------------------------------
Eval num_timesteps=1081000, episode_reward=-57059.16 +/- 36934.89
Episode length: 64.80 +/- 33.17
-----------------------------------
| eval/              |            |
|    mean action     | 0.22838598 |
|    mean velocity x | -0.912     |
|    mean velocity y | -2.24      |
|    mean velocity z | 22.8       |
|    mean_ep_length  | 64.8       |
|    mean_reward     | -5.71e+04  |
| time/              |            |
|    total_timesteps | 1081000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.5      |
|    ep_rew_mean     | -8.16e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 528       |
|    time_elapsed    | 7666      |
|    total_timesteps | 1081344   |
----------------------------------
Eval num_timesteps=1081500, episode_reward=-66945.40 +/- 35161.98
Episode length: 61.60 +/- 21.68
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.45996135   |
|    mean velocity x      | 1.47          |
|    mean velocity y      | 2.03          |
|    mean velocity z      | 17.2          |
|    mean_ep_length       | 61.6          |
|    mean_reward          | -6.69e+04     |
| time/                   |               |
|    total_timesteps      | 1081500       |
| train/                  |               |
|    approx_kl            | 4.4407818e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.261         |
|    learning_rate        | 0.001         |
|    loss                 | 9.37e+07      |
|    n_updates            | 5280          |
|    policy_gradient_loss | -0.000416     |
|    std                  | 0.904         |
|    value_loss           | 2.13e+08      |
-------------------------------------------
Eval num_timesteps=1082000, episode_reward=-52301.17 +/- 32917.57
Episode length: 50.00 +/- 12.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.20410864 |
|    mean velocity x | 1.16       |
|    mean velocity y | -0.218     |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 50         |
|    mean_reward     | -5.23e+04  |
| time/              |            |
|    total_timesteps | 1082000    |
-----------------------------------
Eval num_timesteps=1082500, episode_reward=-63865.20 +/- 16244.99
Episode length: 59.20 +/- 5.53
-----------------------------------
| eval/              |            |
|    mean action     | 0.71991986 |
|    mean velocity x | -2.97      |
|    mean velocity y | -3.45      |
|    mean velocity z | 15.2       |
|    mean_ep_length  | 59.2       |
|    mean_reward     | -6.39e+04  |
| time/              |            |
|    total_timesteps | 1082500    |
-----------------------------------
Eval num_timesteps=1083000, episode_reward=-81012.73 +/- 26019.25
Episode length: 81.60 +/- 40.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.50390977 |
|    mean velocity x | -0.887     |
|    mean velocity y | -2.91      |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 81.6       |
|    mean_reward     | -8.1e+04   |
| time/              |            |
|    total_timesteps | 1083000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.83e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 529       |
|    time_elapsed    | 7673      |
|    total_timesteps | 1083392   |
----------------------------------
Eval num_timesteps=1083500, episode_reward=-55019.57 +/- 37511.14
Episode length: 52.00 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean action          | -0.05016348  |
|    mean velocity x      | -0.643       |
|    mean velocity y      | -0.585       |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 52           |
|    mean_reward          | -5.5e+04     |
| time/                   |              |
|    total_timesteps      | 1083500      |
| train/                  |              |
|    approx_kl            | 0.0003377184 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.001        |
|    loss                 | 1.24e+08     |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.000452    |
|    std                  | 0.904        |
|    value_loss           | 1.61e+08     |
------------------------------------------
Eval num_timesteps=1084000, episode_reward=-54206.04 +/- 41560.01
Episode length: 55.00 +/- 23.96
-----------------------------------
| eval/              |            |
|    mean action     | -0.5156486 |
|    mean velocity x | 3.15       |
|    mean velocity y | 3.22       |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 55         |
|    mean_reward     | -5.42e+04  |
| time/              |            |
|    total_timesteps | 1084000    |
-----------------------------------
Eval num_timesteps=1084500, episode_reward=-35576.28 +/- 40966.92
Episode length: 35.00 +/- 22.19
------------------------------------
| eval/              |             |
|    mean action     | -0.10069136 |
|    mean velocity x | 0.746       |
|    mean velocity y | 2.19        |
|    mean velocity z | 21          |
|    mean_ep_length  | 35          |
|    mean_reward     | -3.56e+04   |
| time/              |             |
|    total_timesteps | 1084500     |
------------------------------------
Eval num_timesteps=1085000, episode_reward=-72216.00 +/- 31109.80
Episode length: 62.00 +/- 12.18
-----------------------------------
| eval/              |            |
|    mean action     | -0.6447554 |
|    mean velocity x | 0.831      |
|    mean velocity y | 3.75       |
|    mean velocity z | 22         |
|    mean_ep_length  | 62         |
|    mean_reward     | -7.22e+04  |
| time/              |            |
|    total_timesteps | 1085000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -8.04e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 530       |
|    time_elapsed    | 7680      |
|    total_timesteps | 1085440   |
----------------------------------
Eval num_timesteps=1085500, episode_reward=-45432.58 +/- 25430.94
Episode length: 46.20 +/- 16.92
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.04793588   |
|    mean velocity x      | 0.376         |
|    mean velocity y      | 0.828         |
|    mean velocity z      | 18.4          |
|    mean_ep_length       | 46.2          |
|    mean_reward          | -4.54e+04     |
| time/                   |               |
|    total_timesteps      | 1085500       |
| train/                  |               |
|    approx_kl            | 0.00058564823 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+08      |
|    n_updates            | 5300          |
|    policy_gradient_loss | -0.000647     |
|    std                  | 0.904         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=1086000, episode_reward=-69660.68 +/- 38079.52
Episode length: 57.60 +/- 25.71
-----------------------------------
| eval/              |            |
|    mean action     | -0.2752915 |
|    mean velocity x | 2.93       |
|    mean velocity y | 3.53       |
|    mean velocity z | 21.6       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -6.97e+04  |
| time/              |            |
|    total_timesteps | 1086000    |
-----------------------------------
Eval num_timesteps=1086500, episode_reward=-65591.38 +/- 16166.48
Episode length: 58.20 +/- 6.97
------------------------------------
| eval/              |             |
|    mean action     | -0.14709345 |
|    mean velocity x | -0.129      |
|    mean velocity y | 1.83        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -6.56e+04   |
| time/              |             |
|    total_timesteps | 1086500     |
------------------------------------
Eval num_timesteps=1087000, episode_reward=-77299.43 +/- 31221.02
Episode length: 63.00 +/- 10.62
-------------------------------------
| eval/              |              |
|    mean action     | -0.071826205 |
|    mean velocity x | 0.601        |
|    mean velocity y | 0.846        |
|    mean velocity z | 18           |
|    mean_ep_length  | 63           |
|    mean_reward     | -7.73e+04    |
| time/              |              |
|    total_timesteps | 1087000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.3      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 531       |
|    time_elapsed    | 7688      |
|    total_timesteps | 1087488   |
----------------------------------
Eval num_timesteps=1087500, episode_reward=-73497.93 +/- 42862.04
Episode length: 52.60 +/- 20.03
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.20934428    |
|    mean velocity x      | -0.768        |
|    mean velocity y      | 0.22          |
|    mean velocity z      | 18.9          |
|    mean_ep_length       | 52.6          |
|    mean_reward          | -7.35e+04     |
| time/                   |               |
|    total_timesteps      | 1087500       |
| train/                  |               |
|    approx_kl            | 5.7204685e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.279         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+08      |
|    n_updates            | 5310          |
|    policy_gradient_loss | -0.000267     |
|    std                  | 0.904         |
|    value_loss           | 2.05e+08      |
-------------------------------------------
Eval num_timesteps=1088000, episode_reward=-67415.48 +/- 33072.00
Episode length: 58.00 +/- 15.71
-------------------------------------
| eval/              |              |
|    mean action     | -0.013112472 |
|    mean velocity x | 0.954        |
|    mean velocity y | 1.01         |
|    mean velocity z | 21.2         |
|    mean_ep_length  | 58           |
|    mean_reward     | -6.74e+04    |
| time/              |              |
|    total_timesteps | 1088000      |
-------------------------------------
Eval num_timesteps=1088500, episode_reward=-58583.06 +/- 43347.36
Episode length: 48.60 +/- 17.72
------------------------------------
| eval/              |             |
|    mean action     | -0.34119317 |
|    mean velocity x | 2.35        |
|    mean velocity y | 2.4         |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -5.86e+04   |
| time/              |             |
|    total_timesteps | 1088500     |
------------------------------------
Eval num_timesteps=1089000, episode_reward=-81741.28 +/- 41303.77
Episode length: 54.60 +/- 21.68
-----------------------------------
| eval/              |            |
|    mean action     | -0.2652444 |
|    mean velocity x | 2.56       |
|    mean velocity y | 1.89       |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -8.17e+04  |
| time/              |            |
|    total_timesteps | 1089000    |
-----------------------------------
Eval num_timesteps=1089500, episode_reward=-61884.90 +/- 16728.98
Episode length: 58.60 +/- 5.82
----------------------------------
| eval/              |           |
|    mean action     | 0.738656  |
|    mean velocity x | -2.05     |
|    mean velocity y | -5.52     |
|    mean velocity z | 17.2      |
|    mean_ep_length  | 58.6      |
|    mean_reward     | -6.19e+04 |
| time/              |           |
|    total_timesteps | 1089500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.5      |
|    ep_rew_mean     | -7.64e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 532       |
|    time_elapsed    | 7695      |
|    total_timesteps | 1089536   |
----------------------------------
Eval num_timesteps=1090000, episode_reward=-64092.27 +/- 35324.96
Episode length: 51.40 +/- 17.12
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.12601371    |
|    mean velocity x      | 0.718         |
|    mean velocity y      | 0.245         |
|    mean velocity z      | 20.7          |
|    mean_ep_length       | 51.4          |
|    mean_reward          | -6.41e+04     |
| time/                   |               |
|    total_timesteps      | 1090000       |
| train/                  |               |
|    approx_kl            | 0.00024121584 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+08      |
|    n_updates            | 5320          |
|    policy_gradient_loss | -0.000587     |
|    std                  | 0.903         |
|    value_loss           | 2.27e+08      |
-------------------------------------------
Eval num_timesteps=1090500, episode_reward=-78098.18 +/- 13987.22
Episode length: 73.20 +/- 11.41
-----------------------------------
| eval/              |            |
|    mean action     | 0.39928925 |
|    mean velocity x | -2.74      |
|    mean velocity y | -2.51      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 73.2       |
|    mean_reward     | -7.81e+04  |
| time/              |            |
|    total_timesteps | 1090500    |
-----------------------------------
Eval num_timesteps=1091000, episode_reward=-75338.94 +/- 12025.41
Episode length: 61.80 +/- 4.71
----------------------------------
| eval/              |           |
|    mean action     | 0.4359689 |
|    mean velocity x | -1.53     |
|    mean velocity y | -2.7      |
|    mean velocity z | 21        |
|    mean_ep_length  | 61.8      |
|    mean_reward     | -7.53e+04 |
| time/              |           |
|    total_timesteps | 1091000   |
----------------------------------
Eval num_timesteps=1091500, episode_reward=-79526.39 +/- 27199.62
Episode length: 72.20 +/- 27.35
----------------------------------
| eval/              |           |
|    mean action     | 0.2169819 |
|    mean velocity x | -1.75     |
|    mean velocity y | -0.185    |
|    mean velocity z | 16.3      |
|    mean_ep_length  | 72.2      |
|    mean_reward     | -7.95e+04 |
| time/              |           |
|    total_timesteps | 1091500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.4      |
|    ep_rew_mean     | -8.18e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 533       |
|    time_elapsed    | 7702      |
|    total_timesteps | 1091584   |
----------------------------------
Eval num_timesteps=1092000, episode_reward=-71596.07 +/- 27521.96
Episode length: 65.80 +/- 14.96
------------------------------------------
| eval/                   |              |
|    mean action          | 0.24885435   |
|    mean velocity x      | -2.78        |
|    mean velocity y      | -2.99        |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 65.8         |
|    mean_reward          | -7.16e+04    |
| time/                   |              |
|    total_timesteps      | 1092000      |
| train/                  |              |
|    approx_kl            | 0.0020708707 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+08     |
|    n_updates            | 5330         |
|    policy_gradient_loss | -0.00196     |
|    std                  | 0.903        |
|    value_loss           | 1.97e+08     |
------------------------------------------
Eval num_timesteps=1092500, episode_reward=-95893.06 +/- 39047.41
Episode length: 105.40 +/- 51.03
-----------------------------------
| eval/              |            |
|    mean action     | -0.3417522 |
|    mean velocity x | 1.86       |
|    mean velocity y | 2.12       |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 105        |
|    mean_reward     | -9.59e+04  |
| time/              |            |
|    total_timesteps | 1092500    |
-----------------------------------
Eval num_timesteps=1093000, episode_reward=-67678.96 +/- 24009.67
Episode length: 63.00 +/- 14.74
------------------------------------
| eval/              |             |
|    mean action     | 0.018547975 |
|    mean velocity x | 0.693       |
|    mean velocity y | -0.204      |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 63          |
|    mean_reward     | -6.77e+04   |
| time/              |             |
|    total_timesteps | 1093000     |
------------------------------------
Eval num_timesteps=1093500, episode_reward=-79811.63 +/- 34550.60
Episode length: 62.40 +/- 15.29
----------------------------------
| eval/              |           |
|    mean action     | 0.2659968 |
|    mean velocity x | -0.77     |
|    mean velocity y | -2.32     |
|    mean velocity z | 19.4      |
|    mean_ep_length  | 62.4      |
|    mean_reward     | -7.98e+04 |
| time/              |           |
|    total_timesteps | 1093500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.71e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 534       |
|    time_elapsed    | 7710      |
|    total_timesteps | 1093632   |
----------------------------------
Eval num_timesteps=1094000, episode_reward=-62677.06 +/- 31162.69
Episode length: 55.60 +/- 10.86
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.027879067   |
|    mean velocity x      | -0.748        |
|    mean velocity y      | -1.03         |
|    mean velocity z      | 19.6          |
|    mean_ep_length       | 55.6          |
|    mean_reward          | -6.27e+04     |
| time/                   |               |
|    total_timesteps      | 1094000       |
| train/                  |               |
|    approx_kl            | 2.4971378e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 9.51e+07      |
|    n_updates            | 5340          |
|    policy_gradient_loss | -0.000152     |
|    std                  | 0.903         |
|    value_loss           | 1.82e+08      |
-------------------------------------------
Eval num_timesteps=1094500, episode_reward=-45059.31 +/- 35911.94
Episode length: 52.40 +/- 24.37
------------------------------------
| eval/              |             |
|    mean action     | -0.24803177 |
|    mean velocity x | 1.19        |
|    mean velocity y | 1.49        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 52.4        |
|    mean_reward     | -4.51e+04   |
| time/              |             |
|    total_timesteps | 1094500     |
------------------------------------
Eval num_timesteps=1095000, episode_reward=-92409.47 +/- 23875.77
Episode length: 61.20 +/- 3.66
------------------------------------
| eval/              |             |
|    mean action     | -0.32896978 |
|    mean velocity x | 1.84        |
|    mean velocity y | 2.15        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -9.24e+04   |
| time/              |             |
|    total_timesteps | 1095000     |
------------------------------------
Eval num_timesteps=1095500, episode_reward=-71683.38 +/- 34395.10
Episode length: 55.60 +/- 9.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.28656322 |
|    mean velocity x | -1.78      |
|    mean velocity y | -2.02      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 55.6       |
|    mean_reward     | -7.17e+04  |
| time/              |            |
|    total_timesteps | 1095500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.7      |
|    ep_rew_mean     | -7.52e+04 |
| time/              |           |
|    fps             | 141       |
|    iterations      | 535       |
|    time_elapsed    | 7717      |
|    total_timesteps | 1095680   |
----------------------------------
Eval num_timesteps=1096000, episode_reward=-52490.23 +/- 37607.10
Episode length: 56.80 +/- 22.58
------------------------------------------
| eval/                   |              |
|    mean action          | 0.30595306   |
|    mean velocity x      | -0.834       |
|    mean velocity y      | -1.71        |
|    mean velocity z      | 16.9         |
|    mean_ep_length       | 56.8         |
|    mean_reward          | -5.25e+04    |
| time/                   |              |
|    total_timesteps      | 1096000      |
| train/                  |              |
|    approx_kl            | 0.0013550825 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.001        |
|    loss                 | 5.24e+07     |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.903        |
|    value_loss           | 1.91e+08     |
------------------------------------------
Eval num_timesteps=1096500, episode_reward=-43125.39 +/- 44379.14
Episode length: 60.60 +/- 46.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.23170504 |
|    mean velocity x | 1.01       |
|    mean velocity y | -0.0238    |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 60.6       |
|    mean_reward     | -4.31e+04  |
| time/              |            |
|    total_timesteps | 1096500    |
-----------------------------------
Eval num_timesteps=1097000, episode_reward=-52320.15 +/- 36623.10
Episode length: 55.20 +/- 15.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.22966643 |
|    mean velocity x | -1.58      |
|    mean velocity y | -2.09      |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 55.2       |
|    mean_reward     | -5.23e+04  |
| time/              |            |
|    total_timesteps | 1097000    |
-----------------------------------
Eval num_timesteps=1097500, episode_reward=-59270.60 +/- 31789.40
Episode length: 57.60 +/- 13.28
------------------------------------
| eval/              |             |
|    mean action     | -0.07939212 |
|    mean velocity x | 1.44        |
|    mean velocity y | 1.59        |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -5.93e+04   |
| time/              |             |
|    total_timesteps | 1097500     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.6     |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 142      |
|    iterations      | 536      |
|    time_elapsed    | 7724     |
|    total_timesteps | 1097728  |
---------------------------------
Eval num_timesteps=1098000, episode_reward=-93122.67 +/- 15626.21
Episode length: 63.60 +/- 3.77
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.31069937    |
|    mean velocity x      | -1.89         |
|    mean velocity y      | -3.1          |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 63.6          |
|    mean_reward          | -9.31e+04     |
| time/                   |               |
|    total_timesteps      | 1098000       |
| train/                  |               |
|    approx_kl            | 0.00016439945 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 1.4e+08       |
|    n_updates            | 5360          |
|    policy_gradient_loss | -0.000353     |
|    std                  | 0.903         |
|    value_loss           | 2.38e+08      |
-------------------------------------------
Eval num_timesteps=1098500, episode_reward=-86629.52 +/- 25664.07
Episode length: 67.20 +/- 9.93
------------------------------------
| eval/              |             |
|    mean action     | 0.006857254 |
|    mean velocity x | -0.694      |
|    mean velocity y | -0.439      |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 67.2        |
|    mean_reward     | -8.66e+04   |
| time/              |             |
|    total_timesteps | 1098500     |
------------------------------------
Eval num_timesteps=1099000, episode_reward=-73377.49 +/- 22324.92
Episode length: 61.60 +/- 7.86
----------------------------------
| eval/              |           |
|    mean action     | 0.2558274 |
|    mean velocity x | 1.26      |
|    mean velocity y | 1.02      |
|    mean velocity z | 15.6      |
|    mean_ep_length  | 61.6      |
|    mean_reward     | -7.34e+04 |
| time/              |           |
|    total_timesteps | 1099000   |
----------------------------------
Eval num_timesteps=1099500, episode_reward=-64537.22 +/- 36273.59
Episode length: 61.80 +/- 31.01
------------------------------------
| eval/              |             |
|    mean action     | 0.010302955 |
|    mean velocity x | 0.324       |
|    mean velocity y | 0.806       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -6.45e+04   |
| time/              |             |
|    total_timesteps | 1099500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.6      |
|    ep_rew_mean     | -7.58e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 537       |
|    time_elapsed    | 7732      |
|    total_timesteps | 1099776   |
----------------------------------
Eval num_timesteps=1100000, episode_reward=-80373.50 +/- 23194.05
Episode length: 61.60 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.17246409    |
|    mean velocity x      | -0.908        |
|    mean velocity y      | -1.84         |
|    mean velocity z      | 19            |
|    mean_ep_length       | 61.6          |
|    mean_reward          | -8.04e+04     |
| time/                   |               |
|    total_timesteps      | 1100000       |
| train/                  |               |
|    approx_kl            | 0.00019980778 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+08      |
|    n_updates            | 5370          |
|    policy_gradient_loss | -0.000385     |
|    std                  | 0.903         |
|    value_loss           | 2.2e+08       |
-------------------------------------------
Eval num_timesteps=1100500, episode_reward=-46917.44 +/- 28291.85
Episode length: 52.40 +/- 19.61
-----------------------------------
| eval/              |            |
|    mean action     | 0.21004593 |
|    mean velocity x | -1.78      |
|    mean velocity y | -1.88      |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 52.4       |
|    mean_reward     | -4.69e+04  |
| time/              |            |
|    total_timesteps | 1100500    |
-----------------------------------
Eval num_timesteps=1101000, episode_reward=-72287.87 +/- 16070.73
Episode length: 72.80 +/- 27.87
-----------------------------------
| eval/              |            |
|    mean action     | 0.05541499 |
|    mean velocity x | 1.48       |
|    mean velocity y | 0.74       |
|    mean velocity z | 19         |
|    mean_ep_length  | 72.8       |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 1101000    |
-----------------------------------
Eval num_timesteps=1101500, episode_reward=-67728.99 +/- 41355.51
Episode length: 49.20 +/- 18.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.19806589 |
|    mean velocity x | -0.986     |
|    mean velocity y | 0.127      |
|    mean velocity z | 15.4       |
|    mean_ep_length  | 49.2       |
|    mean_reward     | -6.77e+04  |
| time/              |            |
|    total_timesteps | 1101500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.7      |
|    ep_rew_mean     | -7.48e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 538       |
|    time_elapsed    | 7739      |
|    total_timesteps | 1101824   |
----------------------------------
Eval num_timesteps=1102000, episode_reward=-77803.81 +/- 31499.49
Episode length: 71.40 +/- 22.12
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.046087585   |
|    mean velocity x      | 1.04          |
|    mean velocity y      | 0.526         |
|    mean velocity z      | 18.8          |
|    mean_ep_length       | 71.4          |
|    mean_reward          | -7.78e+04     |
| time/                   |               |
|    total_timesteps      | 1102000       |
| train/                  |               |
|    approx_kl            | 0.00015582668 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.263         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 5380          |
|    policy_gradient_loss | -0.000461     |
|    std                  | 0.902         |
|    value_loss           | 1.91e+08      |
-------------------------------------------
Eval num_timesteps=1102500, episode_reward=-77977.13 +/- 42343.63
Episode length: 58.00 +/- 20.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.10503179 |
|    mean velocity x | 1.53       |
|    mean velocity y | 0.152      |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 58         |
|    mean_reward     | -7.8e+04   |
| time/              |            |
|    total_timesteps | 1102500    |
-----------------------------------
Eval num_timesteps=1103000, episode_reward=-82654.39 +/- 21248.12
Episode length: 63.40 +/- 6.47
-----------------------------------
| eval/              |            |
|    mean action     | 0.13413844 |
|    mean velocity x | 0.0913     |
|    mean velocity y | 0.294      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -8.27e+04  |
| time/              |            |
|    total_timesteps | 1103000    |
-----------------------------------
Eval num_timesteps=1103500, episode_reward=-57649.65 +/- 36385.92
Episode length: 52.40 +/- 23.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.05534627 |
|    mean velocity x | 1.41       |
|    mean velocity y | 1.18       |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 52.4       |
|    mean_reward     | -5.76e+04  |
| time/              |            |
|    total_timesteps | 1103500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.7      |
|    ep_rew_mean     | -7.44e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 539       |
|    time_elapsed    | 7746      |
|    total_timesteps | 1103872   |
----------------------------------
Eval num_timesteps=1104000, episode_reward=-75978.05 +/- 10768.69
Episode length: 70.80 +/- 10.53
------------------------------------------
| eval/                   |              |
|    mean action          | 0.13563909   |
|    mean velocity x      | -0.858       |
|    mean velocity y      | -1.47        |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 70.8         |
|    mean_reward          | -7.6e+04     |
| time/                   |              |
|    total_timesteps      | 1104000      |
| train/                  |              |
|    approx_kl            | 0.0008092978 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.001        |
|    loss                 | 9.7e+07      |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.000997    |
|    std                  | 0.902        |
|    value_loss           | 2.25e+08     |
------------------------------------------
Eval num_timesteps=1104500, episode_reward=-61884.50 +/- 30912.38
Episode length: 67.60 +/- 30.06
-----------------------------------
| eval/              |            |
|    mean action     | -0.5901618 |
|    mean velocity x | 1.3        |
|    mean velocity y | 3.19       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 67.6       |
|    mean_reward     | -6.19e+04  |
| time/              |            |
|    total_timesteps | 1104500    |
-----------------------------------
Eval num_timesteps=1105000, episode_reward=-71160.54 +/- 20166.78
Episode length: 67.60 +/- 11.76
------------------------------------
| eval/              |             |
|    mean action     | -0.22039422 |
|    mean velocity x | 0.0507      |
|    mean velocity y | 1.25        |
|    mean velocity z | 18          |
|    mean_ep_length  | 67.6        |
|    mean_reward     | -7.12e+04   |
| time/              |             |
|    total_timesteps | 1105000     |
------------------------------------
Eval num_timesteps=1105500, episode_reward=-86002.71 +/- 20051.09
Episode length: 73.80 +/- 18.73
-------------------------------------
| eval/              |              |
|    mean action     | -0.016981171 |
|    mean velocity x | -1.68        |
|    mean velocity y | -0.781       |
|    mean velocity z | 19.6         |
|    mean_ep_length  | 73.8         |
|    mean_reward     | -8.6e+04     |
| time/              |              |
|    total_timesteps | 1105500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.9      |
|    ep_rew_mean     | -7.35e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 540       |
|    time_elapsed    | 7754      |
|    total_timesteps | 1105920   |
----------------------------------
Eval num_timesteps=1106000, episode_reward=-69463.97 +/- 18052.05
Episode length: 63.00 +/- 9.70
------------------------------------------
| eval/                   |              |
|    mean action          | -0.7765187   |
|    mean velocity x      | 1.92         |
|    mean velocity y      | 4.45         |
|    mean velocity z      | 18.6         |
|    mean_ep_length       | 63           |
|    mean_reward          | -6.95e+04    |
| time/                   |              |
|    total_timesteps      | 1106000      |
| train/                  |              |
|    approx_kl            | 6.196121e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.001        |
|    loss                 | 9.29e+07     |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.000428    |
|    std                  | 0.902        |
|    value_loss           | 1.53e+08     |
------------------------------------------
Eval num_timesteps=1106500, episode_reward=-68345.50 +/- 33267.54
Episode length: 53.40 +/- 19.13
------------------------------------
| eval/              |             |
|    mean action     | -0.03701358 |
|    mean velocity x | 1.16        |
|    mean velocity y | 0.0317      |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 53.4        |
|    mean_reward     | -6.83e+04   |
| time/              |             |
|    total_timesteps | 1106500     |
------------------------------------
Eval num_timesteps=1107000, episode_reward=-58189.28 +/- 37583.29
Episode length: 57.80 +/- 29.65
-----------------------------------
| eval/              |            |
|    mean action     | -0.3037363 |
|    mean velocity x | 1.7        |
|    mean velocity y | 2.1        |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -5.82e+04  |
| time/              |            |
|    total_timesteps | 1107000    |
-----------------------------------
Eval num_timesteps=1107500, episode_reward=-71259.54 +/- 38479.59
Episode length: 57.20 +/- 16.17
------------------------------------
| eval/              |             |
|    mean action     | -0.36940864 |
|    mean velocity x | 0.062       |
|    mean velocity y | 1.04        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 1107500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67        |
|    ep_rew_mean     | -7.36e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 541       |
|    time_elapsed    | 7761      |
|    total_timesteps | 1107968   |
----------------------------------
Eval num_timesteps=1108000, episode_reward=-74292.40 +/- 22909.14
Episode length: 64.80 +/- 3.92
------------------------------------------
| eval/                   |              |
|    mean action          | -0.10460466  |
|    mean velocity x      | 2.43         |
|    mean velocity y      | 1.23         |
|    mean velocity z      | 19.9         |
|    mean_ep_length       | 64.8         |
|    mean_reward          | -7.43e+04    |
| time/                   |              |
|    total_timesteps      | 1108000      |
| train/                  |              |
|    approx_kl            | 0.0004342201 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+08     |
|    n_updates            | 5410         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 0.902        |
|    value_loss           | 2.2e+08      |
------------------------------------------
Eval num_timesteps=1108500, episode_reward=-94542.74 +/- 14209.31
Episode length: 61.40 +/- 2.06
-----------------------------------
| eval/              |            |
|    mean action     | 0.21966024 |
|    mean velocity x | -1.44      |
|    mean velocity y | -0.832     |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 61.4       |
|    mean_reward     | -9.45e+04  |
| time/              |            |
|    total_timesteps | 1108500    |
-----------------------------------
Eval num_timesteps=1109000, episode_reward=-86351.33 +/- 16652.59
Episode length: 68.20 +/- 14.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.14543007 |
|    mean velocity x | -1.59      |
|    mean velocity y | -0.983     |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -8.64e+04  |
| time/              |            |
|    total_timesteps | 1109000    |
-----------------------------------
Eval num_timesteps=1109500, episode_reward=-71956.60 +/- 31539.43
Episode length: 60.80 +/- 3.87
-----------------------------------
| eval/              |            |
|    mean action     | 0.20476767 |
|    mean velocity x | -1.48      |
|    mean velocity y | -1.88      |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 60.8       |
|    mean_reward     | -7.2e+04   |
| time/              |            |
|    total_timesteps | 1109500    |
-----------------------------------
Eval num_timesteps=1110000, episode_reward=-78192.70 +/- 6887.73
Episode length: 72.40 +/- 14.11
------------------------------------
| eval/              |             |
|    mean action     | -0.20205985 |
|    mean velocity x | 1.69        |
|    mean velocity y | 2.33        |
|    mean velocity z | 19          |
|    mean_ep_length  | 72.4        |
|    mean_reward     | -7.82e+04   |
| time/              |             |
|    total_timesteps | 1110000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.4      |
|    ep_rew_mean     | -7.42e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 542       |
|    time_elapsed    | 7769      |
|    total_timesteps | 1110016   |
----------------------------------
Eval num_timesteps=1110500, episode_reward=-73351.07 +/- 33652.58
Episode length: 57.20 +/- 9.13
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4409334    |
|    mean velocity x      | 1.66          |
|    mean velocity y      | 1.79          |
|    mean velocity z      | 21.6          |
|    mean_ep_length       | 57.2          |
|    mean_reward          | -7.34e+04     |
| time/                   |               |
|    total_timesteps      | 1110500       |
| train/                  |               |
|    approx_kl            | 3.2420212e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.001         |
|    loss                 | 8.39e+07      |
|    n_updates            | 5420          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 0.902         |
|    value_loss           | 2.03e+08      |
-------------------------------------------
Eval num_timesteps=1111000, episode_reward=-76782.68 +/- 30104.20
Episode length: 58.20 +/- 13.73
-----------------------------------
| eval/              |            |
|    mean action     | -0.6301795 |
|    mean velocity x | 1.62       |
|    mean velocity y | 3.74       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 58.2       |
|    mean_reward     | -7.68e+04  |
| time/              |            |
|    total_timesteps | 1111000    |
-----------------------------------
Eval num_timesteps=1111500, episode_reward=-101718.88 +/- 15005.46
Episode length: 65.20 +/- 4.75
-----------------------------------
| eval/              |            |
|    mean action     | -0.6683847 |
|    mean velocity x | 3.63       |
|    mean velocity y | 5.54       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 65.2       |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 1111500    |
-----------------------------------
Eval num_timesteps=1112000, episode_reward=-85493.86 +/- 29807.87
Episode length: 69.40 +/- 19.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.30986762 |
|    mean velocity x | -2.07      |
|    mean velocity y | -2.24      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 69.4       |
|    mean_reward     | -8.55e+04  |
| time/              |            |
|    total_timesteps | 1112000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.8      |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 543       |
|    time_elapsed    | 7776      |
|    total_timesteps | 1112064   |
----------------------------------
Eval num_timesteps=1112500, episode_reward=-59567.22 +/- 45459.93
Episode length: 46.80 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean action          | 0.24596047   |
|    mean velocity x      | -1.02        |
|    mean velocity y      | -2.02        |
|    mean velocity z      | 19.5         |
|    mean_ep_length       | 46.8         |
|    mean_reward          | -5.96e+04    |
| time/                   |              |
|    total_timesteps      | 1112500      |
| train/                  |              |
|    approx_kl            | 2.851081e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 5430         |
|    policy_gradient_loss | -0.000418    |
|    std                  | 0.902        |
|    value_loss           | 2.18e+08     |
------------------------------------------
Eval num_timesteps=1113000, episode_reward=-86358.86 +/- 13872.70
Episode length: 69.00 +/- 15.13
------------------------------------
| eval/              |             |
|    mean action     | 0.005417084 |
|    mean velocity x | 0.276       |
|    mean velocity y | 0.121       |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 69          |
|    mean_reward     | -8.64e+04   |
| time/              |             |
|    total_timesteps | 1113000     |
------------------------------------
Eval num_timesteps=1113500, episode_reward=-51663.97 +/- 24055.64
Episode length: 51.60 +/- 9.13
-------------------------------------
| eval/              |              |
|    mean action     | -0.005157257 |
|    mean velocity x | 0.539        |
|    mean velocity y | 0.946        |
|    mean velocity z | 21           |
|    mean_ep_length  | 51.6         |
|    mean_reward     | -5.17e+04    |
| time/              |              |
|    total_timesteps | 1113500      |
-------------------------------------
Eval num_timesteps=1114000, episode_reward=-75638.72 +/- 38877.61
Episode length: 55.00 +/- 20.19
------------------------------------
| eval/              |             |
|    mean action     | -0.34466845 |
|    mean velocity x | 2.44        |
|    mean velocity y | 2.77        |
|    mean velocity z | 21.7        |
|    mean_ep_length  | 55          |
|    mean_reward     | -7.56e+04   |
| time/              |             |
|    total_timesteps | 1114000     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.3     |
|    ep_rew_mean     | -8.2e+04 |
| time/              |          |
|    fps             | 143      |
|    iterations      | 544      |
|    time_elapsed    | 7783     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1114500, episode_reward=-59507.22 +/- 39422.56
Episode length: 51.40 +/- 12.09
------------------------------------------
| eval/                   |              |
|    mean action          | 0.30730018   |
|    mean velocity x      | -2.49        |
|    mean velocity y      | -2.69        |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 51.4         |
|    mean_reward          | -5.95e+04    |
| time/                   |              |
|    total_timesteps      | 1114500      |
| train/                  |              |
|    approx_kl            | 1.531298e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.227        |
|    learning_rate        | 0.001        |
|    loss                 | 7.59e+07     |
|    n_updates            | 5440         |
|    policy_gradient_loss | -0.00027     |
|    std                  | 0.902        |
|    value_loss           | 2.34e+08     |
------------------------------------------
Eval num_timesteps=1115000, episode_reward=-68409.13 +/- 28687.07
Episode length: 57.60 +/- 14.37
-----------------------------------
| eval/              |            |
|    mean action     | -0.9232423 |
|    mean velocity x | 3.5        |
|    mean velocity y | 6          |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -6.84e+04  |
| time/              |            |
|    total_timesteps | 1115000    |
-----------------------------------
Eval num_timesteps=1115500, episode_reward=-67693.89 +/- 36418.78
Episode length: 55.80 +/- 23.39
-----------------------------------
| eval/              |            |
|    mean action     | -0.2807805 |
|    mean velocity x | 1.87       |
|    mean velocity y | 2.39       |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 55.8       |
|    mean_reward     | -6.77e+04  |
| time/              |            |
|    total_timesteps | 1115500    |
-----------------------------------
Eval num_timesteps=1116000, episode_reward=-59228.54 +/- 37183.84
Episode length: 59.80 +/- 22.78
------------------------------------
| eval/              |             |
|    mean action     | -0.79470056 |
|    mean velocity x | 1.78        |
|    mean velocity y | 4.51        |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -5.92e+04   |
| time/              |             |
|    total_timesteps | 1116000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.7      |
|    ep_rew_mean     | -8.62e+04 |
| time/              |           |
|    fps             | 142       |
|    iterations      | 545       |
|    time_elapsed    | 7805      |
|    total_timesteps | 1116160   |
----------------------------------
Eval num_timesteps=1116500, episode_reward=-49034.84 +/- 34341.55
Episode length: 48.60 +/- 23.94
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.2243116     |
|    mean velocity x      | -0.962        |
|    mean velocity y      | -1.45         |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -4.9e+04      |
| time/                   |               |
|    total_timesteps      | 1116500       |
| train/                  |               |
|    approx_kl            | 0.00050367054 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.001         |
|    loss                 | 8.99e+07      |
|    n_updates            | 5450          |
|    policy_gradient_loss | -0.000958     |
|    std                  | 0.903         |
|    value_loss           | 2.17e+08      |
-------------------------------------------
Eval num_timesteps=1117000, episode_reward=-28869.38 +/- 28991.00
Episode length: 45.40 +/- 11.48
------------------------------------
| eval/              |             |
|    mean action     | -0.20342074 |
|    mean velocity x | 1.61        |
|    mean velocity y | 2.27        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 45.4        |
|    mean_reward     | -2.89e+04   |
| time/              |             |
|    total_timesteps | 1117000     |
------------------------------------
Eval num_timesteps=1117500, episode_reward=-61683.27 +/- 32895.37
Episode length: 62.80 +/- 20.74
-------------------------------------
| eval/              |              |
|    mean action     | -0.037145812 |
|    mean velocity x | 1.1          |
|    mean velocity y | 1.11         |
|    mean velocity z | 17.2         |
|    mean_ep_length  | 62.8         |
|    mean_reward     | -6.17e+04    |
| time/              |              |
|    total_timesteps | 1117500      |
-------------------------------------
Eval num_timesteps=1118000, episode_reward=-75618.79 +/- 23358.21
Episode length: 62.20 +/- 14.58
-------------------------------------
| eval/              |              |
|    mean action     | -0.071158476 |
|    mean velocity x | 1.87         |
|    mean velocity y | 1.83         |
|    mean velocity z | 19.3         |
|    mean_ep_length  | 62.2         |
|    mean_reward     | -7.56e+04    |
| time/              |              |
|    total_timesteps | 1118000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.8      |
|    ep_rew_mean     | -8.49e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 546       |
|    time_elapsed    | 7812      |
|    total_timesteps | 1118208   |
----------------------------------
Eval num_timesteps=1118500, episode_reward=-72207.17 +/- 35321.61
Episode length: 60.40 +/- 11.84
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.4282527    |
|    mean velocity x      | 0.218         |
|    mean velocity y      | 2.24          |
|    mean velocity z      | 19.4          |
|    mean_ep_length       | 60.4          |
|    mean_reward          | -7.22e+04     |
| time/                   |               |
|    total_timesteps      | 1118500       |
| train/                  |               |
|    approx_kl            | 0.00022521714 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.291         |
|    learning_rate        | 0.001         |
|    loss                 | 9.59e+07      |
|    n_updates            | 5460          |
|    policy_gradient_loss | -0.000496     |
|    std                  | 0.903         |
|    value_loss           | 1.94e+08      |
-------------------------------------------
Eval num_timesteps=1119000, episode_reward=-62180.72 +/- 40002.70
Episode length: 50.60 +/- 19.73
-----------------------------------
| eval/              |            |
|    mean action     | 0.11768544 |
|    mean velocity x | 2.45       |
|    mean velocity y | -0.455     |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 50.6       |
|    mean_reward     | -6.22e+04  |
| time/              |            |
|    total_timesteps | 1119000    |
-----------------------------------
Eval num_timesteps=1119500, episode_reward=-81275.81 +/- 25593.59
Episode length: 59.60 +/- 5.24
------------------------------------
| eval/              |             |
|    mean action     | -0.15859173 |
|    mean velocity x | -1.28       |
|    mean velocity y | -0.427      |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 1119500     |
------------------------------------
Eval num_timesteps=1120000, episode_reward=-46214.07 +/- 42191.11
Episode length: 43.20 +/- 24.80
------------------------------------
| eval/              |             |
|    mean action     | -0.28629285 |
|    mean velocity x | 1.87        |
|    mean velocity y | 1.48        |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 43.2        |
|    mean_reward     | -4.62e+04   |
| time/              |             |
|    total_timesteps | 1120000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75        |
|    ep_rew_mean     | -8.27e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 547       |
|    time_elapsed    | 7819      |
|    total_timesteps | 1120256   |
----------------------------------
Eval num_timesteps=1120500, episode_reward=-89872.12 +/- 13437.49
Episode length: 66.20 +/- 5.49
------------------------------------------
| eval/                   |              |
|    mean action          | -0.34615916  |
|    mean velocity x      | 1.5          |
|    mean velocity y      | 1.75         |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 66.2         |
|    mean_reward          | -8.99e+04    |
| time/                   |              |
|    total_timesteps      | 1120500      |
| train/                  |              |
|    approx_kl            | 8.152309e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.293        |
|    learning_rate        | 0.001        |
|    loss                 | 7.13e+07     |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.000518    |
|    std                  | 0.903        |
|    value_loss           | 1.67e+08     |
------------------------------------------
Eval num_timesteps=1121000, episode_reward=-96477.39 +/- 18370.04
Episode length: 70.40 +/- 14.97
-----------------------------------
| eval/              |            |
|    mean action     | 0.50258243 |
|    mean velocity x | -3.15      |
|    mean velocity y | -3.49      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 70.4       |
|    mean_reward     | -9.65e+04  |
| time/              |            |
|    total_timesteps | 1121000    |
-----------------------------------
Eval num_timesteps=1121500, episode_reward=-15596.25 +/- 24759.10
Episode length: 25.80 +/- 18.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.19634739 |
|    mean velocity x | -2.13      |
|    mean velocity y | -1.99      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 25.8       |
|    mean_reward     | -1.56e+04  |
| time/              |            |
|    total_timesteps | 1121500    |
-----------------------------------
New best mean reward!
Eval num_timesteps=1122000, episode_reward=-82260.26 +/- 37614.74
Episode length: 60.60 +/- 21.23
------------------------------------
| eval/              |             |
|    mean action     | -0.25239527 |
|    mean velocity x | 0.213       |
|    mean velocity y | 2.59        |
|    mean velocity z | 20          |
|    mean_ep_length  | 60.6        |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1122000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.1      |
|    ep_rew_mean     | -8.14e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 548       |
|    time_elapsed    | 7826      |
|    total_timesteps | 1122304   |
----------------------------------
Eval num_timesteps=1122500, episode_reward=-67128.12 +/- 35115.66
Episode length: 55.80 +/- 21.10
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.1135712  |
|    mean velocity x      | 1.02        |
|    mean velocity y      | 1.04        |
|    mean velocity z      | 20.5        |
|    mean_ep_length       | 55.8        |
|    mean_reward          | -6.71e+04   |
| time/                   |             |
|    total_timesteps      | 1122500     |
| train/                  |             |
|    approx_kl            | 0.000420725 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.275       |
|    learning_rate        | 0.001       |
|    loss                 | 1.02e+08    |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.000848   |
|    std                  | 0.902       |
|    value_loss           | 2.05e+08    |
-----------------------------------------
Eval num_timesteps=1123000, episode_reward=-58536.71 +/- 27052.78
Episode length: 73.00 +/- 24.91
------------------------------------
| eval/              |             |
|    mean action     | -0.26139325 |
|    mean velocity x | 0.955       |
|    mean velocity y | 1.73        |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 73          |
|    mean_reward     | -5.85e+04   |
| time/              |             |
|    total_timesteps | 1123000     |
------------------------------------
Eval num_timesteps=1123500, episode_reward=-79932.99 +/- 20863.33
Episode length: 67.40 +/- 7.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.42812672 |
|    mean velocity x | -1.82      |
|    mean velocity y | -3.28      |
|    mean velocity z | 20.9       |
|    mean_ep_length  | 67.4       |
|    mean_reward     | -7.99e+04  |
| time/              |            |
|    total_timesteps | 1123500    |
-----------------------------------
Eval num_timesteps=1124000, episode_reward=-77225.26 +/- 28661.09
Episode length: 58.40 +/- 8.19
------------------------------------
| eval/              |             |
|    mean action     | -0.35632655 |
|    mean velocity x | 1.22        |
|    mean velocity y | 1.81        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -7.72e+04   |
| time/              |             |
|    total_timesteps | 1124000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.67e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 549       |
|    time_elapsed    | 7833      |
|    total_timesteps | 1124352   |
----------------------------------
Eval num_timesteps=1124500, episode_reward=-63207.73 +/- 30238.80
Episode length: 54.40 +/- 10.15
------------------------------------------
| eval/                   |              |
|    mean action          | -0.043397017 |
|    mean velocity x      | 0.231        |
|    mean velocity y      | 0.174        |
|    mean velocity z      | 17.2         |
|    mean_ep_length       | 54.4         |
|    mean_reward          | -6.32e+04    |
| time/                   |              |
|    total_timesteps      | 1124500      |
| train/                  |              |
|    approx_kl            | 0.0002544266 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.001        |
|    loss                 | 1.28e+08     |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.000491    |
|    std                  | 0.902        |
|    value_loss           | 1.95e+08     |
------------------------------------------
Eval num_timesteps=1125000, episode_reward=-70455.75 +/- 26576.02
Episode length: 65.20 +/- 10.93
------------------------------------
| eval/              |             |
|    mean action     | 0.016408553 |
|    mean velocity x | 0.144       |
|    mean velocity y | -0.396      |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 65.2        |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 1125000     |
------------------------------------
Eval num_timesteps=1125500, episode_reward=-64602.69 +/- 17949.23
Episode length: 62.20 +/- 10.93
-----------------------------------
| eval/              |            |
|    mean action     | 0.25779584 |
|    mean velocity x | -1.43      |
|    mean velocity y | -1.21      |
|    mean velocity z | 21         |
|    mean_ep_length  | 62.2       |
|    mean_reward     | -6.46e+04  |
| time/              |            |
|    total_timesteps | 1125500    |
-----------------------------------
Eval num_timesteps=1126000, episode_reward=-69251.32 +/- 24983.12
Episode length: 64.40 +/- 14.12
------------------------------------
| eval/              |             |
|    mean action     | -0.31518093 |
|    mean velocity x | 0.355       |
|    mean velocity y | 2.37        |
|    mean velocity z | 22.9        |
|    mean_ep_length  | 64.4        |
|    mean_reward     | -6.93e+04   |
| time/              |             |
|    total_timesteps | 1126000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.6      |
|    ep_rew_mean     | -7.89e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 550       |
|    time_elapsed    | 7841      |
|    total_timesteps | 1126400   |
----------------------------------
Eval num_timesteps=1126500, episode_reward=-53193.18 +/- 27944.07
Episode length: 66.60 +/- 9.79
------------------------------------------
| eval/                   |              |
|    mean action          | 0.8735588    |
|    mean velocity x      | -3.72        |
|    mean velocity y      | -4.23        |
|    mean velocity z      | 14           |
|    mean_ep_length       | 66.6         |
|    mean_reward          | -5.32e+04    |
| time/                   |              |
|    total_timesteps      | 1126500      |
| train/                  |              |
|    approx_kl            | 0.0007651806 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.001        |
|    loss                 | 8.64e+07     |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.902        |
|    value_loss           | 2.03e+08     |
------------------------------------------
Eval num_timesteps=1127000, episode_reward=-80090.50 +/- 17410.39
Episode length: 72.60 +/- 15.64
------------------------------------
| eval/              |             |
|    mean action     | -0.14155579 |
|    mean velocity x | -0.122      |
|    mean velocity y | -0.0771     |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 72.6        |
|    mean_reward     | -8.01e+04   |
| time/              |             |
|    total_timesteps | 1127000     |
------------------------------------
Eval num_timesteps=1127500, episode_reward=-80030.46 +/- 32134.37
Episode length: 58.60 +/- 8.89
-------------------------------------
| eval/              |              |
|    mean action     | -0.028322075 |
|    mean velocity x | 0.799        |
|    mean velocity y | 0.307        |
|    mean velocity z | 20           |
|    mean_ep_length  | 58.6         |
|    mean_reward     | -8e+04       |
| time/              |              |
|    total_timesteps | 1127500      |
-------------------------------------
Eval num_timesteps=1128000, episode_reward=-86157.24 +/- 35709.39
Episode length: 57.40 +/- 9.81
--------------------------------------
| eval/              |               |
|    mean action     | -0.0061195833 |
|    mean velocity x | 2.5           |
|    mean velocity y | 2.19          |
|    mean velocity z | 17.3          |
|    mean_ep_length  | 57.4          |
|    mean_reward     | -8.62e+04     |
| time/              |               |
|    total_timesteps | 1128000       |
--------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.3      |
|    ep_rew_mean     | -7.61e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 551       |
|    time_elapsed    | 7848      |
|    total_timesteps | 1128448   |
----------------------------------
Eval num_timesteps=1128500, episode_reward=-68003.48 +/- 24410.67
Episode length: 58.00 +/- 3.16
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.07415373   |
|    mean velocity x      | -1.3          |
|    mean velocity y      | 0.698         |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 58            |
|    mean_reward          | -6.8e+04      |
| time/                   |               |
|    total_timesteps      | 1128500       |
| train/                  |               |
|    approx_kl            | 5.5831246e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.275         |
|    learning_rate        | 0.001         |
|    loss                 | 8.34e+07      |
|    n_updates            | 5510          |
|    policy_gradient_loss | -0.00027      |
|    std                  | 0.902         |
|    value_loss           | 1.99e+08      |
-------------------------------------------
Eval num_timesteps=1129000, episode_reward=-65578.59 +/- 31012.03
Episode length: 59.00 +/- 13.56
-----------------------------------
| eval/              |            |
|    mean action     | -0.1997858 |
|    mean velocity x | -0.197     |
|    mean velocity y | 1.26       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 59         |
|    mean_reward     | -6.56e+04  |
| time/              |            |
|    total_timesteps | 1129000    |
-----------------------------------
Eval num_timesteps=1129500, episode_reward=-68224.61 +/- 29678.77
Episode length: 62.40 +/- 13.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.19638385 |
|    mean velocity x | 0.979      |
|    mean velocity y | -0.312     |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -6.82e+04  |
| time/              |            |
|    total_timesteps | 1129500    |
-----------------------------------
Eval num_timesteps=1130000, episode_reward=-52378.26 +/- 29404.87
Episode length: 52.40 +/- 19.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.20335425 |
|    mean velocity x | -0.204     |
|    mean velocity y | -0.855     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 52.4       |
|    mean_reward     | -5.24e+04  |
| time/              |            |
|    total_timesteps | 1130000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.6      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 143       |
|    iterations      | 552       |
|    time_elapsed    | 7855      |
|    total_timesteps | 1130496   |
----------------------------------
Eval num_timesteps=1130500, episode_reward=-65011.48 +/- 27105.68
Episode length: 67.80 +/- 18.29
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.17042325    |
|    mean velocity x      | 0.975         |
|    mean velocity y      | -0.292        |
|    mean velocity z      | 18            |
|    mean_ep_length       | 67.8          |
|    mean_reward          | -6.5e+04      |
| time/                   |               |
|    total_timesteps      | 1130500       |
| train/                  |               |
|    approx_kl            | 1.9283732e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+08      |
|    n_updates            | 5520          |
|    policy_gradient_loss | -0.000151     |
|    std                  | 0.902         |
|    value_loss           | 2.01e+08      |
-------------------------------------------
Eval num_timesteps=1131000, episode_reward=-80586.64 +/- 38783.89
Episode length: 56.00 +/- 17.47
-----------------------------------
| eval/              |            |
|    mean action     | -0.4740729 |
|    mean velocity x | 1.13       |
|    mean velocity y | 2.63       |
|    mean velocity z | 22         |
|    mean_ep_length  | 56         |
|    mean_reward     | -8.06e+04  |
| time/              |            |
|    total_timesteps | 1131000    |
-----------------------------------
Eval num_timesteps=1131500, episode_reward=-86368.26 +/- 16998.48
Episode length: 61.60 +/- 4.84
-----------------------------------
| eval/              |            |
|    mean action     | 0.15018936 |
|    mean velocity x | 0.0122     |
|    mean velocity y | 0.392      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -8.64e+04  |
| time/              |            |
|    total_timesteps | 1131500    |
-----------------------------------
Eval num_timesteps=1132000, episode_reward=-64343.58 +/- 32265.79
Episode length: 52.60 +/- 9.39
------------------------------------
| eval/              |             |
|    mean action     | 0.097465694 |
|    mean velocity x | 0.813       |
|    mean velocity y | -1.68       |
|    mean velocity z | 20          |
|    mean_ep_length  | 52.6        |
|    mean_reward     | -6.43e+04   |
| time/              |             |
|    total_timesteps | 1132000     |
------------------------------------
Eval num_timesteps=1132500, episode_reward=-74900.76 +/- 21335.97
Episode length: 67.60 +/- 11.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.16248241 |
|    mean velocity x | -0.721     |
|    mean velocity y | -1.23      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 67.6       |
|    mean_reward     | -7.49e+04  |
| time/              |            |
|    total_timesteps | 1132500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.7      |
|    ep_rew_mean     | -7.28e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 553       |
|    time_elapsed    | 7863      |
|    total_timesteps | 1132544   |
----------------------------------
Eval num_timesteps=1133000, episode_reward=-81615.33 +/- 22996.43
Episode length: 60.60 +/- 6.56
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.48625746   |
|    mean velocity x      | 1.54          |
|    mean velocity y      | 2.4           |
|    mean velocity z      | 18.7          |
|    mean_ep_length       | 60.6          |
|    mean_reward          | -8.16e+04     |
| time/                   |               |
|    total_timesteps      | 1133000       |
| train/                  |               |
|    approx_kl            | 0.00013622761 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.001         |
|    loss                 | 9.84e+07      |
|    n_updates            | 5530          |
|    policy_gradient_loss | -0.000584     |
|    std                  | 0.902         |
|    value_loss           | 2.09e+08      |
-------------------------------------------
Eval num_timesteps=1133500, episode_reward=-82304.44 +/- 27369.07
Episode length: 58.20 +/- 6.91
-----------------------------------
| eval/              |            |
|    mean action     | -0.3314157 |
|    mean velocity x | 0.557      |
|    mean velocity y | 3.07       |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 58.2       |
|    mean_reward     | -8.23e+04  |
| time/              |            |
|    total_timesteps | 1133500    |
-----------------------------------
Eval num_timesteps=1134000, episode_reward=-76149.28 +/- 46890.04
Episode length: 68.80 +/- 41.72
-------------------------------------
| eval/              |              |
|    mean action     | -0.004111084 |
|    mean velocity x | -0.641       |
|    mean velocity y | -0.134       |
|    mean velocity z | 20.9         |
|    mean_ep_length  | 68.8         |
|    mean_reward     | -7.61e+04    |
| time/              |              |
|    total_timesteps | 1134000      |
-------------------------------------
Eval num_timesteps=1134500, episode_reward=-73993.40 +/- 45355.35
Episode length: 53.80 +/- 11.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.3067388 |
|    mean velocity x | 1.47       |
|    mean velocity y | 2.1        |
|    mean velocity z | 16         |
|    mean_ep_length  | 53.8       |
|    mean_reward     | -7.4e+04   |
| time/              |            |
|    total_timesteps | 1134500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.8      |
|    ep_rew_mean     | -7.33e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 554       |
|    time_elapsed    | 7870      |
|    total_timesteps | 1134592   |
----------------------------------
Eval num_timesteps=1135000, episode_reward=-62607.40 +/- 36143.77
Episode length: 53.60 +/- 9.89
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.003829193   |
|    mean velocity x      | 0.301         |
|    mean velocity y      | 0.603         |
|    mean velocity z      | 16.6          |
|    mean_ep_length       | 53.6          |
|    mean_reward          | -6.26e+04     |
| time/                   |               |
|    total_timesteps      | 1135000       |
| train/                  |               |
|    approx_kl            | 0.00034151803 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+08      |
|    n_updates            | 5540          |
|    policy_gradient_loss | -0.000584     |
|    std                  | 0.902         |
|    value_loss           | 1.83e+08      |
-------------------------------------------
Eval num_timesteps=1135500, episode_reward=-78912.81 +/- 24588.97
Episode length: 63.00 +/- 14.60
------------------------------------
| eval/              |             |
|    mean action     | -0.15491077 |
|    mean velocity x | -0.42       |
|    mean velocity y | 0.515       |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 63          |
|    mean_reward     | -7.89e+04   |
| time/              |             |
|    total_timesteps | 1135500     |
------------------------------------
Eval num_timesteps=1136000, episode_reward=-49055.01 +/- 38300.45
Episode length: 51.80 +/- 18.88
-------------------------------------
| eval/              |              |
|    mean action     | 0.0010101318 |
|    mean velocity x | -0.152       |
|    mean velocity y | -0.0428      |
|    mean velocity z | 18.3         |
|    mean_ep_length  | 51.8         |
|    mean_reward     | -4.91e+04    |
| time/              |              |
|    total_timesteps | 1136000      |
-------------------------------------
Eval num_timesteps=1136500, episode_reward=-82436.89 +/- 28572.05
Episode length: 61.80 +/- 6.21
-----------------------------------
| eval/              |            |
|    mean action     | 0.60761464 |
|    mean velocity x | -1.81      |
|    mean velocity y | -2.63      |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -8.24e+04  |
| time/              |            |
|    total_timesteps | 1136500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.4      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 555       |
|    time_elapsed    | 7877      |
|    total_timesteps | 1136640   |
----------------------------------
Eval num_timesteps=1137000, episode_reward=-64509.15 +/- 36298.83
Episode length: 54.40 +/- 14.31
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.14970511    |
|    mean velocity x      | -2.13         |
|    mean velocity y      | -1.96         |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 54.4          |
|    mean_reward          | -6.45e+04     |
| time/                   |               |
|    total_timesteps      | 1137000       |
| train/                  |               |
|    approx_kl            | 0.00012843849 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.264         |
|    learning_rate        | 0.001         |
|    loss                 | 7.88e+07      |
|    n_updates            | 5550          |
|    policy_gradient_loss | -0.000337     |
|    std                  | 0.902         |
|    value_loss           | 2.02e+08      |
-------------------------------------------
Eval num_timesteps=1137500, episode_reward=-89730.32 +/- 10133.87
Episode length: 64.00 +/- 5.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.19078018 |
|    mean velocity x | -0.311     |
|    mean velocity y | -1.77      |
|    mean velocity z | 16.1       |
|    mean_ep_length  | 64         |
|    mean_reward     | -8.97e+04  |
| time/              |            |
|    total_timesteps | 1137500    |
-----------------------------------
Eval num_timesteps=1138000, episode_reward=-56731.20 +/- 36390.63
Episode length: 61.80 +/- 31.46
------------------------------------
| eval/              |             |
|    mean action     | -0.36323163 |
|    mean velocity x | 2.58        |
|    mean velocity y | 3.14        |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -5.67e+04   |
| time/              |             |
|    total_timesteps | 1138000     |
------------------------------------
Eval num_timesteps=1138500, episode_reward=-77766.52 +/- 8451.34
Episode length: 83.60 +/- 35.87
-------------------------------------
| eval/              |              |
|    mean action     | -0.049677186 |
|    mean velocity x | -1.05        |
|    mean velocity y | 0.775        |
|    mean velocity z | 19.8         |
|    mean_ep_length  | 83.6         |
|    mean_reward     | -7.78e+04    |
| time/              |              |
|    total_timesteps | 1138500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.8      |
|    ep_rew_mean     | -7.16e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 556       |
|    time_elapsed    | 7885      |
|    total_timesteps | 1138688   |
----------------------------------
Eval num_timesteps=1139000, episode_reward=-56324.62 +/- 44121.37
Episode length: 54.20 +/- 23.25
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.15481684    |
|    mean velocity x      | -0.481        |
|    mean velocity y      | -1.3          |
|    mean velocity z      | 17.3          |
|    mean_ep_length       | 54.2          |
|    mean_reward          | -5.63e+04     |
| time/                   |               |
|    total_timesteps      | 1139000       |
| train/                  |               |
|    approx_kl            | 0.00020607465 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.299         |
|    learning_rate        | 0.001         |
|    loss                 | 7.82e+07      |
|    n_updates            | 5560          |
|    policy_gradient_loss | -0.000593     |
|    std                  | 0.902         |
|    value_loss           | 1.67e+08      |
-------------------------------------------
Eval num_timesteps=1139500, episode_reward=-40172.47 +/- 25440.44
Episode length: 51.00 +/- 23.38
------------------------------------
| eval/              |             |
|    mean action     | 0.058558878 |
|    mean velocity x | 1.73        |
|    mean velocity y | 1.31        |
|    mean velocity z | 16.8        |
|    mean_ep_length  | 51          |
|    mean_reward     | -4.02e+04   |
| time/              |             |
|    total_timesteps | 1139500     |
------------------------------------
Eval num_timesteps=1140000, episode_reward=-69138.73 +/- 10552.24
Episode length: 68.60 +/- 19.78
-------------------------------------
| eval/              |              |
|    mean action     | -0.087886624 |
|    mean velocity x | 0.37         |
|    mean velocity y | 1.2          |
|    mean velocity z | 19.6         |
|    mean_ep_length  | 68.6         |
|    mean_reward     | -6.91e+04    |
| time/              |              |
|    total_timesteps | 1140000      |
-------------------------------------
Eval num_timesteps=1140500, episode_reward=-76354.18 +/- 9022.78
Episode length: 79.00 +/- 21.66
------------------------------------
| eval/              |             |
|    mean action     | -0.27320835 |
|    mean velocity x | -0.0243     |
|    mean velocity y | 1.12        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 79          |
|    mean_reward     | -7.64e+04   |
| time/              |             |
|    total_timesteps | 1140500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.9      |
|    ep_rew_mean     | -7.07e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 557       |
|    time_elapsed    | 7892      |
|    total_timesteps | 1140736   |
----------------------------------
Eval num_timesteps=1141000, episode_reward=-67114.42 +/- 35722.69
Episode length: 61.00 +/- 11.33
------------------------------------------
| eval/                   |              |
|    mean action          | 0.12673908   |
|    mean velocity x      | -0.913       |
|    mean velocity y      | -0.525       |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 61           |
|    mean_reward          | -6.71e+04    |
| time/                   |              |
|    total_timesteps      | 1141000      |
| train/                  |              |
|    approx_kl            | 0.0006398191 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.286        |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+08     |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.902        |
|    value_loss           | 1.89e+08     |
------------------------------------------
Eval num_timesteps=1141500, episode_reward=-35238.08 +/- 30695.08
Episode length: 40.60 +/- 22.71
------------------------------------
| eval/              |             |
|    mean action     | 0.052629907 |
|    mean velocity x | 1.25        |
|    mean velocity y | 0.475       |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 40.6        |
|    mean_reward     | -3.52e+04   |
| time/              |             |
|    total_timesteps | 1141500     |
------------------------------------
Eval num_timesteps=1142000, episode_reward=-39629.12 +/- 36819.02
Episode length: 42.60 +/- 19.77
-----------------------------------
| eval/              |            |
|    mean action     | -0.2447281 |
|    mean velocity x | 0.796      |
|    mean velocity y | 0.824      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 42.6       |
|    mean_reward     | -3.96e+04  |
| time/              |            |
|    total_timesteps | 1142000    |
-----------------------------------
Eval num_timesteps=1142500, episode_reward=-67697.18 +/- 27820.16
Episode length: 67.80 +/- 11.44
-----------------------------------
| eval/              |            |
|    mean action     | -0.6163156 |
|    mean velocity x | 2.08       |
|    mean velocity y | 3.35       |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 67.8       |
|    mean_reward     | -6.77e+04  |
| time/              |            |
|    total_timesteps | 1142500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.9      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 558       |
|    time_elapsed    | 7899      |
|    total_timesteps | 1142784   |
----------------------------------
Eval num_timesteps=1143000, episode_reward=-73978.45 +/- 39691.81
Episode length: 52.60 +/- 18.21
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.62921005   |
|    mean velocity x      | 3.15          |
|    mean velocity y      | 4.16          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 52.6          |
|    mean_reward          | -7.4e+04      |
| time/                   |               |
|    total_timesteps      | 1143000       |
| train/                  |               |
|    approx_kl            | 2.4069042e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.001         |
|    loss                 | 1.19e+08      |
|    n_updates            | 5580          |
|    policy_gradient_loss | -0.000211     |
|    std                  | 0.902         |
|    value_loss           | 2.14e+08      |
-------------------------------------------
Eval num_timesteps=1143500, episode_reward=-73408.83 +/- 24178.87
Episode length: 59.60 +/- 8.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.15078965 |
|    mean velocity x | -0.562     |
|    mean velocity y | -1.16      |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -7.34e+04  |
| time/              |            |
|    total_timesteps | 1143500    |
-----------------------------------
Eval num_timesteps=1144000, episode_reward=-69114.54 +/- 37079.97
Episode length: 56.20 +/- 13.75
------------------------------------
| eval/              |             |
|    mean action     | -0.16572003 |
|    mean velocity x | 0.501       |
|    mean velocity y | 0.283       |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 56.2        |
|    mean_reward     | -6.91e+04   |
| time/              |             |
|    total_timesteps | 1144000     |
------------------------------------
Eval num_timesteps=1144500, episode_reward=-41646.48 +/- 16728.71
Episode length: 51.60 +/- 8.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.03782566 |
|    mean velocity x | 0.861      |
|    mean velocity y | 0.445      |
|    mean velocity z | 22.2       |
|    mean_ep_length  | 51.6       |
|    mean_reward     | -4.16e+04  |
| time/              |            |
|    total_timesteps | 1144500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.9      |
|    ep_rew_mean     | -7.65e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 559       |
|    time_elapsed    | 7906      |
|    total_timesteps | 1144832   |
----------------------------------
Eval num_timesteps=1145000, episode_reward=-65538.87 +/- 25170.38
Episode length: 62.60 +/- 14.08
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.03146179   |
|    mean velocity x      | -1.19         |
|    mean velocity y      | -1.52         |
|    mean velocity z      | 18.7          |
|    mean_ep_length       | 62.6          |
|    mean_reward          | -6.55e+04     |
| time/                   |               |
|    total_timesteps      | 1145000       |
| train/                  |               |
|    approx_kl            | 0.00040930294 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.95         |
|    explained_variance   | 0.264         |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+08      |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.000576     |
|    std                  | 0.902         |
|    value_loss           | 2.46e+08      |
-------------------------------------------
Eval num_timesteps=1145500, episode_reward=-50336.59 +/- 37291.10
Episode length: 54.00 +/- 22.60
-----------------------------------
| eval/              |            |
|    mean action     | 0.01364679 |
|    mean velocity x | -0.151     |
|    mean velocity y | 0.276      |
|    mean velocity z | 21.3       |
|    mean_ep_length  | 54         |
|    mean_reward     | -5.03e+04  |
| time/              |            |
|    total_timesteps | 1145500    |
-----------------------------------
Eval num_timesteps=1146000, episode_reward=-77745.85 +/- 36222.43
Episode length: 54.80 +/- 15.54
------------------------------------
| eval/              |             |
|    mean action     | -0.21267468 |
|    mean velocity x | 0.5         |
|    mean velocity y | 0.905       |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -7.77e+04   |
| time/              |             |
|    total_timesteps | 1146000     |
------------------------------------
Eval num_timesteps=1146500, episode_reward=-45105.99 +/- 15582.61
Episode length: 58.00 +/- 13.91
------------------------------------
| eval/              |             |
|    mean action     | -0.34119773 |
|    mean velocity x | 0.924       |
|    mean velocity y | 2.01        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 58          |
|    mean_reward     | -4.51e+04   |
| time/              |             |
|    total_timesteps | 1146500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.3      |
|    ep_rew_mean     | -7.96e+04 |
| time/              |           |
|    fps             | 144       |
|    iterations      | 560       |
|    time_elapsed    | 7914      |
|    total_timesteps | 1146880   |
----------------------------------
Eval num_timesteps=1147000, episode_reward=-62131.79 +/- 38307.96
Episode length: 61.40 +/- 26.24
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.61429924    |
|    mean velocity x      | -3.15         |
|    mean velocity y      | -4.33         |
|    mean velocity z      | 18.7          |
|    mean_ep_length       | 61.4          |
|    mean_reward          | -6.21e+04     |
| time/                   |               |
|    total_timesteps      | 1147000       |
| train/                  |               |
|    approx_kl            | 0.00025322434 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+08      |
|    n_updates            | 5600          |
|    policy_gradient_loss | -0.000724     |
|    std                  | 0.901         |
|    value_loss           | 2.09e+08      |
-------------------------------------------
Eval num_timesteps=1147500, episode_reward=-57581.43 +/- 46888.67
Episode length: 48.80 +/- 32.67
------------------------------------
| eval/              |             |
|    mean action     | -0.09188952 |
|    mean velocity x | -0.85       |
|    mean velocity y | -1.08       |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -5.76e+04   |
| time/              |             |
|    total_timesteps | 1147500     |
------------------------------------
Eval num_timesteps=1148000, episode_reward=-71131.37 +/- 21088.69
Episode length: 61.20 +/- 9.62
-----------------------------------
| eval/              |            |
|    mean action     | 0.24634477 |
|    mean velocity x | -2         |
|    mean velocity y | -1.97      |
|    mean velocity z | 20.2       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -7.11e+04  |
| time/              |            |
|    total_timesteps | 1148000    |
-----------------------------------
Eval num_timesteps=1148500, episode_reward=-61133.79 +/- 28751.51
Episode length: 64.80 +/- 22.85
------------------------------------
| eval/              |             |
|    mean action     | -0.40020454 |
|    mean velocity x | 2.25        |
|    mean velocity y | 1.93        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -6.11e+04   |
| time/              |             |
|    total_timesteps | 1148500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.7      |
|    ep_rew_mean     | -7.96e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 561       |
|    time_elapsed    | 7921      |
|    total_timesteps | 1148928   |
----------------------------------
Eval num_timesteps=1149000, episode_reward=-47618.66 +/- 38518.56
Episode length: 56.80 +/- 27.21
------------------------------------------
| eval/                   |              |
|    mean action          | 0.6248014    |
|    mean velocity x      | -0.663       |
|    mean velocity y      | -3.91        |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 56.8         |
|    mean_reward          | -4.76e+04    |
| time/                   |              |
|    total_timesteps      | 1149000      |
| train/                  |              |
|    approx_kl            | 0.0026347311 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.275        |
|    learning_rate        | 0.001        |
|    loss                 | 1.11e+08     |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 0.901        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=1149500, episode_reward=-95458.60 +/- 17216.88
Episode length: 63.80 +/- 4.17
------------------------------------
| eval/              |             |
|    mean action     | -0.47844008 |
|    mean velocity x | 0.872       |
|    mean velocity y | 2.68        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -9.55e+04   |
| time/              |             |
|    total_timesteps | 1149500     |
------------------------------------
Eval num_timesteps=1150000, episode_reward=-76594.18 +/- 35015.21
Episode length: 56.80 +/- 10.65
-----------------------------------
| eval/              |            |
|    mean action     | 0.42748046 |
|    mean velocity x | -0.464     |
|    mean velocity y | -2.42      |
|    mean velocity z | 21.6       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -7.66e+04  |
| time/              |            |
|    total_timesteps | 1150000    |
-----------------------------------
Eval num_timesteps=1150500, episode_reward=-92395.63 +/- 17975.75
Episode length: 70.80 +/- 6.65
-----------------------------------
| eval/              |            |
|    mean action     | 0.26988685 |
|    mean velocity x | -1.43      |
|    mean velocity y | -1.85      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 70.8       |
|    mean_reward     | -9.24e+04  |
| time/              |            |
|    total_timesteps | 1150500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.5      |
|    ep_rew_mean     | -8.64e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 562       |
|    time_elapsed    | 7928      |
|    total_timesteps | 1150976   |
----------------------------------
Eval num_timesteps=1151000, episode_reward=-85344.97 +/- 19985.37
Episode length: 61.40 +/- 5.57
------------------------------------------
| eval/                   |              |
|    mean action          | 0.32820785   |
|    mean velocity x      | -1.32        |
|    mean velocity y      | -2.91        |
|    mean velocity z      | 22.2         |
|    mean_ep_length       | 61.4         |
|    mean_reward          | -8.53e+04    |
| time/                   |              |
|    total_timesteps      | 1151000      |
| train/                  |              |
|    approx_kl            | 0.0010423658 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.001        |
|    loss                 | 9.61e+07     |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 0.901        |
|    value_loss           | 2.46e+08     |
------------------------------------------
Eval num_timesteps=1151500, episode_reward=-53063.24 +/- 37312.46
Episode length: 47.00 +/- 15.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.13718368 |
|    mean velocity x | -0.733     |
|    mean velocity y | -0.526     |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 47         |
|    mean_reward     | -5.31e+04  |
| time/              |            |
|    total_timesteps | 1151500    |
-----------------------------------
Eval num_timesteps=1152000, episode_reward=-59994.88 +/- 15848.39
Episode length: 61.60 +/- 10.50
------------------------------------
| eval/              |             |
|    mean action     | -0.15205292 |
|    mean velocity x | -0.297      |
|    mean velocity y | 0.554       |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 61.6        |
|    mean_reward     | -6e+04      |
| time/              |             |
|    total_timesteps | 1152000     |
------------------------------------
Eval num_timesteps=1152500, episode_reward=-68122.28 +/- 30880.38
Episode length: 63.00 +/- 21.21
------------------------------------
| eval/              |             |
|    mean action     | -0.09615156 |
|    mean velocity x | -0.659      |
|    mean velocity y | 0.196       |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 63          |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1152500     |
------------------------------------
Eval num_timesteps=1153000, episode_reward=-81182.77 +/- 16938.48
Episode length: 63.60 +/- 7.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.048688438 |
|    mean velocity x | -0.787       |
|    mean velocity y | 1.7          |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 63.6         |
|    mean_reward     | -8.12e+04    |
| time/              |              |
|    total_timesteps | 1153000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.6      |
|    ep_rew_mean     | -8.29e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 563       |
|    time_elapsed    | 7936      |
|    total_timesteps | 1153024   |
----------------------------------
Eval num_timesteps=1153500, episode_reward=-47439.54 +/- 38036.81
Episode length: 47.00 +/- 15.68
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.19295707   |
|    mean velocity x      | -0.677        |
|    mean velocity y      | -0.231        |
|    mean velocity z      | 18.5          |
|    mean_ep_length       | 47            |
|    mean_reward          | -4.74e+04     |
| time/                   |               |
|    total_timesteps      | 1153500       |
| train/                  |               |
|    approx_kl            | 5.4135016e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.279         |
|    learning_rate        | 0.001         |
|    loss                 | 9.41e+07      |
|    n_updates            | 5630          |
|    policy_gradient_loss | -0.000371     |
|    std                  | 0.901         |
|    value_loss           | 2.08e+08      |
-------------------------------------------
Eval num_timesteps=1154000, episode_reward=-76638.76 +/- 13352.42
Episode length: 68.00 +/- 9.88
------------------------------------
| eval/              |             |
|    mean action     | -0.21538955 |
|    mean velocity x | 0.862       |
|    mean velocity y | 1.31        |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 68          |
|    mean_reward     | -7.66e+04   |
| time/              |             |
|    total_timesteps | 1154000     |
------------------------------------
Eval num_timesteps=1154500, episode_reward=-79576.01 +/- 22264.48
Episode length: 73.20 +/- 28.81
------------------------------------
| eval/              |             |
|    mean action     | 0.052571636 |
|    mean velocity x | 0.167       |
|    mean velocity y | 0.321       |
|    mean velocity z | 20          |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -7.96e+04   |
| time/              |             |
|    total_timesteps | 1154500     |
------------------------------------
Eval num_timesteps=1155000, episode_reward=-64230.86 +/- 18561.50
Episode length: 67.40 +/- 15.70
-----------------------------------
| eval/              |            |
|    mean action     | -0.2432653 |
|    mean velocity x | 0.45       |
|    mean velocity y | 1.85       |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 67.4       |
|    mean_reward     | -6.42e+04  |
| time/              |            |
|    total_timesteps | 1155000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.2      |
|    ep_rew_mean     | -8.29e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 564       |
|    time_elapsed    | 7943      |
|    total_timesteps | 1155072   |
----------------------------------
Eval num_timesteps=1155500, episode_reward=-34811.82 +/- 22306.03
Episode length: 42.80 +/- 15.37
------------------------------------------
| eval/                   |              |
|    mean action          | 0.18270478   |
|    mean velocity x      | 0.00335      |
|    mean velocity y      | -1.17        |
|    mean velocity z      | 20.3         |
|    mean_ep_length       | 42.8         |
|    mean_reward          | -3.48e+04    |
| time/                   |              |
|    total_timesteps      | 1155500      |
| train/                  |              |
|    approx_kl            | 0.0007290218 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.296        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 5640         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.901        |
|    value_loss           | 2.16e+08     |
------------------------------------------
Eval num_timesteps=1156000, episode_reward=-90869.34 +/- 17297.18
Episode length: 62.80 +/- 7.68
-----------------------------------
| eval/              |            |
|    mean action     | 0.28059095 |
|    mean velocity x | 0.576      |
|    mean velocity y | -0.648     |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -9.09e+04  |
| time/              |            |
|    total_timesteps | 1156000    |
-----------------------------------
Eval num_timesteps=1156500, episode_reward=-52747.86 +/- 32760.46
Episode length: 51.40 +/- 9.91
------------------------------------
| eval/              |             |
|    mean action     | -0.32658124 |
|    mean velocity x | 0.11        |
|    mean velocity y | 1.96        |
|    mean velocity z | 16.8        |
|    mean_ep_length  | 51.4        |
|    mean_reward     | -5.27e+04   |
| time/              |             |
|    total_timesteps | 1156500     |
------------------------------------
Eval num_timesteps=1157000, episode_reward=-65993.26 +/- 20976.79
Episode length: 77.20 +/- 18.40
-----------------------------------
| eval/              |            |
|    mean action     | -0.3287282 |
|    mean velocity x | 1.83       |
|    mean velocity y | 1.55       |
|    mean velocity z | 19         |
|    mean_ep_length  | 77.2       |
|    mean_reward     | -6.6e+04   |
| time/              |            |
|    total_timesteps | 1157000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.3      |
|    ep_rew_mean     | -7.99e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 565       |
|    time_elapsed    | 7950      |
|    total_timesteps | 1157120   |
----------------------------------
Eval num_timesteps=1157500, episode_reward=-85208.59 +/- 20492.81
Episode length: 71.40 +/- 14.57
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.04552919   |
|    mean velocity x      | 0.331         |
|    mean velocity y      | -0.579        |
|    mean velocity z      | 18.4          |
|    mean_ep_length       | 71.4          |
|    mean_reward          | -8.52e+04     |
| time/                   |               |
|    total_timesteps      | 1157500       |
| train/                  |               |
|    approx_kl            | 0.00033623804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.317         |
|    learning_rate        | 0.001         |
|    loss                 | 9.79e+07      |
|    n_updates            | 5650          |
|    policy_gradient_loss | -0.000612     |
|    std                  | 0.901         |
|    value_loss           | 1.81e+08      |
-------------------------------------------
Eval num_timesteps=1158000, episode_reward=-46565.13 +/- 29791.83
Episode length: 54.40 +/- 17.15
-----------------------------------
| eval/              |            |
|    mean action     | -0.3228676 |
|    mean velocity x | 0.409      |
|    mean velocity y | 2.73       |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -4.66e+04  |
| time/              |            |
|    total_timesteps | 1158000    |
-----------------------------------
Eval num_timesteps=1158500, episode_reward=-88390.61 +/- 33209.18
Episode length: 60.40 +/- 10.59
------------------------------------
| eval/              |             |
|    mean action     | -0.45162323 |
|    mean velocity x | 0.543       |
|    mean velocity y | 2.46        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 60.4        |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 1158500     |
------------------------------------
Eval num_timesteps=1159000, episode_reward=-87422.94 +/- 18224.64
Episode length: 64.20 +/- 4.40
------------------------------------
| eval/              |             |
|    mean action     | -0.07209906 |
|    mean velocity x | 0.313       |
|    mean velocity y | 0.481       |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 64.2        |
|    mean_reward     | -8.74e+04   |
| time/              |             |
|    total_timesteps | 1159000     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67       |
|    ep_rew_mean     | -7.6e+04 |
| time/              |          |
|    fps             | 145      |
|    iterations      | 566      |
|    time_elapsed    | 7958     |
|    total_timesteps | 1159168  |
---------------------------------
Eval num_timesteps=1159500, episode_reward=-72549.46 +/- 27929.95
Episode length: 56.20 +/- 7.78
------------------------------------------
| eval/                   |              |
|    mean action          | -0.8839518   |
|    mean velocity x      | 4.79         |
|    mean velocity y      | 6.05         |
|    mean velocity z      | 18.1         |
|    mean_ep_length       | 56.2         |
|    mean_reward          | -7.25e+04    |
| time/                   |              |
|    total_timesteps      | 1159500      |
| train/                  |              |
|    approx_kl            | 0.0006553058 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.001        |
|    loss                 | 4.33e+07     |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.901        |
|    value_loss           | 1.94e+08     |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=-46962.48 +/- 33800.17
Episode length: 59.40 +/- 30.92
-----------------------------------
| eval/              |            |
|    mean action     | -0.2197445 |
|    mean velocity x | 0.9        |
|    mean velocity y | 1.24       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -4.7e+04   |
| time/              |            |
|    total_timesteps | 1160000    |
-----------------------------------
Eval num_timesteps=1160500, episode_reward=-49769.66 +/- 37924.74
Episode length: 51.00 +/- 23.25
----------------------------------
| eval/              |           |
|    mean action     | -0.531496 |
|    mean velocity x | 1.49      |
|    mean velocity y | 2.02      |
|    mean velocity z | 18.1      |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.98e+04 |
| time/              |           |
|    total_timesteps | 1160500   |
----------------------------------
Eval num_timesteps=1161000, episode_reward=-83096.84 +/- 36222.36
Episode length: 59.60 +/- 13.29
-----------------------------------
| eval/              |            |
|    mean action     | 0.55432314 |
|    mean velocity x | -2.82      |
|    mean velocity y | -3.95      |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -8.31e+04  |
| time/              |            |
|    total_timesteps | 1161000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.7      |
|    ep_rew_mean     | -7.76e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 567       |
|    time_elapsed    | 7965      |
|    total_timesteps | 1161216   |
----------------------------------
Eval num_timesteps=1161500, episode_reward=-58613.80 +/- 34764.61
Episode length: 52.40 +/- 21.00
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.02777887    |
|    mean velocity x      | 1.79          |
|    mean velocity y      | 0.348         |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 52.4          |
|    mean_reward          | -5.86e+04     |
| time/                   |               |
|    total_timesteps      | 1161500       |
| train/                  |               |
|    approx_kl            | 4.7399837e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.319         |
|    learning_rate        | 0.001         |
|    loss                 | 7.62e+07      |
|    n_updates            | 5670          |
|    policy_gradient_loss | -0.000188     |
|    std                  | 0.901         |
|    value_loss           | 1.82e+08      |
-------------------------------------------
Eval num_timesteps=1162000, episode_reward=-44987.64 +/- 35964.72
Episode length: 49.60 +/- 17.87
-----------------------------------
| eval/              |            |
|    mean action     | 0.13897541 |
|    mean velocity x | -0.0731    |
|    mean velocity y | -1.8       |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 49.6       |
|    mean_reward     | -4.5e+04   |
| time/              |            |
|    total_timesteps | 1162000    |
-----------------------------------
Eval num_timesteps=1162500, episode_reward=-57685.07 +/- 37498.93
Episode length: 48.80 +/- 18.97
------------------------------------
| eval/              |             |
|    mean action     | -0.08282843 |
|    mean velocity x | -0.587      |
|    mean velocity y | -0.533      |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -5.77e+04   |
| time/              |             |
|    total_timesteps | 1162500     |
------------------------------------
Eval num_timesteps=1163000, episode_reward=-78733.14 +/- 7367.32
Episode length: 63.00 +/- 6.99
----------------------------------
| eval/              |           |
|    mean action     | 0.2405753 |
|    mean velocity x | 0.154     |
|    mean velocity y | -1.23     |
|    mean velocity z | 17.5      |
|    mean_ep_length  | 63        |
|    mean_reward     | -7.87e+04 |
| time/              |           |
|    total_timesteps | 1163000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.1      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 145       |
|    iterations      | 568       |
|    time_elapsed    | 7972      |
|    total_timesteps | 1163264   |
----------------------------------
Eval num_timesteps=1163500, episode_reward=-60750.06 +/- 36885.53
Episode length: 49.60 +/- 24.00
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.42047474    |
|    mean velocity x      | -1.15         |
|    mean velocity y      | -2.47         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 49.6          |
|    mean_reward          | -6.08e+04     |
| time/                   |               |
|    total_timesteps      | 1163500       |
| train/                  |               |
|    approx_kl            | 0.00030860907 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+08      |
|    n_updates            | 5680          |
|    policy_gradient_loss | -0.00064      |
|    std                  | 0.901         |
|    value_loss           | 2.02e+08      |
-------------------------------------------
Eval num_timesteps=1164000, episode_reward=-78840.66 +/- 20538.36
Episode length: 61.80 +/- 8.84
-------------------------------------
| eval/              |              |
|    mean action     | -0.058750197 |
|    mean velocity x | -1.08        |
|    mean velocity y | 0.217        |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 61.8         |
|    mean_reward     | -7.88e+04    |
| time/              |              |
|    total_timesteps | 1164000      |
-------------------------------------
Eval num_timesteps=1164500, episode_reward=-40820.20 +/- 26838.73
Episode length: 43.60 +/- 21.77
------------------------------------
| eval/              |             |
|    mean action     | -0.17868239 |
|    mean velocity x | -0.168      |
|    mean velocity y | 1.38        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 43.6        |
|    mean_reward     | -4.08e+04   |
| time/              |             |
|    total_timesteps | 1164500     |
------------------------------------
Eval num_timesteps=1165000, episode_reward=-43493.33 +/- 29664.20
Episode length: 49.80 +/- 13.29
-----------------------------------
| eval/              |            |
|    mean action     | 0.03534127 |
|    mean velocity x | 1.42       |
|    mean velocity y | 0.685      |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 49.8       |
|    mean_reward     | -4.35e+04  |
| time/              |            |
|    total_timesteps | 1165000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.8      |
|    ep_rew_mean     | -7.52e+04 |
| time/              |           |
|    fps             | 146       |
|    iterations      | 569       |
|    time_elapsed    | 7979      |
|    total_timesteps | 1165312   |
----------------------------------
Eval num_timesteps=1165500, episode_reward=-63320.42 +/- 23929.38
Episode length: 61.40 +/- 11.20
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35544887  |
|    mean velocity x      | 0.716        |
|    mean velocity y      | 1.92         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 61.4         |
|    mean_reward          | -6.33e+04    |
| time/                   |              |
|    total_timesteps      | 1165500      |
| train/                  |              |
|    approx_kl            | 0.0012650974 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.001        |
|    loss                 | 6.76e+07     |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.9          |
|    value_loss           | 1.72e+08     |
------------------------------------------
Eval num_timesteps=1166000, episode_reward=-89880.21 +/- 11166.13
Episode length: 74.20 +/- 27.95
-------------------------------------
| eval/              |              |
|    mean action     | -0.094631754 |
|    mean velocity x | 0.85         |
|    mean velocity y | 0.575        |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 74.2         |
|    mean_reward     | -8.99e+04    |
| time/              |              |
|    total_timesteps | 1166000      |
-------------------------------------
Eval num_timesteps=1166500, episode_reward=-81265.41 +/- 41168.30
Episode length: 57.40 +/- 21.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.36003882 |
|    mean velocity x | 0.19       |
|    mean velocity y | -0.626     |
|    mean velocity z | 15.9       |
|    mean_ep_length  | 57.4       |
|    mean_reward     | -8.13e+04  |
| time/              |            |
|    total_timesteps | 1166500    |
-----------------------------------
Eval num_timesteps=1167000, episode_reward=-77569.53 +/- 15014.16
Episode length: 58.60 +/- 3.72
----------------------------------
| eval/              |           |
|    mean action     | 0.1939941 |
|    mean velocity x | -0.124    |
|    mean velocity y | -0.65     |
|    mean velocity z | 19.8      |
|    mean_ep_length  | 58.6      |
|    mean_reward     | -7.76e+04 |
| time/              |           |
|    total_timesteps | 1167000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.5      |
|    ep_rew_mean     | -7.29e+04 |
| time/              |           |
|    fps             | 146       |
|    iterations      | 570       |
|    time_elapsed    | 7986      |
|    total_timesteps | 1167360   |
----------------------------------
Eval num_timesteps=1167500, episode_reward=-54718.48 +/- 43002.07
Episode length: 48.20 +/- 14.43
----------------------------------------
| eval/                   |            |
|    mean action          | 0.2567091  |
|    mean velocity x      | -0.935     |
|    mean velocity y      | -1.49      |
|    mean velocity z      | 17.5       |
|    mean_ep_length       | 48.2       |
|    mean_reward          | -5.47e+04  |
| time/                   |            |
|    total_timesteps      | 1167500    |
| train/                  |            |
|    approx_kl            | 0.00054873 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.328      |
|    learning_rate        | 0.001      |
|    loss                 | 1.11e+08   |
|    n_updates            | 5700       |
|    policy_gradient_loss | -0.0007    |
|    std                  | 0.9        |
|    value_loss           | 1.68e+08   |
----------------------------------------
Eval num_timesteps=1168000, episode_reward=-79006.59 +/- 18144.64
Episode length: 63.40 +/- 6.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.24830262 |
|    mean velocity x | -0.947     |
|    mean velocity y | -1.07      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -7.9e+04   |
| time/              |            |
|    total_timesteps | 1168000    |
-----------------------------------
Eval num_timesteps=1168500, episode_reward=-75492.67 +/- 35691.09
Episode length: 59.40 +/- 10.82
------------------------------------
| eval/              |             |
|    mean action     | -0.26460516 |
|    mean velocity x | 1.53        |
|    mean velocity y | 1.07        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 1168500     |
------------------------------------
Eval num_timesteps=1169000, episode_reward=-68398.89 +/- 17406.64
Episode length: 60.80 +/- 5.95
------------------------------------
| eval/              |             |
|    mean action     | -0.07986956 |
|    mean velocity x | 1.05        |
|    mean velocity y | 0.41        |
|    mean velocity z | 18          |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 1169000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -7.19e+04 |
| time/              |           |
|    fps             | 146       |
|    iterations      | 571       |
|    time_elapsed    | 7993      |
|    total_timesteps | 1169408   |
----------------------------------
Eval num_timesteps=1169500, episode_reward=-82396.90 +/- 13104.45
Episode length: 68.40 +/- 11.27
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.18789418  |
|    mean velocity x      | 1.02        |
|    mean velocity y      | -0.886      |
|    mean velocity z      | 17.9        |
|    mean_ep_length       | 68.4        |
|    mean_reward          | -8.24e+04   |
| time/                   |             |
|    total_timesteps      | 1169500     |
| train/                  |             |
|    approx_kl            | 0.000229808 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.001       |
|    loss                 | 8.4e+07     |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.000851   |
|    std                  | 0.9         |
|    value_loss           | 1.68e+08    |
-----------------------------------------
Eval num_timesteps=1170000, episode_reward=-83551.47 +/- 16978.50
Episode length: 68.80 +/- 9.99
-----------------------------------
| eval/              |            |
|    mean action     | -0.7320138 |
|    mean velocity x | 2.25       |
|    mean velocity y | 3.83       |
|    mean velocity z | 16.8       |
|    mean_ep_length  | 68.8       |
|    mean_reward     | -8.36e+04  |
| time/              |            |
|    total_timesteps | 1170000    |
-----------------------------------
Eval num_timesteps=1170500, episode_reward=-59716.00 +/- 34023.58
Episode length: 55.40 +/- 29.04
----------------------------------
| eval/              |           |
|    mean action     | 0.6388361 |
|    mean velocity x | -3.03     |
|    mean velocity y | -4.44     |
|    mean velocity z | 19.6      |
|    mean_ep_length  | 55.4      |
|    mean_reward     | -5.97e+04 |
| time/              |           |
|    total_timesteps | 1170500   |
----------------------------------
Eval num_timesteps=1171000, episode_reward=-79644.55 +/- 19401.39
Episode length: 64.80 +/- 8.59
------------------------------------
| eval/              |             |
|    mean action     | -0.14911686 |
|    mean velocity x | 1.69        |
|    mean velocity y | 0.86        |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -7.96e+04   |
| time/              |             |
|    total_timesteps | 1171000     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 146      |
|    iterations      | 572      |
|    time_elapsed    | 8001     |
|    total_timesteps | 1171456  |
---------------------------------
Eval num_timesteps=1171500, episode_reward=-46635.34 +/- 38060.89
Episode length: 47.60 +/- 21.23
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.09563504    |
|    mean velocity x      | -0.769        |
|    mean velocity y      | -0.892        |
|    mean velocity z      | 18.9          |
|    mean_ep_length       | 47.6          |
|    mean_reward          | -4.66e+04     |
| time/                   |               |
|    total_timesteps      | 1171500       |
| train/                  |               |
|    approx_kl            | 0.00012963399 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.288         |
|    learning_rate        | 0.001         |
|    loss                 | 8.48e+07      |
|    n_updates            | 5720          |
|    policy_gradient_loss | -0.0004       |
|    std                  | 0.9           |
|    value_loss           | 2.03e+08      |
-------------------------------------------
Eval num_timesteps=1172000, episode_reward=-79552.94 +/- 40304.13
Episode length: 77.20 +/- 32.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.12681487 |
|    mean velocity x | -0.701     |
|    mean velocity y | -1.16      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 77.2       |
|    mean_reward     | -7.96e+04  |
| time/              |            |
|    total_timesteps | 1172000    |
-----------------------------------
Eval num_timesteps=1172500, episode_reward=-75002.26 +/- 16493.95
Episode length: 70.00 +/- 15.30
------------------------------------
| eval/              |             |
|    mean action     | 0.060472503 |
|    mean velocity x | -2.01       |
|    mean velocity y | -1.52       |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 70          |
|    mean_reward     | -7.5e+04    |
| time/              |             |
|    total_timesteps | 1172500     |
------------------------------------
Eval num_timesteps=1173000, episode_reward=-109281.45 +/- 48634.37
Episode length: 103.00 +/- 54.45
----------------------------------
| eval/              |           |
|    mean action     | 0.2619008 |
|    mean velocity x | -1.08     |
|    mean velocity y | -1.73     |
|    mean velocity z | 19.1      |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.09e+05 |
| time/              |           |
|    total_timesteps | 1173000   |
----------------------------------
Eval num_timesteps=1173500, episode_reward=-80737.84 +/- 19587.60
Episode length: 81.20 +/- 31.17
------------------------------------
| eval/              |             |
|    mean action     | -0.18789344 |
|    mean velocity x | -0.589      |
|    mean velocity y | 0.909       |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 81.2        |
|    mean_reward     | -8.07e+04   |
| time/              |             |
|    total_timesteps | 1173500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.6      |
|    ep_rew_mean     | -7.36e+04 |
| time/              |           |
|    fps             | 146       |
|    iterations      | 573       |
|    time_elapsed    | 8009      |
|    total_timesteps | 1173504   |
----------------------------------
Eval num_timesteps=1174000, episode_reward=-81840.33 +/- 17651.63
Episode length: 59.20 +/- 3.31
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1419668     |
|    mean velocity x      | 0.342         |
|    mean velocity y      | -0.162        |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 59.2          |
|    mean_reward          | -8.18e+04     |
| time/                   |               |
|    total_timesteps      | 1174000       |
| train/                  |               |
|    approx_kl            | 3.7825084e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 7.11e+07      |
|    n_updates            | 5730          |
|    policy_gradient_loss | -0.000105     |
|    std                  | 0.9           |
|    value_loss           | 1.81e+08      |
-------------------------------------------
Eval num_timesteps=1174500, episode_reward=-55789.65 +/- 37760.85
Episode length: 59.40 +/- 21.56
------------------------------------
| eval/              |             |
|    mean action     | -0.26245552 |
|    mean velocity x | -0.579      |
|    mean velocity y | 1.21        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -5.58e+04   |
| time/              |             |
|    total_timesteps | 1174500     |
------------------------------------
Eval num_timesteps=1175000, episode_reward=-68996.19 +/- 18519.10
Episode length: 61.80 +/- 6.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.31286183 |
|    mean velocity x | -0.587     |
|    mean velocity y | -0.862     |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -6.9e+04   |
| time/              |            |
|    total_timesteps | 1175000    |
-----------------------------------
Eval num_timesteps=1175500, episode_reward=-66000.64 +/- 24125.54
Episode length: 76.40 +/- 22.65
-----------------------------------
| eval/              |            |
|    mean action     | 0.45090118 |
|    mean velocity x | -1.56      |
|    mean velocity y | -3.15      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 76.4       |
|    mean_reward     | -6.6e+04   |
| time/              |            |
|    total_timesteps | 1175500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69       |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 146      |
|    iterations      | 574      |
|    time_elapsed    | 8016     |
|    total_timesteps | 1175552  |
---------------------------------
Eval num_timesteps=1176000, episode_reward=-80567.38 +/- 15712.63
Episode length: 65.40 +/- 4.92
------------------------------------------
| eval/                   |              |
|    mean action          | 0.23286162   |
|    mean velocity x      | -1.96        |
|    mean velocity y      | -2.52        |
|    mean velocity z      | 18.6         |
|    mean_ep_length       | 65.4         |
|    mean_reward          | -8.06e+04    |
| time/                   |              |
|    total_timesteps      | 1176000      |
| train/                  |              |
|    approx_kl            | 0.0012706362 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.001        |
|    loss                 | 8.17e+07     |
|    n_updates            | 5740         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.899        |
|    value_loss           | 1.75e+08     |
------------------------------------------
Eval num_timesteps=1176500, episode_reward=-74213.08 +/- 8069.93
Episode length: 74.80 +/- 6.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.43597192 |
|    mean velocity x | -2.23      |
|    mean velocity y | -3.77      |
|    mean velocity z | 21.8       |
|    mean_ep_length  | 74.8       |
|    mean_reward     | -7.42e+04  |
| time/              |            |
|    total_timesteps | 1176500    |
-----------------------------------
Eval num_timesteps=1177000, episode_reward=-73527.17 +/- 43604.58
Episode length: 72.80 +/- 36.96
--------------------------------------
| eval/              |               |
|    mean action     | -0.0019388351 |
|    mean velocity x | 0.94          |
|    mean velocity y | 0.728         |
|    mean velocity z | 21.7          |
|    mean_ep_length  | 72.8          |
|    mean_reward     | -7.35e+04     |
| time/              |               |
|    total_timesteps | 1177000       |
--------------------------------------
Eval num_timesteps=1177500, episode_reward=-62527.27 +/- 31836.34
Episode length: 65.00 +/- 24.14
------------------------------------
| eval/              |             |
|    mean action     | -0.23928246 |
|    mean velocity x | 0.171       |
|    mean velocity y | 2.52        |
|    mean velocity z | 15.9        |
|    mean_ep_length  | 65          |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 1177500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.7      |
|    ep_rew_mean     | -7.65e+04 |
| time/              |           |
|    fps             | 146       |
|    iterations      | 575       |
|    time_elapsed    | 8023      |
|    total_timesteps | 1177600   |
----------------------------------
Eval num_timesteps=1178000, episode_reward=-86833.48 +/- 32455.14
Episode length: 59.20 +/- 6.34
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.66148204 |
|    mean velocity x      | 1.03        |
|    mean velocity y      | 3.87        |
|    mean velocity z      | 17.5        |
|    mean_ep_length       | 59.2        |
|    mean_reward          | -8.68e+04   |
| time/                   |             |
|    total_timesteps      | 1178000     |
| train/                  |             |
|    approx_kl            | 0.001644942 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+08    |
|    n_updates            | 5750        |
|    policy_gradient_loss | -0.002      |
|    std                  | 0.9         |
|    value_loss           | 1.92e+08    |
-----------------------------------------
Eval num_timesteps=1178500, episode_reward=-68316.02 +/- 30036.32
Episode length: 58.40 +/- 9.22
------------------------------------
| eval/              |             |
|    mean action     | 0.036790557 |
|    mean velocity x | -1.16       |
|    mean velocity y | -1.03       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 58.4        |
|    mean_reward     | -6.83e+04   |
| time/              |             |
|    total_timesteps | 1178500     |
------------------------------------
Eval num_timesteps=1179000, episode_reward=-73286.31 +/- 37725.63
Episode length: 53.40 +/- 22.50
------------------------------------
| eval/              |             |
|    mean action     | -0.50784695 |
|    mean velocity x | 1.14        |
|    mean velocity y | 2.78        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 53.4        |
|    mean_reward     | -7.33e+04   |
| time/              |             |
|    total_timesteps | 1179000     |
------------------------------------
Eval num_timesteps=1179500, episode_reward=-70569.09 +/- 29106.87
Episode length: 78.40 +/- 28.83
------------------------------------
| eval/              |             |
|    mean action     | -0.32685158 |
|    mean velocity x | 1.04        |
|    mean velocity y | 2.32        |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 78.4        |
|    mean_reward     | -7.06e+04   |
| time/              |             |
|    total_timesteps | 1179500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.7      |
|    ep_rew_mean     | -7.81e+04 |
| time/              |           |
|    fps             | 146       |
|    iterations      | 576       |
|    time_elapsed    | 8031      |
|    total_timesteps | 1179648   |
----------------------------------
Eval num_timesteps=1180000, episode_reward=-92926.81 +/- 43966.88
Episode length: 73.20 +/- 43.27
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.027768554 |
|    mean velocity x      | -1.36       |
|    mean velocity y      | 0.0225      |
|    mean velocity z      | 22.1        |
|    mean_ep_length       | 73.2        |
|    mean_reward          | -9.29e+04   |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.001661261 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.001       |
|    loss                 | 5.8e+07     |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.00148    |
|    std                  | 0.9         |
|    value_loss           | 1.95e+08    |
-----------------------------------------
Eval num_timesteps=1180500, episode_reward=-77700.98 +/- 17527.69
Episode length: 65.00 +/- 7.01
-------------------------------------
| eval/              |              |
|    mean action     | -0.021764161 |
|    mean velocity x | 0.404        |
|    mean velocity y | -0.0021      |
|    mean velocity z | 20.8         |
|    mean_ep_length  | 65           |
|    mean_reward     | -7.77e+04    |
| time/              |              |
|    total_timesteps | 1180500      |
-------------------------------------
Eval num_timesteps=1181000, episode_reward=-24394.68 +/- 38441.14
Episode length: 32.40 +/- 17.11
------------------------------------
| eval/              |             |
|    mean action     | -0.07046038 |
|    mean velocity x | -0.434      |
|    mean velocity y | -0.116      |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 32.4        |
|    mean_reward     | -2.44e+04   |
| time/              |             |
|    total_timesteps | 1181000     |
------------------------------------
Eval num_timesteps=1181500, episode_reward=-71843.05 +/- 29149.15
Episode length: 54.40 +/- 7.09
------------------------------------
| eval/              |             |
|    mean action     | -0.12883772 |
|    mean velocity x | 1.11        |
|    mean velocity y | 1.43        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -7.18e+04   |
| time/              |             |
|    total_timesteps | 1181500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.5      |
|    ep_rew_mean     | -8.11e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 577       |
|    time_elapsed    | 8038      |
|    total_timesteps | 1181696   |
----------------------------------
Eval num_timesteps=1182000, episode_reward=-58762.82 +/- 14498.35
Episode length: 69.40 +/- 29.47
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.3464772     |
|    mean velocity x      | -0.958        |
|    mean velocity y      | -2.4          |
|    mean velocity z      | 18.8          |
|    mean_ep_length       | 69.4          |
|    mean_reward          | -5.88e+04     |
| time/                   |               |
|    total_timesteps      | 1182000       |
| train/                  |               |
|    approx_kl            | 0.00036978844 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.001         |
|    loss                 | 1.87e+08      |
|    n_updates            | 5770          |
|    policy_gradient_loss | -0.000786     |
|    std                  | 0.9           |
|    value_loss           | 2.56e+08      |
-------------------------------------------
Eval num_timesteps=1182500, episode_reward=-65059.57 +/- 33059.27
Episode length: 75.40 +/- 36.71
------------------------------------
| eval/              |             |
|    mean action     | 0.060954314 |
|    mean velocity x | 0.118       |
|    mean velocity y | 0.672       |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 75.4        |
|    mean_reward     | -6.51e+04   |
| time/              |             |
|    total_timesteps | 1182500     |
------------------------------------
Eval num_timesteps=1183000, episode_reward=-66153.01 +/- 21989.88
Episode length: 54.20 +/- 6.65
-----------------------------------
| eval/              |            |
|    mean action     | -0.5875871 |
|    mean velocity x | 1.38       |
|    mean velocity y | 2.85       |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 54.2       |
|    mean_reward     | -6.62e+04  |
| time/              |            |
|    total_timesteps | 1183000    |
-----------------------------------
Eval num_timesteps=1183500, episode_reward=-57337.46 +/- 32854.34
Episode length: 62.80 +/- 32.18
-----------------------------------
| eval/              |            |
|    mean action     | -0.1369841 |
|    mean velocity x | 1.89       |
|    mean velocity y | 1.72       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 62.8       |
|    mean_reward     | -5.73e+04  |
| time/              |            |
|    total_timesteps | 1183500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.7      |
|    ep_rew_mean     | -7.99e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 578       |
|    time_elapsed    | 8045      |
|    total_timesteps | 1183744   |
----------------------------------
Eval num_timesteps=1184000, episode_reward=-86267.52 +/- 21156.66
Episode length: 70.60 +/- 22.01
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.20001101    |
|    mean velocity x      | -1.45         |
|    mean velocity y      | -1.01         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 70.6          |
|    mean_reward          | -8.63e+04     |
| time/                   |               |
|    total_timesteps      | 1184000       |
| train/                  |               |
|    approx_kl            | 5.6363235e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.3           |
|    learning_rate        | 0.001         |
|    loss                 | 8.24e+07      |
|    n_updates            | 5780          |
|    policy_gradient_loss | -0.000236     |
|    std                  | 0.9           |
|    value_loss           | 1.81e+08      |
-------------------------------------------
Eval num_timesteps=1184500, episode_reward=-68054.05 +/- 32327.65
Episode length: 62.00 +/- 16.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.21571057 |
|    mean velocity x | -1.96      |
|    mean velocity y | -1.11      |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 62         |
|    mean_reward     | -6.81e+04  |
| time/              |            |
|    total_timesteps | 1184500    |
-----------------------------------
Eval num_timesteps=1185000, episode_reward=-50593.05 +/- 41289.84
Episode length: 47.60 +/- 19.17
------------------------------------
| eval/              |             |
|    mean action     | -0.66466546 |
|    mean velocity x | 2.67        |
|    mean velocity y | 4.18        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 47.6        |
|    mean_reward     | -5.06e+04   |
| time/              |             |
|    total_timesteps | 1185000     |
------------------------------------
Eval num_timesteps=1185500, episode_reward=-64581.84 +/- 39915.03
Episode length: 50.60 +/- 20.48
------------------------------------
| eval/              |             |
|    mean action     | -0.99992305 |
|    mean velocity x | 4.21        |
|    mean velocity y | 6.12        |
|    mean velocity z | 14.3        |
|    mean_ep_length  | 50.6        |
|    mean_reward     | -6.46e+04   |
| time/              |             |
|    total_timesteps | 1185500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.9      |
|    ep_rew_mean     | -7.65e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 579       |
|    time_elapsed    | 8052      |
|    total_timesteps | 1185792   |
----------------------------------
Eval num_timesteps=1186000, episode_reward=-50649.33 +/- 39738.57
Episode length: 47.20 +/- 16.40
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.10865384    |
|    mean velocity x      | -0.528        |
|    mean velocity y      | 0.0898        |
|    mean velocity z      | 21.2          |
|    mean_ep_length       | 47.2          |
|    mean_reward          | -5.06e+04     |
| time/                   |               |
|    total_timesteps      | 1186000       |
| train/                  |               |
|    approx_kl            | 0.00061191776 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.001         |
|    loss                 | 4.13e+07      |
|    n_updates            | 5790          |
|    policy_gradient_loss | -0.000944     |
|    std                  | 0.9           |
|    value_loss           | 1.48e+08      |
-------------------------------------------
Eval num_timesteps=1186500, episode_reward=-73573.02 +/- 33685.86
Episode length: 61.60 +/- 19.16
----------------------------------
| eval/              |           |
|    mean action     | 0.2814328 |
|    mean velocity x | -0.542    |
|    mean velocity y | -0.997    |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 61.6      |
|    mean_reward     | -7.36e+04 |
| time/              |           |
|    total_timesteps | 1186500   |
----------------------------------
Eval num_timesteps=1187000, episode_reward=-32055.45 +/- 28491.34
Episode length: 44.80 +/- 30.60
-----------------------------------
| eval/              |            |
|    mean action     | 0.11507571 |
|    mean velocity x | -0.551     |
|    mean velocity y | -0.188     |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 44.8       |
|    mean_reward     | -3.21e+04  |
| time/              |            |
|    total_timesteps | 1187000    |
-----------------------------------
Eval num_timesteps=1187500, episode_reward=-68793.05 +/- 36234.82
Episode length: 56.80 +/- 13.17
------------------------------------
| eval/              |             |
|    mean action     | -0.14115603 |
|    mean velocity x | -0.331      |
|    mean velocity y | 0.812       |
|    mean velocity z | 20          |
|    mean_ep_length  | 56.8        |
|    mean_reward     | -6.88e+04   |
| time/              |             |
|    total_timesteps | 1187500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.3      |
|    ep_rew_mean     | -7.61e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 580       |
|    time_elapsed    | 8059      |
|    total_timesteps | 1187840   |
----------------------------------
Eval num_timesteps=1188000, episode_reward=-73533.41 +/- 40140.55
Episode length: 53.60 +/- 14.09
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.06568753    |
|    mean velocity x      | 2.54          |
|    mean velocity y      | 0.512         |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 53.6          |
|    mean_reward          | -7.35e+04     |
| time/                   |               |
|    total_timesteps      | 1188000       |
| train/                  |               |
|    approx_kl            | 3.2045093e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.001         |
|    loss                 | 1.2e+08       |
|    n_updates            | 5800          |
|    policy_gradient_loss | -0.000287     |
|    std                  | 0.9           |
|    value_loss           | 2.08e+08      |
-------------------------------------------
Eval num_timesteps=1188500, episode_reward=-83123.83 +/- 18233.28
Episode length: 66.20 +/- 4.96
-----------------------------------
| eval/              |            |
|    mean action     | 0.13596475 |
|    mean velocity x | -0.499     |
|    mean velocity y | -1.08      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 66.2       |
|    mean_reward     | -8.31e+04  |
| time/              |            |
|    total_timesteps | 1188500    |
-----------------------------------
Eval num_timesteps=1189000, episode_reward=-86936.51 +/- 24281.05
Episode length: 59.40 +/- 5.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.51741576 |
|    mean velocity x | -2.3       |
|    mean velocity y | -4.18      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -8.69e+04  |
| time/              |            |
|    total_timesteps | 1189000    |
-----------------------------------
Eval num_timesteps=1189500, episode_reward=-63772.93 +/- 36798.89
Episode length: 53.60 +/- 13.35
------------------------------------
| eval/              |             |
|    mean action     | -0.37684515 |
|    mean velocity x | 1.64        |
|    mean velocity y | 3.15        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -6.38e+04   |
| time/              |             |
|    total_timesteps | 1189500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.7      |
|    ep_rew_mean     | -7.84e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 581       |
|    time_elapsed    | 8066      |
|    total_timesteps | 1189888   |
----------------------------------
Eval num_timesteps=1190000, episode_reward=-55499.75 +/- 28952.37
Episode length: 57.60 +/- 19.70
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.015423966  |
|    mean velocity x      | 0.566         |
|    mean velocity y      | 0.315         |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 57.6          |
|    mean_reward          | -5.55e+04     |
| time/                   |               |
|    total_timesteps      | 1190000       |
| train/                  |               |
|    approx_kl            | 1.6889884e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.001         |
|    loss                 | 1.23e+08      |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.000188     |
|    std                  | 0.9           |
|    value_loss           | 1.92e+08      |
-------------------------------------------
Eval num_timesteps=1190500, episode_reward=-64952.30 +/- 36786.39
Episode length: 50.40 +/- 17.61
------------------------------------
| eval/              |             |
|    mean action     | -0.07825453 |
|    mean velocity x | 1.19        |
|    mean velocity y | 1.32        |
|    mean velocity z | 20          |
|    mean_ep_length  | 50.4        |
|    mean_reward     | -6.5e+04    |
| time/              |             |
|    total_timesteps | 1190500     |
------------------------------------
Eval num_timesteps=1191000, episode_reward=-61461.24 +/- 33577.38
Episode length: 50.40 +/- 13.37
------------------------------------
| eval/              |             |
|    mean action     | -0.34039533 |
|    mean velocity x | -0.174      |
|    mean velocity y | 0.125       |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 50.4        |
|    mean_reward     | -6.15e+04   |
| time/              |             |
|    total_timesteps | 1191000     |
------------------------------------
Eval num_timesteps=1191500, episode_reward=-69148.02 +/- 39960.34
Episode length: 54.20 +/- 19.96
------------------------------------
| eval/              |             |
|    mean action     | -0.22471602 |
|    mean velocity x | 1.57        |
|    mean velocity y | 1.31        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 54.2        |
|    mean_reward     | -6.91e+04   |
| time/              |             |
|    total_timesteps | 1191500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70        |
|    ep_rew_mean     | -7.77e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 582       |
|    time_elapsed    | 8074      |
|    total_timesteps | 1191936   |
----------------------------------
Eval num_timesteps=1192000, episode_reward=-43064.99 +/- 35626.17
Episode length: 50.60 +/- 15.79
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.556724     |
|    mean velocity x      | 1.51          |
|    mean velocity y      | 3.47          |
|    mean velocity z      | 18.6          |
|    mean_ep_length       | 50.6          |
|    mean_reward          | -4.31e+04     |
| time/                   |               |
|    total_timesteps      | 1192000       |
| train/                  |               |
|    approx_kl            | 0.00048754725 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.279         |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+08      |
|    n_updates            | 5820          |
|    policy_gradient_loss | -0.000405     |
|    std                  | 0.9           |
|    value_loss           | 1.85e+08      |
-------------------------------------------
Eval num_timesteps=1192500, episode_reward=-61994.93 +/- 30400.86
Episode length: 56.20 +/- 18.62
------------------------------------
| eval/              |             |
|    mean action     | -0.21047555 |
|    mean velocity x | 0.0974      |
|    mean velocity y | 1.13        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 56.2        |
|    mean_reward     | -6.2e+04    |
| time/              |             |
|    total_timesteps | 1192500     |
------------------------------------
Eval num_timesteps=1193000, episode_reward=-38137.28 +/- 19979.44
Episode length: 57.80 +/- 23.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.05518579 |
|    mean velocity x | 0.954      |
|    mean velocity y | 0.908      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -3.81e+04  |
| time/              |            |
|    total_timesteps | 1193000    |
-----------------------------------
Eval num_timesteps=1193500, episode_reward=-77130.87 +/- 14804.32
Episode length: 64.60 +/- 7.36
--------------------------------------
| eval/              |               |
|    mean action     | -0.0025269317 |
|    mean velocity x | -0.375        |
|    mean velocity y | -0.646        |
|    mean velocity z | 21.4          |
|    mean_ep_length  | 64.6          |
|    mean_reward     | -7.71e+04     |
| time/              |               |
|    total_timesteps | 1193500       |
--------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.2      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 583       |
|    time_elapsed    | 8081      |
|    total_timesteps | 1193984   |
----------------------------------
Eval num_timesteps=1194000, episode_reward=-59895.34 +/- 31996.85
Episode length: 62.80 +/- 12.69
------------------------------------------
| eval/                   |              |
|    mean action          | -0.06791913  |
|    mean velocity x      | -0.766       |
|    mean velocity y      | -0.116       |
|    mean velocity z      | 16.8         |
|    mean_ep_length       | 62.8         |
|    mean_reward          | -5.99e+04    |
| time/                   |              |
|    total_timesteps      | 1194000      |
| train/                  |              |
|    approx_kl            | 0.0006534723 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.001        |
|    loss                 | 6.17e+07     |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.000799    |
|    std                  | 0.899        |
|    value_loss           | 2.05e+08     |
------------------------------------------
Eval num_timesteps=1194500, episode_reward=-95031.53 +/- 20006.64
Episode length: 60.80 +/- 2.86
------------------------------------
| eval/              |             |
|    mean action     | -0.48637378 |
|    mean velocity x | 2.56        |
|    mean velocity y | 1.87        |
|    mean velocity z | 15.9        |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -9.5e+04    |
| time/              |             |
|    total_timesteps | 1194500     |
------------------------------------
Eval num_timesteps=1195000, episode_reward=-82254.57 +/- 22258.76
Episode length: 62.40 +/- 6.62
------------------------------------
| eval/              |             |
|    mean action     | -0.22317609 |
|    mean velocity x | 1.08        |
|    mean velocity y | 2.39        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 62.4        |
|    mean_reward     | -8.23e+04   |
| time/              |             |
|    total_timesteps | 1195000     |
------------------------------------
Eval num_timesteps=1195500, episode_reward=-40861.68 +/- 35705.32
Episode length: 46.80 +/- 22.38
------------------------------------
| eval/              |             |
|    mean action     | -0.20670322 |
|    mean velocity x | -0.335      |
|    mean velocity y | 1.57        |
|    mean velocity z | 16.3        |
|    mean_ep_length  | 46.8        |
|    mean_reward     | -4.09e+04   |
| time/              |             |
|    total_timesteps | 1195500     |
------------------------------------
Eval num_timesteps=1196000, episode_reward=-32224.54 +/- 28818.96
Episode length: 47.80 +/- 20.86
------------------------------------
| eval/              |             |
|    mean action     | -0.36401552 |
|    mean velocity x | 1.08        |
|    mean velocity y | 1.93        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 47.8        |
|    mean_reward     | -3.22e+04   |
| time/              |             |
|    total_timesteps | 1196000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.1      |
|    ep_rew_mean     | -7.66e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 584       |
|    time_elapsed    | 8088      |
|    total_timesteps | 1196032   |
----------------------------------
Eval num_timesteps=1196500, episode_reward=-71355.06 +/- 36106.24
Episode length: 61.00 +/- 30.36
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24894148  |
|    mean velocity x      | 0.464        |
|    mean velocity y      | 0.974        |
|    mean velocity z      | 18.9         |
|    mean_ep_length       | 61           |
|    mean_reward          | -7.14e+04    |
| time/                   |              |
|    total_timesteps      | 1196500      |
| train/                  |              |
|    approx_kl            | 6.995522e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.001        |
|    loss                 | 9.48e+07     |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.000475    |
|    std                  | 0.899        |
|    value_loss           | 1.73e+08     |
------------------------------------------
Eval num_timesteps=1197000, episode_reward=-66198.22 +/- 43517.84
Episode length: 60.40 +/- 33.45
-----------------------------------
| eval/              |            |
|    mean action     | 0.11150575 |
|    mean velocity x | -1.12      |
|    mean velocity y | -1.17      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 60.4       |
|    mean_reward     | -6.62e+04  |
| time/              |            |
|    total_timesteps | 1197000    |
-----------------------------------
Eval num_timesteps=1197500, episode_reward=-69730.24 +/- 21006.72
Episode length: 61.40 +/- 8.40
----------------------------------
| eval/              |           |
|    mean action     | 0.5701942 |
|    mean velocity x | -2.77     |
|    mean velocity y | -4.02     |
|    mean velocity z | 16.9      |
|    mean_ep_length  | 61.4      |
|    mean_reward     | -6.97e+04 |
| time/              |           |
|    total_timesteps | 1197500   |
----------------------------------
Eval num_timesteps=1198000, episode_reward=-55590.57 +/- 20443.57
Episode length: 69.00 +/- 29.91
------------------------------------
| eval/              |             |
|    mean action     | -0.32215652 |
|    mean velocity x | -0.697      |
|    mean velocity y | 1.36        |
|    mean velocity z | 19          |
|    mean_ep_length  | 69          |
|    mean_reward     | -5.56e+04   |
| time/              |             |
|    total_timesteps | 1198000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.6      |
|    ep_rew_mean     | -7.39e+04 |
| time/              |           |
|    fps             | 147       |
|    iterations      | 585       |
|    time_elapsed    | 8096      |
|    total_timesteps | 1198080   |
----------------------------------
Eval num_timesteps=1198500, episode_reward=-63828.19 +/- 25536.90
Episode length: 61.60 +/- 16.37
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.08875748    |
|    mean velocity x      | -0.33         |
|    mean velocity y      | -0.883        |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 61.6          |
|    mean_reward          | -6.38e+04     |
| time/                   |               |
|    total_timesteps      | 1198500       |
| train/                  |               |
|    approx_kl            | 3.2331067e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 7.83e+07      |
|    n_updates            | 5850          |
|    policy_gradient_loss | -0.000288     |
|    std                  | 0.899         |
|    value_loss           | 1.78e+08      |
-------------------------------------------
Eval num_timesteps=1199000, episode_reward=-76411.83 +/- 25593.76
Episode length: 64.20 +/- 12.61
-----------------------------------
| eval/              |            |
|    mean action     | -0.7611952 |
|    mean velocity x | 2.46       |
|    mean velocity y | 3.61       |
|    mean velocity z | 16.9       |
|    mean_ep_length  | 64.2       |
|    mean_reward     | -7.64e+04  |
| time/              |            |
|    total_timesteps | 1199000    |
-----------------------------------
Eval num_timesteps=1199500, episode_reward=-68526.47 +/- 33114.25
Episode length: 64.00 +/- 20.49
------------------------------------
| eval/              |             |
|    mean action     | -0.36955273 |
|    mean velocity x | 0.426       |
|    mean velocity y | 0.802       |
|    mean velocity z | 20          |
|    mean_ep_length  | 64          |
|    mean_reward     | -6.85e+04   |
| time/              |             |
|    total_timesteps | 1199500     |
------------------------------------
Eval num_timesteps=1200000, episode_reward=-55422.53 +/- 32309.65
Episode length: 51.60 +/- 16.86
------------------------------------
| eval/              |             |
|    mean action     | -0.45518765 |
|    mean velocity x | 1.95        |
|    mean velocity y | 3.49        |
|    mean velocity z | 20          |
|    mean_ep_length  | 51.6        |
|    mean_reward     | -5.54e+04   |
| time/              |             |
|    total_timesteps | 1200000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.46e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 586       |
|    time_elapsed    | 8103      |
|    total_timesteps | 1200128   |
----------------------------------
Eval num_timesteps=1200500, episode_reward=-49366.08 +/- 34325.57
Episode length: 47.40 +/- 18.51
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.087984726   |
|    mean velocity x      | 0.501         |
|    mean velocity y      | -0.456        |
|    mean velocity z      | 19.4          |
|    mean_ep_length       | 47.4          |
|    mean_reward          | -4.94e+04     |
| time/                   |               |
|    total_timesteps      | 1200500       |
| train/                  |               |
|    approx_kl            | 0.00078806153 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+08      |
|    n_updates            | 5860          |
|    policy_gradient_loss | -0.00102      |
|    std                  | 0.899         |
|    value_loss           | 2.02e+08      |
-------------------------------------------
Eval num_timesteps=1201000, episode_reward=-58380.69 +/- 36036.30
Episode length: 55.60 +/- 21.32
----------------------------------
| eval/              |           |
|    mean action     | 0.5806676 |
|    mean velocity x | -1.63     |
|    mean velocity y | -3.76     |
|    mean velocity z | 19.9      |
|    mean_ep_length  | 55.6      |
|    mean_reward     | -5.84e+04 |
| time/              |           |
|    total_timesteps | 1201000   |
----------------------------------
Eval num_timesteps=1201500, episode_reward=-47681.97 +/- 38190.79
Episode length: 58.60 +/- 35.63
------------------------------------
| eval/              |             |
|    mean action     | -0.18763483 |
|    mean velocity x | 0.0589      |
|    mean velocity y | 1.61        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 58.6        |
|    mean_reward     | -4.77e+04   |
| time/              |             |
|    total_timesteps | 1201500     |
------------------------------------
Eval num_timesteps=1202000, episode_reward=-92851.08 +/- 16235.21
Episode length: 61.60 +/- 4.84
----------------------------------
| eval/              |           |
|    mean action     | 0.1970016 |
|    mean velocity x | 0.281     |
|    mean velocity y | -0.674    |
|    mean velocity z | 20.5      |
|    mean_ep_length  | 61.6      |
|    mean_reward     | -9.29e+04 |
| time/              |           |
|    total_timesteps | 1202000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.8      |
|    ep_rew_mean     | -7.82e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 587       |
|    time_elapsed    | 8110      |
|    total_timesteps | 1202176   |
----------------------------------
Eval num_timesteps=1202500, episode_reward=-87720.98 +/- 26108.93
Episode length: 60.20 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean action          | 0.6042166    |
|    mean velocity x      | -1.03        |
|    mean velocity y      | -2.42        |
|    mean velocity z      | 17.5         |
|    mean_ep_length       | 60.2         |
|    mean_reward          | -8.77e+04    |
| time/                   |              |
|    total_timesteps      | 1202500      |
| train/                  |              |
|    approx_kl            | 8.104194e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+08     |
|    n_updates            | 5870         |
|    policy_gradient_loss | -0.000136    |
|    std                  | 0.899        |
|    value_loss           | 2.13e+08     |
------------------------------------------
Eval num_timesteps=1203000, episode_reward=-44252.81 +/- 11738.53
Episode length: 60.20 +/- 10.81
-------------------------------------
| eval/              |              |
|    mean action     | -0.032362085 |
|    mean velocity x | 1.73         |
|    mean velocity y | 1.43         |
|    mean velocity z | 17.7         |
|    mean_ep_length  | 60.2         |
|    mean_reward     | -4.43e+04    |
| time/              |              |
|    total_timesteps | 1203000      |
-------------------------------------
Eval num_timesteps=1203500, episode_reward=-71294.64 +/- 33550.15
Episode length: 57.60 +/- 8.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.20016342 |
|    mean velocity x | -0.16      |
|    mean velocity y | -0.996     |
|    mean velocity z | 15.2       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -7.13e+04  |
| time/              |            |
|    total_timesteps | 1203500    |
-----------------------------------
Eval num_timesteps=1204000, episode_reward=-69139.75 +/- 36694.41
Episode length: 66.80 +/- 38.73
-----------------------------------
| eval/              |            |
|    mean action     | 0.27466953 |
|    mean velocity x | -1.39      |
|    mean velocity y | -1.76      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 66.8       |
|    mean_reward     | -6.91e+04  |
| time/              |            |
|    total_timesteps | 1204000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.4      |
|    ep_rew_mean     | -7.79e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 588       |
|    time_elapsed    | 8117      |
|    total_timesteps | 1204224   |
----------------------------------
Eval num_timesteps=1204500, episode_reward=-48218.40 +/- 27788.78
Episode length: 50.00 +/- 6.72
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.32210827   |
|    mean velocity x      | 1.22          |
|    mean velocity y      | 2.72          |
|    mean velocity z      | 21            |
|    mean_ep_length       | 50            |
|    mean_reward          | -4.82e+04     |
| time/                   |               |
|    total_timesteps      | 1204500       |
| train/                  |               |
|    approx_kl            | 5.3168216e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.001         |
|    loss                 | 7.8e+07       |
|    n_updates            | 5880          |
|    policy_gradient_loss | -0.000175     |
|    std                  | 0.899         |
|    value_loss           | 1.58e+08      |
-------------------------------------------
Eval num_timesteps=1205000, episode_reward=-81117.15 +/- 7631.24
Episode length: 75.00 +/- 20.96
------------------------------------
| eval/              |             |
|    mean action     | -0.44167227 |
|    mean velocity x | 1.12        |
|    mean velocity y | 2.87        |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 75          |
|    mean_reward     | -8.11e+04   |
| time/              |             |
|    total_timesteps | 1205000     |
------------------------------------
Eval num_timesteps=1205500, episode_reward=-75158.74 +/- 59735.25
Episode length: 83.00 +/- 68.05
------------------------------------
| eval/              |             |
|    mean action     | -0.27658412 |
|    mean velocity x | 0.55        |
|    mean velocity y | 0.869       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 83          |
|    mean_reward     | -7.52e+04   |
| time/              |             |
|    total_timesteps | 1205500     |
------------------------------------
Eval num_timesteps=1206000, episode_reward=-50121.78 +/- 29696.67
Episode length: 55.00 +/- 12.70
-----------------------------------
| eval/              |            |
|    mean action     | 0.33442834 |
|    mean velocity x | -1.51      |
|    mean velocity y | -2.64      |
|    mean velocity z | 20         |
|    mean_ep_length  | 55         |
|    mean_reward     | -5.01e+04  |
| time/              |            |
|    total_timesteps | 1206000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.5      |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 589       |
|    time_elapsed    | 8124      |
|    total_timesteps | 1206272   |
----------------------------------
Eval num_timesteps=1206500, episode_reward=-68592.87 +/- 34123.53
Episode length: 55.40 +/- 12.13
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13081542   |
|    mean velocity x      | 0.955         |
|    mean velocity y      | 1.85          |
|    mean velocity z      | 16.9          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -6.86e+04     |
| time/                   |               |
|    total_timesteps      | 1206500       |
| train/                  |               |
|    approx_kl            | 0.00011426036 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.001         |
|    loss                 | 4.56e+07      |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.000413     |
|    std                  | 0.899         |
|    value_loss           | 2.23e+08      |
-------------------------------------------
Eval num_timesteps=1207000, episode_reward=-36585.47 +/- 28223.51
Episode length: 42.00 +/- 15.79
-----------------------------------
| eval/              |            |
|    mean action     | -1.0061598 |
|    mean velocity x | 5.11       |
|    mean velocity y | 7.57       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 42         |
|    mean_reward     | -3.66e+04  |
| time/              |            |
|    total_timesteps | 1207000    |
-----------------------------------
Eval num_timesteps=1207500, episode_reward=-71212.41 +/- 37000.48
Episode length: 58.00 +/- 20.94
-----------------------------------
| eval/              |            |
|    mean action     | 0.27769122 |
|    mean velocity x | 1.19       |
|    mean velocity y | -0.903     |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 58         |
|    mean_reward     | -7.12e+04  |
| time/              |            |
|    total_timesteps | 1207500    |
-----------------------------------
Eval num_timesteps=1208000, episode_reward=-95276.04 +/- 13932.89
Episode length: 68.20 +/- 9.83
------------------------------------
| eval/              |             |
|    mean action     | 0.025819214 |
|    mean velocity x | -0.177      |
|    mean velocity y | -0.697      |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 68.2        |
|    mean_reward     | -9.53e+04   |
| time/              |             |
|    total_timesteps | 1208000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -7.53e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 590       |
|    time_elapsed    | 8132      |
|    total_timesteps | 1208320   |
----------------------------------
Eval num_timesteps=1208500, episode_reward=-68640.43 +/- 25793.61
Episode length: 68.20 +/- 14.12
------------------------------------------
| eval/                   |              |
|    mean action          | 0.08050406   |
|    mean velocity x      | -0.37        |
|    mean velocity y      | -0.804       |
|    mean velocity z      | 16.7         |
|    mean_ep_length       | 68.2         |
|    mean_reward          | -6.86e+04    |
| time/                   |              |
|    total_timesteps      | 1208500      |
| train/                  |              |
|    approx_kl            | 0.0023599947 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.001        |
|    loss                 | 7.7e+07      |
|    n_updates            | 5900         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 0.899        |
|    value_loss           | 1.66e+08     |
------------------------------------------
Eval num_timesteps=1209000, episode_reward=-61740.57 +/- 31485.32
Episode length: 57.40 +/- 22.46
------------------------------------
| eval/              |             |
|    mean action     | -0.35449696 |
|    mean velocity x | 1.56        |
|    mean velocity y | 1.95        |
|    mean velocity z | 17.5        |
|    mean_ep_length  | 57.4        |
|    mean_reward     | -6.17e+04   |
| time/              |             |
|    total_timesteps | 1209000     |
------------------------------------
Eval num_timesteps=1209500, episode_reward=-57330.08 +/- 45016.89
Episode length: 53.00 +/- 15.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.13468319 |
|    mean velocity x | -0.959     |
|    mean velocity y | -0.897     |
|    mean velocity z | 15.7       |
|    mean_ep_length  | 53         |
|    mean_reward     | -5.73e+04  |
| time/              |            |
|    total_timesteps | 1209500    |
-----------------------------------
Eval num_timesteps=1210000, episode_reward=-68315.93 +/- 38043.78
Episode length: 57.40 +/- 14.53
------------------------------------
| eval/              |             |
|    mean action     | -0.26621422 |
|    mean velocity x | 1.26        |
|    mean velocity y | 2.42        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 57.4        |
|    mean_reward     | -6.83e+04   |
| time/              |             |
|    total_timesteps | 1210000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.6      |
|    ep_rew_mean     | -7.32e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 591       |
|    time_elapsed    | 8139      |
|    total_timesteps | 1210368   |
----------------------------------
Eval num_timesteps=1210500, episode_reward=-53356.76 +/- 32861.27
Episode length: 53.00 +/- 29.22
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.49874714   |
|    mean velocity x      | 1.35          |
|    mean velocity y      | 2.75          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 53            |
|    mean_reward          | -5.34e+04     |
| time/                   |               |
|    total_timesteps      | 1210500       |
| train/                  |               |
|    approx_kl            | 0.00043144106 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.338         |
|    learning_rate        | 0.001         |
|    loss                 | 8.56e+07      |
|    n_updates            | 5910          |
|    policy_gradient_loss | -0.000893     |
|    std                  | 0.899         |
|    value_loss           | 1.65e+08      |
-------------------------------------------
Eval num_timesteps=1211000, episode_reward=-76784.33 +/- 27486.17
Episode length: 69.40 +/- 28.88
----------------------------------
| eval/              |           |
|    mean action     | 0.6998681 |
|    mean velocity x | -2.82     |
|    mean velocity y | -3.86     |
|    mean velocity z | 18        |
|    mean_ep_length  | 69.4      |
|    mean_reward     | -7.68e+04 |
| time/              |           |
|    total_timesteps | 1211000   |
----------------------------------
Eval num_timesteps=1211500, episode_reward=-46050.57 +/- 38513.45
Episode length: 50.00 +/- 19.98
-----------------------------------
| eval/              |            |
|    mean action     | -0.3593612 |
|    mean velocity x | 0.853      |
|    mean velocity y | 2.8        |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 50         |
|    mean_reward     | -4.61e+04  |
| time/              |            |
|    total_timesteps | 1211500    |
-----------------------------------
Eval num_timesteps=1212000, episode_reward=-83356.65 +/- 26423.30
Episode length: 58.80 +/- 5.00
-------------------------------------
| eval/              |              |
|    mean action     | -0.121913396 |
|    mean velocity x | 2.13         |
|    mean velocity y | 1.82         |
|    mean velocity z | 19.8         |
|    mean_ep_length  | 58.8         |
|    mean_reward     | -8.34e+04    |
| time/              |              |
|    total_timesteps | 1212000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.4      |
|    ep_rew_mean     | -7.17e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 592       |
|    time_elapsed    | 8146      |
|    total_timesteps | 1212416   |
----------------------------------
Eval num_timesteps=1212500, episode_reward=-59245.53 +/- 31740.18
Episode length: 57.40 +/- 11.20
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.6706819     |
|    mean velocity x      | -2.96         |
|    mean velocity y      | -2.84         |
|    mean velocity z      | 16.7          |
|    mean_ep_length       | 57.4          |
|    mean_reward          | -5.92e+04     |
| time/                   |               |
|    total_timesteps      | 1212500       |
| train/                  |               |
|    approx_kl            | 2.9350893e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.339         |
|    learning_rate        | 0.001         |
|    loss                 | 6.43e+07      |
|    n_updates            | 5920          |
|    policy_gradient_loss | -0.000122     |
|    std                  | 0.9           |
|    value_loss           | 1.64e+08      |
-------------------------------------------
Eval num_timesteps=1213000, episode_reward=-51809.44 +/- 30313.70
Episode length: 48.80 +/- 16.23
------------------------------------
| eval/              |             |
|    mean action     | -0.29178864 |
|    mean velocity x | 0.157       |
|    mean velocity y | 0.325       |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -5.18e+04   |
| time/              |             |
|    total_timesteps | 1213000     |
------------------------------------
Eval num_timesteps=1213500, episode_reward=-73732.34 +/- 39162.50
Episode length: 78.20 +/- 40.16
------------------------------------
| eval/              |             |
|    mean action     | -0.24195571 |
|    mean velocity x | 0.855       |
|    mean velocity y | 1.76        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 78.2        |
|    mean_reward     | -7.37e+04   |
| time/              |             |
|    total_timesteps | 1213500     |
------------------------------------
Eval num_timesteps=1214000, episode_reward=-60752.87 +/- 42008.93
Episode length: 49.40 +/- 16.72
----------------------------------
| eval/              |           |
|    mean action     | 0.5422022 |
|    mean velocity x | -0.606    |
|    mean velocity y | -2.87     |
|    mean velocity z | 23.1      |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -6.08e+04 |
| time/              |           |
|    total_timesteps | 1214000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.4      |
|    ep_rew_mean     | -7.24e+04 |
| time/              |           |
|    fps             | 148       |
|    iterations      | 593       |
|    time_elapsed    | 8153      |
|    total_timesteps | 1214464   |
----------------------------------
Eval num_timesteps=1214500, episode_reward=-70255.57 +/- 60719.73
Episode length: 86.40 +/- 72.46
------------------------------------------
| eval/                   |              |
|    mean action          | -0.2907621   |
|    mean velocity x      | 0.496        |
|    mean velocity y      | 1.45         |
|    mean velocity z      | 21.9         |
|    mean_ep_length       | 86.4         |
|    mean_reward          | -7.03e+04    |
| time/                   |              |
|    total_timesteps      | 1214500      |
| train/                  |              |
|    approx_kl            | 0.0004524615 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.259        |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+08     |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.000536    |
|    std                  | 0.9          |
|    value_loss           | 2.28e+08     |
------------------------------------------
Eval num_timesteps=1215000, episode_reward=-84580.04 +/- 22820.50
Episode length: 63.80 +/- 3.12
------------------------------------
| eval/              |             |
|    mean action     | -0.14104599 |
|    mean velocity x | 0.595       |
|    mean velocity y | 1.16        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -8.46e+04   |
| time/              |             |
|    total_timesteps | 1215000     |
------------------------------------
Eval num_timesteps=1215500, episode_reward=-54702.59 +/- 39419.51
Episode length: 53.60 +/- 16.61
------------------------------------
| eval/              |             |
|    mean action     | -0.39964864 |
|    mean velocity x | 0.871       |
|    mean velocity y | 2.31        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -5.47e+04   |
| time/              |             |
|    total_timesteps | 1215500     |
------------------------------------
Eval num_timesteps=1216000, episode_reward=-63013.46 +/- 43018.21
Episode length: 63.60 +/- 44.11
------------------------------------
| eval/              |             |
|    mean action     | -0.11165618 |
|    mean velocity x | 0.212       |
|    mean velocity y | -0.299      |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 63.6        |
|    mean_reward     | -6.3e+04    |
| time/              |             |
|    total_timesteps | 1216000     |
------------------------------------
Eval num_timesteps=1216500, episode_reward=-61237.53 +/- 36465.20
Episode length: 51.40 +/- 23.44
------------------------------------
| eval/              |             |
|    mean action     | -0.17398989 |
|    mean velocity x | -1.34       |
|    mean velocity y | 0.176       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 51.4        |
|    mean_reward     | -6.12e+04   |
| time/              |             |
|    total_timesteps | 1216500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.7      |
|    ep_rew_mean     | -7.87e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 594       |
|    time_elapsed    | 8161      |
|    total_timesteps | 1216512   |
----------------------------------
Eval num_timesteps=1217000, episode_reward=-53730.09 +/- 36946.63
Episode length: 54.40 +/- 22.42
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.21463658   |
|    mean velocity x      | 0.498         |
|    mean velocity y      | 1.79          |
|    mean velocity z      | 19.5          |
|    mean_ep_length       | 54.4          |
|    mean_reward          | -5.37e+04     |
| time/                   |               |
|    total_timesteps      | 1217000       |
| train/                  |               |
|    approx_kl            | 1.9565341e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+08      |
|    n_updates            | 5940          |
|    policy_gradient_loss | -0.000181     |
|    std                  | 0.9           |
|    value_loss           | 2.07e+08      |
-------------------------------------------
Eval num_timesteps=1217500, episode_reward=-52682.50 +/- 30663.74
Episode length: 61.40 +/- 20.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.27947637 |
|    mean velocity x | -1.96      |
|    mean velocity y | -2.74      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 61.4       |
|    mean_reward     | -5.27e+04  |
| time/              |            |
|    total_timesteps | 1217500    |
-----------------------------------
Eval num_timesteps=1218000, episode_reward=-40528.11 +/- 36382.93
Episode length: 47.20 +/- 27.49
------------------------------------
| eval/              |             |
|    mean action     | 0.010249169 |
|    mean velocity x | -0.267      |
|    mean velocity y | 0.333       |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 47.2        |
|    mean_reward     | -4.05e+04   |
| time/              |             |
|    total_timesteps | 1218000     |
------------------------------------
Eval num_timesteps=1218500, episode_reward=-52691.92 +/- 20502.28
Episode length: 63.80 +/- 22.62
-----------------------------------
| eval/              |            |
|    mean action     | 0.18546447 |
|    mean velocity x | -2.66      |
|    mean velocity y | -1.66      |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 63.8       |
|    mean_reward     | -5.27e+04  |
| time/              |            |
|    total_timesteps | 1218500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69        |
|    ep_rew_mean     | -7.72e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 595       |
|    time_elapsed    | 8168      |
|    total_timesteps | 1218560   |
----------------------------------
Eval num_timesteps=1219000, episode_reward=-41296.21 +/- 33269.40
Episode length: 43.60 +/- 15.98
------------------------------------------
| eval/                   |              |
|    mean action          | 0.100148685  |
|    mean velocity x      | 2.41         |
|    mean velocity y      | 0.397        |
|    mean velocity z      | 18.6         |
|    mean_ep_length       | 43.6         |
|    mean_reward          | -4.13e+04    |
| time/                   |              |
|    total_timesteps      | 1219000      |
| train/                  |              |
|    approx_kl            | 0.0009354544 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.319        |
|    learning_rate        | 0.001        |
|    loss                 | 9.71e+07     |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000816    |
|    std                  | 0.9          |
|    value_loss           | 1.49e+08     |
------------------------------------------
Eval num_timesteps=1219500, episode_reward=-90408.15 +/- 24388.28
Episode length: 73.80 +/- 27.85
------------------------------------
| eval/              |             |
|    mean action     | -0.63721853 |
|    mean velocity x | 1.12        |
|    mean velocity y | 4           |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 73.8        |
|    mean_reward     | -9.04e+04   |
| time/              |             |
|    total_timesteps | 1219500     |
------------------------------------
Eval num_timesteps=1220000, episode_reward=-57985.22 +/- 35383.59
Episode length: 61.20 +/- 18.49
------------------------------------
| eval/              |             |
|    mean action     | -0.16198696 |
|    mean velocity x | 1.45        |
|    mean velocity y | 1.46        |
|    mean velocity z | 22.7        |
|    mean_ep_length  | 61.2        |
|    mean_reward     | -5.8e+04    |
| time/              |             |
|    total_timesteps | 1220000     |
------------------------------------
Eval num_timesteps=1220500, episode_reward=-106103.40 +/- 26277.50
Episode length: 88.40 +/- 39.04
------------------------------------
| eval/              |             |
|    mean action     | -0.23152246 |
|    mean velocity x | -0.17       |
|    mean velocity y | -0.869      |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 88.4        |
|    mean_reward     | -1.06e+05   |
| time/              |             |
|    total_timesteps | 1220500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.2      |
|    ep_rew_mean     | -8.34e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 596       |
|    time_elapsed    | 8175      |
|    total_timesteps | 1220608   |
----------------------------------
Eval num_timesteps=1221000, episode_reward=-61584.64 +/- 31430.34
Episode length: 59.60 +/- 22.63
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.026719024  |
|    mean velocity x      | -0.169        |
|    mean velocity y      | 0.828         |
|    mean velocity z      | 19.7          |
|    mean_ep_length       | 59.6          |
|    mean_reward          | -6.16e+04     |
| time/                   |               |
|    total_timesteps      | 1221000       |
| train/                  |               |
|    approx_kl            | 0.00042378713 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.278         |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+08      |
|    n_updates            | 5960          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 0.9           |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=1221500, episode_reward=-74648.38 +/- 34091.96
Episode length: 56.20 +/- 7.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.27835143 |
|    mean velocity x | -2.63      |
|    mean velocity y | -1.57      |
|    mean velocity z | 15.8       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -7.46e+04  |
| time/              |            |
|    total_timesteps | 1221500    |
-----------------------------------
Eval num_timesteps=1222000, episode_reward=-72582.31 +/- 15648.58
Episode length: 59.20 +/- 3.31
-------------------------------------
| eval/              |              |
|    mean action     | -0.045719773 |
|    mean velocity x | -0.18        |
|    mean velocity y | -0.74        |
|    mean velocity z | 17.9         |
|    mean_ep_length  | 59.2         |
|    mean_reward     | -7.26e+04    |
| time/              |              |
|    total_timesteps | 1222000      |
-------------------------------------
Eval num_timesteps=1222500, episode_reward=-72326.89 +/- 27912.09
Episode length: 57.60 +/- 5.08
-----------------------------------
| eval/              |            |
|    mean action     | 0.40929908 |
|    mean velocity x | -0.928     |
|    mean velocity y | -2.43      |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 1222500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.3      |
|    ep_rew_mean     | -7.78e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 597       |
|    time_elapsed    | 8183      |
|    total_timesteps | 1222656   |
----------------------------------
Eval num_timesteps=1223000, episode_reward=-80461.91 +/- 18531.44
Episode length: 61.40 +/- 8.33
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.020525932  |
|    mean velocity x      | 1.16          |
|    mean velocity y      | 0.749         |
|    mean velocity z      | 15.8          |
|    mean_ep_length       | 61.4          |
|    mean_reward          | -8.05e+04     |
| time/                   |               |
|    total_timesteps      | 1223000       |
| train/                  |               |
|    approx_kl            | 9.5427706e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.001         |
|    loss                 | 7.92e+07      |
|    n_updates            | 5970          |
|    policy_gradient_loss | -0.000471     |
|    std                  | 0.9           |
|    value_loss           | 1.71e+08      |
-------------------------------------------
Eval num_timesteps=1223500, episode_reward=-56411.90 +/- 43330.56
Episode length: 53.40 +/- 27.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.26136714 |
|    mean velocity x | -1.14      |
|    mean velocity y | -1.62      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 53.4       |
|    mean_reward     | -5.64e+04  |
| time/              |            |
|    total_timesteps | 1223500    |
-----------------------------------
Eval num_timesteps=1224000, episode_reward=-79946.60 +/- 20744.86
Episode length: 61.80 +/- 7.47
----------------------------------
| eval/              |           |
|    mean action     | 0.236673  |
|    mean velocity x | -0.331    |
|    mean velocity y | -1.73     |
|    mean velocity z | 20.5      |
|    mean_ep_length  | 61.8      |
|    mean_reward     | -7.99e+04 |
| time/              |           |
|    total_timesteps | 1224000   |
----------------------------------
Eval num_timesteps=1224500, episode_reward=-61205.63 +/- 16693.98
Episode length: 57.40 +/- 7.12
-----------------------------------
| eval/              |            |
|    mean action     | -0.3541374 |
|    mean velocity x | -0.374     |
|    mean velocity y | 1.52       |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 57.4       |
|    mean_reward     | -6.12e+04  |
| time/              |            |
|    total_timesteps | 1224500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70        |
|    ep_rew_mean     | -7.44e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 598       |
|    time_elapsed    | 8190      |
|    total_timesteps | 1224704   |
----------------------------------
Eval num_timesteps=1225000, episode_reward=-75542.26 +/- 19777.99
Episode length: 65.60 +/- 11.04
------------------------------------------
| eval/                   |              |
|    mean action          | -0.12190961  |
|    mean velocity x      | 0.437        |
|    mean velocity y      | 0.18         |
|    mean velocity z      | 20.6         |
|    mean_ep_length       | 65.6         |
|    mean_reward          | -7.55e+04    |
| time/                   |              |
|    total_timesteps      | 1225000      |
| train/                  |              |
|    approx_kl            | 3.294341e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.001        |
|    loss                 | 9.3e+07      |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.000149    |
|    std                  | 0.9          |
|    value_loss           | 2.09e+08     |
------------------------------------------
Eval num_timesteps=1225500, episode_reward=-71283.06 +/- 31194.10
Episode length: 60.20 +/- 10.57
------------------------------------
| eval/              |             |
|    mean action     | 0.047455247 |
|    mean velocity x | -0.414      |
|    mean velocity y | -0.843      |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 60.2        |
|    mean_reward     | -7.13e+04   |
| time/              |             |
|    total_timesteps | 1225500     |
------------------------------------
Eval num_timesteps=1226000, episode_reward=-61919.22 +/- 29889.06
Episode length: 58.20 +/- 13.11
------------------------------------
| eval/              |             |
|    mean action     | -0.13608049 |
|    mean velocity x | 0.472       |
|    mean velocity y | 0.444       |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -6.19e+04   |
| time/              |             |
|    total_timesteps | 1226000     |
------------------------------------
Eval num_timesteps=1226500, episode_reward=-76732.00 +/- 8808.81
Episode length: 64.60 +/- 7.71
------------------------------------
| eval/              |             |
|    mean action     | -0.17701116 |
|    mean velocity x | -0.937      |
|    mean velocity y | 0.2         |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 64.6        |
|    mean_reward     | -7.67e+04   |
| time/              |             |
|    total_timesteps | 1226500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.4      |
|    ep_rew_mean     | -7.53e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 599       |
|    time_elapsed    | 8197      |
|    total_timesteps | 1226752   |
----------------------------------
Eval num_timesteps=1227000, episode_reward=-62735.70 +/- 31826.48
Episode length: 70.60 +/- 35.83
------------------------------------------
| eval/                   |              |
|    mean action          | 0.29648095   |
|    mean velocity x      | -1.11        |
|    mean velocity y      | -1.73        |
|    mean velocity z      | 16.8         |
|    mean_ep_length       | 70.6         |
|    mean_reward          | -6.27e+04    |
| time/                   |              |
|    total_timesteps      | 1227000      |
| train/                  |              |
|    approx_kl            | 0.0018926059 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.001        |
|    loss                 | 5.14e+07     |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00196     |
|    std                  | 0.9          |
|    value_loss           | 1.83e+08     |
------------------------------------------
Eval num_timesteps=1227500, episode_reward=-37181.80 +/- 24898.87
Episode length: 61.00 +/- 28.99
------------------------------------
| eval/              |             |
|    mean action     | -0.21944238 |
|    mean velocity x | -0.49       |
|    mean velocity y | 0.111       |
|    mean velocity z | 21          |
|    mean_ep_length  | 61          |
|    mean_reward     | -3.72e+04   |
| time/              |             |
|    total_timesteps | 1227500     |
------------------------------------
Eval num_timesteps=1228000, episode_reward=-68921.76 +/- 37445.85
Episode length: 56.60 +/- 24.08
----------------------------------
| eval/              |           |
|    mean action     | 0.5874872 |
|    mean velocity x | -0.394    |
|    mean velocity y | -3.06     |
|    mean velocity z | 17.2      |
|    mean_ep_length  | 56.6      |
|    mean_reward     | -6.89e+04 |
| time/              |           |
|    total_timesteps | 1228000   |
----------------------------------
Eval num_timesteps=1228500, episode_reward=-73520.31 +/- 38930.81
Episode length: 57.40 +/- 23.62
-------------------------------------
| eval/              |              |
|    mean action     | -0.053325113 |
|    mean velocity x | -0.775       |
|    mean velocity y | -0.142       |
|    mean velocity z | 17.6         |
|    mean_ep_length  | 57.4         |
|    mean_reward     | -7.35e+04    |
| time/              |              |
|    total_timesteps | 1228500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.1      |
|    ep_rew_mean     | -7.07e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 600       |
|    time_elapsed    | 8205      |
|    total_timesteps | 1228800   |
----------------------------------
Eval num_timesteps=1229000, episode_reward=-55415.43 +/- 52433.84
Episode length: 70.80 +/- 55.55
------------------------------------------
| eval/                   |              |
|    mean action          | 0.052538104  |
|    mean velocity x      | -1.3         |
|    mean velocity y      | 0.0775       |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 70.8         |
|    mean_reward          | -5.54e+04    |
| time/                   |              |
|    total_timesteps      | 1229000      |
| train/                  |              |
|    approx_kl            | 0.0011102352 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+08     |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.9          |
|    value_loss           | 1.74e+08     |
------------------------------------------
Eval num_timesteps=1229500, episode_reward=-72793.66 +/- 25081.99
Episode length: 54.80 +/- 9.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.05644467 |
|    mean velocity x | -0.467     |
|    mean velocity y | -1.43      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 54.8       |
|    mean_reward     | -7.28e+04  |
| time/              |            |
|    total_timesteps | 1229500    |
-----------------------------------
Eval num_timesteps=1230000, episode_reward=-89786.05 +/- 31691.36
Episode length: 78.60 +/- 36.71
------------------------------------
| eval/              |             |
|    mean action     | -0.63089484 |
|    mean velocity x | 2.97        |
|    mean velocity y | 4.12        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 78.6        |
|    mean_reward     | -8.98e+04   |
| time/              |             |
|    total_timesteps | 1230000     |
------------------------------------
Eval num_timesteps=1230500, episode_reward=-66417.17 +/- 35214.77
Episode length: 72.20 +/- 36.23
------------------------------------
| eval/              |             |
|    mean action     | 0.122028746 |
|    mean velocity x | 0.342       |
|    mean velocity y | -0.249      |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 72.2        |
|    mean_reward     | -6.64e+04   |
| time/              |             |
|    total_timesteps | 1230500     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.9     |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 149      |
|    iterations      | 601      |
|    time_elapsed    | 8212     |
|    total_timesteps | 1230848  |
---------------------------------
Eval num_timesteps=1231000, episode_reward=-76418.92 +/- 30421.13
Episode length: 61.40 +/- 9.26
------------------------------------------
| eval/                   |              |
|    mean action          | 0.04664285   |
|    mean velocity x      | 0.478        |
|    mean velocity y      | -0.683       |
|    mean velocity z      | 20.6         |
|    mean_ep_length       | 61.4         |
|    mean_reward          | -7.64e+04    |
| time/                   |              |
|    total_timesteps      | 1231000      |
| train/                  |              |
|    approx_kl            | 0.0009893787 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.001        |
|    loss                 | 1.21e+08     |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.9          |
|    value_loss           | 2.13e+08     |
------------------------------------------
Eval num_timesteps=1231500, episode_reward=-60680.02 +/- 29160.60
Episode length: 64.60 +/- 21.00
------------------------------------
| eval/              |             |
|    mean action     | -0.65938914 |
|    mean velocity x | 2.42        |
|    mean velocity y | 3.93        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 64.6        |
|    mean_reward     | -6.07e+04   |
| time/              |             |
|    total_timesteps | 1231500     |
------------------------------------
Eval num_timesteps=1232000, episode_reward=-84775.59 +/- 12841.30
Episode length: 71.60 +/- 13.14
-----------------------------------
| eval/              |            |
|    mean action     | 0.05214847 |
|    mean velocity x | -0.00787   |
|    mean velocity y | -1.5       |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 71.6       |
|    mean_reward     | -8.48e+04  |
| time/              |            |
|    total_timesteps | 1232000    |
-----------------------------------
Eval num_timesteps=1232500, episode_reward=-77846.19 +/- 22333.10
Episode length: 68.40 +/- 13.15
------------------------------------
| eval/              |             |
|    mean action     | -0.20812373 |
|    mean velocity x | -0.268      |
|    mean velocity y | 1.15        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 1232500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.1      |
|    ep_rew_mean     | -7.56e+04 |
| time/              |           |
|    fps             | 149       |
|    iterations      | 602       |
|    time_elapsed    | 8219      |
|    total_timesteps | 1232896   |
----------------------------------
Eval num_timesteps=1233000, episode_reward=-84713.11 +/- 32187.54
Episode length: 61.00 +/- 11.92
------------------------------------------
| eval/                   |              |
|    mean action          | 0.067560546  |
|    mean velocity x      | -1.18        |
|    mean velocity y      | -1.41        |
|    mean velocity z      | 20.3         |
|    mean_ep_length       | 61           |
|    mean_reward          | -8.47e+04    |
| time/                   |              |
|    total_timesteps      | 1233000      |
| train/                  |              |
|    approx_kl            | 0.0007678938 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.001        |
|    loss                 | 1.3e+08      |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.000986    |
|    std                  | 0.9          |
|    value_loss           | 1.98e+08     |
------------------------------------------
Eval num_timesteps=1233500, episode_reward=-85393.61 +/- 12583.98
Episode length: 79.80 +/- 22.02
------------------------------------
| eval/              |             |
|    mean action     | -0.58552796 |
|    mean velocity x | 1.7         |
|    mean velocity y | 3.05        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 79.8        |
|    mean_reward     | -8.54e+04   |
| time/              |             |
|    total_timesteps | 1233500     |
------------------------------------
Eval num_timesteps=1234000, episode_reward=-51661.48 +/- 19462.03
Episode length: 58.80 +/- 12.35
----------------------------------
| eval/              |           |
|    mean action     | 0.4139813 |
|    mean velocity x | -1.37     |
|    mean velocity y | -2.89     |
|    mean velocity z | 19.2      |
|    mean_ep_length  | 58.8      |
|    mean_reward     | -5.17e+04 |
| time/              |           |
|    total_timesteps | 1234000   |
----------------------------------
Eval num_timesteps=1234500, episode_reward=-83374.84 +/- 24882.80
Episode length: 59.60 +/- 5.64
------------------------------------
| eval/              |             |
|    mean action     | -0.68802404 |
|    mean velocity x | 1.43        |
|    mean velocity y | 2.64        |
|    mean velocity z | 16.2        |
|    mean_ep_length  | 59.6        |
|    mean_reward     | -8.34e+04   |
| time/              |             |
|    total_timesteps | 1234500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.7      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 603       |
|    time_elapsed    | 8227      |
|    total_timesteps | 1234944   |
----------------------------------
Eval num_timesteps=1235000, episode_reward=-69642.72 +/- 26801.23
Episode length: 59.20 +/- 5.71
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.2633092     |
|    mean velocity x      | -1.57         |
|    mean velocity y      | -2.06         |
|    mean velocity z      | 19            |
|    mean_ep_length       | 59.2          |
|    mean_reward          | -6.96e+04     |
| time/                   |               |
|    total_timesteps      | 1235000       |
| train/                  |               |
|    approx_kl            | 2.7846865e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.302         |
|    learning_rate        | 0.001         |
|    loss                 | 4.68e+07      |
|    n_updates            | 6030          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 0.9           |
|    value_loss           | 1.63e+08      |
-------------------------------------------
Eval num_timesteps=1235500, episode_reward=-112971.78 +/- 29669.29
Episode length: 96.20 +/- 69.91
-----------------------------------
| eval/              |            |
|    mean action     | -0.5607078 |
|    mean velocity x | 3.02       |
|    mean velocity y | 3.82       |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 96.2       |
|    mean_reward     | -1.13e+05  |
| time/              |            |
|    total_timesteps | 1235500    |
-----------------------------------
Eval num_timesteps=1236000, episode_reward=-82887.21 +/- 31973.09
Episode length: 90.60 +/- 43.95
------------------------------------
| eval/              |             |
|    mean action     | 0.034158904 |
|    mean velocity x | 1.18        |
|    mean velocity y | -0.425      |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 90.6        |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 1236000     |
------------------------------------
Eval num_timesteps=1236500, episode_reward=-60785.50 +/- 31283.27
Episode length: 65.40 +/- 27.77
------------------------------------
| eval/              |             |
|    mean action     | -0.25808302 |
|    mean velocity x | 0.394       |
|    mean velocity y | 1.68        |
|    mean velocity z | 23.4        |
|    mean_ep_length  | 65.4        |
|    mean_reward     | -6.08e+04   |
| time/              |             |
|    total_timesteps | 1236500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.9      |
|    ep_rew_mean     | -8.65e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 604       |
|    time_elapsed    | 8234      |
|    total_timesteps | 1236992   |
----------------------------------
Eval num_timesteps=1237000, episode_reward=-59900.16 +/- 27932.11
Episode length: 55.60 +/- 6.05
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.05035608   |
|    mean velocity x      | -0.53         |
|    mean velocity y      | -0.52         |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 55.6          |
|    mean_reward          | -5.99e+04     |
| time/                   |               |
|    total_timesteps      | 1237000       |
| train/                  |               |
|    approx_kl            | 8.9003384e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.231         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+08      |
|    n_updates            | 6040          |
|    policy_gradient_loss | -0.000241     |
|    std                  | 0.9           |
|    value_loss           | 2.43e+08      |
-------------------------------------------
Eval num_timesteps=1237500, episode_reward=-77317.30 +/- 16856.12
Episode length: 60.20 +/- 3.60
------------------------------------
| eval/              |             |
|    mean action     | 0.022101479 |
|    mean velocity x | -0.234      |
|    mean velocity y | -1.35       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 60.2        |
|    mean_reward     | -7.73e+04   |
| time/              |             |
|    total_timesteps | 1237500     |
------------------------------------
Eval num_timesteps=1238000, episode_reward=-66336.36 +/- 43711.06
Episode length: 49.20 +/- 17.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.92804646 |
|    mean velocity x | -4.7       |
|    mean velocity y | -5.75      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 49.2       |
|    mean_reward     | -6.63e+04  |
| time/              |            |
|    total_timesteps | 1238000    |
-----------------------------------
Eval num_timesteps=1238500, episode_reward=-38224.35 +/- 32737.16
Episode length: 53.40 +/- 27.14
-------------------------------------
| eval/              |              |
|    mean action     | -0.111678876 |
|    mean velocity x | -0.713       |
|    mean velocity y | 0.49         |
|    mean velocity z | 20.6         |
|    mean_ep_length  | 53.4         |
|    mean_reward     | -3.82e+04    |
| time/              |              |
|    total_timesteps | 1238500      |
-------------------------------------
Eval num_timesteps=1239000, episode_reward=-74388.37 +/- 35532.90
Episode length: 52.20 +/- 15.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.04421678 |
|    mean velocity x | -1.9       |
|    mean velocity y | -1.33      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 52.2       |
|    mean_reward     | -7.44e+04  |
| time/              |            |
|    total_timesteps | 1239000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 77.8      |
|    ep_rew_mean     | -8.72e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 605       |
|    time_elapsed    | 8241      |
|    total_timesteps | 1239040   |
----------------------------------
Eval num_timesteps=1239500, episode_reward=-75572.77 +/- 36951.52
Episode length: 66.80 +/- 36.61
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.27563408    |
|    mean velocity x      | -1.12         |
|    mean velocity y      | -1.49         |
|    mean velocity z      | 17.1          |
|    mean_ep_length       | 66.8          |
|    mean_reward          | -7.56e+04     |
| time/                   |               |
|    total_timesteps      | 1239500       |
| train/                  |               |
|    approx_kl            | 0.00049392274 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.253         |
|    learning_rate        | 0.001         |
|    loss                 | 1.27e+08      |
|    n_updates            | 6050          |
|    policy_gradient_loss | -0.0008       |
|    std                  | 0.9           |
|    value_loss           | 2.24e+08      |
-------------------------------------------
Eval num_timesteps=1240000, episode_reward=-78520.20 +/- 30814.49
Episode length: 67.60 +/- 21.46
----------------------------------
| eval/              |           |
|    mean action     | -0.591517 |
|    mean velocity x | 2.59      |
|    mean velocity y | 3.39      |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 67.6      |
|    mean_reward     | -7.85e+04 |
| time/              |           |
|    total_timesteps | 1240000   |
----------------------------------
Eval num_timesteps=1240500, episode_reward=-86954.56 +/- 30353.78
Episode length: 80.40 +/- 42.41
------------------------------------
| eval/              |             |
|    mean action     | -0.12523557 |
|    mean velocity x | 1.53        |
|    mean velocity y | 0.788       |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 80.4        |
|    mean_reward     | -8.7e+04    |
| time/              |             |
|    total_timesteps | 1240500     |
------------------------------------
Eval num_timesteps=1241000, episode_reward=-84253.90 +/- 39775.49
Episode length: 59.00 +/- 12.39
-----------------------------------
| eval/              |            |
|    mean action     | -0.6200781 |
|    mean velocity x | 2.47       |
|    mean velocity y | 3.67       |
|    mean velocity z | 18         |
|    mean_ep_length  | 59         |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 1241000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.7      |
|    ep_rew_mean     | -8.29e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 606       |
|    time_elapsed    | 8249      |
|    total_timesteps | 1241088   |
----------------------------------
Eval num_timesteps=1241500, episode_reward=-61384.13 +/- 37162.54
Episode length: 51.00 +/- 11.33
------------------------------------------
| eval/                   |              |
|    mean action          | -0.17795174  |
|    mean velocity x      | 1.26         |
|    mean velocity y      | 1.66         |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 51           |
|    mean_reward          | -6.14e+04    |
| time/                   |              |
|    total_timesteps      | 1241500      |
| train/                  |              |
|    approx_kl            | 0.0018188015 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.001        |
|    loss                 | 4.53e+07     |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.899        |
|    value_loss           | 1.35e+08     |
------------------------------------------
Eval num_timesteps=1242000, episode_reward=-92895.10 +/- 10525.16
Episode length: 61.80 +/- 2.32
----------------------------------
| eval/              |           |
|    mean action     | 0.21295   |
|    mean velocity x | -1.37     |
|    mean velocity y | -2.12     |
|    mean velocity z | 17.7      |
|    mean_ep_length  | 61.8      |
|    mean_reward     | -9.29e+04 |
| time/              |           |
|    total_timesteps | 1242000   |
----------------------------------
Eval num_timesteps=1242500, episode_reward=-66215.32 +/- 36181.98
Episode length: 55.00 +/- 25.34
------------------------------------
| eval/              |             |
|    mean action     | -0.18655881 |
|    mean velocity x | 0.993       |
|    mean velocity y | 0.404       |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 55          |
|    mean_reward     | -6.62e+04   |
| time/              |             |
|    total_timesteps | 1242500     |
------------------------------------
Eval num_timesteps=1243000, episode_reward=-78787.26 +/- 34161.65
Episode length: 68.40 +/- 24.08
------------------------------------
| eval/              |             |
|    mean action     | -0.22334936 |
|    mean velocity x | 1.33        |
|    mean velocity y | 2.04        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 68.4        |
|    mean_reward     | -7.88e+04   |
| time/              |             |
|    total_timesteps | 1243000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.3      |
|    ep_rew_mean     | -8.22e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 607       |
|    time_elapsed    | 8256      |
|    total_timesteps | 1243136   |
----------------------------------
Eval num_timesteps=1243500, episode_reward=-58230.45 +/- 30064.09
Episode length: 55.80 +/- 11.72
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2719445    |
|    mean velocity x      | 1.57          |
|    mean velocity y      | 2.88          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 55.8          |
|    mean_reward          | -5.82e+04     |
| time/                   |               |
|    total_timesteps      | 1243500       |
| train/                  |               |
|    approx_kl            | 0.00096762105 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+08      |
|    n_updates            | 6070          |
|    policy_gradient_loss | -0.000935     |
|    std                  | 0.899         |
|    value_loss           | 2.04e+08      |
-------------------------------------------
Eval num_timesteps=1244000, episode_reward=-75529.92 +/- 42202.92
Episode length: 57.20 +/- 10.11
------------------------------------
| eval/              |             |
|    mean action     | -0.30215028 |
|    mean velocity x | 1.14        |
|    mean velocity y | 2.84        |
|    mean velocity z | 16.1        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -7.55e+04   |
| time/              |             |
|    total_timesteps | 1244000     |
------------------------------------
Eval num_timesteps=1244500, episode_reward=-39096.03 +/- 36323.08
Episode length: 44.20 +/- 28.72
----------------------------------
| eval/              |           |
|    mean action     | 0.5855389 |
|    mean velocity x | -0.493    |
|    mean velocity y | -2.34     |
|    mean velocity z | 18.7      |
|    mean_ep_length  | 44.2      |
|    mean_reward     | -3.91e+04 |
| time/              |           |
|    total_timesteps | 1244500   |
----------------------------------
Eval num_timesteps=1245000, episode_reward=-91031.48 +/- 11259.74
Episode length: 79.20 +/- 20.36
------------------------------------
| eval/              |             |
|    mean action     | -0.14522195 |
|    mean velocity x | -1.22       |
|    mean velocity y | -0.745      |
|    mean velocity z | 18          |
|    mean_ep_length  | 79.2        |
|    mean_reward     | -9.1e+04    |
| time/              |             |
|    total_timesteps | 1245000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.3      |
|    ep_rew_mean     | -7.14e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 608       |
|    time_elapsed    | 8263      |
|    total_timesteps | 1245184   |
----------------------------------
Eval num_timesteps=1245500, episode_reward=-57565.35 +/- 28224.21
Episode length: 65.20 +/- 22.18
------------------------------------------
| eval/                   |              |
|    mean action          | 0.15579419   |
|    mean velocity x      | 0.204        |
|    mean velocity y      | -0.806       |
|    mean velocity z      | 17.5         |
|    mean_ep_length       | 65.2         |
|    mean_reward          | -5.76e+04    |
| time/                   |              |
|    total_timesteps      | 1245500      |
| train/                  |              |
|    approx_kl            | 0.0006340914 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.001        |
|    loss                 | 5.59e+07     |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.000952    |
|    std                  | 0.899        |
|    value_loss           | 1.59e+08     |
------------------------------------------
Eval num_timesteps=1246000, episode_reward=-46846.66 +/- 40925.30
Episode length: 58.00 +/- 44.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.15045752 |
|    mean velocity x | -0.702     |
|    mean velocity y | -1.01      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 58         |
|    mean_reward     | -4.68e+04  |
| time/              |            |
|    total_timesteps | 1246000    |
-----------------------------------
Eval num_timesteps=1246500, episode_reward=-72291.83 +/- 23683.78
Episode length: 63.60 +/- 15.03
-----------------------------------
| eval/              |            |
|    mean action     | 0.20595588 |
|    mean velocity x | 0.246      |
|    mean velocity y | -0.169     |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 63.6       |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 1246500    |
-----------------------------------
Eval num_timesteps=1247000, episode_reward=-59383.10 +/- 40487.55
Episode length: 56.80 +/- 28.73
------------------------------------
| eval/              |             |
|    mean action     | -0.35786104 |
|    mean velocity x | 1.22        |
|    mean velocity y | 1.28        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 56.8        |
|    mean_reward     | -5.94e+04   |
| time/              |             |
|    total_timesteps | 1247000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.65e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 609       |
|    time_elapsed    | 8270      |
|    total_timesteps | 1247232   |
----------------------------------
Eval num_timesteps=1247500, episode_reward=-58965.02 +/- 48616.01
Episode length: 46.80 +/- 24.96
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.087946564  |
|    mean velocity x      | -0.475        |
|    mean velocity y      | -0.443        |
|    mean velocity z      | 18.1          |
|    mean_ep_length       | 46.8          |
|    mean_reward          | -5.9e+04      |
| time/                   |               |
|    total_timesteps      | 1247500       |
| train/                  |               |
|    approx_kl            | 3.3402757e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+08      |
|    n_updates            | 6090          |
|    policy_gradient_loss | -0.000341     |
|    std                  | 0.899         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=1248000, episode_reward=-69291.18 +/- 16554.24
Episode length: 71.00 +/- 30.29
------------------------------------
| eval/              |             |
|    mean action     | -0.05386844 |
|    mean velocity x | 0.192       |
|    mean velocity y | 0.411       |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 71          |
|    mean_reward     | -6.93e+04   |
| time/              |             |
|    total_timesteps | 1248000     |
------------------------------------
Eval num_timesteps=1248500, episode_reward=-64043.63 +/- 33764.19
Episode length: 54.60 +/- 15.70
-----------------------------------
| eval/              |            |
|    mean action     | -0.5606441 |
|    mean velocity x | 2.49       |
|    mean velocity y | 3.2        |
|    mean velocity z | 14.5       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -6.4e+04   |
| time/              |            |
|    total_timesteps | 1248500    |
-----------------------------------
Eval num_timesteps=1249000, episode_reward=-91896.69 +/- 21348.73
Episode length: 61.60 +/- 4.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.28163525 |
|    mean velocity x | 1.22       |
|    mean velocity y | -0.947     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -9.19e+04  |
| time/              |            |
|    total_timesteps | 1249000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.7      |
|    ep_rew_mean     | -7.07e+04 |
| time/              |           |
|    fps             | 150       |
|    iterations      | 610       |
|    time_elapsed    | 8278      |
|    total_timesteps | 1249280   |
----------------------------------
Eval num_timesteps=1249500, episode_reward=-79365.54 +/- 11966.15
Episode length: 61.20 +/- 1.72
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.08958238 |
|    mean velocity x      | 1.64        |
|    mean velocity y      | 1.79        |
|    mean velocity z      | 18          |
|    mean_ep_length       | 61.2        |
|    mean_reward          | -7.94e+04   |
| time/                   |             |
|    total_timesteps      | 1249500     |
| train/                  |             |
|    approx_kl            | 0.001914267 |
|    clip_fraction        | 0.00366     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.001       |
|    loss                 | 9.25e+07    |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.00109    |
|    std                  | 0.899       |
|    value_loss           | 1.41e+08    |
-----------------------------------------
Eval num_timesteps=1250000, episode_reward=-53984.68 +/- 25643.82
Episode length: 57.00 +/- 19.03
-----------------------------------
| eval/              |            |
|    mean action     | -0.2331351 |
|    mean velocity x | 0.488      |
|    mean velocity y | 1.44       |
|    mean velocity z | 20.9       |
|    mean_ep_length  | 57         |
|    mean_reward     | -5.4e+04   |
| time/              |            |
|    total_timesteps | 1250000    |
-----------------------------------
Eval num_timesteps=1250500, episode_reward=-90472.56 +/- 17111.62
Episode length: 72.80 +/- 14.22
----------------------------------
| eval/              |           |
|    mean action     | 0.696849  |
|    mean velocity x | -1.46     |
|    mean velocity y | -4.13     |
|    mean velocity z | 15.3      |
|    mean_ep_length  | 72.8      |
|    mean_reward     | -9.05e+04 |
| time/              |           |
|    total_timesteps | 1250500   |
----------------------------------
Eval num_timesteps=1251000, episode_reward=-83255.33 +/- 60015.64
Episode length: 79.80 +/- 62.41
-------------------------------------
| eval/              |              |
|    mean action     | -0.051548332 |
|    mean velocity x | 0.36         |
|    mean velocity y | 1.08         |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 79.8         |
|    mean_reward     | -8.33e+04    |
| time/              |              |
|    total_timesteps | 1251000      |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.2     |
|    ep_rew_mean     | -7.1e+04 |
| time/              |          |
|    fps             | 151      |
|    iterations      | 611      |
|    time_elapsed    | 8285     |
|    total_timesteps | 1251328  |
---------------------------------
Eval num_timesteps=1251500, episode_reward=-40794.41 +/- 42565.83
Episode length: 48.00 +/- 26.08
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.56902474    |
|    mean velocity x      | -1.32         |
|    mean velocity y      | -3.93         |
|    mean velocity z      | 21.5          |
|    mean_ep_length       | 48            |
|    mean_reward          | -4.08e+04     |
| time/                   |               |
|    total_timesteps      | 1251500       |
| train/                  |               |
|    approx_kl            | 0.00054533634 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.33          |
|    learning_rate        | 0.001         |
|    loss                 | 5e+07         |
|    n_updates            | 6110          |
|    policy_gradient_loss | -0.00165      |
|    std                  | 0.899         |
|    value_loss           | 1.88e+08      |
-------------------------------------------
Eval num_timesteps=1252000, episode_reward=-63789.45 +/- 34612.74
Episode length: 52.20 +/- 16.45
------------------------------------
| eval/              |             |
|    mean action     | -0.26423192 |
|    mean velocity x | 0.239       |
|    mean velocity y | 1.21        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -6.38e+04   |
| time/              |             |
|    total_timesteps | 1252000     |
------------------------------------
Eval num_timesteps=1252500, episode_reward=-76979.09 +/- 30031.03
Episode length: 60.00 +/- 6.23
-----------------------------------
| eval/              |            |
|    mean action     | -0.3205457 |
|    mean velocity x | 0.68       |
|    mean velocity y | 1.84       |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 60         |
|    mean_reward     | -7.7e+04   |
| time/              |            |
|    total_timesteps | 1252500    |
-----------------------------------
Eval num_timesteps=1253000, episode_reward=-59129.74 +/- 41838.61
Episode length: 49.20 +/- 15.79
------------------------------------
| eval/              |             |
|    mean action     | -0.03867343 |
|    mean velocity x | -0.212      |
|    mean velocity y | -0.144      |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 49.2        |
|    mean_reward     | -5.91e+04   |
| time/              |             |
|    total_timesteps | 1253000     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 151      |
|    iterations      | 612      |
|    time_elapsed    | 8292     |
|    total_timesteps | 1253376  |
---------------------------------
Eval num_timesteps=1253500, episode_reward=-57197.33 +/- 20208.80
Episode length: 57.00 +/- 12.12
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.80192363    |
|    mean velocity x      | -1.92         |
|    mean velocity y      | -4.11         |
|    mean velocity z      | 17.2          |
|    mean_ep_length       | 57            |
|    mean_reward          | -5.72e+04     |
| time/                   |               |
|    total_timesteps      | 1253500       |
| train/                  |               |
|    approx_kl            | 4.6585134e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.001         |
|    loss                 | 8.08e+07      |
|    n_updates            | 6120          |
|    policy_gradient_loss | -0.000173     |
|    std                  | 0.899         |
|    value_loss           | 1.65e+08      |
-------------------------------------------
Eval num_timesteps=1254000, episode_reward=-83699.51 +/- 41367.92
Episode length: 55.20 +/- 15.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.23383664 |
|    mean velocity x | 0.842      |
|    mean velocity y | -0.793     |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 55.2       |
|    mean_reward     | -8.37e+04  |
| time/              |            |
|    total_timesteps | 1254000    |
-----------------------------------
Eval num_timesteps=1254500, episode_reward=-75831.52 +/- 40338.01
Episode length: 67.80 +/- 43.12
------------------------------------
| eval/              |             |
|    mean action     | -0.49074534 |
|    mean velocity x | 1.34        |
|    mean velocity y | 2.98        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 67.8        |
|    mean_reward     | -7.58e+04   |
| time/              |             |
|    total_timesteps | 1254500     |
------------------------------------
Eval num_timesteps=1255000, episode_reward=-58046.46 +/- 35167.25
Episode length: 62.40 +/- 25.56
-----------------------------------
| eval/              |            |
|    mean action     | -0.1669557 |
|    mean velocity x | 0.161      |
|    mean velocity y | 1.54       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -5.8e+04   |
| time/              |            |
|    total_timesteps | 1255000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.4      |
|    ep_rew_mean     | -7.68e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 613       |
|    time_elapsed    | 8299      |
|    total_timesteps | 1255424   |
----------------------------------
Eval num_timesteps=1255500, episode_reward=-41515.81 +/- 33155.12
Episode length: 47.20 +/- 26.29
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.11262991   |
|    mean velocity x      | 0.997         |
|    mean velocity y      | 0.424         |
|    mean velocity z      | 20.9          |
|    mean_ep_length       | 47.2          |
|    mean_reward          | -4.15e+04     |
| time/                   |               |
|    total_timesteps      | 1255500       |
| train/                  |               |
|    approx_kl            | 0.00018716586 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+08      |
|    n_updates            | 6130          |
|    policy_gradient_loss | -0.000328     |
|    std                  | 0.899         |
|    value_loss           | 2.11e+08      |
-------------------------------------------
Eval num_timesteps=1256000, episode_reward=-95721.25 +/- 13049.58
Episode length: 68.20 +/- 13.04
-------------------------------------
| eval/              |              |
|    mean action     | -0.011495765 |
|    mean velocity x | 0.657        |
|    mean velocity y | -0.0992      |
|    mean velocity z | 20.9         |
|    mean_ep_length  | 68.2         |
|    mean_reward     | -9.57e+04    |
| time/              |              |
|    total_timesteps | 1256000      |
-------------------------------------
Eval num_timesteps=1256500, episode_reward=-46999.93 +/- 37239.22
Episode length: 47.20 +/- 23.30
-----------------------------------
| eval/              |            |
|    mean action     | -0.2913018 |
|    mean velocity x | 1.01       |
|    mean velocity y | 2.48       |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 47.2       |
|    mean_reward     | -4.7e+04   |
| time/              |            |
|    total_timesteps | 1256500    |
-----------------------------------
Eval num_timesteps=1257000, episode_reward=-70241.64 +/- 21682.06
Episode length: 68.20 +/- 10.53
-----------------------------------
| eval/              |            |
|    mean action     | 0.14434002 |
|    mean velocity x | -0.0764    |
|    mean velocity y | -1.48      |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 68.2       |
|    mean_reward     | -7.02e+04  |
| time/              |            |
|    total_timesteps | 1257000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71        |
|    ep_rew_mean     | -8.06e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 614       |
|    time_elapsed    | 8306      |
|    total_timesteps | 1257472   |
----------------------------------
Eval num_timesteps=1257500, episode_reward=-86138.07 +/- 18771.09
Episode length: 77.60 +/- 30.62
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.16820732    |
|    mean velocity x      | 0.303         |
|    mean velocity y      | 0.0826        |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 77.6          |
|    mean_reward          | -8.61e+04     |
| time/                   |               |
|    total_timesteps      | 1257500       |
| train/                  |               |
|    approx_kl            | 0.00051628344 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 1.17e+08      |
|    n_updates            | 6140          |
|    policy_gradient_loss | -0.000619     |
|    std                  | 0.899         |
|    value_loss           | 2.48e+08      |
-------------------------------------------
Eval num_timesteps=1258000, episode_reward=-78430.48 +/- 27691.75
Episode length: 63.00 +/- 14.04
----------------------------------
| eval/              |           |
|    mean action     | 0.8456958 |
|    mean velocity x | -3.63     |
|    mean velocity y | -5.06     |
|    mean velocity z | 20.3      |
|    mean_ep_length  | 63        |
|    mean_reward     | -7.84e+04 |
| time/              |           |
|    total_timesteps | 1258000   |
----------------------------------
Eval num_timesteps=1258500, episode_reward=-63920.20 +/- 40561.17
Episode length: 53.80 +/- 12.56
-------------------------------------
| eval/              |              |
|    mean action     | -0.013027512 |
|    mean velocity x | 1.25         |
|    mean velocity y | -0.379       |
|    mean velocity z | 18.5         |
|    mean_ep_length  | 53.8         |
|    mean_reward     | -6.39e+04    |
| time/              |              |
|    total_timesteps | 1258500      |
-------------------------------------
Eval num_timesteps=1259000, episode_reward=-59481.03 +/- 35375.03
Episode length: 52.00 +/- 11.54
-----------------------------------
| eval/              |            |
|    mean action     | 0.41706306 |
|    mean velocity x | -1.27      |
|    mean velocity y | -2.49      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 52         |
|    mean_reward     | -5.95e+04  |
| time/              |            |
|    total_timesteps | 1259000    |
-----------------------------------
Eval num_timesteps=1259500, episode_reward=-80272.96 +/- 23501.79
Episode length: 72.60 +/- 22.65
-------------------------------------
| eval/              |              |
|    mean action     | -0.049571715 |
|    mean velocity x | -0.887       |
|    mean velocity y | -1.3         |
|    mean velocity z | 21.2         |
|    mean_ep_length  | 72.6         |
|    mean_reward     | -8.03e+04    |
| time/              |              |
|    total_timesteps | 1259500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.3      |
|    ep_rew_mean     | -8.32e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 615       |
|    time_elapsed    | 8314      |
|    total_timesteps | 1259520   |
----------------------------------
Eval num_timesteps=1260000, episode_reward=-60521.43 +/- 32894.94
Episode length: 59.60 +/- 17.30
------------------------------------------
| eval/                   |              |
|    mean action          | -0.15973797  |
|    mean velocity x      | 1.79         |
|    mean velocity y      | 2.19         |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 59.6         |
|    mean_reward          | -6.05e+04    |
| time/                   |              |
|    total_timesteps      | 1260000      |
| train/                  |              |
|    approx_kl            | 7.877621e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.001        |
|    loss                 | 1.47e+08     |
|    n_updates            | 6150         |
|    policy_gradient_loss | -0.000166    |
|    std                  | 0.899        |
|    value_loss           | 2.53e+08     |
------------------------------------------
Eval num_timesteps=1260500, episode_reward=-38634.62 +/- 32630.92
Episode length: 45.20 +/- 17.78
-----------------------------------
| eval/              |            |
|    mean action     | 0.34850976 |
|    mean velocity x | -0.64      |
|    mean velocity y | -2.74      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 45.2       |
|    mean_reward     | -3.86e+04  |
| time/              |            |
|    total_timesteps | 1260500    |
-----------------------------------
Eval num_timesteps=1261000, episode_reward=-67175.49 +/- 41658.64
Episode length: 53.00 +/- 21.84
-----------------------------------
| eval/              |            |
|    mean action     | 0.08794481 |
|    mean velocity x | -0.287     |
|    mean velocity y | -1.14      |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 53         |
|    mean_reward     | -6.72e+04  |
| time/              |            |
|    total_timesteps | 1261000    |
-----------------------------------
Eval num_timesteps=1261500, episode_reward=-44555.75 +/- 35177.91
Episode length: 43.40 +/- 17.82
------------------------------------
| eval/              |             |
|    mean action     | -0.49387738 |
|    mean velocity x | 0.898       |
|    mean velocity y | 3.01        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 43.4        |
|    mean_reward     | -4.46e+04   |
| time/              |             |
|    total_timesteps | 1261500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -8.59e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 616       |
|    time_elapsed    | 8321      |
|    total_timesteps | 1261568   |
----------------------------------
Eval num_timesteps=1262000, episode_reward=-62276.58 +/- 48861.19
Episode length: 43.80 +/- 25.29
------------------------------------------
| eval/                   |              |
|    mean action          | 0.07808865   |
|    mean velocity x      | -1           |
|    mean velocity y      | -0.489       |
|    mean velocity z      | 21.6         |
|    mean_ep_length       | 43.8         |
|    mean_reward          | -6.23e+04    |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 0.0021004772 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.001        |
|    loss                 | 9.79e+07     |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 0.899        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=1262500, episode_reward=-78672.43 +/- 30912.50
Episode length: 68.20 +/- 15.83
------------------------------------
| eval/              |             |
|    mean action     | -0.34144777 |
|    mean velocity x | 0.82        |
|    mean velocity y | 1.76        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 68.2        |
|    mean_reward     | -7.87e+04   |
| time/              |             |
|    total_timesteps | 1262500     |
------------------------------------
Eval num_timesteps=1263000, episode_reward=-82221.31 +/- 26140.54
Episode length: 61.20 +/- 8.57
-----------------------------------
| eval/              |            |
|    mean action     | -0.2815322 |
|    mean velocity x | 1.58       |
|    mean velocity y | 1.66       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -8.22e+04  |
| time/              |            |
|    total_timesteps | 1263000    |
-----------------------------------
Eval num_timesteps=1263500, episode_reward=-78994.31 +/- 23622.23
Episode length: 60.80 +/- 9.54
------------------------------------
| eval/              |             |
|    mean action     | -0.13878943 |
|    mean velocity x | 0.0655      |
|    mean velocity y | -1.02       |
|    mean velocity z | 18          |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 1263500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.5      |
|    ep_rew_mean     | -8.46e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 617       |
|    time_elapsed    | 8328      |
|    total_timesteps | 1263616   |
----------------------------------
Eval num_timesteps=1264000, episode_reward=-66338.15 +/- 15787.60
Episode length: 63.60 +/- 7.81
------------------------------------------
| eval/                   |              |
|    mean action          | 0.17784025   |
|    mean velocity x      | -0.884       |
|    mean velocity y      | -1.82        |
|    mean velocity z      | 18.8         |
|    mean_ep_length       | 63.6         |
|    mean_reward          | -6.63e+04    |
| time/                   |              |
|    total_timesteps      | 1264000      |
| train/                  |              |
|    approx_kl            | 0.0009390514 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.001        |
|    loss                 | 1.5e+08      |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 0.899        |
|    value_loss           | 1.8e+08      |
------------------------------------------
Eval num_timesteps=1264500, episode_reward=-94867.33 +/- 8081.67
Episode length: 61.40 +/- 1.62
-----------------------------------
| eval/              |            |
|    mean action     | 0.44149676 |
|    mean velocity x | -1.9       |
|    mean velocity y | -3.72      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 61.4       |
|    mean_reward     | -9.49e+04  |
| time/              |            |
|    total_timesteps | 1264500    |
-----------------------------------
Eval num_timesteps=1265000, episode_reward=-70717.10 +/- 42106.11
Episode length: 52.00 +/- 15.80
------------------------------------
| eval/              |             |
|    mean action     | -0.29595637 |
|    mean velocity x | 1.53        |
|    mean velocity y | 2.16        |
|    mean velocity z | 20.9        |
|    mean_ep_length  | 52          |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 1265000     |
------------------------------------
Eval num_timesteps=1265500, episode_reward=-72568.79 +/- 31307.66
Episode length: 58.60 +/- 12.82
----------------------------------
| eval/              |           |
|    mean action     | -0.84015  |
|    mean velocity x | 3.87      |
|    mean velocity y | 6.82      |
|    mean velocity z | 17.9      |
|    mean_ep_length  | 58.6      |
|    mean_reward     | -7.26e+04 |
| time/              |           |
|    total_timesteps | 1265500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.8      |
|    ep_rew_mean     | -8.48e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 618       |
|    time_elapsed    | 8335      |
|    total_timesteps | 1265664   |
----------------------------------
Eval num_timesteps=1266000, episode_reward=-47309.39 +/- 41571.70
Episode length: 44.60 +/- 26.03
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.49379778    |
|    mean velocity x      | -0.933        |
|    mean velocity y      | -2.71         |
|    mean velocity z      | 16.8          |
|    mean_ep_length       | 44.6          |
|    mean_reward          | -4.73e+04     |
| time/                   |               |
|    total_timesteps      | 1266000       |
| train/                  |               |
|    approx_kl            | 0.00037326873 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.322         |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 6180          |
|    policy_gradient_loss | -0.000872     |
|    std                  | 0.899         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=1266500, episode_reward=-56011.68 +/- 25765.66
Episode length: 52.00 +/- 6.26
------------------------------------
| eval/              |             |
|    mean action     | -0.06533452 |
|    mean velocity x | 1.57        |
|    mean velocity y | 1.39        |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 52          |
|    mean_reward     | -5.6e+04    |
| time/              |             |
|    total_timesteps | 1266500     |
------------------------------------
Eval num_timesteps=1267000, episode_reward=-41618.00 +/- 34124.49
Episode length: 46.80 +/- 10.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.20148946 |
|    mean velocity x | -0.805     |
|    mean velocity y | -1.29      |
|    mean velocity z | 15.4       |
|    mean_ep_length  | 46.8       |
|    mean_reward     | -4.16e+04  |
| time/              |            |
|    total_timesteps | 1267000    |
-----------------------------------
Eval num_timesteps=1267500, episode_reward=-52228.40 +/- 40831.10
Episode length: 46.20 +/- 17.08
------------------------------------
| eval/              |             |
|    mean action     | -0.13606042 |
|    mean velocity x | 1.5         |
|    mean velocity y | 1.01        |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 46.2        |
|    mean_reward     | -5.22e+04   |
| time/              |             |
|    total_timesteps | 1267500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.2      |
|    ep_rew_mean     | -7.71e+04 |
| time/              |           |
|    fps             | 151       |
|    iterations      | 619       |
|    time_elapsed    | 8343      |
|    total_timesteps | 1267712   |
----------------------------------
Eval num_timesteps=1268000, episode_reward=-93720.04 +/- 15772.50
Episode length: 68.40 +/- 12.88
------------------------------------------
| eval/                   |              |
|    mean action          | -0.17162463  |
|    mean velocity x      | 2.31         |
|    mean velocity y      | 2.81         |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 68.4         |
|    mean_reward          | -9.37e+04    |
| time/                   |              |
|    total_timesteps      | 1268000      |
| train/                  |              |
|    approx_kl            | 0.0026272875 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.001        |
|    loss                 | 7.84e+07     |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 0.899        |
|    value_loss           | 1.42e+08     |
------------------------------------------
Eval num_timesteps=1268500, episode_reward=-72421.86 +/- 37879.51
Episode length: 61.20 +/- 15.20
-----------------------------------
| eval/              |            |
|    mean action     | 0.30003718 |
|    mean velocity x | -1.87      |
|    mean velocity y | -2.97      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -7.24e+04  |
| time/              |            |
|    total_timesteps | 1268500    |
-----------------------------------
Eval num_timesteps=1269000, episode_reward=-72336.54 +/- 13117.72
Episode length: 61.00 +/- 7.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.07037908 |
|    mean velocity x | 0.0228     |
|    mean velocity y | -0.963     |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 61         |
|    mean_reward     | -7.23e+04  |
| time/              |            |
|    total_timesteps | 1269000    |
-----------------------------------
Eval num_timesteps=1269500, episode_reward=-79736.78 +/- 12315.53
Episode length: 71.20 +/- 9.15
----------------------------------
| eval/              |           |
|    mean action     | 0.4103324 |
|    mean velocity x | -0.941    |
|    mean velocity y | -1.5      |
|    mean velocity z | 20        |
|    mean_ep_length  | 71.2      |
|    mean_reward     | -7.97e+04 |
| time/              |           |
|    total_timesteps | 1269500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.1      |
|    ep_rew_mean     | -7.28e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 620       |
|    time_elapsed    | 8350      |
|    total_timesteps | 1269760   |
----------------------------------
Eval num_timesteps=1270000, episode_reward=-92654.05 +/- 13108.19
Episode length: 73.80 +/- 25.61
------------------------------------------
| eval/                   |              |
|    mean action          | 0.1299364    |
|    mean velocity x      | -1.23        |
|    mean velocity y      | -0.972       |
|    mean velocity z      | 17.9         |
|    mean_ep_length       | 73.8         |
|    mean_reward          | -9.27e+04    |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0015107314 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.001        |
|    loss                 | 8.56e+07     |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.899        |
|    value_loss           | 1.81e+08     |
------------------------------------------
Eval num_timesteps=1270500, episode_reward=-54380.34 +/- 41498.47
Episode length: 55.60 +/- 26.01
-----------------------------------
| eval/              |            |
|    mean action     | 0.24957007 |
|    mean velocity x | -2.16      |
|    mean velocity y | -3.54      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 55.6       |
|    mean_reward     | -5.44e+04  |
| time/              |            |
|    total_timesteps | 1270500    |
-----------------------------------
Eval num_timesteps=1271000, episode_reward=-58792.80 +/- 40467.94
Episode length: 48.40 +/- 22.92
-----------------------------------
| eval/              |            |
|    mean action     | 0.10716818 |
|    mean velocity x | -0.332     |
|    mean velocity y | -0.765     |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 48.4       |
|    mean_reward     | -5.88e+04  |
| time/              |            |
|    total_timesteps | 1271000    |
-----------------------------------
Eval num_timesteps=1271500, episode_reward=-82171.33 +/- 23452.83
Episode length: 60.40 +/- 5.68
-------------------------------------
| eval/              |              |
|    mean action     | -0.022589676 |
|    mean velocity x | -0.692       |
|    mean velocity y | -0.158       |
|    mean velocity z | 19.3         |
|    mean_ep_length  | 60.4         |
|    mean_reward     | -8.22e+04    |
| time/              |              |
|    total_timesteps | 1271500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.1      |
|    ep_rew_mean     | -6.89e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 621       |
|    time_elapsed    | 8357      |
|    total_timesteps | 1271808   |
----------------------------------
Eval num_timesteps=1272000, episode_reward=-86405.47 +/- 19687.97
Episode length: 63.20 +/- 5.74
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40907544   |
|    mean velocity x      | 2.47          |
|    mean velocity y      | 1.67          |
|    mean velocity z      | 17            |
|    mean_ep_length       | 63.2          |
|    mean_reward          | -8.64e+04     |
| time/                   |               |
|    total_timesteps      | 1272000       |
| train/                  |               |
|    approx_kl            | 0.00026270794 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.329         |
|    learning_rate        | 0.001         |
|    loss                 | 5.02e+07      |
|    n_updates            | 6210          |
|    policy_gradient_loss | -0.000335     |
|    std                  | 0.899         |
|    value_loss           | 1.96e+08      |
-------------------------------------------
Eval num_timesteps=1272500, episode_reward=-71752.00 +/- 39392.53
Episode length: 59.40 +/- 15.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.03223784 |
|    mean velocity x | -0.454     |
|    mean velocity y | -0.582     |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -7.18e+04  |
| time/              |            |
|    total_timesteps | 1272500    |
-----------------------------------
Eval num_timesteps=1273000, episode_reward=-57421.90 +/- 31756.32
Episode length: 57.80 +/- 29.82
------------------------------------
| eval/              |             |
|    mean action     | 0.006405853 |
|    mean velocity x | 0.127       |
|    mean velocity y | -0.478      |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 57.8        |
|    mean_reward     | -5.74e+04   |
| time/              |             |
|    total_timesteps | 1273000     |
------------------------------------
Eval num_timesteps=1273500, episode_reward=-87596.42 +/- 15125.83
Episode length: 63.00 +/- 5.40
----------------------------------
| eval/              |           |
|    mean action     | 0.4636782 |
|    mean velocity x | -2.27     |
|    mean velocity y | -3.27     |
|    mean velocity z | 19.7      |
|    mean_ep_length  | 63        |
|    mean_reward     | -8.76e+04 |
| time/              |           |
|    total_timesteps | 1273500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.6      |
|    ep_rew_mean     | -7.41e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 622       |
|    time_elapsed    | 8365      |
|    total_timesteps | 1273856   |
----------------------------------
Eval num_timesteps=1274000, episode_reward=-75537.05 +/- 36330.72
Episode length: 53.40 +/- 14.75
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.17750058    |
|    mean velocity x      | -0.615        |
|    mean velocity y      | -1.35         |
|    mean velocity z      | 19.9          |
|    mean_ep_length       | 53.4          |
|    mean_reward          | -7.55e+04     |
| time/                   |               |
|    total_timesteps      | 1274000       |
| train/                  |               |
|    approx_kl            | 0.00028837335 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.311         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+08      |
|    n_updates            | 6220          |
|    policy_gradient_loss | -0.000455     |
|    std                  | 0.899         |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=1274500, episode_reward=-61559.58 +/- 41757.68
Episode length: 55.00 +/- 14.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.22798692 |
|    mean velocity x | -2.71      |
|    mean velocity y | -1.86      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 55         |
|    mean_reward     | -6.16e+04  |
| time/              |            |
|    total_timesteps | 1274500    |
-----------------------------------
Eval num_timesteps=1275000, episode_reward=-83070.28 +/- 50360.51
Episode length: 78.60 +/- 44.52
-------------------------------------
| eval/              |              |
|    mean action     | -0.004207802 |
|    mean velocity x | -0.347       |
|    mean velocity y | -0.443       |
|    mean velocity z | 21.1         |
|    mean_ep_length  | 78.6         |
|    mean_reward     | -8.31e+04    |
| time/              |              |
|    total_timesteps | 1275000      |
-------------------------------------
Eval num_timesteps=1275500, episode_reward=-76154.58 +/- 21806.75
Episode length: 67.80 +/- 17.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.23508255 |
|    mean velocity x | -1.63      |
|    mean velocity y | -2.42      |
|    mean velocity z | 22         |
|    mean_ep_length  | 67.8       |
|    mean_reward     | -7.62e+04  |
| time/              |            |
|    total_timesteps | 1275500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -8.12e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 623       |
|    time_elapsed    | 8372      |
|    total_timesteps | 1275904   |
----------------------------------
Eval num_timesteps=1276000, episode_reward=-53765.64 +/- 28493.15
Episode length: 57.60 +/- 11.48
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.37233555   |
|    mean velocity x      | -0.121        |
|    mean velocity y      | 1.53          |
|    mean velocity z      | 18.8          |
|    mean_ep_length       | 57.6          |
|    mean_reward          | -5.38e+04     |
| time/                   |               |
|    total_timesteps      | 1276000       |
| train/                  |               |
|    approx_kl            | 4.7739566e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.282         |
|    learning_rate        | 0.001         |
|    loss                 | 1.49e+08      |
|    n_updates            | 6230          |
|    policy_gradient_loss | -0.000205     |
|    std                  | 0.899         |
|    value_loss           | 2.31e+08      |
-------------------------------------------
Eval num_timesteps=1276500, episode_reward=-93561.75 +/- 12301.47
Episode length: 64.20 +/- 3.12
------------------------------------
| eval/              |             |
|    mean action     | -0.13186291 |
|    mean velocity x | 0.366       |
|    mean velocity y | 1.96        |
|    mean velocity z | 18          |
|    mean_ep_length  | 64.2        |
|    mean_reward     | -9.36e+04   |
| time/              |             |
|    total_timesteps | 1276500     |
------------------------------------
Eval num_timesteps=1277000, episode_reward=-88441.84 +/- 15879.57
Episode length: 70.00 +/- 10.55
------------------------------------
| eval/              |             |
|    mean action     | -0.46306098 |
|    mean velocity x | 1.55        |
|    mean velocity y | 1.87        |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 70          |
|    mean_reward     | -8.84e+04   |
| time/              |             |
|    total_timesteps | 1277000     |
------------------------------------
Eval num_timesteps=1277500, episode_reward=-74227.36 +/- 20811.19
Episode length: 75.20 +/- 23.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.69451046 |
|    mean velocity x | -2.2       |
|    mean velocity y | -3.37      |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 75.2       |
|    mean_reward     | -7.42e+04  |
| time/              |            |
|    total_timesteps | 1277500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.7      |
|    ep_rew_mean     | -7.83e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 624       |
|    time_elapsed    | 8379      |
|    total_timesteps | 1277952   |
----------------------------------
Eval num_timesteps=1278000, episode_reward=-67245.67 +/- 31002.45
Episode length: 60.20 +/- 12.83
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.55536807 |
|    mean velocity x      | 2.09        |
|    mean velocity y      | 3.21        |
|    mean velocity z      | 17.2        |
|    mean_ep_length       | 60.2        |
|    mean_reward          | -6.72e+04   |
| time/                   |             |
|    total_timesteps      | 1278000     |
| train/                  |             |
|    approx_kl            | 3.82977e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.001       |
|    loss                 | 8.89e+07    |
|    n_updates            | 6240        |
|    policy_gradient_loss | -0.000312   |
|    std                  | 0.899       |
|    value_loss           | 1.79e+08    |
-----------------------------------------
Eval num_timesteps=1278500, episode_reward=-85946.23 +/- 18521.12
Episode length: 65.80 +/- 4.66
------------------------------------
| eval/              |             |
|    mean action     | 0.045955047 |
|    mean velocity x | 1.22        |
|    mean velocity y | 1.36        |
|    mean velocity z | 17.2        |
|    mean_ep_length  | 65.8        |
|    mean_reward     | -8.59e+04   |
| time/              |             |
|    total_timesteps | 1278500     |
------------------------------------
Eval num_timesteps=1279000, episode_reward=-71750.63 +/- 40922.43
Episode length: 52.40 +/- 14.09
-----------------------------------
| eval/              |            |
|    mean action     | 0.42085657 |
|    mean velocity x | -2.3       |
|    mean velocity y | -4.17      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 52.4       |
|    mean_reward     | -7.18e+04  |
| time/              |            |
|    total_timesteps | 1279000    |
-----------------------------------
Eval num_timesteps=1279500, episode_reward=-47064.15 +/- 39153.31
Episode length: 62.60 +/- 18.62
----------------------------------
| eval/              |           |
|    mean action     | 0.5166051 |
|    mean velocity x | -0.724    |
|    mean velocity y | -2.91     |
|    mean velocity z | 20.2      |
|    mean_ep_length  | 62.6      |
|    mean_reward     | -4.71e+04 |
| time/              |           |
|    total_timesteps | 1279500   |
----------------------------------
Eval num_timesteps=1280000, episode_reward=-79224.27 +/- 23205.89
Episode length: 63.20 +/- 2.48
-----------------------------------
| eval/              |            |
|    mean action     | 0.38445008 |
|    mean velocity x | -0.76      |
|    mean velocity y | -1.71      |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -7.92e+04  |
| time/              |            |
|    total_timesteps | 1280000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.7      |
|    ep_rew_mean     | -8.23e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 625       |
|    time_elapsed    | 8387      |
|    total_timesteps | 1280000   |
----------------------------------
Eval num_timesteps=1280500, episode_reward=-82620.67 +/- 22042.89
Episode length: 63.00 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1255252     |
|    mean velocity x      | -1.48         |
|    mean velocity y      | -1.64         |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 63            |
|    mean_reward          | -8.26e+04     |
| time/                   |               |
|    total_timesteps      | 1280500       |
| train/                  |               |
|    approx_kl            | 0.00035551254 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.94         |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 1.22e+08      |
|    n_updates            | 6250          |
|    policy_gradient_loss | -0.000882     |
|    std                  | 0.899         |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=1281000, episode_reward=-51593.71 +/- 36189.11
Episode length: 51.00 +/- 22.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.12148901 |
|    mean velocity x | -2.15      |
|    mean velocity y | -1.1       |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 51         |
|    mean_reward     | -5.16e+04  |
| time/              |            |
|    total_timesteps | 1281000    |
-----------------------------------
Eval num_timesteps=1281500, episode_reward=-45810.14 +/- 37542.22
Episode length: 48.80 +/- 20.00
----------------------------------
| eval/              |           |
|    mean action     | 0.7141172 |
|    mean velocity x | -1.23     |
|    mean velocity y | -3.94     |
|    mean velocity z | 20.3      |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.58e+04 |
| time/              |           |
|    total_timesteps | 1281500   |
----------------------------------
Eval num_timesteps=1282000, episode_reward=-35183.12 +/- 23911.11
Episode length: 46.20 +/- 7.86
-----------------------------------
| eval/              |            |
|    mean action     | -0.1551796 |
|    mean velocity x | 0.777      |
|    mean velocity y | 0.363      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 46.2       |
|    mean_reward     | -3.52e+04  |
| time/              |            |
|    total_timesteps | 1282000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73        |
|    ep_rew_mean     | -8.36e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 626       |
|    time_elapsed    | 8394      |
|    total_timesteps | 1282048   |
----------------------------------
Eval num_timesteps=1282500, episode_reward=-78366.68 +/- 34079.82
Episode length: 71.20 +/- 27.59
------------------------------------------
| eval/                   |              |
|    mean action          | 0.6287482    |
|    mean velocity x      | -3.14        |
|    mean velocity y      | -4.72        |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 71.2         |
|    mean_reward          | -7.84e+04    |
| time/                   |              |
|    total_timesteps      | 1282500      |
| train/                  |              |
|    approx_kl            | 0.0017140197 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.001        |
|    loss                 | 7.65e+07     |
|    n_updates            | 6260         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 0.898        |
|    value_loss           | 2.25e+08     |
------------------------------------------
Eval num_timesteps=1283000, episode_reward=-51423.50 +/- 40006.61
Episode length: 48.80 +/- 12.92
-----------------------------------
| eval/              |            |
|    mean action     | -0.2757586 |
|    mean velocity x | 2.38       |
|    mean velocity y | 1.85       |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 48.8       |
|    mean_reward     | -5.14e+04  |
| time/              |            |
|    total_timesteps | 1283000    |
-----------------------------------
Eval num_timesteps=1283500, episode_reward=-63386.92 +/- 17461.54
Episode length: 56.80 +/- 6.68
----------------------------------
| eval/              |           |
|    mean action     | 0.5797783 |
|    mean velocity x | -1.06     |
|    mean velocity y | -2.12     |
|    mean velocity z | 18        |
|    mean_ep_length  | 56.8      |
|    mean_reward     | -6.34e+04 |
| time/              |           |
|    total_timesteps | 1283500   |
----------------------------------
Eval num_timesteps=1284000, episode_reward=-56550.28 +/- 34433.99
Episode length: 53.00 +/- 14.51
-----------------------------------
| eval/              |            |
|    mean action     | 0.46232155 |
|    mean velocity x | -2.22      |
|    mean velocity y | -3.24      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 53         |
|    mean_reward     | -5.66e+04  |
| time/              |            |
|    total_timesteps | 1284000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.8      |
|    ep_rew_mean     | -8.17e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 627       |
|    time_elapsed    | 8401      |
|    total_timesteps | 1284096   |
----------------------------------
Eval num_timesteps=1284500, episode_reward=-92688.77 +/- 28601.30
Episode length: 78.80 +/- 29.44
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.44489825   |
|    mean velocity x      | 2.22          |
|    mean velocity y      | 3.2           |
|    mean velocity z      | 18.9          |
|    mean_ep_length       | 78.8          |
|    mean_reward          | -9.27e+04     |
| time/                   |               |
|    total_timesteps      | 1284500       |
| train/                  |               |
|    approx_kl            | 0.00010175968 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.355         |
|    learning_rate        | 0.001         |
|    loss                 | 8.22e+07      |
|    n_updates            | 6270          |
|    policy_gradient_loss | -0.000574     |
|    std                  | 0.898         |
|    value_loss           | 1.53e+08      |
-------------------------------------------
Eval num_timesteps=1285000, episode_reward=-63436.46 +/- 39474.32
Episode length: 51.00 +/- 18.78
----------------------------------
| eval/              |           |
|    mean action     | 1.4027568 |
|    mean velocity x | -4.39     |
|    mean velocity y | -7.89     |
|    mean velocity z | 18.2      |
|    mean_ep_length  | 51        |
|    mean_reward     | -6.34e+04 |
| time/              |           |
|    total_timesteps | 1285000   |
----------------------------------
Eval num_timesteps=1285500, episode_reward=-47350.28 +/- 43908.02
Episode length: 42.40 +/- 19.45
-------------------------------------
| eval/              |              |
|    mean action     | -0.055321716 |
|    mean velocity x | 0.662        |
|    mean velocity y | 0.386        |
|    mean velocity z | 19.3         |
|    mean_ep_length  | 42.4         |
|    mean_reward     | -4.74e+04    |
| time/              |              |
|    total_timesteps | 1285500      |
-------------------------------------
Eval num_timesteps=1286000, episode_reward=-62740.27 +/- 30868.07
Episode length: 61.80 +/- 5.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.17616728 |
|    mean velocity x | -1.33      |
|    mean velocity y | -1.02      |
|    mean velocity z | 19.5       |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -6.27e+04  |
| time/              |            |
|    total_timesteps | 1286000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.5      |
|    ep_rew_mean     | -8.67e+04 |
| time/              |           |
|    fps             | 152       |
|    iterations      | 628       |
|    time_elapsed    | 8408      |
|    total_timesteps | 1286144   |
----------------------------------
Eval num_timesteps=1286500, episode_reward=-77736.89 +/- 18607.20
Episode length: 66.60 +/- 6.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.027659943 |
|    mean velocity x      | 0.44         |
|    mean velocity y      | 0.0557       |
|    mean velocity z      | 20.6         |
|    mean_ep_length       | 66.6         |
|    mean_reward          | -7.77e+04    |
| time/                   |              |
|    total_timesteps      | 1286500      |
| train/                  |              |
|    approx_kl            | 0.0023844386 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.001        |
|    loss                 | 9.71e+07     |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.00353     |
|    std                  | 0.898        |
|    value_loss           | 1.62e+08     |
------------------------------------------
Eval num_timesteps=1287000, episode_reward=-56704.10 +/- 33209.36
Episode length: 51.80 +/- 20.01
-----------------------------------
| eval/              |            |
|    mean action     | 0.11198744 |
|    mean velocity x | 1.39       |
|    mean velocity y | 0.343      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 51.8       |
|    mean_reward     | -5.67e+04  |
| time/              |            |
|    total_timesteps | 1287000    |
-----------------------------------
Eval num_timesteps=1287500, episode_reward=-38718.27 +/- 35298.61
Episode length: 40.80 +/- 18.08
------------------------------------
| eval/              |             |
|    mean action     | -0.01138337 |
|    mean velocity x | 0.738       |
|    mean velocity y | 0.188       |
|    mean velocity z | 21          |
|    mean_ep_length  | 40.8        |
|    mean_reward     | -3.87e+04   |
| time/              |             |
|    total_timesteps | 1287500     |
------------------------------------
Eval num_timesteps=1288000, episode_reward=-56207.93 +/- 13294.16
Episode length: 53.60 +/- 5.08
------------------------------------
| eval/              |             |
|    mean action     | -0.29026073 |
|    mean velocity x | 0.623       |
|    mean velocity y | 2.2         |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -5.62e+04   |
| time/              |             |
|    total_timesteps | 1288000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.9      |
|    ep_rew_mean     | -8.76e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 629       |
|    time_elapsed    | 8415      |
|    total_timesteps | 1288192   |
----------------------------------
Eval num_timesteps=1288500, episode_reward=-55209.75 +/- 30349.75
Episode length: 52.20 +/- 19.70
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.21644217    |
|    mean velocity x      | -1.39         |
|    mean velocity y      | -1.44         |
|    mean velocity z      | 18            |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -5.52e+04     |
| time/                   |               |
|    total_timesteps      | 1288500       |
| train/                  |               |
|    approx_kl            | 0.00016262016 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+08      |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.000552     |
|    std                  | 0.898         |
|    value_loss           | 2.23e+08      |
-------------------------------------------
Eval num_timesteps=1289000, episode_reward=-47174.24 +/- 37996.13
Episode length: 48.60 +/- 13.87
-----------------------------------
| eval/              |            |
|    mean action     | 0.31650543 |
|    mean velocity x | -0.676     |
|    mean velocity y | -2.39      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 48.6       |
|    mean_reward     | -4.72e+04  |
| time/              |            |
|    total_timesteps | 1289000    |
-----------------------------------
Eval num_timesteps=1289500, episode_reward=-60843.33 +/- 45217.98
Episode length: 48.80 +/- 16.86
-----------------------------------
| eval/              |            |
|    mean action     | -0.5539856 |
|    mean velocity x | 0.86       |
|    mean velocity y | 3          |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 48.8       |
|    mean_reward     | -6.08e+04  |
| time/              |            |
|    total_timesteps | 1289500    |
-----------------------------------
Eval num_timesteps=1290000, episode_reward=-82272.70 +/- 22685.71
Episode length: 65.60 +/- 9.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.19423522 |
|    mean velocity x | -1.36      |
|    mean velocity y | -1         |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 65.6       |
|    mean_reward     | -8.23e+04  |
| time/              |            |
|    total_timesteps | 1290000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 78.8      |
|    ep_rew_mean     | -8.68e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 630       |
|    time_elapsed    | 8422      |
|    total_timesteps | 1290240   |
----------------------------------
Eval num_timesteps=1290500, episode_reward=-56967.95 +/- 46060.88
Episode length: 43.40 +/- 24.09
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.20781098   |
|    mean velocity x      | -0.55         |
|    mean velocity y      | 0.0892        |
|    mean velocity z      | 22.1          |
|    mean_ep_length       | 43.4          |
|    mean_reward          | -5.7e+04      |
| time/                   |               |
|    total_timesteps      | 1290500       |
| train/                  |               |
|    approx_kl            | 0.00016042459 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.319         |
|    learning_rate        | 0.001         |
|    loss                 | 6.13e+07      |
|    n_updates            | 6300          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 0.898         |
|    value_loss           | 1.92e+08      |
-------------------------------------------
Eval num_timesteps=1291000, episode_reward=-45619.63 +/- 36640.02
Episode length: 49.40 +/- 24.90
------------------------------------
| eval/              |             |
|    mean action     | 0.008900391 |
|    mean velocity x | -0.957      |
|    mean velocity y | -0.0968     |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 49.4        |
|    mean_reward     | -4.56e+04   |
| time/              |             |
|    total_timesteps | 1291000     |
------------------------------------
Eval num_timesteps=1291500, episode_reward=-73375.86 +/- 52102.65
Episode length: 70.40 +/- 47.81
-----------------------------------
| eval/              |            |
|    mean action     | 0.35905838 |
|    mean velocity x | -1.51      |
|    mean velocity y | -2.59      |
|    mean velocity z | 19.7       |
|    mean_ep_length  | 70.4       |
|    mean_reward     | -7.34e+04  |
| time/              |            |
|    total_timesteps | 1291500    |
-----------------------------------
Eval num_timesteps=1292000, episode_reward=-48365.34 +/- 25810.25
Episode length: 54.60 +/- 16.03
-----------------------------------
| eval/              |            |
|    mean action     | 0.11353048 |
|    mean velocity x | -0.504     |
|    mean velocity y | -0.354     |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -4.84e+04  |
| time/              |            |
|    total_timesteps | 1292000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 76.3      |
|    ep_rew_mean     | -8.85e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 631       |
|    time_elapsed    | 8429      |
|    total_timesteps | 1292288   |
----------------------------------
Eval num_timesteps=1292500, episode_reward=-63669.59 +/- 31859.76
Episode length: 54.40 +/- 7.34
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.25221005    |
|    mean velocity x      | -0.686        |
|    mean velocity y      | -2.93         |
|    mean velocity z      | 22            |
|    mean_ep_length       | 54.4          |
|    mean_reward          | -6.37e+04     |
| time/                   |               |
|    total_timesteps      | 1292500       |
| train/                  |               |
|    approx_kl            | 2.7089845e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+08      |
|    n_updates            | 6310          |
|    policy_gradient_loss | -0.000135     |
|    std                  | 0.898         |
|    value_loss           | 2.43e+08      |
-------------------------------------------
Eval num_timesteps=1293000, episode_reward=-74495.02 +/- 24631.59
Episode length: 72.40 +/- 31.32
------------------------------------
| eval/              |             |
|    mean action     | -0.29059342 |
|    mean velocity x | 1.38        |
|    mean velocity y | 1.81        |
|    mean velocity z | 22.5        |
|    mean_ep_length  | 72.4        |
|    mean_reward     | -7.45e+04   |
| time/              |             |
|    total_timesteps | 1293000     |
------------------------------------
Eval num_timesteps=1293500, episode_reward=-70601.29 +/- 34332.31
Episode length: 62.60 +/- 20.24
-----------------------------------
| eval/              |            |
|    mean action     | 0.37137195 |
|    mean velocity x | -1.23      |
|    mean velocity y | -2.26      |
|    mean velocity z | 21.6       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -7.06e+04  |
| time/              |            |
|    total_timesteps | 1293500    |
-----------------------------------
Eval num_timesteps=1294000, episode_reward=-64964.85 +/- 27481.36
Episode length: 58.60 +/- 11.81
-------------------------------------
| eval/              |              |
|    mean action     | 0.0071236724 |
|    mean velocity x | 0.727        |
|    mean velocity y | 0.832        |
|    mean velocity z | 19.5         |
|    mean_ep_length  | 58.6         |
|    mean_reward     | -6.5e+04     |
| time/              |              |
|    total_timesteps | 1294000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -8.66e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 632       |
|    time_elapsed    | 8437      |
|    total_timesteps | 1294336   |
----------------------------------
Eval num_timesteps=1294500, episode_reward=-74011.72 +/- 28749.95
Episode length: 62.00 +/- 12.41
------------------------------------------
| eval/                   |              |
|    mean action          | -0.62830836  |
|    mean velocity x      | 1.18         |
|    mean velocity y      | 3.43         |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 62           |
|    mean_reward          | -7.4e+04     |
| time/                   |              |
|    total_timesteps      | 1294500      |
| train/                  |              |
|    approx_kl            | 0.0023310562 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.001        |
|    loss                 | 2.34e+08     |
|    n_updates            | 6320         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 0.898        |
|    value_loss           | 2.66e+08     |
------------------------------------------
Eval num_timesteps=1295000, episode_reward=-85161.94 +/- 22927.67
Episode length: 60.00 +/- 2.37
------------------------------------
| eval/              |             |
|    mean action     | -0.34705105 |
|    mean velocity x | 1.65        |
|    mean velocity y | 3.25        |
|    mean velocity z | 21.5        |
|    mean_ep_length  | 60          |
|    mean_reward     | -8.52e+04   |
| time/              |             |
|    total_timesteps | 1295000     |
------------------------------------
Eval num_timesteps=1295500, episode_reward=-63031.80 +/- 36604.13
Episode length: 54.40 +/- 18.83
-----------------------------------
| eval/              |            |
|    mean action     | 0.13433576 |
|    mean velocity x | -1.88      |
|    mean velocity y | -1.89      |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -6.3e+04   |
| time/              |            |
|    total_timesteps | 1295500    |
-----------------------------------
Eval num_timesteps=1296000, episode_reward=-82843.34 +/- 13213.42
Episode length: 64.60 +/- 3.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.26311034 |
|    mean velocity x | -1.92      |
|    mean velocity y | -2.83      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 64.6       |
|    mean_reward     | -8.28e+04  |
| time/              |            |
|    total_timesteps | 1296000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.4      |
|    ep_rew_mean     | -8.04e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 633       |
|    time_elapsed    | 8444      |
|    total_timesteps | 1296384   |
----------------------------------
Eval num_timesteps=1296500, episode_reward=-33263.45 +/- 31890.17
Episode length: 49.60 +/- 23.18
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.26964664 |
|    mean velocity x      | 0.488       |
|    mean velocity y      | 0.303       |
|    mean velocity z      | 20.1        |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -3.33e+04   |
| time/                   |             |
|    total_timesteps      | 1296500     |
| train/                  |             |
|    approx_kl            | 0.002350816 |
|    clip_fraction        | 0.00474     |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.001       |
|    loss                 | 7.06e+07    |
|    n_updates            | 6330        |
|    policy_gradient_loss | -0.00182    |
|    std                  | 0.898       |
|    value_loss           | 1.94e+08    |
-----------------------------------------
Eval num_timesteps=1297000, episode_reward=-76809.83 +/- 42101.90
Episode length: 54.40 +/- 10.93
------------------------------------
| eval/              |             |
|    mean action     | -0.37365845 |
|    mean velocity x | -0.519      |
|    mean velocity y | 1.15        |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 54.4        |
|    mean_reward     | -7.68e+04   |
| time/              |             |
|    total_timesteps | 1297000     |
------------------------------------
Eval num_timesteps=1297500, episode_reward=-66519.34 +/- 30802.22
Episode length: 57.60 +/- 12.83
-----------------------------------
| eval/              |            |
|    mean action     | 0.16347525 |
|    mean velocity x | -0.0685    |
|    mean velocity y | -1.47      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 57.6       |
|    mean_reward     | -6.65e+04  |
| time/              |            |
|    total_timesteps | 1297500    |
-----------------------------------
Eval num_timesteps=1298000, episode_reward=-56202.33 +/- 26472.70
Episode length: 59.60 +/- 15.60
-----------------------------------
| eval/              |            |
|    mean action     | 0.14799397 |
|    mean velocity x | -0.604     |
|    mean velocity y | -0.342     |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -5.62e+04  |
| time/              |            |
|    total_timesteps | 1298000    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.4     |
|    ep_rew_mean     | -8e+04   |
| time/              |          |
|    fps             | 153      |
|    iterations      | 634      |
|    time_elapsed    | 8451     |
|    total_timesteps | 1298432  |
---------------------------------
Eval num_timesteps=1298500, episode_reward=-78942.94 +/- 36056.91
Episode length: 63.00 +/- 12.84
------------------------------------------
| eval/                   |              |
|    mean action          | 0.08010001   |
|    mean velocity x      | 0.279        |
|    mean velocity y      | 0.575        |
|    mean velocity z      | 15.4         |
|    mean_ep_length       | 63           |
|    mean_reward          | -7.89e+04    |
| time/                   |              |
|    total_timesteps      | 1298500      |
| train/                  |              |
|    approx_kl            | 6.913682e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.001        |
|    loss                 | 8.65e+07     |
|    n_updates            | 6340         |
|    policy_gradient_loss | -0.000236    |
|    std                  | 0.898        |
|    value_loss           | 1.85e+08     |
------------------------------------------
Eval num_timesteps=1299000, episode_reward=-90163.36 +/- 18026.40
Episode length: 86.60 +/- 27.64
-----------------------------------
| eval/              |            |
|    mean action     | -0.1798309 |
|    mean velocity x | 0.419      |
|    mean velocity y | 0.415      |
|    mean velocity z | 17.7       |
|    mean_ep_length  | 86.6       |
|    mean_reward     | -9.02e+04  |
| time/              |            |
|    total_timesteps | 1299000    |
-----------------------------------
Eval num_timesteps=1299500, episode_reward=-66000.46 +/- 31365.17
Episode length: 55.80 +/- 18.55
------------------------------------
| eval/              |             |
|    mean action     | -0.72514004 |
|    mean velocity x | 2.54        |
|    mean velocity y | 5.34        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 55.8        |
|    mean_reward     | -6.6e+04    |
| time/              |             |
|    total_timesteps | 1299500     |
------------------------------------
Eval num_timesteps=1300000, episode_reward=-78009.61 +/- 19753.98
Episode length: 66.40 +/- 7.94
------------------------------------
| eval/              |             |
|    mean action     | 0.031518105 |
|    mean velocity x | -0.0286     |
|    mean velocity y | -0.6        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -7.8e+04    |
| time/              |             |
|    total_timesteps | 1300000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.8      |
|    ep_rew_mean     | -7.59e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 635       |
|    time_elapsed    | 8458      |
|    total_timesteps | 1300480   |
----------------------------------
Eval num_timesteps=1300500, episode_reward=-62845.89 +/- 34175.33
Episode length: 55.00 +/- 26.96
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3294253   |
|    mean velocity x      | 0.52         |
|    mean velocity y      | 1.36         |
|    mean velocity z      | 18.5         |
|    mean_ep_length       | 55           |
|    mean_reward          | -6.28e+04    |
| time/                   |              |
|    total_timesteps      | 1300500      |
| train/                  |              |
|    approx_kl            | 0.0012147739 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.001        |
|    loss                 | 5.04e+07     |
|    n_updates            | 6350         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.897        |
|    value_loss           | 1.62e+08     |
------------------------------------------
Eval num_timesteps=1301000, episode_reward=-53335.01 +/- 40733.62
Episode length: 49.20 +/- 23.51
-----------------------------------
| eval/              |            |
|    mean action     | 0.52650523 |
|    mean velocity x | -1.93      |
|    mean velocity y | -3.95      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 49.2       |
|    mean_reward     | -5.33e+04  |
| time/              |            |
|    total_timesteps | 1301000    |
-----------------------------------
Eval num_timesteps=1301500, episode_reward=-72589.26 +/- 18822.29
Episode length: 67.80 +/- 9.09
------------------------------------
| eval/              |             |
|    mean action     | 0.115817904 |
|    mean velocity x | 0.277       |
|    mean velocity y | -0.563      |
|    mean velocity z | 21.2        |
|    mean_ep_length  | 67.8        |
|    mean_reward     | -7.26e+04   |
| time/              |             |
|    total_timesteps | 1301500     |
------------------------------------
Eval num_timesteps=1302000, episode_reward=-47840.10 +/- 36305.05
Episode length: 49.60 +/- 15.00
----------------------------------
| eval/              |           |
|    mean action     | 0.3041649 |
|    mean velocity x | -1.64     |
|    mean velocity y | -1.49     |
|    mean velocity z | 19.2      |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.78e+04 |
| time/              |           |
|    total_timesteps | 1302000   |
----------------------------------
Eval num_timesteps=1302500, episode_reward=-80675.94 +/- 20572.89
Episode length: 61.20 +/- 5.71
----------------------------------
| eval/              |           |
|    mean action     | 0.1880686 |
|    mean velocity x | -1.01     |
|    mean velocity y | -2.34     |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 61.2      |
|    mean_reward     | -8.07e+04 |
| time/              |           |
|    total_timesteps | 1302500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.6      |
|    ep_rew_mean     | -7.35e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 636       |
|    time_elapsed    | 8466      |
|    total_timesteps | 1302528   |
----------------------------------
Eval num_timesteps=1303000, episode_reward=-50671.72 +/- 33019.91
Episode length: 50.80 +/- 15.60
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.13330022    |
|    mean velocity x      | 0.234         |
|    mean velocity y      | -1.11         |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 50.8          |
|    mean_reward          | -5.07e+04     |
| time/                   |               |
|    total_timesteps      | 1303000       |
| train/                  |               |
|    approx_kl            | 0.00031560537 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.347         |
|    learning_rate        | 0.001         |
|    loss                 | 5.57e+07      |
|    n_updates            | 6360          |
|    policy_gradient_loss | -0.000748     |
|    std                  | 0.897         |
|    value_loss           | 1.78e+08      |
-------------------------------------------
Eval num_timesteps=1303500, episode_reward=-59446.24 +/- 28631.83
Episode length: 55.60 +/- 20.12
------------------------------------
| eval/              |             |
|    mean action     | -0.17067456 |
|    mean velocity x | -0.119      |
|    mean velocity y | 0.823       |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 55.6        |
|    mean_reward     | -5.94e+04   |
| time/              |             |
|    total_timesteps | 1303500     |
------------------------------------
Eval num_timesteps=1304000, episode_reward=-87833.33 +/- 43622.18
Episode length: 54.60 +/- 17.33
------------------------------------
| eval/              |             |
|    mean action     | -0.17079237 |
|    mean velocity x | 1.09        |
|    mean velocity y | 0.524       |
|    mean velocity z | 18.3        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -8.78e+04   |
| time/              |             |
|    total_timesteps | 1304000     |
------------------------------------
Eval num_timesteps=1304500, episode_reward=-80988.54 +/- 32168.96
Episode length: 57.80 +/- 8.54
------------------------------------
| eval/              |             |
|    mean action     | -0.22198002 |
|    mean velocity x | 0.579       |
|    mean velocity y | 0.974       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 57.8        |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 1304500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.3      |
|    ep_rew_mean     | -7.28e+04 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 637       |
|    time_elapsed    | 8473      |
|    total_timesteps | 1304576   |
----------------------------------
Eval num_timesteps=1305000, episode_reward=-79728.85 +/- 26899.38
Episode length: 58.60 +/- 4.32
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.7752035     |
|    mean velocity x      | -2.03         |
|    mean velocity y      | -3.87         |
|    mean velocity z      | 17.2          |
|    mean_ep_length       | 58.6          |
|    mean_reward          | -7.97e+04     |
| time/                   |               |
|    total_timesteps      | 1305000       |
| train/                  |               |
|    approx_kl            | 0.00041956446 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.326         |
|    learning_rate        | 0.001         |
|    loss                 | 1.08e+08      |
|    n_updates            | 6370          |
|    policy_gradient_loss | -0.000968     |
|    std                  | 0.897         |
|    value_loss           | 1.88e+08      |
-------------------------------------------
Eval num_timesteps=1305500, episode_reward=-38047.64 +/- 32697.64
Episode length: 43.20 +/- 25.14
------------------------------------
| eval/              |             |
|    mean action     | 0.043186415 |
|    mean velocity x | -0.434      |
|    mean velocity y | -0.517      |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 43.2        |
|    mean_reward     | -3.8e+04    |
| time/              |             |
|    total_timesteps | 1305500     |
------------------------------------
Eval num_timesteps=1306000, episode_reward=-73509.68 +/- 20942.29
Episode length: 66.20 +/- 6.58
-----------------------------------
| eval/              |            |
|    mean action     | -0.2599332 |
|    mean velocity x | 0.143      |
|    mean velocity y | 1.01       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 66.2       |
|    mean_reward     | -7.35e+04  |
| time/              |            |
|    total_timesteps | 1306000    |
-----------------------------------
Eval num_timesteps=1306500, episode_reward=-75685.30 +/- 22594.41
Episode length: 62.20 +/- 13.69
-------------------------------------
| eval/              |              |
|    mean action     | -0.022169681 |
|    mean velocity x | -1.4         |
|    mean velocity y | 1.13         |
|    mean velocity z | 17.5         |
|    mean_ep_length  | 62.2         |
|    mean_reward     | -7.57e+04    |
| time/              |              |
|    total_timesteps | 1306500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.3      |
|    ep_rew_mean     | -6.93e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 638       |
|    time_elapsed    | 8481      |
|    total_timesteps | 1306624   |
----------------------------------
Eval num_timesteps=1307000, episode_reward=-71277.09 +/- 20838.63
Episode length: 64.80 +/- 15.75
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.28810468   |
|    mean velocity x      | -0.326        |
|    mean velocity y      | 1.92          |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 64.8          |
|    mean_reward          | -7.13e+04     |
| time/                   |               |
|    total_timesteps      | 1307000       |
| train/                  |               |
|    approx_kl            | 0.00017895593 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.357         |
|    learning_rate        | 0.001         |
|    loss                 | 7.74e+07      |
|    n_updates            | 6380          |
|    policy_gradient_loss | -0.000828     |
|    std                  | 0.897         |
|    value_loss           | 1.52e+08      |
-------------------------------------------
Eval num_timesteps=1307500, episode_reward=-75613.67 +/- 39822.42
Episode length: 54.20 +/- 15.17
-----------------------------------
| eval/              |            |
|    mean action     | -0.4239535 |
|    mean velocity x | 1.58       |
|    mean velocity y | 2.18       |
|    mean velocity z | 21         |
|    mean_ep_length  | 54.2       |
|    mean_reward     | -7.56e+04  |
| time/              |            |
|    total_timesteps | 1307500    |
-----------------------------------
Eval num_timesteps=1308000, episode_reward=-70331.97 +/- 24334.84
Episode length: 63.20 +/- 14.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.22252265 |
|    mean velocity x | -1.81      |
|    mean velocity y | -1.44      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -7.03e+04  |
| time/              |            |
|    total_timesteps | 1308000    |
-----------------------------------
Eval num_timesteps=1308500, episode_reward=-57385.17 +/- 32447.79
Episode length: 57.20 +/- 19.94
-------------------------------------
| eval/              |              |
|    mean action     | -0.043766845 |
|    mean velocity x | 0.47         |
|    mean velocity y | 1.37         |
|    mean velocity z | 18.1         |
|    mean_ep_length  | 57.2         |
|    mean_reward     | -5.74e+04    |
| time/              |              |
|    total_timesteps | 1308500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.6      |
|    ep_rew_mean     | -7.09e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 639       |
|    time_elapsed    | 8488      |
|    total_timesteps | 1308672   |
----------------------------------
Eval num_timesteps=1309000, episode_reward=-101651.35 +/- 14922.77
Episode length: 67.00 +/- 5.76
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.56062526    |
|    mean velocity x      | -2.26         |
|    mean velocity y      | -3.81         |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 67            |
|    mean_reward          | -1.02e+05     |
| time/                   |               |
|    total_timesteps      | 1309000       |
| train/                  |               |
|    approx_kl            | 0.00072890683 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.001         |
|    loss                 | 4.04e+07      |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.00122      |
|    std                  | 0.897         |
|    value_loss           | 1.59e+08      |
-------------------------------------------
Eval num_timesteps=1309500, episode_reward=-34493.89 +/- 21537.54
Episode length: 52.20 +/- 20.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.28840244 |
|    mean velocity x | -0.279     |
|    mean velocity y | -1.77      |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 52.2       |
|    mean_reward     | -3.45e+04  |
| time/              |            |
|    total_timesteps | 1309500    |
-----------------------------------
Eval num_timesteps=1310000, episode_reward=-51104.01 +/- 19243.14
Episode length: 54.80 +/- 3.66
----------------------------------
| eval/              |           |
|    mean action     | 0.7397108 |
|    mean velocity x | -1.04     |
|    mean velocity y | -3.97     |
|    mean velocity z | 18.7      |
|    mean_ep_length  | 54.8      |
|    mean_reward     | -5.11e+04 |
| time/              |           |
|    total_timesteps | 1310000   |
----------------------------------
Eval num_timesteps=1310500, episode_reward=-40924.96 +/- 36957.32
Episode length: 47.40 +/- 33.17
----------------------------------
| eval/              |           |
|    mean action     | 0.3973498 |
|    mean velocity x | -2.38     |
|    mean velocity y | -3.74     |
|    mean velocity z | 22.4      |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.09e+04 |
| time/              |           |
|    total_timesteps | 1310500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.1      |
|    ep_rew_mean     | -7.56e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 640       |
|    time_elapsed    | 8495      |
|    total_timesteps | 1310720   |
----------------------------------
Eval num_timesteps=1311000, episode_reward=-75691.07 +/- 28929.69
Episode length: 68.40 +/- 23.43
------------------------------------------
| eval/                   |              |
|    mean action          | -0.08200472  |
|    mean velocity x      | 0.854        |
|    mean velocity y      | 0.643        |
|    mean velocity z      | 19.9         |
|    mean_ep_length       | 68.4         |
|    mean_reward          | -7.57e+04    |
| time/                   |              |
|    total_timesteps      | 1311000      |
| train/                  |              |
|    approx_kl            | 0.0006363293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.329        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.000669    |
|    std                  | 0.898        |
|    value_loss           | 2.06e+08     |
------------------------------------------
Eval num_timesteps=1311500, episode_reward=-64141.18 +/- 35156.96
Episode length: 69.80 +/- 37.31
------------------------------------
| eval/              |             |
|    mean action     | -0.01688919 |
|    mean velocity x | -0.354      |
|    mean velocity y | -0.521      |
|    mean velocity z | 22          |
|    mean_ep_length  | 69.8        |
|    mean_reward     | -6.41e+04   |
| time/              |             |
|    total_timesteps | 1311500     |
------------------------------------
Eval num_timesteps=1312000, episode_reward=-81173.81 +/- 20455.62
Episode length: 64.60 +/- 4.63
------------------------------------
| eval/              |             |
|    mean action     | -0.07095535 |
|    mean velocity x | 0.511       |
|    mean velocity y | 0.244       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 64.6        |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 1312000     |
------------------------------------
Eval num_timesteps=1312500, episode_reward=-71081.02 +/- 4069.50
Episode length: 56.80 +/- 1.47
----------------------------------
| eval/              |           |
|    mean action     | 0.3673257 |
|    mean velocity x | 0.806     |
|    mean velocity y | -1.38     |
|    mean velocity z | 16.3      |
|    mean_ep_length  | 56.8      |
|    mean_reward     | -7.11e+04 |
| time/              |           |
|    total_timesteps | 1312500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.6      |
|    ep_rew_mean     | -7.55e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 641       |
|    time_elapsed    | 8502      |
|    total_timesteps | 1312768   |
----------------------------------
Eval num_timesteps=1313000, episode_reward=-55176.61 +/- 32639.20
Episode length: 48.20 +/- 17.05
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.05164694   |
|    mean velocity x      | 1.26          |
|    mean velocity y      | 1.38          |
|    mean velocity z      | 21.5          |
|    mean_ep_length       | 48.2          |
|    mean_reward          | -5.52e+04     |
| time/                   |               |
|    total_timesteps      | 1313000       |
| train/                  |               |
|    approx_kl            | 6.8939844e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.324         |
|    learning_rate        | 0.001         |
|    loss                 | 1.02e+08      |
|    n_updates            | 6410          |
|    policy_gradient_loss | -0.000403     |
|    std                  | 0.898         |
|    value_loss           | 2.12e+08      |
-------------------------------------------
Eval num_timesteps=1313500, episode_reward=-81914.45 +/- 21484.89
Episode length: 63.80 +/- 8.18
------------------------------------
| eval/              |             |
|    mean action     | -0.13161466 |
|    mean velocity x | -0.031      |
|    mean velocity y | 0.623       |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -8.19e+04   |
| time/              |             |
|    total_timesteps | 1313500     |
------------------------------------
Eval num_timesteps=1314000, episode_reward=-80451.76 +/- 15294.63
Episode length: 64.20 +/- 6.68
------------------------------------
| eval/              |             |
|    mean action     | -0.07480243 |
|    mean velocity x | -1.37       |
|    mean velocity y | -0.501      |
|    mean velocity z | 18          |
|    mean_ep_length  | 64.2        |
|    mean_reward     | -8.05e+04   |
| time/              |             |
|    total_timesteps | 1314000     |
------------------------------------
Eval num_timesteps=1314500, episode_reward=-45929.39 +/- 38044.62
Episode length: 47.00 +/- 21.91
-----------------------------------
| eval/              |            |
|    mean action     | -0.2974078 |
|    mean velocity x | -0.285     |
|    mean velocity y | 1.42       |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 47         |
|    mean_reward     | -4.59e+04  |
| time/              |            |
|    total_timesteps | 1314500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.5      |
|    ep_rew_mean     | -7.81e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 642       |
|    time_elapsed    | 8510      |
|    total_timesteps | 1314816   |
----------------------------------
Eval num_timesteps=1315000, episode_reward=-62067.73 +/- 30716.79
Episode length: 56.60 +/- 8.01
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.19322337    |
|    mean velocity x      | -0.289        |
|    mean velocity y      | -1.2          |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 56.6          |
|    mean_reward          | -6.21e+04     |
| time/                   |               |
|    total_timesteps      | 1315000       |
| train/                  |               |
|    approx_kl            | 2.9664778e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.319         |
|    learning_rate        | 0.001         |
|    loss                 | 1.06e+08      |
|    n_updates            | 6420          |
|    policy_gradient_loss | -5.43e-05     |
|    std                  | 0.898         |
|    value_loss           | 2.04e+08      |
-------------------------------------------
Eval num_timesteps=1315500, episode_reward=-58975.19 +/- 28444.71
Episode length: 54.60 +/- 10.87
----------------------------------
| eval/              |           |
|    mean action     | 0.1272721 |
|    mean velocity x | -0.267    |
|    mean velocity y | -1.31     |
|    mean velocity z | 19.9      |
|    mean_ep_length  | 54.6      |
|    mean_reward     | -5.9e+04  |
| time/              |           |
|    total_timesteps | 1315500   |
----------------------------------
Eval num_timesteps=1316000, episode_reward=-59954.58 +/- 26022.00
Episode length: 57.60 +/- 9.93
------------------------------------
| eval/              |             |
|    mean action     | 0.010231346 |
|    mean velocity x | 1.68        |
|    mean velocity y | 1.16        |
|    mean velocity z | 20          |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -6e+04      |
| time/              |             |
|    total_timesteps | 1316000     |
------------------------------------
Eval num_timesteps=1316500, episode_reward=-71991.13 +/- 23924.20
Episode length: 60.60 +/- 8.73
----------------------------------
| eval/              |           |
|    mean action     | 1.0671532 |
|    mean velocity x | -4.71     |
|    mean velocity y | -6.92     |
|    mean velocity z | 16.6      |
|    mean_ep_length  | 60.6      |
|    mean_reward     | -7.2e+04  |
| time/              |           |
|    total_timesteps | 1316500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 64.3      |
|    ep_rew_mean     | -7.52e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 643       |
|    time_elapsed    | 8517      |
|    total_timesteps | 1316864   |
----------------------------------
Eval num_timesteps=1317000, episode_reward=-62697.71 +/- 21582.20
Episode length: 61.40 +/- 8.04
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.556649      |
|    mean velocity x      | -1.23         |
|    mean velocity y      | -2.75         |
|    mean velocity z      | 20.2          |
|    mean_ep_length       | 61.4          |
|    mean_reward          | -6.27e+04     |
| time/                   |               |
|    total_timesteps      | 1317000       |
| train/                  |               |
|    approx_kl            | 8.7964465e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.343         |
|    learning_rate        | 0.001         |
|    loss                 | 8.69e+07      |
|    n_updates            | 6430          |
|    policy_gradient_loss | -0.000633     |
|    std                  | 0.898         |
|    value_loss           | 1.93e+08      |
-------------------------------------------
Eval num_timesteps=1317500, episode_reward=-52586.73 +/- 40910.67
Episode length: 48.60 +/- 21.17
------------------------------------
| eval/              |             |
|    mean action     | -0.46297345 |
|    mean velocity x | 1.2         |
|    mean velocity y | 3.05        |
|    mean velocity z | 19.7        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -5.26e+04   |
| time/              |             |
|    total_timesteps | 1317500     |
------------------------------------
Eval num_timesteps=1318000, episode_reward=-78879.03 +/- 11719.01
Episode length: 70.20 +/- 14.44
-------------------------------------
| eval/              |              |
|    mean action     | -0.027379913 |
|    mean velocity x | 0.957        |
|    mean velocity y | 0.518        |
|    mean velocity z | 16.9         |
|    mean_ep_length  | 70.2         |
|    mean_reward     | -7.89e+04    |
| time/              |              |
|    total_timesteps | 1318000      |
-------------------------------------
Eval num_timesteps=1318500, episode_reward=-81488.09 +/- 44314.81
Episode length: 70.60 +/- 43.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.07262103 |
|    mean velocity x | -1.34      |
|    mean velocity y | -1.05      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 70.6       |
|    mean_reward     | -8.15e+04  |
| time/              |            |
|    total_timesteps | 1318500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.7      |
|    ep_rew_mean     | -7.39e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 644       |
|    time_elapsed    | 8524      |
|    total_timesteps | 1318912   |
----------------------------------
Eval num_timesteps=1319000, episode_reward=-94143.51 +/- 11764.48
Episode length: 61.20 +/- 2.04
------------------------------------------
| eval/                   |              |
|    mean action          | 0.118936196  |
|    mean velocity x      | -0.227       |
|    mean velocity y      | -0.182       |
|    mean velocity z      | 15.4         |
|    mean_ep_length       | 61.2         |
|    mean_reward          | -9.41e+04    |
| time/                   |              |
|    total_timesteps      | 1319000      |
| train/                  |              |
|    approx_kl            | 0.0006139684 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.368        |
|    learning_rate        | 0.001        |
|    loss                 | 8.98e+07     |
|    n_updates            | 6440         |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.897        |
|    value_loss           | 1.45e+08     |
------------------------------------------
Eval num_timesteps=1319500, episode_reward=-69541.47 +/- 28933.63
Episode length: 59.60 +/- 6.62
-----------------------------------
| eval/              |            |
|    mean action     | -0.3166682 |
|    mean velocity x | 0.406      |
|    mean velocity y | 0.653      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -6.95e+04  |
| time/              |            |
|    total_timesteps | 1319500    |
-----------------------------------
Eval num_timesteps=1320000, episode_reward=-58204.11 +/- 31564.15
Episode length: 56.80 +/- 20.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.24694635 |
|    mean velocity x | 0.281      |
|    mean velocity y | -0.736     |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 56.8       |
|    mean_reward     | -5.82e+04  |
| time/              |            |
|    total_timesteps | 1320000    |
-----------------------------------
Eval num_timesteps=1320500, episode_reward=-61845.93 +/- 29879.14
Episode length: 56.40 +/- 21.10
-----------------------------------
| eval/              |            |
|    mean action     | 0.12845585 |
|    mean velocity x | -0.834     |
|    mean velocity y | -0.569     |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 56.4       |
|    mean_reward     | -6.18e+04  |
| time/              |            |
|    total_timesteps | 1320500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.5      |
|    ep_rew_mean     | -7.65e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 645       |
|    time_elapsed    | 8531      |
|    total_timesteps | 1320960   |
----------------------------------
Eval num_timesteps=1321000, episode_reward=-32157.10 +/- 37101.54
Episode length: 43.60 +/- 19.65
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.33253077    |
|    mean velocity x      | -1.86         |
|    mean velocity y      | -2.41         |
|    mean velocity z      | 20.5          |
|    mean_ep_length       | 43.6          |
|    mean_reward          | -3.22e+04     |
| time/                   |               |
|    total_timesteps      | 1321000       |
| train/                  |               |
|    approx_kl            | 0.00027641142 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.001         |
|    loss                 | 7.44e+07      |
|    n_updates            | 6450          |
|    policy_gradient_loss | -0.000409     |
|    std                  | 0.897         |
|    value_loss           | 2.24e+08      |
-------------------------------------------
Eval num_timesteps=1321500, episode_reward=-72119.69 +/- 13135.07
Episode length: 63.40 +/- 7.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.53294766 |
|    mean velocity x | -1.63      |
|    mean velocity y | -3.92      |
|    mean velocity z | 15.3       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -7.21e+04  |
| time/              |            |
|    total_timesteps | 1321500    |
-----------------------------------
Eval num_timesteps=1322000, episode_reward=-61167.01 +/- 28072.76
Episode length: 64.00 +/- 26.50
------------------------------------
| eval/              |             |
|    mean action     | 0.028540177 |
|    mean velocity x | -0.11       |
|    mean velocity y | 0.226       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 64          |
|    mean_reward     | -6.12e+04   |
| time/              |             |
|    total_timesteps | 1322000     |
------------------------------------
Eval num_timesteps=1322500, episode_reward=-66757.53 +/- 33350.91
Episode length: 57.20 +/- 9.50
------------------------------------
| eval/              |             |
|    mean action     | -0.03968611 |
|    mean velocity x | -0.762      |
|    mean velocity y | -0.00311    |
|    mean velocity z | 22.6        |
|    mean_ep_length  | 57.2        |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 1322500     |
------------------------------------
Eval num_timesteps=1323000, episode_reward=-64831.88 +/- 32629.14
Episode length: 56.60 +/- 6.41
------------------------------------
| eval/              |             |
|    mean action     | 0.058478545 |
|    mean velocity x | 0.0803      |
|    mean velocity y | -0.777      |
|    mean velocity z | 14.4        |
|    mean_ep_length  | 56.6        |
|    mean_reward     | -6.48e+04   |
| time/              |             |
|    total_timesteps | 1323000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.9      |
|    ep_rew_mean     | -7.68e+04 |
| time/              |           |
|    fps             | 154       |
|    iterations      | 646       |
|    time_elapsed    | 8539      |
|    total_timesteps | 1323008   |
----------------------------------
Eval num_timesteps=1323500, episode_reward=-57037.91 +/- 31536.25
Episode length: 53.80 +/- 13.82
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13732079   |
|    mean velocity x      | 1.44          |
|    mean velocity y      | 1.44          |
|    mean velocity z      | 18.5          |
|    mean_ep_length       | 53.8          |
|    mean_reward          | -5.7e+04      |
| time/                   |               |
|    total_timesteps      | 1323500       |
| train/                  |               |
|    approx_kl            | 0.00042152032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.337         |
|    learning_rate        | 0.001         |
|    loss                 | 9.04e+07      |
|    n_updates            | 6460          |
|    policy_gradient_loss | -0.000941     |
|    std                  | 0.897         |
|    value_loss           | 2.07e+08      |
-------------------------------------------
Eval num_timesteps=1324000, episode_reward=-87046.00 +/- 23116.24
Episode length: 61.20 +/- 4.71
-----------------------------------
| eval/              |            |
|    mean action     | -0.3691449 |
|    mean velocity x | 1.28       |
|    mean velocity y | 1.53       |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -8.7e+04   |
| time/              |            |
|    total_timesteps | 1324000    |
-----------------------------------
Eval num_timesteps=1324500, episode_reward=-78666.27 +/- 27039.43
Episode length: 61.20 +/- 6.88
-----------------------------------
| eval/              |            |
|    mean action     | -0.7061497 |
|    mean velocity x | 1.87       |
|    mean velocity y | 4.29       |
|    mean velocity z | 17.2       |
|    mean_ep_length  | 61.2       |
|    mean_reward     | -7.87e+04  |
| time/              |            |
|    total_timesteps | 1324500    |
-----------------------------------
Eval num_timesteps=1325000, episode_reward=-38445.27 +/- 32079.84
Episode length: 40.40 +/- 18.58
------------------------------------
| eval/              |             |
|    mean action     | -0.48380077 |
|    mean velocity x | 2.27        |
|    mean velocity y | 3.2         |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 40.4        |
|    mean_reward     | -3.84e+04   |
| time/              |             |
|    total_timesteps | 1325000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.3      |
|    ep_rew_mean     | -7.37e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 647       |
|    time_elapsed    | 8546      |
|    total_timesteps | 1325056   |
----------------------------------
Eval num_timesteps=1325500, episode_reward=-66626.00 +/- 21669.90
Episode length: 63.20 +/- 15.35
------------------------------------------
| eval/                   |              |
|    mean action          | -0.27020696  |
|    mean velocity x      | 0.457        |
|    mean velocity y      | 0.734        |
|    mean velocity z      | 19.6         |
|    mean_ep_length       | 63.2         |
|    mean_reward          | -6.66e+04    |
| time/                   |              |
|    total_timesteps      | 1325500      |
| train/                  |              |
|    approx_kl            | 0.0012867954 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.001        |
|    loss                 | 6.6e+07      |
|    n_updates            | 6470         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.898        |
|    value_loss           | 1.68e+08     |
------------------------------------------
Eval num_timesteps=1326000, episode_reward=-65415.85 +/- 39149.28
Episode length: 53.00 +/- 11.26
-----------------------------------
| eval/              |            |
|    mean action     | 0.33729437 |
|    mean velocity x | -2.92      |
|    mean velocity y | -2.57      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 53         |
|    mean_reward     | -6.54e+04  |
| time/              |            |
|    total_timesteps | 1326000    |
-----------------------------------
Eval num_timesteps=1326500, episode_reward=-49999.22 +/- 22727.75
Episode length: 57.60 +/- 13.89
------------------------------------
| eval/              |             |
|    mean action     | -0.86402017 |
|    mean velocity x | 3.42        |
|    mean velocity y | 4.27        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 57.6        |
|    mean_reward     | -5e+04      |
| time/              |             |
|    total_timesteps | 1326500     |
------------------------------------
Eval num_timesteps=1327000, episode_reward=-66544.69 +/- 27317.50
Episode length: 61.80 +/- 8.63
-------------------------------------
| eval/              |              |
|    mean action     | -0.008289803 |
|    mean velocity x | 0.157        |
|    mean velocity y | 0.75         |
|    mean velocity z | 19.4         |
|    mean_ep_length  | 61.8         |
|    mean_reward     | -6.65e+04    |
| time/              |              |
|    total_timesteps | 1327000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -7.89e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 648       |
|    time_elapsed    | 8553      |
|    total_timesteps | 1327104   |
----------------------------------
Eval num_timesteps=1327500, episode_reward=-73187.72 +/- 27099.42
Episode length: 59.60 +/- 6.68
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.12648447    |
|    mean velocity x      | 1.09          |
|    mean velocity y      | -0.831        |
|    mean velocity z      | 17.2          |
|    mean_ep_length       | 59.6          |
|    mean_reward          | -7.32e+04     |
| time/                   |               |
|    total_timesteps      | 1327500       |
| train/                  |               |
|    approx_kl            | 0.00020276834 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.329         |
|    learning_rate        | 0.001         |
|    loss                 | 6.68e+07      |
|    n_updates            | 6480          |
|    policy_gradient_loss | -0.000746     |
|    std                  | 0.898         |
|    value_loss           | 1.74e+08      |
-------------------------------------------
Eval num_timesteps=1328000, episode_reward=-53672.91 +/- 39269.77
Episode length: 48.80 +/- 15.99
----------------------------------
| eval/              |           |
|    mean action     | 0.10752   |
|    mean velocity x | 0.525     |
|    mean velocity y | -0.644    |
|    mean velocity z | 20.8      |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -5.37e+04 |
| time/              |           |
|    total_timesteps | 1328000   |
----------------------------------
Eval num_timesteps=1328500, episode_reward=-76630.21 +/- 13695.06
Episode length: 64.40 +/- 6.97
------------------------------------
| eval/              |             |
|    mean action     | 0.059313215 |
|    mean velocity x | -0.351      |
|    mean velocity y | -0.801      |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 64.4        |
|    mean_reward     | -7.66e+04   |
| time/              |             |
|    total_timesteps | 1328500     |
------------------------------------
Eval num_timesteps=1329000, episode_reward=-86893.19 +/- 37203.14
Episode length: 60.20 +/- 12.70
----------------------------------
| eval/              |           |
|    mean action     | 0.6276805 |
|    mean velocity x | -1.77     |
|    mean velocity y | -4.54     |
|    mean velocity z | 20.2      |
|    mean_ep_length  | 60.2      |
|    mean_reward     | -8.69e+04 |
| time/              |           |
|    total_timesteps | 1329000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.2      |
|    ep_rew_mean     | -7.77e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 649       |
|    time_elapsed    | 8560      |
|    total_timesteps | 1329152   |
----------------------------------
Eval num_timesteps=1329500, episode_reward=-62884.80 +/- 42319.21
Episode length: 51.80 +/- 9.50
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.18407837    |
|    mean velocity x      | 0.963         |
|    mean velocity y      | -0.485        |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 51.8          |
|    mean_reward          | -6.29e+04     |
| time/                   |               |
|    total_timesteps      | 1329500       |
| train/                  |               |
|    approx_kl            | 3.5681413e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.254         |
|    learning_rate        | 0.001         |
|    loss                 | 6.64e+07      |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000334     |
|    std                  | 0.898         |
|    value_loss           | 2.03e+08      |
-------------------------------------------
Eval num_timesteps=1330000, episode_reward=-65924.15 +/- 19472.92
Episode length: 65.80 +/- 14.93
-----------------------------------
| eval/              |            |
|    mean action     | -0.7580935 |
|    mean velocity x | 3.21       |
|    mean velocity y | 3.97       |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -6.59e+04  |
| time/              |            |
|    total_timesteps | 1330000    |
-----------------------------------
Eval num_timesteps=1330500, episode_reward=-96553.67 +/- 14211.58
Episode length: 65.40 +/- 4.72
----------------------------------
| eval/              |           |
|    mean action     | 0.5002686 |
|    mean velocity x | -1.09     |
|    mean velocity y | -3.8      |
|    mean velocity z | 16        |
|    mean_ep_length  | 65.4      |
|    mean_reward     | -9.66e+04 |
| time/              |           |
|    total_timesteps | 1330500   |
----------------------------------
Eval num_timesteps=1331000, episode_reward=-72529.22 +/- 33888.70
Episode length: 66.40 +/- 22.69
------------------------------------
| eval/              |             |
|    mean action     | 0.020461975 |
|    mean velocity x | -0.685      |
|    mean velocity y | -0.495      |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -7.25e+04   |
| time/              |             |
|    total_timesteps | 1331000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.6      |
|    ep_rew_mean     | -7.77e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 650       |
|    time_elapsed    | 8567      |
|    total_timesteps | 1331200   |
----------------------------------
Eval num_timesteps=1331500, episode_reward=-44499.99 +/- 23337.97
Episode length: 55.40 +/- 11.46
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.045147087  |
|    mean velocity x      | -1.08         |
|    mean velocity y      | -0.75         |
|    mean velocity z      | 22.7          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -4.45e+04     |
| time/                   |               |
|    total_timesteps      | 1331500       |
| train/                  |               |
|    approx_kl            | 5.5394456e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.3           |
|    learning_rate        | 0.001         |
|    loss                 | 6.82e+07      |
|    n_updates            | 6500          |
|    policy_gradient_loss | -0.000391     |
|    std                  | 0.897         |
|    value_loss           | 1.6e+08       |
-------------------------------------------
Eval num_timesteps=1332000, episode_reward=-79774.54 +/- 27844.93
Episode length: 66.40 +/- 13.46
------------------------------------
| eval/              |             |
|    mean action     | -0.24958506 |
|    mean velocity x | 2.66        |
|    mean velocity y | 3.43        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -7.98e+04   |
| time/              |             |
|    total_timesteps | 1332000     |
------------------------------------
Eval num_timesteps=1332500, episode_reward=-52943.80 +/- 42399.24
Episode length: 44.20 +/- 25.33
------------------------------------
| eval/              |             |
|    mean action     | -0.33022642 |
|    mean velocity x | 1.77        |
|    mean velocity y | 2.94        |
|    mean velocity z | 16.8        |
|    mean_ep_length  | 44.2        |
|    mean_reward     | -5.29e+04   |
| time/              |             |
|    total_timesteps | 1332500     |
------------------------------------
Eval num_timesteps=1333000, episode_reward=-71352.74 +/- 11793.00
Episode length: 70.20 +/- 12.35
-----------------------------------
| eval/              |            |
|    mean action     | -0.6024347 |
|    mean velocity x | 1.73       |
|    mean velocity y | 3.82       |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 70.2       |
|    mean_reward     | -7.14e+04  |
| time/              |            |
|    total_timesteps | 1333000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.1      |
|    ep_rew_mean     | -7.55e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 651       |
|    time_elapsed    | 8575      |
|    total_timesteps | 1333248   |
----------------------------------
Eval num_timesteps=1333500, episode_reward=-53754.51 +/- 34414.52
Episode length: 49.80 +/- 17.30
--------------------------------------------
| eval/                   |                |
|    mean action          | 0.215582       |
|    mean velocity x      | -0.578         |
|    mean velocity y      | -1.23          |
|    mean velocity z      | 19.2           |
|    mean_ep_length       | 49.8           |
|    mean_reward          | -5.38e+04      |
| time/                   |                |
|    total_timesteps      | 1333500        |
| train/                  |                |
|    approx_kl            | 0.000110450084 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.93          |
|    explained_variance   | 0.306          |
|    learning_rate        | 0.001          |
|    loss                 | 8.8e+07        |
|    n_updates            | 6510           |
|    policy_gradient_loss | -0.000347      |
|    std                  | 0.897          |
|    value_loss           | 2.03e+08       |
--------------------------------------------
Eval num_timesteps=1334000, episode_reward=-85454.25 +/- 16393.09
Episode length: 64.80 +/- 6.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.44303825 |
|    mean velocity x | 0.0217     |
|    mean velocity y | -1.33      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 64.8       |
|    mean_reward     | -8.55e+04  |
| time/              |            |
|    total_timesteps | 1334000    |
-----------------------------------
Eval num_timesteps=1334500, episode_reward=-75078.19 +/- 32124.87
Episode length: 59.00 +/- 11.88
------------------------------------
| eval/              |             |
|    mean action     | -0.08621315 |
|    mean velocity x | -0.357      |
|    mean velocity y | -0.513      |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 59          |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1334500     |
------------------------------------
Eval num_timesteps=1335000, episode_reward=-62893.79 +/- 15284.15
Episode length: 62.60 +/- 11.64
----------------------------------
| eval/              |           |
|    mean action     | 0.3486892 |
|    mean velocity x | -1.63     |
|    mean velocity y | -2.12     |
|    mean velocity z | 18.3      |
|    mean_ep_length  | 62.6      |
|    mean_reward     | -6.29e+04 |
| time/              |           |
|    total_timesteps | 1335000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.1      |
|    ep_rew_mean     | -7.46e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 652       |
|    time_elapsed    | 8582      |
|    total_timesteps | 1335296   |
----------------------------------
Eval num_timesteps=1335500, episode_reward=-63840.06 +/- 29653.90
Episode length: 66.20 +/- 22.65
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.47587782    |
|    mean velocity x      | -1.92         |
|    mean velocity y      | -3.08         |
|    mean velocity z      | 14.5          |
|    mean_ep_length       | 66.2          |
|    mean_reward          | -6.38e+04     |
| time/                   |               |
|    total_timesteps      | 1335500       |
| train/                  |               |
|    approx_kl            | 0.00021999219 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.001         |
|    loss                 | 9.27e+07      |
|    n_updates            | 6520          |
|    policy_gradient_loss | -0.000671     |
|    std                  | 0.898         |
|    value_loss           | 1.86e+08      |
-------------------------------------------
Eval num_timesteps=1336000, episode_reward=-75771.81 +/- 43865.17
Episode length: 51.80 +/- 17.24
-------------------------------------
| eval/              |              |
|    mean action     | -0.068991855 |
|    mean velocity x | 0.766        |
|    mean velocity y | -0.578       |
|    mean velocity z | 19.8         |
|    mean_ep_length  | 51.8         |
|    mean_reward     | -7.58e+04    |
| time/              |              |
|    total_timesteps | 1336000      |
-------------------------------------
Eval num_timesteps=1336500, episode_reward=-79312.72 +/- 14375.78
Episode length: 80.20 +/- 19.05
----------------------------------
| eval/              |           |
|    mean action     | 0.4481225 |
|    mean velocity x | -1.51     |
|    mean velocity y | -3.08     |
|    mean velocity z | 18.4      |
|    mean_ep_length  | 80.2      |
|    mean_reward     | -7.93e+04 |
| time/              |           |
|    total_timesteps | 1336500   |
----------------------------------
Eval num_timesteps=1337000, episode_reward=-59079.23 +/- 20232.96
Episode length: 53.80 +/- 9.66
------------------------------------
| eval/              |             |
|    mean action     | -0.04040226 |
|    mean velocity x | 0.726       |
|    mean velocity y | 0.211       |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -5.91e+04   |
| time/              |             |
|    total_timesteps | 1337000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.4      |
|    ep_rew_mean     | -7.72e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 653       |
|    time_elapsed    | 8589      |
|    total_timesteps | 1337344   |
----------------------------------
Eval num_timesteps=1337500, episode_reward=-48397.52 +/- 32568.21
Episode length: 52.00 +/- 8.15
------------------------------------------
| eval/                   |              |
|    mean action          | -0.4460051   |
|    mean velocity x      | -0.479       |
|    mean velocity y      | 1.66         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 52           |
|    mean_reward          | -4.84e+04    |
| time/                   |              |
|    total_timesteps      | 1337500      |
| train/                  |              |
|    approx_kl            | 0.0004684151 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.335        |
|    learning_rate        | 0.001        |
|    loss                 | 7.3e+07      |
|    n_updates            | 6530         |
|    policy_gradient_loss | -0.000812    |
|    std                  | 0.898        |
|    value_loss           | 1.82e+08     |
------------------------------------------
Eval num_timesteps=1338000, episode_reward=-72254.13 +/- 37240.54
Episode length: 58.00 +/- 13.61
------------------------------------
| eval/              |             |
|    mean action     | -0.26213494 |
|    mean velocity x | 0.397       |
|    mean velocity y | 1.92        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 58          |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 1338000     |
------------------------------------
Eval num_timesteps=1338500, episode_reward=-58569.07 +/- 28846.25
Episode length: 59.00 +/- 25.05
------------------------------------
| eval/              |             |
|    mean action     | -0.08768573 |
|    mean velocity x | -0.333      |
|    mean velocity y | 0.294       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 59          |
|    mean_reward     | -5.86e+04   |
| time/              |             |
|    total_timesteps | 1338500     |
------------------------------------
Eval num_timesteps=1339000, episode_reward=-86004.12 +/- 32602.54
Episode length: 68.60 +/- 20.82
------------------------------------
| eval/              |             |
|    mean action     | -0.15511173 |
|    mean velocity x | -0.119      |
|    mean velocity y | 0.964       |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 68.6        |
|    mean_reward     | -8.6e+04    |
| time/              |             |
|    total_timesteps | 1339000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.61e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 654       |
|    time_elapsed    | 8596      |
|    total_timesteps | 1339392   |
----------------------------------
Eval num_timesteps=1339500, episode_reward=-88611.83 +/- 13121.63
Episode length: 77.20 +/- 18.31
------------------------------------------
| eval/                   |              |
|    mean action          | -0.26427472  |
|    mean velocity x      | 0.831        |
|    mean velocity y      | 2.72         |
|    mean velocity z      | 17.6         |
|    mean_ep_length       | 77.2         |
|    mean_reward          | -8.86e+04    |
| time/                   |              |
|    total_timesteps      | 1339500      |
| train/                  |              |
|    approx_kl            | 0.0004028965 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.001        |
|    loss                 | 6.86e+07     |
|    n_updates            | 6540         |
|    policy_gradient_loss | -0.000659    |
|    std                  | 0.898        |
|    value_loss           | 1.6e+08      |
------------------------------------------
Eval num_timesteps=1340000, episode_reward=-70888.24 +/- 39609.23
Episode length: 57.80 +/- 22.69
-----------------------------------
| eval/              |            |
|    mean action     | 0.11830825 |
|    mean velocity x | -0.952     |
|    mean velocity y | -1.73      |
|    mean velocity z | 21         |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -7.09e+04  |
| time/              |            |
|    total_timesteps | 1340000    |
-----------------------------------
Eval num_timesteps=1340500, episode_reward=-57145.47 +/- 28676.75
Episode length: 55.20 +/- 24.40
------------------------------------
| eval/              |             |
|    mean action     | -0.15377735 |
|    mean velocity x | 1.37        |
|    mean velocity y | 0.274       |
|    mean velocity z | 19          |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -5.71e+04   |
| time/              |             |
|    total_timesteps | 1340500     |
------------------------------------
Eval num_timesteps=1341000, episode_reward=-84460.06 +/- 32417.14
Episode length: 81.40 +/- 41.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.14119205 |
|    mean velocity x | -0.753     |
|    mean velocity y | -0.218     |
|    mean velocity z | 17         |
|    mean_ep_length  | 81.4       |
|    mean_reward     | -8.45e+04  |
| time/              |            |
|    total_timesteps | 1341000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.9      |
|    ep_rew_mean     | -7.62e+04 |
| time/              |           |
|    fps             | 155       |
|    iterations      | 655       |
|    time_elapsed    | 8604      |
|    total_timesteps | 1341440   |
----------------------------------
Eval num_timesteps=1341500, episode_reward=-73344.85 +/- 24982.22
Episode length: 61.00 +/- 8.81
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.18131027    |
|    mean velocity x      | 2.81           |
|    mean velocity y      | 2.05           |
|    mean velocity z      | 19.2           |
|    mean_ep_length       | 61             |
|    mean_reward          | -7.33e+04      |
| time/                   |                |
|    total_timesteps      | 1341500        |
| train/                  |                |
|    approx_kl            | 0.000113583315 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.93          |
|    explained_variance   | 0.333          |
|    learning_rate        | 0.001          |
|    loss                 | 7.18e+07       |
|    n_updates            | 6550           |
|    policy_gradient_loss | -0.000349      |
|    std                  | 0.898          |
|    value_loss           | 1.89e+08       |
--------------------------------------------
Eval num_timesteps=1342000, episode_reward=-44877.71 +/- 30012.42
Episode length: 48.00 +/- 17.99
-----------------------------------
| eval/              |            |
|    mean action     | 0.39949682 |
|    mean velocity x | -0.924     |
|    mean velocity y | -1.7       |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 48         |
|    mean_reward     | -4.49e+04  |
| time/              |            |
|    total_timesteps | 1342000    |
-----------------------------------
Eval num_timesteps=1342500, episode_reward=-86247.65 +/- 20803.74
Episode length: 62.60 +/- 5.46
------------------------------------
| eval/              |             |
|    mean action     | -0.19982658 |
|    mean velocity x | 0.424       |
|    mean velocity y | 1.54        |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 62.6        |
|    mean_reward     | -8.62e+04   |
| time/              |             |
|    total_timesteps | 1342500     |
------------------------------------
Eval num_timesteps=1343000, episode_reward=-45675.38 +/- 43255.29
Episode length: 48.80 +/- 13.73
------------------------------------
| eval/              |             |
|    mean action     | -0.13071942 |
|    mean velocity x | 0.624       |
|    mean velocity y | 0.581       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -4.57e+04   |
| time/              |             |
|    total_timesteps | 1343000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.9      |
|    ep_rew_mean     | -7.68e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 656       |
|    time_elapsed    | 8611      |
|    total_timesteps | 1343488   |
----------------------------------
Eval num_timesteps=1343500, episode_reward=-86082.09 +/- 12905.76
Episode length: 67.40 +/- 11.25
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.29478267    |
|    mean velocity x      | -1.43         |
|    mean velocity y      | -2.69         |
|    mean velocity z      | 21.8          |
|    mean_ep_length       | 67.4          |
|    mean_reward          | -8.61e+04     |
| time/                   |               |
|    total_timesteps      | 1343500       |
| train/                  |               |
|    approx_kl            | 0.00040917273 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.357         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+08      |
|    n_updates            | 6560          |
|    policy_gradient_loss | -0.000928     |
|    std                  | 0.897         |
|    value_loss           | 1.73e+08      |
-------------------------------------------
Eval num_timesteps=1344000, episode_reward=-91188.23 +/- 26764.40
Episode length: 61.40 +/- 5.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.14060748 |
|    mean velocity x | 0.251      |
|    mean velocity y | -1.3       |
|    mean velocity z | 16.9       |
|    mean_ep_length  | 61.4       |
|    mean_reward     | -9.12e+04  |
| time/              |            |
|    total_timesteps | 1344000    |
-----------------------------------
Eval num_timesteps=1344500, episode_reward=-54477.98 +/- 36127.29
Episode length: 49.80 +/- 14.63
-----------------------------------
| eval/              |            |
|    mean action     | 0.26566532 |
|    mean velocity x | -0.665     |
|    mean velocity y | -2.76      |
|    mean velocity z | 19.2       |
|    mean_ep_length  | 49.8       |
|    mean_reward     | -5.45e+04  |
| time/              |            |
|    total_timesteps | 1344500    |
-----------------------------------
Eval num_timesteps=1345000, episode_reward=-64418.63 +/- 25981.49
Episode length: 65.80 +/- 18.33
-----------------------------------
| eval/              |            |
|    mean action     | 0.08524423 |
|    mean velocity x | -1.11      |
|    mean velocity y | -1.66      |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 65.8       |
|    mean_reward     | -6.44e+04  |
| time/              |            |
|    total_timesteps | 1345000    |
-----------------------------------
Eval num_timesteps=1345500, episode_reward=-61955.07 +/- 34658.46
Episode length: 59.00 +/- 21.88
------------------------------------
| eval/              |             |
|    mean action     | -0.41293204 |
|    mean velocity x | 2.45        |
|    mean velocity y | 2.6         |
|    mean velocity z | 17.8        |
|    mean_ep_length  | 59          |
|    mean_reward     | -6.2e+04    |
| time/              |             |
|    total_timesteps | 1345500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66        |
|    ep_rew_mean     | -7.16e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 657       |
|    time_elapsed    | 8619      |
|    total_timesteps | 1345536   |
----------------------------------
Eval num_timesteps=1346000, episode_reward=-50735.32 +/- 35252.95
Episode length: 48.60 +/- 11.07
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.5150476     |
|    mean velocity x      | -0.743        |
|    mean velocity y      | -3.35         |
|    mean velocity z      | 16.8          |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -5.07e+04     |
| time/                   |               |
|    total_timesteps      | 1346000       |
| train/                  |               |
|    approx_kl            | 5.3779077e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.335         |
|    learning_rate        | 0.001         |
|    loss                 | 6.29e+07      |
|    n_updates            | 6570          |
|    policy_gradient_loss | -0.000219     |
|    std                  | 0.897         |
|    value_loss           | 1.59e+08      |
-------------------------------------------
Eval num_timesteps=1346500, episode_reward=-48400.56 +/- 42236.98
Episode length: 46.00 +/- 21.84
----------------------------------
| eval/              |           |
|    mean action     | 0.3874094 |
|    mean velocity x | -0.744    |
|    mean velocity y | -1.96     |
|    mean velocity z | 18        |
|    mean_ep_length  | 46        |
|    mean_reward     | -4.84e+04 |
| time/              |           |
|    total_timesteps | 1346500   |
----------------------------------
Eval num_timesteps=1347000, episode_reward=-72413.13 +/- 19973.77
Episode length: 65.60 +/- 13.89
------------------------------------
| eval/              |             |
|    mean action     | -0.13843964 |
|    mean velocity x | -1.16       |
|    mean velocity y | 0.507       |
|    mean velocity z | 18          |
|    mean_ep_length  | 65.6        |
|    mean_reward     | -7.24e+04   |
| time/              |             |
|    total_timesteps | 1347000     |
------------------------------------
Eval num_timesteps=1347500, episode_reward=-54530.30 +/- 16451.95
Episode length: 56.00 +/- 7.29
-----------------------------------
| eval/              |            |
|    mean action     | 0.46364453 |
|    mean velocity x | -2.68      |
|    mean velocity y | -4.72      |
|    mean velocity z | 20         |
|    mean_ep_length  | 56         |
|    mean_reward     | -5.45e+04  |
| time/              |            |
|    total_timesteps | 1347500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.8      |
|    ep_rew_mean     | -7.01e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 658       |
|    time_elapsed    | 8626      |
|    total_timesteps | 1347584   |
----------------------------------
Eval num_timesteps=1348000, episode_reward=-57503.31 +/- 35164.69
Episode length: 55.40 +/- 15.91
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1993294     |
|    mean velocity x      | -0.634        |
|    mean velocity y      | -0.501        |
|    mean velocity z      | 20.5          |
|    mean_ep_length       | 55.4          |
|    mean_reward          | -5.75e+04     |
| time/                   |               |
|    total_timesteps      | 1348000       |
| train/                  |               |
|    approx_kl            | 0.00011958217 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.313         |
|    learning_rate        | 0.001         |
|    loss                 | 9.38e+07      |
|    n_updates            | 6580          |
|    policy_gradient_loss | -0.000404     |
|    std                  | 0.897         |
|    value_loss           | 1.8e+08       |
-------------------------------------------
Eval num_timesteps=1348500, episode_reward=-103110.07 +/- 8356.71
Episode length: 69.60 +/- 15.73
----------------------------------
| eval/              |           |
|    mean action     | 0.5181432 |
|    mean velocity x | -2.48     |
|    mean velocity y | -3.71     |
|    mean velocity z | 19.9      |
|    mean_ep_length  | 69.6      |
|    mean_reward     | -1.03e+05 |
| time/              |           |
|    total_timesteps | 1348500   |
----------------------------------
Eval num_timesteps=1349000, episode_reward=-94502.42 +/- 17111.59
Episode length: 65.40 +/- 5.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.48720592 |
|    mean velocity x | 0.882      |
|    mean velocity y | -2.31      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 65.4       |
|    mean_reward     | -9.45e+04  |
| time/              |            |
|    total_timesteps | 1349000    |
-----------------------------------
Eval num_timesteps=1349500, episode_reward=-76203.49 +/- 29153.19
Episode length: 61.40 +/- 7.34
-------------------------------------
| eval/              |              |
|    mean action     | -0.058006607 |
|    mean velocity x | 0.969        |
|    mean velocity y | -0.623       |
|    mean velocity z | 17.6         |
|    mean_ep_length  | 61.4         |
|    mean_reward     | -7.62e+04    |
| time/              |              |
|    total_timesteps | 1349500      |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.6     |
|    ep_rew_mean     | -7.3e+04 |
| time/              |          |
|    fps             | 156      |
|    iterations      | 659      |
|    time_elapsed    | 8633     |
|    total_timesteps | 1349632  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=-67890.61 +/- 40881.03
Episode length: 51.00 +/- 14.51
------------------------------------------
| eval/                   |              |
|    mean action          | 0.11011162   |
|    mean velocity x      | -0.344       |
|    mean velocity y      | -2.28        |
|    mean velocity z      | 18.7         |
|    mean_ep_length       | 51           |
|    mean_reward          | -6.79e+04    |
| time/                   |              |
|    total_timesteps      | 1350000      |
| train/                  |              |
|    approx_kl            | 0.0003977084 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.001        |
|    loss                 | 7e+07        |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.000556    |
|    std                  | 0.897        |
|    value_loss           | 2.03e+08     |
------------------------------------------
Eval num_timesteps=1350500, episode_reward=-80265.04 +/- 30573.87
Episode length: 59.00 +/- 6.78
------------------------------------
| eval/              |             |
|    mean action     | -0.32747927 |
|    mean velocity x | 1.62        |
|    mean velocity y | 1.3         |
|    mean velocity z | 21          |
|    mean_ep_length  | 59          |
|    mean_reward     | -8.03e+04   |
| time/              |             |
|    total_timesteps | 1350500     |
------------------------------------
Eval num_timesteps=1351000, episode_reward=-74713.40 +/- 37836.57
Episode length: 56.00 +/- 15.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.71863693 |
|    mean velocity x | -3.71      |
|    mean velocity y | -5.79      |
|    mean velocity z | 16         |
|    mean_ep_length  | 56         |
|    mean_reward     | -7.47e+04  |
| time/              |            |
|    total_timesteps | 1351000    |
-----------------------------------
Eval num_timesteps=1351500, episode_reward=-75045.26 +/- 34621.82
Episode length: 65.20 +/- 22.21
----------------------------------
| eval/              |           |
|    mean action     | 0.2764725 |
|    mean velocity x | -2.54     |
|    mean velocity y | -1.6      |
|    mean velocity z | 18.7      |
|    mean_ep_length  | 65.2      |
|    mean_reward     | -7.5e+04  |
| time/              |           |
|    total_timesteps | 1351500   |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 156      |
|    iterations      | 660      |
|    time_elapsed    | 8640     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1352000, episode_reward=-73319.08 +/- 39174.80
Episode length: 53.40 +/- 17.21
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13281025   |
|    mean velocity x      | -0.099        |
|    mean velocity y      | 0.396         |
|    mean velocity z      | 18.1          |
|    mean_ep_length       | 53.4          |
|    mean_reward          | -7.33e+04     |
| time/                   |               |
|    total_timesteps      | 1352000       |
| train/                  |               |
|    approx_kl            | 0.00058774516 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.339         |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 6600          |
|    policy_gradient_loss | -0.00106      |
|    std                  | 0.897         |
|    value_loss           | 1.79e+08      |
-------------------------------------------
Eval num_timesteps=1352500, episode_reward=-72540.85 +/- 33266.42
Episode length: 57.20 +/- 16.74
-------------------------------------
| eval/              |              |
|    mean action     | -0.093181886 |
|    mean velocity x | 1.69         |
|    mean velocity y | 0.387        |
|    mean velocity z | 19.5         |
|    mean_ep_length  | 57.2         |
|    mean_reward     | -7.25e+04    |
| time/              |              |
|    total_timesteps | 1352500      |
-------------------------------------
Eval num_timesteps=1353000, episode_reward=-59990.73 +/- 30446.52
Episode length: 62.60 +/- 25.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.10397453 |
|    mean velocity x | -0.0412    |
|    mean velocity y | -0.489     |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -6e+04     |
| time/              |            |
|    total_timesteps | 1353000    |
-----------------------------------
Eval num_timesteps=1353500, episode_reward=-63468.35 +/- 32480.17
Episode length: 53.00 +/- 18.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.11486732 |
|    mean velocity x | 0.235      |
|    mean velocity y | -0.191     |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 53         |
|    mean_reward     | -6.35e+04  |
| time/              |            |
|    total_timesteps | 1353500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.8      |
|    ep_rew_mean     | -7.69e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 661       |
|    time_elapsed    | 8648      |
|    total_timesteps | 1353728   |
----------------------------------
Eval num_timesteps=1354000, episode_reward=-48760.02 +/- 29950.19
Episode length: 58.20 +/- 26.53
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.26216143    |
|    mean velocity x      | -0.668        |
|    mean velocity y      | -1.03         |
|    mean velocity z      | 19.1          |
|    mean_ep_length       | 58.2          |
|    mean_reward          | -4.88e+04     |
| time/                   |               |
|    total_timesteps      | 1354000       |
| train/                  |               |
|    approx_kl            | 0.00047473476 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.366         |
|    learning_rate        | 0.001         |
|    loss                 | 6.47e+07      |
|    n_updates            | 6610          |
|    policy_gradient_loss | -0.000783     |
|    std                  | 0.897         |
|    value_loss           | 1.47e+08      |
-------------------------------------------
Eval num_timesteps=1354500, episode_reward=-74675.50 +/- 19964.20
Episode length: 63.40 +/- 5.43
----------------------------------
| eval/              |           |
|    mean action     | 0.3444967 |
|    mean velocity x | -1.3      |
|    mean velocity y | -2.81     |
|    mean velocity z | 19.1      |
|    mean_ep_length  | 63.4      |
|    mean_reward     | -7.47e+04 |
| time/              |           |
|    total_timesteps | 1354500   |
----------------------------------
Eval num_timesteps=1355000, episode_reward=-61968.86 +/- 31746.07
Episode length: 60.80 +/- 15.28
------------------------------------
| eval/              |             |
|    mean action     | -0.07324996 |
|    mean velocity x | 0.912       |
|    mean velocity y | 0.95        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 60.8        |
|    mean_reward     | -6.2e+04    |
| time/              |             |
|    total_timesteps | 1355000     |
------------------------------------
Eval num_timesteps=1355500, episode_reward=-37007.39 +/- 36111.94
Episode length: 48.00 +/- 29.51
-----------------------------------
| eval/              |            |
|    mean action     | 0.24422307 |
|    mean velocity x | -1.79      |
|    mean velocity y | -3.04      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 48         |
|    mean_reward     | -3.7e+04   |
| time/              |            |
|    total_timesteps | 1355500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69        |
|    ep_rew_mean     | -7.52e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 662       |
|    time_elapsed    | 8655      |
|    total_timesteps | 1355776   |
----------------------------------
Eval num_timesteps=1356000, episode_reward=-93516.75 +/- 62357.14
Episode length: 96.60 +/- 67.23
------------------------------------------
| eval/                   |              |
|    mean action          | -0.29377776  |
|    mean velocity x      | 0.961        |
|    mean velocity y      | 2.38         |
|    mean velocity z      | 19.1         |
|    mean_ep_length       | 96.6         |
|    mean_reward          | -9.35e+04    |
| time/                   |              |
|    total_timesteps      | 1356000      |
| train/                  |              |
|    approx_kl            | 0.0001728897 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.33         |
|    learning_rate        | 0.001        |
|    loss                 | 7.72e+07     |
|    n_updates            | 6620         |
|    policy_gradient_loss | -0.000308    |
|    std                  | 0.897        |
|    value_loss           | 1.97e+08     |
------------------------------------------
Eval num_timesteps=1356500, episode_reward=-75572.64 +/- 16009.54
Episode length: 67.20 +/- 6.34
------------------------------------
| eval/              |             |
|    mean action     | 0.046451233 |
|    mean velocity x | -0.501      |
|    mean velocity y | -0.829      |
|    mean velocity z | 23          |
|    mean_ep_length  | 67.2        |
|    mean_reward     | -7.56e+04   |
| time/              |             |
|    total_timesteps | 1356500     |
------------------------------------
Eval num_timesteps=1357000, episode_reward=-75552.32 +/- 42651.38
Episode length: 76.40 +/- 51.22
------------------------------------
| eval/              |             |
|    mean action     | -0.29983428 |
|    mean velocity x | 2.39        |
|    mean velocity y | 1.8         |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 76.4        |
|    mean_reward     | -7.56e+04   |
| time/              |             |
|    total_timesteps | 1357000     |
------------------------------------
Eval num_timesteps=1357500, episode_reward=-74966.03 +/- 37647.56
Episode length: 63.20 +/- 25.99
-----------------------------------
| eval/              |            |
|    mean action     | -0.5234411 |
|    mean velocity x | 2.4        |
|    mean velocity y | 3.87       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -7.5e+04   |
| time/              |            |
|    total_timesteps | 1357500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -7.98e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 663       |
|    time_elapsed    | 8681      |
|    total_timesteps | 1357824   |
----------------------------------
Eval num_timesteps=1358000, episode_reward=-48322.25 +/- 40180.71
Episode length: 49.40 +/- 29.38
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.030825958 |
|    mean velocity x      | -1.36       |
|    mean velocity y      | -1.11       |
|    mean velocity z      | 21.2        |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -4.83e+04   |
| time/                   |             |
|    total_timesteps      | 1358000     |
| train/                  |             |
|    approx_kl            | 2.77363e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.001       |
|    loss                 | 1.99e+08    |
|    n_updates            | 6630        |
|    policy_gradient_loss | -0.000184   |
|    std                  | 0.897       |
|    value_loss           | 2.79e+08    |
-----------------------------------------
Eval num_timesteps=1358500, episode_reward=-76302.96 +/- 22266.51
Episode length: 86.20 +/- 36.50
-----------------------------------
| eval/              |            |
|    mean action     | 0.28531742 |
|    mean velocity x | -1.65      |
|    mean velocity y | -2.1       |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 86.2       |
|    mean_reward     | -7.63e+04  |
| time/              |            |
|    total_timesteps | 1358500    |
-----------------------------------
Eval num_timesteps=1359000, episode_reward=-69763.88 +/- 32507.45
Episode length: 64.80 +/- 23.45
------------------------------------
| eval/              |             |
|    mean action     | -0.21750316 |
|    mean velocity x | 0.942       |
|    mean velocity y | 0.782       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 64.8        |
|    mean_reward     | -6.98e+04   |
| time/              |             |
|    total_timesteps | 1359000     |
------------------------------------
Eval num_timesteps=1359500, episode_reward=-60351.36 +/- 35696.81
Episode length: 51.00 +/- 11.64
-----------------------------------
| eval/              |            |
|    mean action     | -0.0584236 |
|    mean velocity x | -0.582     |
|    mean velocity y | 0.252      |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 51         |
|    mean_reward     | -6.04e+04  |
| time/              |            |
|    total_timesteps | 1359500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -8e+04   |
| time/              |          |
|    fps             | 156      |
|    iterations      | 664      |
|    time_elapsed    | 8688     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=-53422.31 +/- 40100.41
Episode length: 49.80 +/- 12.29
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3607668   |
|    mean velocity x      | 1.35         |
|    mean velocity y      | 1.74         |
|    mean velocity z      | 18.8         |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -5.34e+04    |
| time/                   |              |
|    total_timesteps      | 1360000      |
| train/                  |              |
|    approx_kl            | 8.611631e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+08     |
|    n_updates            | 6640         |
|    policy_gradient_loss | -0.000416    |
|    std                  | 0.897        |
|    value_loss           | 1.75e+08     |
------------------------------------------
Eval num_timesteps=1360500, episode_reward=-55978.70 +/- 38200.20
Episode length: 53.00 +/- 25.27
-----------------------------------
| eval/              |            |
|    mean action     | 0.17999347 |
|    mean velocity x | -0.919     |
|    mean velocity y | -1.29      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 53         |
|    mean_reward     | -5.6e+04   |
| time/              |            |
|    total_timesteps | 1360500    |
-----------------------------------
Eval num_timesteps=1361000, episode_reward=-53028.33 +/- 41924.67
Episode length: 49.80 +/- 20.37
-----------------------------------
| eval/              |            |
|    mean action     | 0.14116338 |
|    mean velocity x | 0.482      |
|    mean velocity y | -1.15      |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 49.8       |
|    mean_reward     | -5.3e+04   |
| time/              |            |
|    total_timesteps | 1361000    |
-----------------------------------
Eval num_timesteps=1361500, episode_reward=-80251.36 +/- 22178.93
Episode length: 62.20 +/- 5.46
-----------------------------------
| eval/              |            |
|    mean action     | 0.34183192 |
|    mean velocity x | -1.41      |
|    mean velocity y | -2.75      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 62.2       |
|    mean_reward     | -8.03e+04  |
| time/              |            |
|    total_timesteps | 1361500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -8.05e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 665       |
|    time_elapsed    | 8695      |
|    total_timesteps | 1361920   |
----------------------------------
Eval num_timesteps=1362000, episode_reward=-73608.15 +/- 29651.37
Episode length: 59.20 +/- 7.47
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.3536087     |
|    mean velocity x      | -0.645        |
|    mean velocity y      | -2.13         |
|    mean velocity z      | 19.9          |
|    mean_ep_length       | 59.2          |
|    mean_reward          | -7.36e+04     |
| time/                   |               |
|    total_timesteps      | 1362000       |
| train/                  |               |
|    approx_kl            | 5.1576353e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.294         |
|    learning_rate        | 0.001         |
|    loss                 | 6.83e+07      |
|    n_updates            | 6650          |
|    policy_gradient_loss | -8.72e-05     |
|    std                  | 0.897         |
|    value_loss           | 2.05e+08      |
-------------------------------------------
Eval num_timesteps=1362500, episode_reward=-75972.01 +/- 38982.31
Episode length: 56.40 +/- 21.84
------------------------------------
| eval/              |             |
|    mean action     | -0.14136074 |
|    mean velocity x | -0.195      |
|    mean velocity y | 0.177       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 56.4        |
|    mean_reward     | -7.6e+04    |
| time/              |             |
|    total_timesteps | 1362500     |
------------------------------------
Eval num_timesteps=1363000, episode_reward=-54779.71 +/- 29001.52
Episode length: 49.80 +/- 13.35
-----------------------------------
| eval/              |            |
|    mean action     | 0.40263465 |
|    mean velocity x | -1.27      |
|    mean velocity y | -1.65      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 49.8       |
|    mean_reward     | -5.48e+04  |
| time/              |            |
|    total_timesteps | 1363000    |
-----------------------------------
Eval num_timesteps=1363500, episode_reward=-44755.92 +/- 19599.68
Episode length: 50.60 +/- 10.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.01621231 |
|    mean velocity x | -0.218     |
|    mean velocity y | 0.646      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 50.6       |
|    mean_reward     | -4.48e+04  |
| time/              |            |
|    total_timesteps | 1363500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.2      |
|    ep_rew_mean     | -7.53e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 666       |
|    time_elapsed    | 8702      |
|    total_timesteps | 1363968   |
----------------------------------
Eval num_timesteps=1364000, episode_reward=-54307.71 +/- 34747.51
Episode length: 57.40 +/- 16.13
------------------------------------------
| eval/                   |              |
|    mean action          | -0.22287284  |
|    mean velocity x      | 0.000893     |
|    mean velocity y      | 0.391        |
|    mean velocity z      | 16.3         |
|    mean_ep_length       | 57.4         |
|    mean_reward          | -5.43e+04    |
| time/                   |              |
|    total_timesteps      | 1364000      |
| train/                  |              |
|    approx_kl            | 4.393616e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.001        |
|    loss                 | 6.22e+07     |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.000286    |
|    std                  | 0.897        |
|    value_loss           | 1.93e+08     |
------------------------------------------
Eval num_timesteps=1364500, episode_reward=-50595.06 +/- 32122.20
Episode length: 62.60 +/- 29.36
----------------------------------
| eval/              |           |
|    mean action     | 0.3787812 |
|    mean velocity x | -0.266    |
|    mean velocity y | -1.86     |
|    mean velocity z | 20.5      |
|    mean_ep_length  | 62.6      |
|    mean_reward     | -5.06e+04 |
| time/              |           |
|    total_timesteps | 1364500   |
----------------------------------
Eval num_timesteps=1365000, episode_reward=-67879.66 +/- 24369.85
Episode length: 63.40 +/- 13.12
------------------------------------
| eval/              |             |
|    mean action     | -0.28358966 |
|    mean velocity x | 2.03        |
|    mean velocity y | 0.869       |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 63.4        |
|    mean_reward     | -6.79e+04   |
| time/              |             |
|    total_timesteps | 1365000     |
------------------------------------
Eval num_timesteps=1365500, episode_reward=-53812.99 +/- 36666.22
Episode length: 48.80 +/- 18.80
------------------------------------
| eval/              |             |
|    mean action     | -0.84427845 |
|    mean velocity x | 1.75        |
|    mean velocity y | 4.67        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -5.38e+04   |
| time/              |             |
|    total_timesteps | 1365500     |
------------------------------------
Eval num_timesteps=1366000, episode_reward=-78422.33 +/- 19121.60
Episode length: 63.40 +/- 7.06
-----------------------------------
| eval/              |            |
|    mean action     | 0.53424233 |
|    mean velocity x | -2.79      |
|    mean velocity y | -4.54      |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 63.4       |
|    mean_reward     | -7.84e+04  |
| time/              |            |
|    total_timesteps | 1366000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.5      |
|    ep_rew_mean     | -7.34e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 667       |
|    time_elapsed    | 8710      |
|    total_timesteps | 1366016   |
----------------------------------
Eval num_timesteps=1366500, episode_reward=-58011.33 +/- 36641.51
Episode length: 65.00 +/- 27.96
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.28670454    |
|    mean velocity x      | -0.891        |
|    mean velocity y      | -1.83         |
|    mean velocity z      | 22.3          |
|    mean_ep_length       | 65            |
|    mean_reward          | -5.8e+04      |
| time/                   |               |
|    total_timesteps      | 1366500       |
| train/                  |               |
|    approx_kl            | 1.7490907e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.337         |
|    learning_rate        | 0.001         |
|    loss                 | 9.12e+07      |
|    n_updates            | 6670          |
|    policy_gradient_loss | -0.00018      |
|    std                  | 0.896         |
|    value_loss           | 1.66e+08      |
-------------------------------------------
Eval num_timesteps=1367000, episode_reward=-71800.29 +/- 22702.56
Episode length: 67.80 +/- 18.05
-----------------------------------
| eval/              |            |
|    mean action     | 0.39796898 |
|    mean velocity x | -1.95      |
|    mean velocity y | -3.5       |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 67.8       |
|    mean_reward     | -7.18e+04  |
| time/              |            |
|    total_timesteps | 1367000    |
-----------------------------------
Eval num_timesteps=1367500, episode_reward=-79169.00 +/- 9367.35
Episode length: 66.80 +/- 9.43
------------------------------------
| eval/              |             |
|    mean action     | 0.045373917 |
|    mean velocity x | -0.722      |
|    mean velocity y | -0.676      |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 66.8        |
|    mean_reward     | -7.92e+04   |
| time/              |             |
|    total_timesteps | 1367500     |
------------------------------------
Eval num_timesteps=1368000, episode_reward=-66430.29 +/- 21339.19
Episode length: 57.80 +/- 8.66
-----------------------------------
| eval/              |            |
|    mean action     | 0.40989873 |
|    mean velocity x | -1.13      |
|    mean velocity y | -2.34      |
|    mean velocity z | 18.9       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -6.64e+04  |
| time/              |            |
|    total_timesteps | 1368000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.4      |
|    ep_rew_mean     | -7.76e+04 |
| time/              |           |
|    fps             | 156       |
|    iterations      | 668       |
|    time_elapsed    | 8717      |
|    total_timesteps | 1368064   |
----------------------------------
Eval num_timesteps=1368500, episode_reward=-80224.52 +/- 21871.61
Episode length: 58.60 +/- 7.26
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2580797    |
|    mean velocity x      | -0.846       |
|    mean velocity y      | -1.2         |
|    mean velocity z      | 17.1         |
|    mean_ep_length       | 58.6         |
|    mean_reward          | -8.02e+04    |
| time/                   |              |
|    total_timesteps      | 1368500      |
| train/                  |              |
|    approx_kl            | 7.246548e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.296        |
|    learning_rate        | 0.001        |
|    loss                 | 6.85e+07     |
|    n_updates            | 6680         |
|    policy_gradient_loss | -0.000222    |
|    std                  | 0.897        |
|    value_loss           | 2.17e+08     |
------------------------------------------
Eval num_timesteps=1369000, episode_reward=-67351.45 +/- 22233.57
Episode length: 62.40 +/- 14.91
----------------------------------
| eval/              |           |
|    mean action     | 0.6571317 |
|    mean velocity x | -1.74     |
|    mean velocity y | -3.21     |
|    mean velocity z | 14.3      |
|    mean_ep_length  | 62.4      |
|    mean_reward     | -6.74e+04 |
| time/              |           |
|    total_timesteps | 1369000   |
----------------------------------
Eval num_timesteps=1369500, episode_reward=-75134.68 +/- 37181.85
Episode length: 53.80 +/- 15.92
------------------------------------
| eval/              |             |
|    mean action     | 0.014307312 |
|    mean velocity x | 0.874       |
|    mean velocity y | 0.707       |
|    mean velocity z | 20.6        |
|    mean_ep_length  | 53.8        |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1369500     |
------------------------------------
Eval num_timesteps=1370000, episode_reward=-52812.22 +/- 36460.78
Episode length: 66.40 +/- 14.22
----------------------------------
| eval/              |           |
|    mean action     | 0.5768747 |
|    mean velocity x | -1.99     |
|    mean velocity y | -3.05     |
|    mean velocity z | 18.1      |
|    mean_ep_length  | 66.4      |
|    mean_reward     | -5.28e+04 |
| time/              |           |
|    total_timesteps | 1370000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67        |
|    ep_rew_mean     | -7.24e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 669       |
|    time_elapsed    | 8725      |
|    total_timesteps | 1370112   |
----------------------------------
Eval num_timesteps=1370500, episode_reward=-42765.75 +/- 34241.69
Episode length: 50.00 +/- 27.46
------------------------------------------
| eval/                   |              |
|    mean action          | 0.06907992   |
|    mean velocity x      | -0.358       |
|    mean velocity y      | -1.04        |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 50           |
|    mean_reward          | -4.28e+04    |
| time/                   |              |
|    total_timesteps      | 1370500      |
| train/                  |              |
|    approx_kl            | 0.0003347555 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.348        |
|    learning_rate        | 0.001        |
|    loss                 | 8.81e+07     |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.000775    |
|    std                  | 0.897        |
|    value_loss           | 1.57e+08     |
------------------------------------------
Eval num_timesteps=1371000, episode_reward=-64131.41 +/- 26975.82
Episode length: 59.80 +/- 7.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.12163826 |
|    mean velocity x | -0.742     |
|    mean velocity y | -2.27      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 59.8       |
|    mean_reward     | -6.41e+04  |
| time/              |            |
|    total_timesteps | 1371000    |
-----------------------------------
Eval num_timesteps=1371500, episode_reward=-101931.85 +/- 13673.09
Episode length: 63.80 +/- 4.71
-----------------------------------
| eval/              |            |
|    mean action     | 0.15339932 |
|    mean velocity x | -0.526     |
|    mean velocity y | 0.0566     |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 63.8       |
|    mean_reward     | -1.02e+05  |
| time/              |            |
|    total_timesteps | 1371500    |
-----------------------------------
Eval num_timesteps=1372000, episode_reward=-50754.90 +/- 33738.10
Episode length: 68.00 +/- 41.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.07756951 |
|    mean velocity x | -1.26      |
|    mean velocity y | -0.602     |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 68         |
|    mean_reward     | -5.08e+04  |
| time/              |            |
|    total_timesteps | 1372000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 64.9      |
|    ep_rew_mean     | -7.27e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 670       |
|    time_elapsed    | 8732      |
|    total_timesteps | 1372160   |
----------------------------------
Eval num_timesteps=1372500, episode_reward=-42120.01 +/- 32625.40
Episode length: 46.00 +/- 22.05
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35494348  |
|    mean velocity x      | 1.63         |
|    mean velocity y      | 1.84         |
|    mean velocity z      | 18.4         |
|    mean_ep_length       | 46           |
|    mean_reward          | -4.21e+04    |
| time/                   |              |
|    total_timesteps      | 1372500      |
| train/                  |              |
|    approx_kl            | 0.0005125234 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+08     |
|    n_updates            | 6700         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 0.897        |
|    value_loss           | 2.02e+08     |
------------------------------------------
Eval num_timesteps=1373000, episode_reward=-49709.56 +/- 40791.03
Episode length: 48.80 +/- 21.23
------------------------------------
| eval/              |             |
|    mean action     | 0.024166271 |
|    mean velocity x | 1.38        |
|    mean velocity y | -0.159      |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 48.8        |
|    mean_reward     | -4.97e+04   |
| time/              |             |
|    total_timesteps | 1373000     |
------------------------------------
Eval num_timesteps=1373500, episode_reward=-72016.03 +/- 33459.98
Episode length: 60.60 +/- 12.37
------------------------------------
| eval/              |             |
|    mean action     | -0.27969784 |
|    mean velocity x | 2.2         |
|    mean velocity y | 1.88        |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 60.6        |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 1373500     |
------------------------------------
Eval num_timesteps=1374000, episode_reward=-77127.09 +/- 38362.62
Episode length: 56.00 +/- 14.18
------------------------------------
| eval/              |             |
|    mean action     | -0.33535668 |
|    mean velocity x | 0.312       |
|    mean velocity y | 1.02        |
|    mean velocity z | 17.6        |
|    mean_ep_length  | 56          |
|    mean_reward     | -7.71e+04   |
| time/              |             |
|    total_timesteps | 1374000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 62.9      |
|    ep_rew_mean     | -6.66e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 671       |
|    time_elapsed    | 8739      |
|    total_timesteps | 1374208   |
----------------------------------
Eval num_timesteps=1374500, episode_reward=-92739.05 +/- 14930.56
Episode length: 64.20 +/- 5.34
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.027826507  |
|    mean velocity x      | -0.872        |
|    mean velocity y      | -0.554        |
|    mean velocity z      | 21.4          |
|    mean_ep_length       | 64.2          |
|    mean_reward          | -9.27e+04     |
| time/                   |               |
|    total_timesteps      | 1374500       |
| train/                  |               |
|    approx_kl            | 0.00045498708 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.362         |
|    learning_rate        | 0.001         |
|    loss                 | 6.57e+07      |
|    n_updates            | 6710          |
|    policy_gradient_loss | -0.000975     |
|    std                  | 0.897         |
|    value_loss           | 1.62e+08      |
-------------------------------------------
Eval num_timesteps=1375000, episode_reward=-35988.80 +/- 40514.99
Episode length: 37.20 +/- 16.82
-----------------------------------
| eval/              |            |
|    mean action     | -0.6176469 |
|    mean velocity x | 2.52       |
|    mean velocity y | 3.94       |
|    mean velocity z | 17.5       |
|    mean_ep_length  | 37.2       |
|    mean_reward     | -3.6e+04   |
| time/              |            |
|    total_timesteps | 1375000    |
-----------------------------------
Eval num_timesteps=1375500, episode_reward=-56245.54 +/- 16280.18
Episode length: 55.40 +/- 5.46
----------------------------------
| eval/              |           |
|    mean action     | 0.7530449 |
|    mean velocity x | -2.88     |
|    mean velocity y | -4.93     |
|    mean velocity z | 17.9      |
|    mean_ep_length  | 55.4      |
|    mean_reward     | -5.62e+04 |
| time/              |           |
|    total_timesteps | 1375500   |
----------------------------------
Eval num_timesteps=1376000, episode_reward=-57911.82 +/- 34231.53
Episode length: 46.60 +/- 20.09
------------------------------------
| eval/              |             |
|    mean action     | -0.19251986 |
|    mean velocity x | 0.856       |
|    mean velocity y | 0.711       |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 46.6        |
|    mean_reward     | -5.79e+04   |
| time/              |             |
|    total_timesteps | 1376000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66        |
|    ep_rew_mean     | -7.18e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 672       |
|    time_elapsed    | 8746      |
|    total_timesteps | 1376256   |
----------------------------------
Eval num_timesteps=1376500, episode_reward=-50373.07 +/- 21969.60
Episode length: 59.80 +/- 6.73
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.11321317   |
|    mean velocity x      | -0.57         |
|    mean velocity y      | 0.241         |
|    mean velocity z      | 19.2          |
|    mean_ep_length       | 59.8          |
|    mean_reward          | -5.04e+04     |
| time/                   |               |
|    total_timesteps      | 1376500       |
| train/                  |               |
|    approx_kl            | 0.00014569054 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.37          |
|    learning_rate        | 0.001         |
|    loss                 | 1.12e+08      |
|    n_updates            | 6720          |
|    policy_gradient_loss | -0.000278     |
|    std                  | 0.897         |
|    value_loss           | 1.79e+08      |
-------------------------------------------
Eval num_timesteps=1377000, episode_reward=-66995.02 +/- 34715.36
Episode length: 60.00 +/- 15.19
-----------------------------------
| eval/              |            |
|    mean action     | 0.18647997 |
|    mean velocity x | -3.02      |
|    mean velocity y | -3.2       |
|    mean velocity z | 21         |
|    mean_ep_length  | 60         |
|    mean_reward     | -6.7e+04   |
| time/              |            |
|    total_timesteps | 1377000    |
-----------------------------------
Eval num_timesteps=1377500, episode_reward=-50828.26 +/- 40845.97
Episode length: 44.40 +/- 18.88
----------------------------------
| eval/              |           |
|    mean action     | -0.538859 |
|    mean velocity x | 2.09      |
|    mean velocity y | 2.99      |
|    mean velocity z | 17.4      |
|    mean_ep_length  | 44.4      |
|    mean_reward     | -5.08e+04 |
| time/              |           |
|    total_timesteps | 1377500   |
----------------------------------
Eval num_timesteps=1378000, episode_reward=-18886.38 +/- 25539.85
Episode length: 26.00 +/- 18.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.35244215 |
|    mean velocity x | -0.204     |
|    mean velocity y | -2         |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 26         |
|    mean_reward     | -1.89e+04  |
| time/              |            |
|    total_timesteps | 1378000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.4      |
|    ep_rew_mean     | -7.34e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 673       |
|    time_elapsed    | 8753      |
|    total_timesteps | 1378304   |
----------------------------------
Eval num_timesteps=1378500, episode_reward=-27927.22 +/- 23056.29
Episode length: 37.60 +/- 22.50
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.4582673  |
|    mean velocity x      | 1.9         |
|    mean velocity y      | 3.11        |
|    mean velocity z      | 15.8        |
|    mean_ep_length       | 37.6        |
|    mean_reward          | -2.79e+04   |
| time/                   |             |
|    total_timesteps      | 1378500     |
| train/                  |             |
|    approx_kl            | 6.38286e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.001       |
|    loss                 | 8.79e+07    |
|    n_updates            | 6730        |
|    policy_gradient_loss | -0.000401   |
|    std                  | 0.897       |
|    value_loss           | 1.71e+08    |
-----------------------------------------
Eval num_timesteps=1379000, episode_reward=-46675.65 +/- 31482.21
Episode length: 59.80 +/- 27.05
------------------------------------
| eval/              |             |
|    mean action     | -0.12281317 |
|    mean velocity x | -0.624      |
|    mean velocity y | 0.134       |
|    mean velocity z | 19          |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -4.67e+04   |
| time/              |             |
|    total_timesteps | 1379000     |
------------------------------------
Eval num_timesteps=1379500, episode_reward=-54115.88 +/- 43486.30
Episode length: 44.60 +/- 20.91
----------------------------------
| eval/              |           |
|    mean action     | 0.4556383 |
|    mean velocity x | -0.589    |
|    mean velocity y | -2.57     |
|    mean velocity z | 20        |
|    mean_ep_length  | 44.6      |
|    mean_reward     | -5.41e+04 |
| time/              |           |
|    total_timesteps | 1379500   |
----------------------------------
Eval num_timesteps=1380000, episode_reward=-81568.19 +/- 27736.52
Episode length: 74.00 +/- 30.07
------------------------------------
| eval/              |             |
|    mean action     | -0.21574333 |
|    mean velocity x | 0.857       |
|    mean velocity y | 1.66        |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 74          |
|    mean_reward     | -8.16e+04   |
| time/              |             |
|    total_timesteps | 1380000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.1      |
|    ep_rew_mean     | -7.53e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 674       |
|    time_elapsed    | 8760      |
|    total_timesteps | 1380352   |
----------------------------------
Eval num_timesteps=1380500, episode_reward=-60757.28 +/- 30743.04
Episode length: 59.60 +/- 29.78
--------------------------------------------
| eval/                   |                |
|    mean action          | -0.6719685     |
|    mean velocity x      | 2.63           |
|    mean velocity y      | 4.66           |
|    mean velocity z      | 19.2           |
|    mean_ep_length       | 59.6           |
|    mean_reward          | -6.08e+04      |
| time/                   |                |
|    total_timesteps      | 1380500        |
| train/                  |                |
|    approx_kl            | 0.000115449104 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.93          |
|    explained_variance   | 0.31           |
|    learning_rate        | 0.001          |
|    loss                 | 1.3e+08        |
|    n_updates            | 6740           |
|    policy_gradient_loss | -0.000343      |
|    std                  | 0.897          |
|    value_loss           | 1.96e+08       |
--------------------------------------------
Eval num_timesteps=1381000, episode_reward=-78681.80 +/- 16300.19
Episode length: 74.60 +/- 24.37
------------------------------------
| eval/              |             |
|    mean action     | -0.35760263 |
|    mean velocity x | 0.347       |
|    mean velocity y | 1.15        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 74.6        |
|    mean_reward     | -7.87e+04   |
| time/              |             |
|    total_timesteps | 1381000     |
------------------------------------
Eval num_timesteps=1381500, episode_reward=-69716.44 +/- 33902.15
Episode length: 60.60 +/- 15.08
------------------------------------
| eval/              |             |
|    mean action     | -0.13411541 |
|    mean velocity x | 2.72        |
|    mean velocity y | 1.77        |
|    mean velocity z | 19.5        |
|    mean_ep_length  | 60.6        |
|    mean_reward     | -6.97e+04   |
| time/              |             |
|    total_timesteps | 1381500     |
------------------------------------
Eval num_timesteps=1382000, episode_reward=-70993.50 +/- 22749.52
Episode length: 67.00 +/- 14.48
-----------------------------------
| eval/              |            |
|    mean action     | 0.12451793 |
|    mean velocity x | -0.96      |
|    mean velocity y | -1.16      |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 67         |
|    mean_reward     | -7.1e+04   |
| time/              |            |
|    total_timesteps | 1382000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.2      |
|    ep_rew_mean     | -7.44e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 675       |
|    time_elapsed    | 8768      |
|    total_timesteps | 1382400   |
----------------------------------
Eval num_timesteps=1382500, episode_reward=-67033.95 +/- 39340.97
Episode length: 71.20 +/- 39.88
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.21340585   |
|    mean velocity x      | 0.877         |
|    mean velocity y      | 1.16          |
|    mean velocity z      | 19.9          |
|    mean_ep_length       | 71.2          |
|    mean_reward          | -6.7e+04      |
| time/                   |               |
|    total_timesteps      | 1382500       |
| train/                  |               |
|    approx_kl            | 0.00097621325 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.001         |
|    loss                 | 1.47e+08      |
|    n_updates            | 6750          |
|    policy_gradient_loss | -0.00083      |
|    std                  | 0.897         |
|    value_loss           | 1.91e+08      |
-------------------------------------------
Eval num_timesteps=1383000, episode_reward=-67579.83 +/- 36434.97
Episode length: 54.40 +/- 8.16
-----------------------------------
| eval/              |            |
|    mean action     | 0.17422068 |
|    mean velocity x | -0.693     |
|    mean velocity y | -0.105     |
|    mean velocity z | 17.1       |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -6.76e+04  |
| time/              |            |
|    total_timesteps | 1383000    |
-----------------------------------
Eval num_timesteps=1383500, episode_reward=-60250.90 +/- 26906.07
Episode length: 69.80 +/- 24.09
----------------------------------
| eval/              |           |
|    mean action     | 0.902966  |
|    mean velocity x | -2.89     |
|    mean velocity y | -4.69     |
|    mean velocity z | 17.8      |
|    mean_ep_length  | 69.8      |
|    mean_reward     | -6.03e+04 |
| time/              |           |
|    total_timesteps | 1383500   |
----------------------------------
Eval num_timesteps=1384000, episode_reward=-53047.63 +/- 44106.84
Episode length: 55.20 +/- 35.82
-----------------------------------
| eval/              |            |
|    mean action     | 0.06804265 |
|    mean velocity x | -0.43      |
|    mean velocity y | -0.0566    |
|    mean velocity z | 21.8       |
|    mean_ep_length  | 55.2       |
|    mean_reward     | -5.3e+04   |
| time/              |            |
|    total_timesteps | 1384000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.4      |
|    ep_rew_mean     | -7.32e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 676       |
|    time_elapsed    | 8775      |
|    total_timesteps | 1384448   |
----------------------------------
Eval num_timesteps=1384500, episode_reward=-75997.92 +/- 19343.25
Episode length: 60.00 +/- 2.83
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1341382     |
|    mean velocity x      | -2.37         |
|    mean velocity y      | -1.28         |
|    mean velocity z      | 17.2          |
|    mean_ep_length       | 60            |
|    mean_reward          | -7.6e+04      |
| time/                   |               |
|    total_timesteps      | 1384500       |
| train/                  |               |
|    approx_kl            | 0.00014837508 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.348         |
|    learning_rate        | 0.001         |
|    loss                 | 1.29e+08      |
|    n_updates            | 6760          |
|    policy_gradient_loss | -0.000557     |
|    std                  | 0.897         |
|    value_loss           | 1.64e+08      |
-------------------------------------------
Eval num_timesteps=1385000, episode_reward=-84289.11 +/- 20990.69
Episode length: 62.40 +/- 3.88
-----------------------------------
| eval/              |            |
|    mean action     | 0.27489954 |
|    mean velocity x | -0.734     |
|    mean velocity y | -2.49      |
|    mean velocity z | 21.9       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 1385000    |
-----------------------------------
Eval num_timesteps=1385500, episode_reward=-55242.81 +/- 38693.31
Episode length: 48.60 +/- 24.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.08854541 |
|    mean velocity x | -0.937     |
|    mean velocity y | -0.358     |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 48.6       |
|    mean_reward     | -5.52e+04  |
| time/              |            |
|    total_timesteps | 1385500    |
-----------------------------------
Eval num_timesteps=1386000, episode_reward=-82908.50 +/- 32454.87
Episode length: 68.00 +/- 24.22
------------------------------------
| eval/              |             |
|    mean action     | -0.21264075 |
|    mean velocity x | 0.0695      |
|    mean velocity y | 1.22        |
|    mean velocity z | 17          |
|    mean_ep_length  | 68          |
|    mean_reward     | -8.29e+04   |
| time/              |             |
|    total_timesteps | 1386000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.5      |
|    ep_rew_mean     | -7.31e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 677       |
|    time_elapsed    | 8782      |
|    total_timesteps | 1386496   |
----------------------------------
Eval num_timesteps=1386500, episode_reward=-47205.83 +/- 45771.98
Episode length: 41.60 +/- 23.17
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.09760587 |
|    mean velocity x      | 2.43        |
|    mean velocity y      | 0.464       |
|    mean velocity z      | 14.5        |
|    mean_ep_length       | 41.6        |
|    mean_reward          | -4.72e+04   |
| time/                   |             |
|    total_timesteps      | 1386500     |
| train/                  |             |
|    approx_kl            | 0.001397775 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.001       |
|    loss                 | 6.97e+07    |
|    n_updates            | 6770        |
|    policy_gradient_loss | -0.0017     |
|    std                  | 0.896       |
|    value_loss           | 1.75e+08    |
-----------------------------------------
Eval num_timesteps=1387000, episode_reward=-39563.51 +/- 32463.86
Episode length: 41.00 +/- 20.93
------------------------------------
| eval/              |             |
|    mean action     | -0.46937433 |
|    mean velocity x | 0.209       |
|    mean velocity y | 1.75        |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 41          |
|    mean_reward     | -3.96e+04   |
| time/              |             |
|    total_timesteps | 1387000     |
------------------------------------
Eval num_timesteps=1387500, episode_reward=-35819.75 +/- 43552.46
Episode length: 33.80 +/- 26.50
----------------------------------
| eval/              |           |
|    mean action     | 0.8235821 |
|    mean velocity x | -1.56     |
|    mean velocity y | -4.4      |
|    mean velocity z | 17        |
|    mean_ep_length  | 33.8      |
|    mean_reward     | -3.58e+04 |
| time/              |           |
|    total_timesteps | 1387500   |
----------------------------------
Eval num_timesteps=1388000, episode_reward=-82396.25 +/- 21493.71
Episode length: 66.40 +/- 19.98
------------------------------------
| eval/              |             |
|    mean action     | 0.044008117 |
|    mean velocity x | -2.65       |
|    mean velocity y | -1.77       |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 66.4        |
|    mean_reward     | -8.24e+04   |
| time/              |             |
|    total_timesteps | 1388000     |
------------------------------------
Eval num_timesteps=1388500, episode_reward=-66765.57 +/- 25943.40
Episode length: 59.40 +/- 6.34
-----------------------------------
| eval/              |            |
|    mean action     | 0.26244518 |
|    mean velocity x | -0.943     |
|    mean velocity y | -2.55      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -6.68e+04  |
| time/              |            |
|    total_timesteps | 1388500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.6      |
|    ep_rew_mean     | -7.23e+04 |
| time/              |           |
|    fps             | 157       |
|    iterations      | 678       |
|    time_elapsed    | 8790      |
|    total_timesteps | 1388544   |
----------------------------------
Eval num_timesteps=1389000, episode_reward=-91685.40 +/- 16231.51
Episode length: 63.80 +/- 1.94
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.51805824   |
|    mean velocity x      | 2.52          |
|    mean velocity y      | 3.97          |
|    mean velocity z      | 18.3          |
|    mean_ep_length       | 63.8          |
|    mean_reward          | -9.17e+04     |
| time/                   |               |
|    total_timesteps      | 1389000       |
| train/                  |               |
|    approx_kl            | 8.2688406e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+08      |
|    n_updates            | 6780          |
|    policy_gradient_loss | -0.000442     |
|    std                  | 0.896         |
|    value_loss           | 1.86e+08      |
-------------------------------------------
Eval num_timesteps=1389500, episode_reward=-74140.10 +/- 22300.82
Episode length: 73.80 +/- 27.07
------------------------------------
| eval/              |             |
|    mean action     | 0.026433082 |
|    mean velocity x | -1.28       |
|    mean velocity y | -1.21       |
|    mean velocity z | 23.4        |
|    mean_ep_length  | 73.8        |
|    mean_reward     | -7.41e+04   |
| time/              |             |
|    total_timesteps | 1389500     |
------------------------------------
Eval num_timesteps=1390000, episode_reward=-78063.07 +/- 40008.28
Episode length: 54.60 +/- 23.85
------------------------------------
| eval/              |             |
|    mean action     | -0.30143172 |
|    mean velocity x | 0.537       |
|    mean velocity y | 2.9         |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 54.6        |
|    mean_reward     | -7.81e+04   |
| time/              |             |
|    total_timesteps | 1390000     |
------------------------------------
Eval num_timesteps=1390500, episode_reward=-38071.50 +/- 31347.02
Episode length: 52.00 +/- 28.16
------------------------------------
| eval/              |             |
|    mean action     | -0.25771976 |
|    mean velocity x | 1.42        |
|    mean velocity y | 1.41        |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 52          |
|    mean_reward     | -3.81e+04   |
| time/              |             |
|    total_timesteps | 1390500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70        |
|    ep_rew_mean     | -7.82e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 679       |
|    time_elapsed    | 8797      |
|    total_timesteps | 1390592   |
----------------------------------
Eval num_timesteps=1391000, episode_reward=-78300.03 +/- 26207.38
Episode length: 57.20 +/- 6.85
------------------------------------------
| eval/                   |              |
|    mean action          | 0.044918884  |
|    mean velocity x      | 0.896        |
|    mean velocity y      | 1.02         |
|    mean velocity z      | 19           |
|    mean_ep_length       | 57.2         |
|    mean_reward          | -7.83e+04    |
| time/                   |              |
|    total_timesteps      | 1391000      |
| train/                  |              |
|    approx_kl            | 6.806338e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+08     |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.000302    |
|    std                  | 0.896        |
|    value_loss           | 2.29e+08     |
------------------------------------------
Eval num_timesteps=1391500, episode_reward=-72228.35 +/- 37741.64
Episode length: 58.00 +/- 18.25
-----------------------------------
| eval/              |            |
|    mean action     | -0.6822598 |
|    mean velocity x | 3.09       |
|    mean velocity y | 4.53       |
|    mean velocity z | 18         |
|    mean_ep_length  | 58         |
|    mean_reward     | -7.22e+04  |
| time/              |            |
|    total_timesteps | 1391500    |
-----------------------------------
Eval num_timesteps=1392000, episode_reward=-65841.26 +/- 34533.49
Episode length: 55.00 +/- 18.48
----------------------------------
| eval/              |           |
|    mean action     | 0.8629962 |
|    mean velocity x | -1.87     |
|    mean velocity y | -4.65     |
|    mean velocity z | 18        |
|    mean_ep_length  | 55        |
|    mean_reward     | -6.58e+04 |
| time/              |           |
|    total_timesteps | 1392000   |
----------------------------------
Eval num_timesteps=1392500, episode_reward=-78780.48 +/- 25048.51
Episode length: 62.60 +/- 9.05
----------------------------------
| eval/              |           |
|    mean action     | 0.4357462 |
|    mean velocity x | -1.21     |
|    mean velocity y | -2.77     |
|    mean velocity z | 21.6      |
|    mean_ep_length  | 62.6      |
|    mean_reward     | -7.88e+04 |
| time/              |           |
|    total_timesteps | 1392500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72        |
|    ep_rew_mean     | -7.93e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 680       |
|    time_elapsed    | 8804      |
|    total_timesteps | 1392640   |
----------------------------------
Eval num_timesteps=1393000, episode_reward=-81664.28 +/- 18777.51
Episode length: 70.40 +/- 14.44
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.40729365   |
|    mean velocity x      | 3.52          |
|    mean velocity y      | 3.76          |
|    mean velocity z      | 18.7          |
|    mean_ep_length       | 70.4          |
|    mean_reward          | -8.17e+04     |
| time/                   |               |
|    total_timesteps      | 1393000       |
| train/                  |               |
|    approx_kl            | 8.4044616e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+08      |
|    n_updates            | 6800          |
|    policy_gradient_loss | -0.000266     |
|    std                  | 0.896         |
|    value_loss           | 1.79e+08      |
-------------------------------------------
Eval num_timesteps=1393500, episode_reward=-79963.56 +/- 9498.83
Episode length: 59.60 +/- 4.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.11352374 |
|    mean velocity x | -0.422     |
|    mean velocity y | -1.26      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 59.6       |
|    mean_reward     | -8e+04     |
| time/              |            |
|    total_timesteps | 1393500    |
-----------------------------------
Eval num_timesteps=1394000, episode_reward=-56693.69 +/- 43721.07
Episode length: 46.40 +/- 20.64
------------------------------------
| eval/              |             |
|    mean action     | -0.48766327 |
|    mean velocity x | 1.62        |
|    mean velocity y | 3.04        |
|    mean velocity z | 18.9        |
|    mean_ep_length  | 46.4        |
|    mean_reward     | -5.67e+04   |
| time/              |             |
|    total_timesteps | 1394000     |
------------------------------------
Eval num_timesteps=1394500, episode_reward=-95235.27 +/- 10222.31
Episode length: 66.20 +/- 9.43
-----------------------------------
| eval/              |            |
|    mean action     | 0.38958886 |
|    mean velocity x | -2.42      |
|    mean velocity y | -2.52      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 66.2       |
|    mean_reward     | -9.52e+04  |
| time/              |            |
|    total_timesteps | 1394500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -8.06e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 681       |
|    time_elapsed    | 8811      |
|    total_timesteps | 1394688   |
----------------------------------
Eval num_timesteps=1395000, episode_reward=-67813.53 +/- 43302.53
Episode length: 71.80 +/- 40.29
------------------------------------------
| eval/                   |              |
|    mean action          | -0.11127284  |
|    mean velocity x      | 0.347        |
|    mean velocity y      | 1.59         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 71.8         |
|    mean_reward          | -6.78e+04    |
| time/                   |              |
|    total_timesteps      | 1395000      |
| train/                  |              |
|    approx_kl            | 8.163648e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.333        |
|    learning_rate        | 0.001        |
|    loss                 | 1.04e+08     |
|    n_updates            | 6810         |
|    policy_gradient_loss | -0.000283    |
|    std                  | 0.896        |
|    value_loss           | 1.78e+08     |
------------------------------------------
Eval num_timesteps=1395500, episode_reward=-61127.71 +/- 42942.97
Episode length: 49.00 +/- 15.63
-------------------------------------
| eval/              |              |
|    mean action     | 0.0052409973 |
|    mean velocity x | -0.136       |
|    mean velocity y | -0.311       |
|    mean velocity z | 21.2         |
|    mean_ep_length  | 49           |
|    mean_reward     | -6.11e+04    |
| time/              |              |
|    total_timesteps | 1395500      |
-------------------------------------
Eval num_timesteps=1396000, episode_reward=-84991.20 +/- 18709.85
Episode length: 68.20 +/- 16.92
------------------------------------
| eval/              |             |
|    mean action     | -0.11996871 |
|    mean velocity x | 1.64        |
|    mean velocity y | 0.386       |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 68.2        |
|    mean_reward     | -8.5e+04    |
| time/              |             |
|    total_timesteps | 1396000     |
------------------------------------
Eval num_timesteps=1396500, episode_reward=-83128.44 +/- 16702.92
Episode length: 71.80 +/- 11.25
-----------------------------------
| eval/              |            |
|    mean action     | 0.17655462 |
|    mean velocity x | -0.628     |
|    mean velocity y | -1.1       |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 71.8       |
|    mean_reward     | -8.31e+04  |
| time/              |            |
|    total_timesteps | 1396500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.6     |
|    ep_rew_mean     | -8.2e+04 |
| time/              |          |
|    fps             | 158      |
|    iterations      | 682      |
|    time_elapsed    | 8819     |
|    total_timesteps | 1396736  |
---------------------------------
Eval num_timesteps=1397000, episode_reward=-70913.80 +/- 27966.03
Episode length: 56.80 +/- 4.53
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.0057994844  |
|    mean velocity x      | 0.945         |
|    mean velocity y      | 0.334         |
|    mean velocity z      | 20            |
|    mean_ep_length       | 56.8          |
|    mean_reward          | -7.09e+04     |
| time/                   |               |
|    total_timesteps      | 1397000       |
| train/                  |               |
|    approx_kl            | 2.8340408e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.001         |
|    loss                 | 1.24e+08      |
|    n_updates            | 6820          |
|    policy_gradient_loss | -0.000387     |
|    std                  | 0.896         |
|    value_loss           | 2.46e+08      |
-------------------------------------------
Eval num_timesteps=1397500, episode_reward=-65551.98 +/- 33773.01
Episode length: 59.80 +/- 13.57
----------------------------------
| eval/              |           |
|    mean action     | 0.0836042 |
|    mean velocity x | 0.899     |
|    mean velocity y | 0.904     |
|    mean velocity z | 23.2      |
|    mean_ep_length  | 59.8      |
|    mean_reward     | -6.56e+04 |
| time/              |           |
|    total_timesteps | 1397500   |
----------------------------------
Eval num_timesteps=1398000, episode_reward=-57875.18 +/- 30300.93
Episode length: 52.60 +/- 6.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.25196898 |
|    mean velocity x | -0.917     |
|    mean velocity y | -1.21      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 52.6       |
|    mean_reward     | -5.79e+04  |
| time/              |            |
|    total_timesteps | 1398000    |
-----------------------------------
Eval num_timesteps=1398500, episode_reward=-67423.13 +/- 46907.65
Episode length: 78.60 +/- 52.71
------------------------------------
| eval/              |             |
|    mean action     | -0.52107906 |
|    mean velocity x | -0.484      |
|    mean velocity y | 0.558       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 78.6        |
|    mean_reward     | -6.74e+04   |
| time/              |             |
|    total_timesteps | 1398500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -8.34e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 683       |
|    time_elapsed    | 8826      |
|    total_timesteps | 1398784   |
----------------------------------
Eval num_timesteps=1399000, episode_reward=-72976.58 +/- 14287.81
Episode length: 59.60 +/- 5.92
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.2829973     |
|    mean velocity x      | -1.55         |
|    mean velocity y      | -2.15         |
|    mean velocity z      | 21.3          |
|    mean_ep_length       | 59.6          |
|    mean_reward          | -7.3e+04      |
| time/                   |               |
|    total_timesteps      | 1399000       |
| train/                  |               |
|    approx_kl            | 0.00015804224 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.001         |
|    loss                 | 9.56e+07      |
|    n_updates            | 6830          |
|    policy_gradient_loss | -0.00049      |
|    std                  | 0.896         |
|    value_loss           | 2.14e+08      |
-------------------------------------------
Eval num_timesteps=1399500, episode_reward=-40239.63 +/- 31895.79
Episode length: 44.80 +/- 21.58
-----------------------------------
| eval/              |            |
|    mean action     | 0.39207837 |
|    mean velocity x | -0.0474    |
|    mean velocity y | -2.52      |
|    mean velocity z | 19.9       |
|    mean_ep_length  | 44.8       |
|    mean_reward     | -4.02e+04  |
| time/              |            |
|    total_timesteps | 1399500    |
-----------------------------------
Eval num_timesteps=1400000, episode_reward=-42016.88 +/- 31251.21
Episode length: 50.80 +/- 18.02
-----------------------------------
| eval/              |            |
|    mean action     | 0.23453632 |
|    mean velocity x | -2.23      |
|    mean velocity y | -0.8       |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 50.8       |
|    mean_reward     | -4.2e+04   |
| time/              |            |
|    total_timesteps | 1400000    |
-----------------------------------
Eval num_timesteps=1400500, episode_reward=-70596.37 +/- 39162.78
Episode length: 59.40 +/- 18.71
------------------------------------
| eval/              |             |
|    mean action     | -0.31065083 |
|    mean velocity x | 1.16        |
|    mean velocity y | 1.59        |
|    mean velocity z | 20.4        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -7.06e+04   |
| time/              |             |
|    total_timesteps | 1400500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.6      |
|    ep_rew_mean     | -7.91e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 684       |
|    time_elapsed    | 8833      |
|    total_timesteps | 1400832   |
----------------------------------
Eval num_timesteps=1401000, episode_reward=-72031.05 +/- 35464.64
Episode length: 67.00 +/- 22.90
------------------------------------------
| eval/                   |              |
|    mean action          | 0.21430688   |
|    mean velocity x      | -0.261       |
|    mean velocity y      | -0.548       |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 67           |
|    mean_reward          | -7.2e+04     |
| time/                   |              |
|    total_timesteps      | 1401000      |
| train/                  |              |
|    approx_kl            | 7.778639e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+08     |
|    n_updates            | 6840         |
|    policy_gradient_loss | -0.000376    |
|    std                  | 0.896        |
|    value_loss           | 2.21e+08     |
------------------------------------------
Eval num_timesteps=1401500, episode_reward=-72659.77 +/- 37101.42
Episode length: 56.60 +/- 14.85
-----------------------------------
| eval/              |            |
|    mean action     | 0.24524882 |
|    mean velocity x | -1.12      |
|    mean velocity y | -2.28      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -7.27e+04  |
| time/              |            |
|    total_timesteps | 1401500    |
-----------------------------------
Eval num_timesteps=1402000, episode_reward=-69733.59 +/- 13983.01
Episode length: 62.40 +/- 4.72
----------------------------------
| eval/              |           |
|    mean action     | 0.5716957 |
|    mean velocity x | -1.35     |
|    mean velocity y | -3.13     |
|    mean velocity z | 16.7      |
|    mean_ep_length  | 62.4      |
|    mean_reward     | -6.97e+04 |
| time/              |           |
|    total_timesteps | 1402000   |
----------------------------------
Eval num_timesteps=1402500, episode_reward=-26101.41 +/- 20096.06
Episode length: 40.20 +/- 15.38
------------------------------------
| eval/              |             |
|    mean action     | 0.009946335 |
|    mean velocity x | -1.13       |
|    mean velocity y | -1.58       |
|    mean velocity z | 19.8        |
|    mean_ep_length  | 40.2        |
|    mean_reward     | -2.61e+04   |
| time/              |             |
|    total_timesteps | 1402500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67        |
|    ep_rew_mean     | -7.86e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 685       |
|    time_elapsed    | 8841      |
|    total_timesteps | 1402880   |
----------------------------------
Eval num_timesteps=1403000, episode_reward=-79064.13 +/- 41977.76
Episode length: 57.60 +/- 25.04
------------------------------------------
| eval/                   |              |
|    mean action          | 0.1646621    |
|    mean velocity x      | -0.625       |
|    mean velocity y      | -1.47        |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 57.6         |
|    mean_reward          | -7.91e+04    |
| time/                   |              |
|    total_timesteps      | 1403000      |
| train/                  |              |
|    approx_kl            | 0.0015425056 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.001        |
|    loss                 | 7.22e+07     |
|    n_updates            | 6850         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 0.896        |
|    value_loss           | 1.63e+08     |
------------------------------------------
Eval num_timesteps=1403500, episode_reward=-70486.31 +/- 28657.08
Episode length: 61.40 +/- 14.68
------------------------------------
| eval/              |             |
|    mean action     | -0.10056992 |
|    mean velocity x | -0.852      |
|    mean velocity y | 0.144       |
|    mean velocity z | 17.2        |
|    mean_ep_length  | 61.4        |
|    mean_reward     | -7.05e+04   |
| time/              |             |
|    total_timesteps | 1403500     |
------------------------------------
Eval num_timesteps=1404000, episode_reward=-45032.75 +/- 43505.10
Episode length: 44.20 +/- 15.95
----------------------------------
| eval/              |           |
|    mean action     | 0.2899914 |
|    mean velocity x | -0.335    |
|    mean velocity y | -2.09     |
|    mean velocity z | 19        |
|    mean_ep_length  | 44.2      |
|    mean_reward     | -4.5e+04  |
| time/              |           |
|    total_timesteps | 1404000   |
----------------------------------
Eval num_timesteps=1404500, episode_reward=-56039.30 +/- 26120.99
Episode length: 59.40 +/- 11.31
------------------------------------
| eval/              |             |
|    mean action     | -0.21807723 |
|    mean velocity x | 1.07        |
|    mean velocity y | 2.02        |
|    mean velocity z | 21.8        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -5.6e+04    |
| time/              |             |
|    total_timesteps | 1404500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.1      |
|    ep_rew_mean     | -7.54e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 686       |
|    time_elapsed    | 8848      |
|    total_timesteps | 1404928   |
----------------------------------
Eval num_timesteps=1405000, episode_reward=-79690.56 +/- 22730.50
Episode length: 66.60 +/- 17.86
------------------------------------------
| eval/                   |              |
|    mean action          | 0.05038238   |
|    mean velocity x      | -1.1         |
|    mean velocity y      | 1.02         |
|    mean velocity z      | 18.2         |
|    mean_ep_length       | 66.6         |
|    mean_reward          | -7.97e+04    |
| time/                   |              |
|    total_timesteps      | 1405000      |
| train/                  |              |
|    approx_kl            | 7.179333e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.001        |
|    loss                 | 7.33e+07     |
|    n_updates            | 6860         |
|    policy_gradient_loss | -0.000154    |
|    std                  | 0.896        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=1405500, episode_reward=-46972.74 +/- 26324.79
Episode length: 64.40 +/- 21.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.42551807 |
|    mean velocity x | -1.93      |
|    mean velocity y | -2.51      |
|    mean velocity z | 18.7       |
|    mean_ep_length  | 64.4       |
|    mean_reward     | -4.7e+04   |
| time/              |            |
|    total_timesteps | 1405500    |
-----------------------------------
Eval num_timesteps=1406000, episode_reward=-84339.32 +/- 21847.99
Episode length: 61.00 +/- 6.23
-----------------------------------
| eval/              |            |
|    mean action     | -1.0202217 |
|    mean velocity x | 3.58       |
|    mean velocity y | 5.76       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 61         |
|    mean_reward     | -8.43e+04  |
| time/              |            |
|    total_timesteps | 1406000    |
-----------------------------------
Eval num_timesteps=1406500, episode_reward=-90752.31 +/- 21204.79
Episode length: 64.80 +/- 10.55
-----------------------------------
| eval/              |            |
|    mean action     | 0.23505728 |
|    mean velocity x | 1.25       |
|    mean velocity y | -0.569     |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 64.8       |
|    mean_reward     | -9.08e+04  |
| time/              |            |
|    total_timesteps | 1406500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -7.7e+04 |
| time/              |          |
|    fps             | 158      |
|    iterations      | 687      |
|    time_elapsed    | 8855     |
|    total_timesteps | 1406976  |
---------------------------------
Eval num_timesteps=1407000, episode_reward=-60071.74 +/- 33608.68
Episode length: 64.60 +/- 29.57
-------------------------------------------
| eval/                   |               |
|    mean action          | 1.3455898     |
|    mean velocity x      | -4.39         |
|    mean velocity y      | -7.95         |
|    mean velocity z      | 17.3          |
|    mean_ep_length       | 64.6          |
|    mean_reward          | -6.01e+04     |
| time/                   |               |
|    total_timesteps      | 1407000       |
| train/                  |               |
|    approx_kl            | 5.9147715e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.313         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+08      |
|    n_updates            | 6870          |
|    policy_gradient_loss | -9.3e-05      |
|    std                  | 0.896         |
|    value_loss           | 1.78e+08      |
-------------------------------------------
Eval num_timesteps=1407500, episode_reward=-99237.53 +/- 11337.59
Episode length: 61.80 +/- 1.17
------------------------------------
| eval/              |             |
|    mean action     | -0.42110777 |
|    mean velocity x | 1.83        |
|    mean velocity y | 3.44        |
|    mean velocity z | 19          |
|    mean_ep_length  | 61.8        |
|    mean_reward     | -9.92e+04   |
| time/              |             |
|    total_timesteps | 1407500     |
------------------------------------
Eval num_timesteps=1408000, episode_reward=-50978.93 +/- 37011.52
Episode length: 62.00 +/- 34.31
------------------------------------
| eval/              |             |
|    mean action     | -0.48325568 |
|    mean velocity x | 2.75        |
|    mean velocity y | 3.21        |
|    mean velocity z | 17.7        |
|    mean_ep_length  | 62          |
|    mean_reward     | -5.1e+04    |
| time/              |             |
|    total_timesteps | 1408000     |
------------------------------------
Eval num_timesteps=1408500, episode_reward=-88126.25 +/- 17679.90
Episode length: 67.40 +/- 9.46
------------------------------------
| eval/              |             |
|    mean action     | -0.07927628 |
|    mean velocity x | 0.345       |
|    mean velocity y | 0.775       |
|    mean velocity z | 21.4        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -8.81e+04   |
| time/              |             |
|    total_timesteps | 1408500     |
------------------------------------
Eval num_timesteps=1409000, episode_reward=-58830.40 +/- 37013.18
Episode length: 51.20 +/- 16.24
-------------------------------------
| eval/              |              |
|    mean action     | 0.0016194611 |
|    mean velocity x | 0.422        |
|    mean velocity y | -0.389       |
|    mean velocity z | 22.2         |
|    mean_ep_length  | 51.2         |
|    mean_reward     | -5.88e+04    |
| time/              |              |
|    total_timesteps | 1409000      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 72.7      |
|    ep_rew_mean     | -8.11e+04 |
| time/              |           |
|    fps             | 158       |
|    iterations      | 688       |
|    time_elapsed    | 8863      |
|    total_timesteps | 1409024   |
----------------------------------
Eval num_timesteps=1409500, episode_reward=-55206.92 +/- 20716.18
Episode length: 54.60 +/- 9.26
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.13143691   |
|    mean velocity x      | 1.33          |
|    mean velocity y      | 1.08          |
|    mean velocity z      | 19.6          |
|    mean_ep_length       | 54.6          |
|    mean_reward          | -5.52e+04     |
| time/                   |               |
|    total_timesteps      | 1409500       |
| train/                  |               |
|    approx_kl            | 0.00013990887 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.001         |
|    loss                 | 4.99e+07      |
|    n_updates            | 6880          |
|    policy_gradient_loss | -0.000579     |
|    std                  | 0.896         |
|    value_loss           | 2.35e+08      |
-------------------------------------------
Eval num_timesteps=1410000, episode_reward=-33429.22 +/- 25842.30
Episode length: 39.60 +/- 13.48
-----------------------------------
| eval/              |            |
|    mean action     | -0.5434946 |
|    mean velocity x | 1.6        |
|    mean velocity y | 3.7        |
|    mean velocity z | 18.8       |
|    mean_ep_length  | 39.6       |
|    mean_reward     | -3.34e+04  |
| time/              |            |
|    total_timesteps | 1410000    |
-----------------------------------
Eval num_timesteps=1410500, episode_reward=-52603.02 +/- 42220.57
Episode length: 45.00 +/- 20.10
-----------------------------------
| eval/              |            |
|    mean action     | 0.13499977 |
|    mean velocity x | 1.19       |
|    mean velocity y | -0.414     |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 45         |
|    mean_reward     | -5.26e+04  |
| time/              |            |
|    total_timesteps | 1410500    |
-----------------------------------
Eval num_timesteps=1411000, episode_reward=-61733.42 +/- 16293.21
Episode length: 64.20 +/- 8.30
-----------------------------------
| eval/              |            |
|    mean action     | 0.69546115 |
|    mean velocity x | -3.97      |
|    mean velocity y | -4.19      |
|    mean velocity z | 17         |
|    mean_ep_length  | 64.2       |
|    mean_reward     | -6.17e+04  |
| time/              |            |
|    total_timesteps | 1411000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.6      |
|    ep_rew_mean     | -7.76e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 689       |
|    time_elapsed    | 8870      |
|    total_timesteps | 1411072   |
----------------------------------
Eval num_timesteps=1411500, episode_reward=-46913.50 +/- 16064.23
Episode length: 59.20 +/- 19.33
------------------------------------------
| eval/                   |              |
|    mean action          | -0.24042349  |
|    mean velocity x      | 3.21         |
|    mean velocity y      | 2.62         |
|    mean velocity z      | 15.5         |
|    mean_ep_length       | 59.2         |
|    mean_reward          | -4.69e+04    |
| time/                   |              |
|    total_timesteps      | 1411500      |
| train/                  |              |
|    approx_kl            | 5.757177e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.001        |
|    loss                 | 7.86e+07     |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.000313    |
|    std                  | 0.896        |
|    value_loss           | 1.44e+08     |
------------------------------------------
Eval num_timesteps=1412000, episode_reward=-73186.99 +/- 22080.72
Episode length: 64.60 +/- 9.26
------------------------------------
| eval/              |             |
|    mean action     | -0.29217362 |
|    mean velocity x | 1.84        |
|    mean velocity y | 1.7         |
|    mean velocity z | 19          |
|    mean_ep_length  | 64.6        |
|    mean_reward     | -7.32e+04   |
| time/              |             |
|    total_timesteps | 1412000     |
------------------------------------
Eval num_timesteps=1412500, episode_reward=-48504.70 +/- 23391.70
Episode length: 62.00 +/- 26.05
------------------------------------
| eval/              |             |
|    mean action     | -0.56758624 |
|    mean velocity x | 1.13        |
|    mean velocity y | 3.41        |
|    mean velocity z | 21.9        |
|    mean_ep_length  | 62          |
|    mean_reward     | -4.85e+04   |
| time/              |             |
|    total_timesteps | 1412500     |
------------------------------------
Eval num_timesteps=1413000, episode_reward=-52147.73 +/- 35761.04
Episode length: 49.80 +/- 16.42
-----------------------------------
| eval/              |            |
|    mean action     | -0.3934217 |
|    mean velocity x | 1.32       |
|    mean velocity y | 2.93       |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 49.8       |
|    mean_reward     | -5.21e+04  |
| time/              |            |
|    total_timesteps | 1413000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.9      |
|    ep_rew_mean     | -7.14e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 690       |
|    time_elapsed    | 8877      |
|    total_timesteps | 1413120   |
----------------------------------
Eval num_timesteps=1413500, episode_reward=-67012.91 +/- 37638.36
Episode length: 54.80 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.44040784    |
|    mean velocity x      | -1.72         |
|    mean velocity y      | -3.52         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 54.8          |
|    mean_reward          | -6.7e+04      |
| time/                   |               |
|    total_timesteps      | 1413500       |
| train/                  |               |
|    approx_kl            | 0.00047583744 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.93         |
|    explained_variance   | 0.348         |
|    learning_rate        | 0.001         |
|    loss                 | 1.43e+08      |
|    n_updates            | 6900          |
|    policy_gradient_loss | -0.000841     |
|    std                  | 0.895         |
|    value_loss           | 1.89e+08      |
-------------------------------------------
Eval num_timesteps=1414000, episode_reward=-67714.08 +/- 37508.63
Episode length: 56.20 +/- 21.52
-------------------------------------
| eval/              |              |
|    mean action     | -0.020569725 |
|    mean velocity x | -1.66        |
|    mean velocity y | -0.611       |
|    mean velocity z | 20.8         |
|    mean_ep_length  | 56.2         |
|    mean_reward     | -6.77e+04    |
| time/              |              |
|    total_timesteps | 1414000      |
-------------------------------------
Eval num_timesteps=1414500, episode_reward=-61461.51 +/- 46724.90
Episode length: 61.80 +/- 34.01
-----------------------------------
| eval/              |            |
|    mean action     | 0.32817328 |
|    mean velocity x | -1.4       |
|    mean velocity y | -2.04      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 61.8       |
|    mean_reward     | -6.15e+04  |
| time/              |            |
|    total_timesteps | 1414500    |
-----------------------------------
Eval num_timesteps=1415000, episode_reward=-50033.18 +/- 31739.26
Episode length: 57.20 +/- 23.41
----------------------------------
| eval/              |           |
|    mean action     | 0.5021948 |
|    mean velocity x | -3.3      |
|    mean velocity y | -4.45     |
|    mean velocity z | 17.8      |
|    mean_ep_length  | 57.2      |
|    mean_reward     | -5e+04    |
| time/              |           |
|    total_timesteps | 1415000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 64        |
|    ep_rew_mean     | -7.11e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 691       |
|    time_elapsed    | 8884      |
|    total_timesteps | 1415168   |
----------------------------------
Eval num_timesteps=1415500, episode_reward=-75178.17 +/- 21569.72
Episode length: 62.20 +/- 9.72
------------------------------------------
| eval/                   |              |
|    mean action          | -0.1971532   |
|    mean velocity x      | 0.986        |
|    mean velocity y      | 0.998        |
|    mean velocity z      | 19.7         |
|    mean_ep_length       | 62.2         |
|    mean_reward          | -7.52e+04    |
| time/                   |              |
|    total_timesteps      | 1415500      |
| train/                  |              |
|    approx_kl            | 8.378003e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.001        |
|    loss                 | 1.44e+08     |
|    n_updates            | 6910         |
|    policy_gradient_loss | -1.12e-05    |
|    std                  | 0.895        |
|    value_loss           | 1.99e+08     |
------------------------------------------
Eval num_timesteps=1416000, episode_reward=-65950.80 +/- 33483.58
Episode length: 52.20 +/- 15.64
-----------------------------------
| eval/              |            |
|    mean action     | 0.21974884 |
|    mean velocity x | -1.48      |
|    mean velocity y | -1.72      |
|    mean velocity z | 19.6       |
|    mean_ep_length  | 52.2       |
|    mean_reward     | -6.6e+04   |
| time/              |            |
|    total_timesteps | 1416000    |
-----------------------------------
Eval num_timesteps=1416500, episode_reward=-72815.86 +/- 20021.42
Episode length: 67.80 +/- 10.53
-----------------------------------
| eval/              |            |
|    mean action     | 0.48592168 |
|    mean velocity x | -1.29      |
|    mean velocity y | -3.15      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 67.8       |
|    mean_reward     | -7.28e+04  |
| time/              |            |
|    total_timesteps | 1416500    |
-----------------------------------
Eval num_timesteps=1417000, episode_reward=-58930.71 +/- 40820.52
Episode length: 58.40 +/- 21.95
----------------------------------
| eval/              |           |
|    mean action     | 0.6054121 |
|    mean velocity x | -3.67     |
|    mean velocity y | -4.92     |
|    mean velocity z | 20        |
|    mean_ep_length  | 58.4      |
|    mean_reward     | -5.89e+04 |
| time/              |           |
|    total_timesteps | 1417000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.6      |
|    ep_rew_mean     | -7.45e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 692       |
|    time_elapsed    | 8892      |
|    total_timesteps | 1417216   |
----------------------------------
Eval num_timesteps=1417500, episode_reward=-49435.06 +/- 32503.36
Episode length: 54.80 +/- 18.95
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.0030204772 |
|    mean velocity x      | -0.941        |
|    mean velocity y      | -1.68         |
|    mean velocity z      | 16.8          |
|    mean_ep_length       | 54.8          |
|    mean_reward          | -4.94e+04     |
| time/                   |               |
|    total_timesteps      | 1417500       |
| train/                  |               |
|    approx_kl            | 0.00017815764 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.329         |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+08      |
|    n_updates            | 6920          |
|    policy_gradient_loss | -0.000455     |
|    std                  | 0.895         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=1418000, episode_reward=-74284.32 +/- 8895.56
Episode length: 63.00 +/- 12.33
-----------------------------------
| eval/              |            |
|    mean action     | 0.12721792 |
|    mean velocity x | 0.238      |
|    mean velocity y | -0.477     |
|    mean velocity z | 22.5       |
|    mean_ep_length  | 63         |
|    mean_reward     | -7.43e+04  |
| time/              |            |
|    total_timesteps | 1418000    |
-----------------------------------
Eval num_timesteps=1418500, episode_reward=-59638.56 +/- 38725.34
Episode length: 53.40 +/- 17.90
-----------------------------------
| eval/              |            |
|    mean action     | 0.18589352 |
|    mean velocity x | -1.94      |
|    mean velocity y | -1.93      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 53.4       |
|    mean_reward     | -5.96e+04  |
| time/              |            |
|    total_timesteps | 1418500    |
-----------------------------------
Eval num_timesteps=1419000, episode_reward=-76609.13 +/- 37099.02
Episode length: 55.40 +/- 16.81
------------------------------------
| eval/              |             |
|    mean action     | -0.08456784 |
|    mean velocity x | 1.33        |
|    mean velocity y | 0.841       |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 55.4        |
|    mean_reward     | -7.66e+04   |
| time/              |             |
|    total_timesteps | 1419000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.5      |
|    ep_rew_mean     | -7.91e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 693       |
|    time_elapsed    | 8899      |
|    total_timesteps | 1419264   |
----------------------------------
Eval num_timesteps=1419500, episode_reward=-43422.96 +/- 35976.99
Episode length: 44.80 +/- 14.39
------------------------------------------
| eval/                   |              |
|    mean action          | -0.35733703  |
|    mean velocity x      | 0.62         |
|    mean velocity y      | 2.22         |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 44.8         |
|    mean_reward          | -4.34e+04    |
| time/                   |              |
|    total_timesteps      | 1419500      |
| train/                  |              |
|    approx_kl            | 0.0006089258 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.001        |
|    loss                 | 1.38e+08     |
|    n_updates            | 6930         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.895        |
|    value_loss           | 1.95e+08     |
------------------------------------------
Eval num_timesteps=1420000, episode_reward=-48287.98 +/- 25486.61
Episode length: 56.60 +/- 11.55
----------------------------------
| eval/              |           |
|    mean action     | 0.512054  |
|    mean velocity x | -3.6      |
|    mean velocity y | -4.81     |
|    mean velocity z | 18.6      |
|    mean_ep_length  | 56.6      |
|    mean_reward     | -4.83e+04 |
| time/              |           |
|    total_timesteps | 1420000   |
----------------------------------
Eval num_timesteps=1420500, episode_reward=-81293.48 +/- 19582.86
Episode length: 73.20 +/- 16.94
------------------------------------
| eval/              |             |
|    mean action     | -0.31676564 |
|    mean velocity x | 1.88        |
|    mean velocity y | 2.7         |
|    mean velocity z | 22          |
|    mean_ep_length  | 73.2        |
|    mean_reward     | -8.13e+04   |
| time/              |             |
|    total_timesteps | 1420500     |
------------------------------------
Eval num_timesteps=1421000, episode_reward=-54935.83 +/- 36387.51
Episode length: 50.60 +/- 11.22
-----------------------------------
| eval/              |            |
|    mean action     | 0.32101962 |
|    mean velocity x | -0.292     |
|    mean velocity y | -2.42      |
|    mean velocity z | 20.3       |
|    mean_ep_length  | 50.6       |
|    mean_reward     | -5.49e+04  |
| time/              |            |
|    total_timesteps | 1421000    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.5     |
|    ep_rew_mean     | -8.1e+04 |
| time/              |          |
|    fps             | 159      |
|    iterations      | 694      |
|    time_elapsed    | 8906     |
|    total_timesteps | 1421312  |
---------------------------------
Eval num_timesteps=1421500, episode_reward=-55168.38 +/- 34829.34
Episode length: 53.20 +/- 19.34
------------------------------------------
| eval/                   |              |
|    mean action          | 0.094115034  |
|    mean velocity x      | 0.422        |
|    mean velocity y      | -0.799       |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 53.2         |
|    mean_reward          | -5.52e+04    |
| time/                   |              |
|    total_timesteps      | 1421500      |
| train/                  |              |
|    approx_kl            | 0.0001078386 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.001        |
|    loss                 | 9.16e+07     |
|    n_updates            | 6940         |
|    policy_gradient_loss | -0.000573    |
|    std                  | 0.895        |
|    value_loss           | 2.15e+08     |
------------------------------------------
Eval num_timesteps=1422000, episode_reward=-62265.03 +/- 40081.88
Episode length: 50.40 +/- 13.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.39935476 |
|    mean velocity x | -1.58      |
|    mean velocity y | -2.79      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 50.4       |
|    mean_reward     | -6.23e+04  |
| time/              |            |
|    total_timesteps | 1422000    |
-----------------------------------
Eval num_timesteps=1422500, episode_reward=-93120.64 +/- 18090.86
Episode length: 73.40 +/- 18.85
----------------------------------
| eval/              |           |
|    mean action     | 0.4844804 |
|    mean velocity x | -2.17     |
|    mean velocity y | -2.81     |
|    mean velocity z | 18.9      |
|    mean_ep_length  | 73.4      |
|    mean_reward     | -9.31e+04 |
| time/              |           |
|    total_timesteps | 1422500   |
----------------------------------
Eval num_timesteps=1423000, episode_reward=-56120.00 +/- 47887.30
Episode length: 43.60 +/- 19.83
------------------------------------
| eval/              |             |
|    mean action     | -0.16598141 |
|    mean velocity x | 0.495       |
|    mean velocity y | 1.18        |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 43.6        |
|    mean_reward     | -5.61e+04   |
| time/              |             |
|    total_timesteps | 1423000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.9      |
|    ep_rew_mean     | -8.15e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 695       |
|    time_elapsed    | 8913      |
|    total_timesteps | 1423360   |
----------------------------------
Eval num_timesteps=1423500, episode_reward=-57002.93 +/- 35164.92
Episode length: 53.60 +/- 12.66
------------------------------------------
| eval/                   |              |
|    mean action          | -0.37849396  |
|    mean velocity x      | 2.49         |
|    mean velocity y      | 3            |
|    mean velocity z      | 20.8         |
|    mean_ep_length       | 53.6         |
|    mean_reward          | -5.7e+04     |
| time/                   |              |
|    total_timesteps      | 1423500      |
| train/                  |              |
|    approx_kl            | 0.0021525542 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.001        |
|    loss                 | 1.03e+08     |
|    n_updates            | 6950         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.895        |
|    value_loss           | 1.96e+08     |
------------------------------------------
Eval num_timesteps=1424000, episode_reward=-54548.14 +/- 29523.00
Episode length: 53.20 +/- 18.14
-----------------------------------
| eval/              |            |
|    mean action     | 0.21726406 |
|    mean velocity x | -0.42      |
|    mean velocity y | -1.54      |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 53.2       |
|    mean_reward     | -5.45e+04  |
| time/              |            |
|    total_timesteps | 1424000    |
-----------------------------------
Eval num_timesteps=1424500, episode_reward=-78874.67 +/- 41862.77
Episode length: 52.20 +/- 21.60
-----------------------------------
| eval/              |            |
|    mean action     | 0.61632884 |
|    mean velocity x | -2.11      |
|    mean velocity y | -2.77      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 52.2       |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 1424500    |
-----------------------------------
Eval num_timesteps=1425000, episode_reward=-68144.05 +/- 37776.33
Episode length: 58.80 +/- 10.38
------------------------------------
| eval/              |             |
|    mean action     | -0.60237634 |
|    mean velocity x | 3.3         |
|    mean velocity y | 5.07        |
|    mean velocity z | 18.2        |
|    mean_ep_length  | 58.8        |
|    mean_reward     | -6.81e+04   |
| time/              |             |
|    total_timesteps | 1425000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.5      |
|    ep_rew_mean     | -7.96e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 696       |
|    time_elapsed    | 8920      |
|    total_timesteps | 1425408   |
----------------------------------
Eval num_timesteps=1425500, episode_reward=-54650.53 +/- 34425.59
Episode length: 55.40 +/- 10.46
------------------------------------------
| eval/                   |              |
|    mean action          | -0.098600075 |
|    mean velocity x      | 0.227        |
|    mean velocity y      | 0.497        |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 55.4         |
|    mean_reward          | -5.47e+04    |
| time/                   |              |
|    total_timesteps      | 1425500      |
| train/                  |              |
|    approx_kl            | 0.0002633921 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.364        |
|    learning_rate        | 0.001        |
|    loss                 | 6.35e+07     |
|    n_updates            | 6960         |
|    policy_gradient_loss | -0.000722    |
|    std                  | 0.895        |
|    value_loss           | 1.75e+08     |
------------------------------------------
Eval num_timesteps=1426000, episode_reward=-66214.20 +/- 26080.95
Episode length: 59.00 +/- 18.09
----------------------------------
| eval/              |           |
|    mean action     | 0.3051801 |
|    mean velocity x | -1.43     |
|    mean velocity y | -2.19     |
|    mean velocity z | 18.4      |
|    mean_ep_length  | 59        |
|    mean_reward     | -6.62e+04 |
| time/              |           |
|    total_timesteps | 1426000   |
----------------------------------
Eval num_timesteps=1426500, episode_reward=-51235.43 +/- 26922.03
Episode length: 54.60 +/- 17.72
-----------------------------------
| eval/              |            |
|    mean action     | 0.23533313 |
|    mean velocity x | -0.543     |
|    mean velocity y | -0.655     |
|    mean velocity z | 19.4       |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -5.12e+04  |
| time/              |            |
|    total_timesteps | 1426500    |
-----------------------------------
Eval num_timesteps=1427000, episode_reward=-67914.06 +/- 44734.95
Episode length: 49.00 +/- 17.56
-----------------------------------
| eval/              |            |
|    mean action     | 0.47969982 |
|    mean velocity x | -1.11      |
|    mean velocity y | -2.03      |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 49         |
|    mean_reward     | -6.79e+04  |
| time/              |            |
|    total_timesteps | 1427000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.5      |
|    ep_rew_mean     | -7.67e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 697       |
|    time_elapsed    | 8928      |
|    total_timesteps | 1427456   |
----------------------------------
Eval num_timesteps=1427500, episode_reward=-74986.92 +/- 29671.57
Episode length: 70.80 +/- 28.99
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.124838315  |
|    mean velocity x      | 2.26          |
|    mean velocity y      | 1.33          |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 70.8          |
|    mean_reward          | -7.5e+04      |
| time/                   |               |
|    total_timesteps      | 1427500       |
| train/                  |               |
|    approx_kl            | 0.00021450987 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.366         |
|    learning_rate        | 0.001         |
|    loss                 | 8.28e+07      |
|    n_updates            | 6970          |
|    policy_gradient_loss | -0.00113      |
|    std                  | 0.894         |
|    value_loss           | 1.84e+08      |
-------------------------------------------
Eval num_timesteps=1428000, episode_reward=-75873.82 +/- 12719.13
Episode length: 80.00 +/- 21.14
------------------------------------
| eval/              |             |
|    mean action     | -0.10716907 |
|    mean velocity x | -1.12       |
|    mean velocity y | -0.0124     |
|    mean velocity z | 18.1        |
|    mean_ep_length  | 80          |
|    mean_reward     | -7.59e+04   |
| time/              |             |
|    total_timesteps | 1428000     |
------------------------------------
Eval num_timesteps=1428500, episode_reward=-79488.27 +/- 16874.15
Episode length: 63.80 +/- 7.08
------------------------------------
| eval/              |             |
|    mean action     | 0.055420656 |
|    mean velocity x | -2.03       |
|    mean velocity y | -1.35       |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 63.8        |
|    mean_reward     | -7.95e+04   |
| time/              |             |
|    total_timesteps | 1428500     |
------------------------------------
Eval num_timesteps=1429000, episode_reward=-56660.18 +/- 30886.29
Episode length: 54.00 +/- 22.18
-----------------------------------
| eval/              |            |
|    mean action     | 0.41047978 |
|    mean velocity x | -1.12      |
|    mean velocity y | -3.66      |
|    mean velocity z | 19.8       |
|    mean_ep_length  | 54         |
|    mean_reward     | -5.67e+04  |
| time/              |            |
|    total_timesteps | 1429000    |
-----------------------------------
Eval num_timesteps=1429500, episode_reward=-75110.03 +/- 41544.10
Episode length: 55.20 +/- 13.24
------------------------------------
| eval/              |             |
|    mean action     | -0.48319146 |
|    mean velocity x | 1           |
|    mean velocity y | 2.54        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 55.2        |
|    mean_reward     | -7.51e+04   |
| time/              |             |
|    total_timesteps | 1429500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.8      |
|    ep_rew_mean     | -7.57e+04 |
| time/              |           |
|    fps             | 159       |
|    iterations      | 698       |
|    time_elapsed    | 8936      |
|    total_timesteps | 1429504   |
----------------------------------
Eval num_timesteps=1430000, episode_reward=-81403.76 +/- 17009.31
Episode length: 75.20 +/- 13.67
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.4655274     |
|    mean velocity x      | -0.908        |
|    mean velocity y      | -2.8          |
|    mean velocity z      | 21.1          |
|    mean_ep_length       | 75.2          |
|    mean_reward          | -8.14e+04     |
| time/                   |               |
|    total_timesteps      | 1430000       |
| train/                  |               |
|    approx_kl            | 3.7457474e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.001         |
|    loss                 | 8.87e+07      |
|    n_updates            | 6980          |
|    policy_gradient_loss | -0.000204     |
|    std                  | 0.894         |
|    value_loss           | 1.81e+08      |
-------------------------------------------
Eval num_timesteps=1430500, episode_reward=-80958.76 +/- 35691.60
Episode length: 95.60 +/- 39.46
------------------------------------
| eval/              |             |
|    mean action     | 0.035190888 |
|    mean velocity x | -0.818      |
|    mean velocity y | -1.43       |
|    mean velocity z | 16.9        |
|    mean_ep_length  | 95.6        |
|    mean_reward     | -8.1e+04    |
| time/              |             |
|    total_timesteps | 1430500     |
------------------------------------
Eval num_timesteps=1431000, episode_reward=-103712.03 +/- 6080.58
Episode length: 64.20 +/- 2.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.34460452 |
|    mean velocity x | -2.26      |
|    mean velocity y | -3.48      |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 64.2       |
|    mean_reward     | -1.04e+05  |
| time/              |            |
|    total_timesteps | 1431000    |
-----------------------------------
Eval num_timesteps=1431500, episode_reward=-72333.71 +/- 16035.26
Episode length: 62.80 +/- 7.55
------------------------------------
| eval/              |             |
|    mean action     | -0.43380943 |
|    mean velocity x | 2.48        |
|    mean velocity y | 2.4         |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 62.8        |
|    mean_reward     | -7.23e+04   |
| time/              |             |
|    total_timesteps | 1431500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.2      |
|    ep_rew_mean     | -7.46e+04 |
| time/              |           |
|    fps             | 160       |
|    iterations      | 699       |
|    time_elapsed    | 8943      |
|    total_timesteps | 1431552   |
----------------------------------
Eval num_timesteps=1432000, episode_reward=-44917.39 +/- 41130.04
Episode length: 46.60 +/- 22.95
-----------------------------------------
| eval/                   |             |
|    mean action          | -0.12328709 |
|    mean velocity x      | 0.0185      |
|    mean velocity y      | 0.00651     |
|    mean velocity z      | 22.5        |
|    mean_ep_length       | 46.6        |
|    mean_reward          | -4.49e+04   |
| time/                   |             |
|    total_timesteps      | 1432000     |
| train/                  |             |
|    approx_kl            | 0.00149816  |
|    clip_fraction        | 0.000635    |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.001       |
|    loss                 | 7.88e+07    |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.000728   |
|    std                  | 0.895       |
|    value_loss           | 1.94e+08    |
-----------------------------------------
Eval num_timesteps=1432500, episode_reward=-44465.24 +/- 24285.16
Episode length: 55.80 +/- 12.73
------------------------------------
| eval/              |             |
|    mean action     | -0.39726946 |
|    mean velocity x | 1.88        |
|    mean velocity y | 1.16        |
|    mean velocity z | 15.9        |
|    mean_ep_length  | 55.8        |
|    mean_reward     | -4.45e+04   |
| time/              |             |
|    total_timesteps | 1432500     |
------------------------------------
Eval num_timesteps=1433000, episode_reward=-86807.53 +/- 9242.22
Episode length: 62.40 +/- 5.43
-----------------------------------
| eval/              |            |
|    mean action     | -0.5538765 |
|    mean velocity x | 0.754      |
|    mean velocity y | 4.42       |
|    mean velocity z | 18.2       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -8.68e+04  |
| time/              |            |
|    total_timesteps | 1433000    |
-----------------------------------
Eval num_timesteps=1433500, episode_reward=-40988.83 +/- 22690.06
Episode length: 50.00 +/- 16.02
-----------------------------------
| eval/              |            |
|    mean action     | -0.6271937 |
|    mean velocity x | 2.26       |
|    mean velocity y | 2.7        |
|    mean velocity z | 18.5       |
|    mean_ep_length  | 50         |
|    mean_reward     | -4.1e+04   |
| time/              |            |
|    total_timesteps | 1433500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64.9     |
|    ep_rew_mean     | -7.2e+04 |
| time/              |          |
|    fps             | 160      |
|    iterations      | 700      |
|    time_elapsed    | 8950     |
|    total_timesteps | 1433600  |
---------------------------------
Eval num_timesteps=1434000, episode_reward=-77179.93 +/- 27186.31
Episode length: 63.60 +/- 15.08
------------------------------------------
| eval/                   |              |
|    mean action          | 0.40726006   |
|    mean velocity x      | -1.96        |
|    mean velocity y      | -3.1         |
|    mean velocity z      | 20.4         |
|    mean_ep_length       | 63.6         |
|    mean_reward          | -7.72e+04    |
| time/                   |              |
|    total_timesteps      | 1434000      |
| train/                  |              |
|    approx_kl            | 0.0014371723 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.001        |
|    loss                 | 1.64e+08     |
|    n_updates            | 7000         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 0.894        |
|    value_loss           | 1.89e+08     |
------------------------------------------
Eval num_timesteps=1434500, episode_reward=-32921.85 +/- 28254.76
Episode length: 39.20 +/- 17.43
------------------------------------
| eval/              |             |
|    mean action     | -0.21111304 |
|    mean velocity x | -1.24       |
|    mean velocity y | 0.229       |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 39.2        |
|    mean_reward     | -3.29e+04   |
| time/              |             |
|    total_timesteps | 1434500     |
------------------------------------
Eval num_timesteps=1435000, episode_reward=-71186.76 +/- 24052.39
Episode length: 58.20 +/- 9.93
------------------------------------
| eval/              |             |
|    mean action     | -0.82245654 |
|    mean velocity x | 1.64        |
|    mean velocity y | 5.03        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -7.12e+04   |
| time/              |             |
|    total_timesteps | 1435000     |
------------------------------------
Eval num_timesteps=1435500, episode_reward=-90148.03 +/- 11538.10
Episode length: 71.80 +/- 10.91
-----------------------------------
| eval/              |            |
|    mean action     | 0.21307592 |
|    mean velocity x | 0.886      |
|    mean velocity y | -0.948     |
|    mean velocity z | 20.6       |
|    mean_ep_length  | 71.8       |
|    mean_reward     | -9.01e+04  |
| time/              |            |
|    total_timesteps | 1435500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.9      |
|    ep_rew_mean     | -7.37e+04 |
| time/              |           |
|    fps             | 160       |
|    iterations      | 701       |
|    time_elapsed    | 8958      |
|    total_timesteps | 1435648   |
----------------------------------
Eval num_timesteps=1436000, episode_reward=-59962.15 +/- 39234.05
Episode length: 50.60 +/- 18.78
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.03028302   |
|    mean velocity x      | 0.438         |
|    mean velocity y      | 0.623         |
|    mean velocity z      | 16.4          |
|    mean_ep_length       | 50.6          |
|    mean_reward          | -6e+04        |
| time/                   |               |
|    total_timesteps      | 1436000       |
| train/                  |               |
|    approx_kl            | 0.00025119976 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.368         |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+07      |
|    n_updates            | 7010          |
|    policy_gradient_loss | -0.000501     |
|    std                  | 0.894         |
|    value_loss           | 1.74e+08      |
-------------------------------------------
Eval num_timesteps=1436500, episode_reward=-38493.06 +/- 41069.16
Episode length: 43.20 +/- 16.33
-----------------------------------
| eval/              |            |
|    mean action     | 0.38755518 |
|    mean velocity x | -0.649     |
|    mean velocity y | -2.21      |
|    mean velocity z | 20.4       |
|    mean_ep_length  | 43.2       |
|    mean_reward     | -3.85e+04  |
| time/              |            |
|    total_timesteps | 1436500    |
-----------------------------------
Eval num_timesteps=1437000, episode_reward=-60634.25 +/- 33414.07
Episode length: 61.00 +/- 21.85
----------------------------------
| eval/              |           |
|    mean action     | 0.1456928 |
|    mean velocity x | -0.228    |
|    mean velocity y | -1.83     |
|    mean velocity z | 19        |
|    mean_ep_length  | 61        |
|    mean_reward     | -6.06e+04 |
| time/              |           |
|    total_timesteps | 1437000   |
----------------------------------
Eval num_timesteps=1437500, episode_reward=-65749.73 +/- 30710.52
Episode length: 59.00 +/- 13.30
------------------------------------
| eval/              |             |
|    mean action     | -0.03459123 |
|    mean velocity x | -1.91       |
|    mean velocity y | 0.104       |
|    mean velocity z | 18.4        |
|    mean_ep_length  | 59          |
|    mean_reward     | -6.57e+04   |
| time/              |             |
|    total_timesteps | 1437500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.4      |
|    ep_rew_mean     | -7.55e+04 |
| time/              |           |
|    fps             | 160       |
|    iterations      | 702       |
|    time_elapsed    | 8965      |
|    total_timesteps | 1437696   |
----------------------------------
Eval num_timesteps=1438000, episode_reward=-52009.00 +/- 34993.45
Episode length: 52.20 +/- 10.72
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.49903336    |
|    mean velocity x      | -0.261        |
|    mean velocity y      | -1.68         |
|    mean velocity z      | 18.1          |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -5.2e+04      |
| time/                   |               |
|    total_timesteps      | 1438000       |
| train/                  |               |
|    approx_kl            | 0.00016429974 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.345         |
|    learning_rate        | 0.001         |
|    loss                 | 6.96e+07      |
|    n_updates            | 7020          |
|    policy_gradient_loss | -0.000352     |
|    std                  | 0.894         |
|    value_loss           | 1.7e+08       |
-------------------------------------------
Eval num_timesteps=1438500, episode_reward=-67865.91 +/- 36317.59
Episode length: 54.80 +/- 17.78
------------------------------------
| eval/              |             |
|    mean action     | -0.19429822 |
|    mean velocity x | -0.118      |
|    mean velocity y | 1.72        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -6.79e+04   |
| time/              |             |
|    total_timesteps | 1438500     |
------------------------------------
Eval num_timesteps=1439000, episode_reward=-91734.88 +/- 20442.96
Episode length: 64.60 +/- 3.44
----------------------------------
| eval/              |           |
|    mean action     | 0.1808665 |
|    mean velocity x | -0.0773   |
|    mean velocity y | 0.0187    |
|    mean velocity z | 17        |
|    mean_ep_length  | 64.6      |
|    mean_reward     | -9.17e+04 |
| time/              |           |
|    total_timesteps | 1439000   |
----------------------------------
Eval num_timesteps=1439500, episode_reward=-70066.23 +/- 38283.72
Episode length: 57.00 +/- 21.48
-----------------------------------
| eval/              |            |
|    mean action     | 0.06056247 |
|    mean velocity x | -0.14      |
|    mean velocity y | 0.579      |
|    mean velocity z | 20.8       |
|    mean_ep_length  | 57         |
|    mean_reward     | -7.01e+04  |
| time/              |            |
|    total_timesteps | 1439500    |
-----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -7.6e+04 |
| time/              |          |
|    fps             | 160      |
|    iterations      | 703      |
|    time_elapsed    | 8972     |
|    total_timesteps | 1439744  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=-87711.24 +/- 29102.77
Episode length: 59.20 +/- 6.14
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.3030539    |
|    mean velocity x      | 1.18          |
|    mean velocity y      | 2.27          |
|    mean velocity z      | 19.8          |
|    mean_ep_length       | 59.2          |
|    mean_reward          | -8.77e+04     |
| time/                   |               |
|    total_timesteps      | 1440000       |
| train/                  |               |
|    approx_kl            | 6.0669874e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.001         |
|    loss                 | 8.49e+07      |
|    n_updates            | 7030          |
|    policy_gradient_loss | -0.000323     |
|    std                  | 0.894         |
|    value_loss           | 2.18e+08      |
-------------------------------------------
Eval num_timesteps=1440500, episode_reward=-69221.63 +/- 22721.71
Episode length: 77.00 +/- 27.57
-----------------------------------
| eval/              |            |
|    mean action     | 0.19195572 |
|    mean velocity x | -0.391     |
|    mean velocity y | -1.18      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 77         |
|    mean_reward     | -6.92e+04  |
| time/              |            |
|    total_timesteps | 1440500    |
-----------------------------------
Eval num_timesteps=1441000, episode_reward=-61463.49 +/- 46167.77
Episode length: 51.40 +/- 24.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.04986059 |
|    mean velocity x | 0.0395     |
|    mean velocity y | 0.0666     |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 51.4       |
|    mean_reward     | -6.15e+04  |
| time/              |            |
|    total_timesteps | 1441000    |
-----------------------------------
Eval num_timesteps=1441500, episode_reward=-98837.48 +/- 15488.64
Episode length: 67.40 +/- 6.77
------------------------------------
| eval/              |             |
|    mean action     | -0.17933339 |
|    mean velocity x | -0.614      |
|    mean velocity y | 0.0569      |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 67.4        |
|    mean_reward     | -9.88e+04   |
| time/              |             |
|    total_timesteps | 1441500     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -7.8e+04 |
| time/              |          |
|    fps             | 160      |
|    iterations      | 704      |
|    time_elapsed    | 8979     |
|    total_timesteps | 1441792  |
---------------------------------
Eval num_timesteps=1442000, episode_reward=-66858.87 +/- 20236.18
Episode length: 65.60 +/- 17.95
------------------------------------------
| eval/                   |              |
|    mean action          | 0.2772119    |
|    mean velocity x      | -0.0425      |
|    mean velocity y      | -0.579       |
|    mean velocity z      | 18.1         |
|    mean_ep_length       | 65.6         |
|    mean_reward          | -6.69e+04    |
| time/                   |              |
|    total_timesteps      | 1442000      |
| train/                  |              |
|    approx_kl            | 0.0004516435 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.001        |
|    loss                 | 8.74e+07     |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 0.894        |
|    value_loss           | 1.9e+08      |
------------------------------------------
Eval num_timesteps=1442500, episode_reward=-68084.50 +/- 33335.67
Episode length: 55.00 +/- 9.80
-----------------------------------
| eval/              |            |
|    mean action     | 0.34837753 |
|    mean velocity x | 0.586      |
|    mean velocity y | -2.69      |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 55         |
|    mean_reward     | -6.81e+04  |
| time/              |            |
|    total_timesteps | 1442500    |
-----------------------------------
Eval num_timesteps=1443000, episode_reward=-52772.67 +/- 26662.88
Episode length: 55.00 +/- 11.28
------------------------------------
| eval/              |             |
|    mean action     | -0.12663735 |
|    mean velocity x | 2.01        |
|    mean velocity y | 2.33        |
|    mean velocity z | 17          |
|    mean_ep_length  | 55          |
|    mean_reward     | -5.28e+04   |
| time/              |             |
|    total_timesteps | 1443000     |
------------------------------------
Eval num_timesteps=1443500, episode_reward=-73073.11 +/- 23095.74
Episode length: 64.20 +/- 9.04
-----------------------------------
| eval/              |            |
|    mean action     | -0.5460637 |
|    mean velocity x | 1.7        |
|    mean velocity y | 4.05       |
|    mean velocity z | 21         |
|    mean_ep_length  | 64.2       |
|    mean_reward     | -7.31e+04  |
| time/              |            |
|    total_timesteps | 1443500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69.3      |
|    ep_rew_mean     | -7.73e+04 |
| time/              |           |
|    fps             | 160       |
|    iterations      | 705       |
|    time_elapsed    | 8986      |
|    total_timesteps | 1443840   |
----------------------------------
Eval num_timesteps=1444000, episode_reward=-68633.13 +/- 36497.01
Episode length: 53.20 +/- 20.60
------------------------------------------
| eval/                   |              |
|    mean action          | 0.10874539   |
|    mean velocity x      | -2.08        |
|    mean velocity y      | -1.2         |
|    mean velocity z      | 17.8         |
|    mean_ep_length       | 53.2         |
|    mean_reward          | -6.86e+04    |
| time/                   |              |
|    total_timesteps      | 1444000      |
| train/                  |              |
|    approx_kl            | 0.0009853557 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.001        |
|    loss                 | 9.81e+07     |
|    n_updates            | 7050         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.894        |
|    value_loss           | 1.72e+08     |
------------------------------------------
Eval num_timesteps=1444500, episode_reward=-67048.42 +/- 31259.30
Episode length: 54.40 +/- 9.67
-----------------------------------
| eval/              |            |
|    mean action     | 0.51031536 |
|    mean velocity x | -1.18      |
|    mean velocity y | -2.52      |
|    mean velocity z | 17.4       |
|    mean_ep_length  | 54.4       |
|    mean_reward     | -6.7e+04   |
| time/              |            |
|    total_timesteps | 1444500    |
-----------------------------------
Eval num_timesteps=1445000, episode_reward=-71772.21 +/- 22355.77
Episode length: 77.20 +/- 31.57
-----------------------------------
| eval/              |            |
|    mean action     | 0.14368759 |
|    mean velocity x | 0.224      |
|    mean velocity y | 0.49       |
|    mean velocity z | 18.6       |
|    mean_ep_length  | 77.2       |
|    mean_reward     | -7.18e+04  |
| time/              |            |
|    total_timesteps | 1445000    |
-----------------------------------
Eval num_timesteps=1445500, episode_reward=-71932.99 +/- 32627.12
Episode length: 60.40 +/- 12.32
--------------------------------------
| eval/              |               |
|    mean action     | -0.0050774687 |
|    mean velocity x | -0.547        |
|    mean velocity y | 0.0588        |
|    mean velocity z | 19.1          |
|    mean_ep_length  | 60.4          |
|    mean_reward     | -7.19e+04     |
| time/              |               |
|    total_timesteps | 1445500       |
--------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.2      |
|    ep_rew_mean     | -7.88e+04 |
| time/              |           |
|    fps             | 160       |
|    iterations      | 706       |
|    time_elapsed    | 8994      |
|    total_timesteps | 1445888   |
----------------------------------
Eval num_timesteps=1446000, episode_reward=-47470.10 +/- 49630.65
Episode length: 45.20 +/- 13.73
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.2511201    |
|    mean velocity x      | 0.655         |
|    mean velocity y      | 1.82          |
|    mean velocity z      | 20.8          |
|    mean_ep_length       | 45.2          |
|    mean_reward          | -4.75e+04     |
| time/                   |               |
|    total_timesteps      | 1446000       |
| train/                  |               |
|    approx_kl            | 2.3867091e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.352         |
|    learning_rate        | 0.001         |
|    loss                 | 6.18e+07      |
|    n_updates            | 7060          |
|    policy_gradient_loss | -0.00021      |
|    std                  | 0.894         |
|    value_loss           | 1.9e+08       |
-------------------------------------------
Eval num_timesteps=1446500, episode_reward=-77404.57 +/- 7469.98
Episode length: 66.00 +/- 13.42
------------------------------------
| eval/              |             |
|    mean action     | -0.18000378 |
|    mean velocity x | 0.818       |
|    mean velocity y | 1.69        |
|    mean velocity z | 20.1        |
|    mean_ep_length  | 66          |
|    mean_reward     | -7.74e+04   |
| time/              |             |
|    total_timesteps | 1446500     |
------------------------------------
Eval num_timesteps=1447000, episode_reward=-62454.18 +/- 34210.09
Episode length: 60.60 +/- 9.44
------------------------------------
| eval/              |             |
|    mean action     | -0.39875916 |
|    mean velocity x | 0.552       |
|    mean velocity y | 1.81        |
|    mean velocity z | 16.4        |
|    mean_ep_length  | 60.6        |
|    mean_reward     | -6.25e+04   |
| time/              |             |
|    total_timesteps | 1447000     |
------------------------------------
Eval num_timesteps=1447500, episode_reward=-69183.58 +/- 105448.69
Episode length: 87.00 +/- 105.32
----------------------------------
| eval/              |           |
|    mean action     | 0.5012624 |
|    mean velocity x | -1.26     |
|    mean velocity y | -3.14     |
|    mean velocity z | 20.8      |
|    mean_ep_length  | 87        |
|    mean_reward     | -6.92e+04 |
| time/              |           |
|    total_timesteps | 1447500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.9      |
|    ep_rew_mean     | -7.51e+04 |
| time/              |           |
|    fps             | 160       |
|    iterations      | 707       |
|    time_elapsed    | 9001      |
|    total_timesteps | 1447936   |
----------------------------------
Eval num_timesteps=1448000, episode_reward=-93726.64 +/- 15097.28
Episode length: 61.80 +/- 3.49
------------------------------------------
| eval/                   |              |
|    mean action          | -0.3486514   |
|    mean velocity x      | 0.298        |
|    mean velocity y      | 1.34         |
|    mean velocity z      | 17           |
|    mean_ep_length       | 61.8         |
|    mean_reward          | -9.37e+04    |
| time/                   |              |
|    total_timesteps      | 1448000      |
| train/                  |              |
|    approx_kl            | 9.342152e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.001        |
|    loss                 | 1.07e+08     |
|    n_updates            | 7070         |
|    policy_gradient_loss | -0.000288    |
|    std                  | 0.894        |
|    value_loss           | 1.79e+08     |
------------------------------------------
Eval num_timesteps=1448500, episode_reward=-63206.71 +/- 36134.05
Episode length: 54.60 +/- 8.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.44166547 |
|    mean velocity x | -1.84      |
|    mean velocity y | -3.07      |
|    mean velocity z | 21         |
|    mean_ep_length  | 54.6       |
|    mean_reward     | -6.32e+04  |
| time/              |            |
|    total_timesteps | 1448500    |
-----------------------------------
Eval num_timesteps=1449000, episode_reward=-68818.25 +/- 32201.41
Episode length: 60.40 +/- 8.52
-----------------------------------
| eval/              |            |
|    mean action     | 0.15933886 |
|    mean velocity x | 0.187      |
|    mean velocity y | -1.45      |
|    mean velocity z | 18.1       |
|    mean_ep_length  | 60.4       |
|    mean_reward     | -6.88e+04  |
| time/              |            |
|    total_timesteps | 1449000    |
-----------------------------------
Eval num_timesteps=1449500, episode_reward=-81211.14 +/- 14747.58
Episode length: 72.40 +/- 9.93
------------------------------------
| eval/              |             |
|    mean action     | -0.23867038 |
|    mean velocity x | 0.545       |
|    mean velocity y | 1.96        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 72.4        |
|    mean_reward     | -8.12e+04   |
| time/              |             |
|    total_timesteps | 1449500     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67       |
|    ep_rew_mean     | -7.4e+04 |
| time/              |          |
|    fps             | 160      |
|    iterations      | 708      |
|    time_elapsed    | 9009     |
|    total_timesteps | 1449984  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=-75906.10 +/- 37746.72
Episode length: 52.20 +/- 18.69
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.14655685   |
|    mean velocity x      | 0.519         |
|    mean velocity y      | 1.65          |
|    mean velocity z      | 21.4          |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -7.59e+04     |
| time/                   |               |
|    total_timesteps      | 1450000       |
| train/                  |               |
|    approx_kl            | 0.00013725332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 5.65e+07      |
|    n_updates            | 7080          |
|    policy_gradient_loss | -0.000631     |
|    std                  | 0.895         |
|    value_loss           | 1.83e+08      |
-------------------------------------------
Eval num_timesteps=1450500, episode_reward=-86992.77 +/- 8122.94
Episode length: 73.40 +/- 20.35
------------------------------------
| eval/              |             |
|    mean action     | -0.22470891 |
|    mean velocity x | 2.94        |
|    mean velocity y | 3.08        |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 73.4        |
|    mean_reward     | -8.7e+04    |
| time/              |             |
|    total_timesteps | 1450500     |
------------------------------------
Eval num_timesteps=1451000, episode_reward=-70712.83 +/- 36355.48
Episode length: 59.40 +/- 19.76
------------------------------------
| eval/              |             |
|    mean action     | 0.050893683 |
|    mean velocity x | 0.0755      |
|    mean velocity y | -1.51       |
|    mean velocity z | 18.6        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -7.07e+04   |
| time/              |             |
|    total_timesteps | 1451000     |
------------------------------------
Eval num_timesteps=1451500, episode_reward=-50134.84 +/- 38375.16
Episode length: 71.40 +/- 40.10
-----------------------------------
| eval/              |            |
|    mean action     | 0.13297437 |
|    mean velocity x | -1.02      |
|    mean velocity y | -0.421     |
|    mean velocity z | 17.8       |
|    mean_ep_length  | 71.4       |
|    mean_reward     | -5.01e+04  |
| time/              |            |
|    total_timesteps | 1451500    |
-----------------------------------
Eval num_timesteps=1452000, episode_reward=-90460.92 +/- 23405.92
Episode length: 63.60 +/- 6.89
------------------------------------
| eval/              |             |
|    mean action     | -0.05820479 |
|    mean velocity x | 0.708       |
|    mean velocity y | 0.444       |
|    mean velocity z | 19.2        |
|    mean_ep_length  | 63.6        |
|    mean_reward     | -9.05e+04   |
| time/              |             |
|    total_timesteps | 1452000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 63        |
|    ep_rew_mean     | -7.03e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 709       |
|    time_elapsed    | 9017      |
|    total_timesteps | 1452032   |
----------------------------------
Eval num_timesteps=1452500, episode_reward=-62393.81 +/- 23237.42
Episode length: 61.20 +/- 9.93
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.5459364     |
|    mean velocity x      | -1.74         |
|    mean velocity y      | -2.87         |
|    mean velocity z      | 17.6          |
|    mean_ep_length       | 61.2          |
|    mean_reward          | -6.24e+04     |
| time/                   |               |
|    total_timesteps      | 1452500       |
| train/                  |               |
|    approx_kl            | 0.00050981174 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.339         |
|    learning_rate        | 0.001         |
|    loss                 | 8.7e+07       |
|    n_updates            | 7090          |
|    policy_gradient_loss | -0.00123      |
|    std                  | 0.895         |
|    value_loss           | 1.97e+08      |
-------------------------------------------
Eval num_timesteps=1453000, episode_reward=-76104.30 +/- 40192.82
Episode length: 53.20 +/- 16.44
------------------------------------
| eval/              |             |
|    mean action     | -0.23176306 |
|    mean velocity x | 0.893       |
|    mean velocity y | 0.515       |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 53.2        |
|    mean_reward     | -7.61e+04   |
| time/              |             |
|    total_timesteps | 1453000     |
------------------------------------
Eval num_timesteps=1453500, episode_reward=-65613.30 +/- 35735.07
Episode length: 53.20 +/- 11.16
-----------------------------------
| eval/              |            |
|    mean action     | -0.2638706 |
|    mean velocity x | 0.62       |
|    mean velocity y | 1.63       |
|    mean velocity z | 20         |
|    mean_ep_length  | 53.2       |
|    mean_reward     | -6.56e+04  |
| time/              |            |
|    total_timesteps | 1453500    |
-----------------------------------
Eval num_timesteps=1454000, episode_reward=-81806.89 +/- 35746.10
Episode length: 63.20 +/- 11.07
-----------------------------------
| eval/              |            |
|    mean action     | 0.41743708 |
|    mean velocity x | -0.946     |
|    mean velocity y | -2.55      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 63.2       |
|    mean_reward     | -8.18e+04  |
| time/              |            |
|    total_timesteps | 1454000    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 63.8      |
|    ep_rew_mean     | -7.19e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 710       |
|    time_elapsed    | 9024      |
|    total_timesteps | 1454080   |
----------------------------------
Eval num_timesteps=1454500, episode_reward=-93579.41 +/- 14431.16
Episode length: 65.20 +/- 4.31
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.042204283   |
|    mean velocity x      | 0.544         |
|    mean velocity y      | -0.543        |
|    mean velocity z      | 20.3          |
|    mean_ep_length       | 65.2          |
|    mean_reward          | -9.36e+04     |
| time/                   |               |
|    total_timesteps      | 1454500       |
| train/                  |               |
|    approx_kl            | 2.2356864e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.321         |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+08      |
|    n_updates            | 7100          |
|    policy_gradient_loss | -0.000283     |
|    std                  | 0.895         |
|    value_loss           | 2.02e+08      |
-------------------------------------------
Eval num_timesteps=1455000, episode_reward=-41773.44 +/- 24945.64
Episode length: 61.60 +/- 28.95
-----------------------------------
| eval/              |            |
|    mean action     | 0.10514512 |
|    mean velocity x | 0.266      |
|    mean velocity y | -1.13      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 61.6       |
|    mean_reward     | -4.18e+04  |
| time/              |            |
|    total_timesteps | 1455000    |
-----------------------------------
Eval num_timesteps=1455500, episode_reward=-66896.75 +/- 27292.65
Episode length: 57.80 +/- 12.61
-----------------------------------
| eval/              |            |
|    mean action     | -0.3135881 |
|    mean velocity x | -1.55      |
|    mean velocity y | 0.164      |
|    mean velocity z | 20.9       |
|    mean_ep_length  | 57.8       |
|    mean_reward     | -6.69e+04  |
| time/              |            |
|    total_timesteps | 1455500    |
-----------------------------------
Eval num_timesteps=1456000, episode_reward=-99343.70 +/- 3643.23
Episode length: 61.40 +/- 0.49
------------------------------------
| eval/              |             |
|    mean action     | 0.049154297 |
|    mean velocity x | 0.513       |
|    mean velocity y | 0.825       |
|    mean velocity z | 17          |
|    mean_ep_length  | 61.4        |
|    mean_reward     | -9.93e+04   |
| time/              |             |
|    total_timesteps | 1456000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.8      |
|    ep_rew_mean     | -7.62e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 711       |
|    time_elapsed    | 9031      |
|    total_timesteps | 1456128   |
----------------------------------
Eval num_timesteps=1456500, episode_reward=-52334.13 +/- 28942.39
Episode length: 61.80 +/- 24.09
------------------------------------------
| eval/                   |              |
|    mean action          | 0.15805697   |
|    mean velocity x      | -1.36        |
|    mean velocity y      | -1.78        |
|    mean velocity z      | 19.4         |
|    mean_ep_length       | 61.8         |
|    mean_reward          | -5.23e+04    |
| time/                   |              |
|    total_timesteps      | 1456500      |
| train/                  |              |
|    approx_kl            | 0.0001045939 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.368        |
|    learning_rate        | 0.001        |
|    loss                 | 5.05e+07     |
|    n_updates            | 7110         |
|    policy_gradient_loss | -0.000196    |
|    std                  | 0.895        |
|    value_loss           | 1.87e+08     |
------------------------------------------
Eval num_timesteps=1457000, episode_reward=-41460.15 +/- 33337.22
Episode length: 56.20 +/- 28.58
-----------------------------------
| eval/              |            |
|    mean action     | -0.9276031 |
|    mean velocity x | 2.24       |
|    mean velocity y | 5.38       |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 56.2       |
|    mean_reward     | -4.15e+04  |
| time/              |            |
|    total_timesteps | 1457000    |
-----------------------------------
Eval num_timesteps=1457500, episode_reward=-79339.42 +/- 40646.95
Episode length: 53.60 +/- 23.09
------------------------------------
| eval/              |             |
|    mean action     | -0.79727966 |
|    mean velocity x | 2.99        |
|    mean velocity y | 4.36        |
|    mean velocity z | 17.2        |
|    mean_ep_length  | 53.6        |
|    mean_reward     | -7.93e+04   |
| time/              |             |
|    total_timesteps | 1457500     |
------------------------------------
Eval num_timesteps=1458000, episode_reward=-68378.53 +/- 36393.39
Episode length: 54.00 +/- 15.63
------------------------------------
| eval/              |             |
|    mean action     | -0.16826016 |
|    mean velocity x | -0.0376     |
|    mean velocity y | 0.735       |
|    mean velocity z | 22          |
|    mean_ep_length  | 54          |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 1458000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.8      |
|    ep_rew_mean     | -8.23e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 712       |
|    time_elapsed    | 9038      |
|    total_timesteps | 1458176   |
----------------------------------
Eval num_timesteps=1458500, episode_reward=-47571.24 +/- 39982.99
Episode length: 43.40 +/- 27.43
------------------------------------------
| eval/                   |              |
|    mean action          | 0.33886412   |
|    mean velocity x      | -2.07        |
|    mean velocity y      | -2.43        |
|    mean velocity z      | 19.3         |
|    mean_ep_length       | 43.4         |
|    mean_reward          | -4.76e+04    |
| time/                   |              |
|    total_timesteps      | 1458500      |
| train/                  |              |
|    approx_kl            | 0.0004677389 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.344        |
|    learning_rate        | 0.001        |
|    loss                 | 1.43e+08     |
|    n_updates            | 7120         |
|    policy_gradient_loss | -0.000692    |
|    std                  | 0.895        |
|    value_loss           | 1.99e+08     |
------------------------------------------
Eval num_timesteps=1459000, episode_reward=-50396.23 +/- 28324.43
Episode length: 55.60 +/- 13.15
-----------------------------------
| eval/              |            |
|    mean action     | 0.34929138 |
|    mean velocity x | -0.893     |
|    mean velocity y | -1.09      |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 55.6       |
|    mean_reward     | -5.04e+04  |
| time/              |            |
|    total_timesteps | 1459000    |
-----------------------------------
Eval num_timesteps=1459500, episode_reward=-65083.63 +/- 32120.77
Episode length: 55.00 +/- 14.24
---------------------------------------
| eval/              |                |
|    mean action     | -0.00011039734 |
|    mean velocity x | 0.246          |
|    mean velocity y | -0.548         |
|    mean velocity z | 17.9           |
|    mean_ep_length  | 55             |
|    mean_reward     | -6.51e+04      |
| time/              |                |
|    total_timesteps | 1459500        |
---------------------------------------
Eval num_timesteps=1460000, episode_reward=-43955.13 +/- 36873.51
Episode length: 47.00 +/- 17.91
------------------------------------
| eval/              |             |
|    mean action     | -0.38053608 |
|    mean velocity x | 1.03        |
|    mean velocity y | 1.88        |
|    mean velocity z | 19          |
|    mean_ep_length  | 47          |
|    mean_reward     | -4.4e+04    |
| time/              |             |
|    total_timesteps | 1460000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.6      |
|    ep_rew_mean     | -8.52e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 713       |
|    time_elapsed    | 9045      |
|    total_timesteps | 1460224   |
----------------------------------
Eval num_timesteps=1460500, episode_reward=-60929.26 +/- 31976.52
Episode length: 57.60 +/- 23.97
------------------------------------------
| eval/                   |              |
|    mean action          | -0.12740228  |
|    mean velocity x      | -0.244       |
|    mean velocity y      | 0.12         |
|    mean velocity z      | 16.8         |
|    mean_ep_length       | 57.6         |
|    mean_reward          | -6.09e+04    |
| time/                   |              |
|    total_timesteps      | 1460500      |
| train/                  |              |
|    approx_kl            | 0.0001932144 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.001        |
|    loss                 | 4.09e+07     |
|    n_updates            | 7130         |
|    policy_gradient_loss | -0.000535    |
|    std                  | 0.895        |
|    value_loss           | 1.64e+08     |
------------------------------------------
Eval num_timesteps=1461000, episode_reward=-65573.75 +/- 22983.97
Episode length: 64.00 +/- 16.01
-----------------------------------
| eval/              |            |
|    mean action     | -0.4303092 |
|    mean velocity x | 0.882      |
|    mean velocity y | 2.68       |
|    mean velocity z | 22.3       |
|    mean_ep_length  | 64         |
|    mean_reward     | -6.56e+04  |
| time/              |            |
|    total_timesteps | 1461000    |
-----------------------------------
Eval num_timesteps=1461500, episode_reward=-98295.07 +/- 21327.11
Episode length: 87.60 +/- 39.80
----------------------------------
| eval/              |           |
|    mean action     | -0.158227 |
|    mean velocity x | 1.92      |
|    mean velocity y | 1.07      |
|    mean velocity z | 20.1      |
|    mean_ep_length  | 87.6      |
|    mean_reward     | -9.83e+04 |
| time/              |           |
|    total_timesteps | 1461500   |
----------------------------------
Eval num_timesteps=1462000, episode_reward=-69077.82 +/- 34573.67
Episode length: 51.60 +/- 15.83
------------------------------------
| eval/              |             |
|    mean action     | -0.08401515 |
|    mean velocity x | -0.924      |
|    mean velocity y | 0.266       |
|    mean velocity z | 19.3        |
|    mean_ep_length  | 51.6        |
|    mean_reward     | -6.91e+04   |
| time/              |             |
|    total_timesteps | 1462000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.5      |
|    ep_rew_mean     | -8.34e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 714       |
|    time_elapsed    | 9053      |
|    total_timesteps | 1462272   |
----------------------------------
Eval num_timesteps=1462500, episode_reward=-74359.27 +/- 38040.95
Episode length: 53.80 +/- 20.11
------------------------------------------
| eval/                   |              |
|    mean action          | -0.14131661  |
|    mean velocity x      | 0.773        |
|    mean velocity y      | 0.822        |
|    mean velocity z      | 20.2         |
|    mean_ep_length       | 53.8         |
|    mean_reward          | -7.44e+04    |
| time/                   |              |
|    total_timesteps      | 1462500      |
| train/                  |              |
|    approx_kl            | 0.0028233225 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.001        |
|    loss                 | 4.78e+07     |
|    n_updates            | 7140         |
|    policy_gradient_loss | -0.0025      |
|    std                  | 0.895        |
|    value_loss           | 1.85e+08     |
------------------------------------------
Eval num_timesteps=1463000, episode_reward=-63917.82 +/- 38749.93
Episode length: 48.60 +/- 18.25
----------------------------------
| eval/              |           |
|    mean action     | 0.5728667 |
|    mean velocity x | -2.19     |
|    mean velocity y | -3.13     |
|    mean velocity z | 18.5      |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -6.39e+04 |
| time/              |           |
|    total_timesteps | 1463000   |
----------------------------------
Eval num_timesteps=1463500, episode_reward=-63195.26 +/- 41586.52
Episode length: 52.20 +/- 19.01
------------------------------------
| eval/              |             |
|    mean action     | -0.73398983 |
|    mean velocity x | 2.84        |
|    mean velocity y | 5.33        |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 52.2        |
|    mean_reward     | -6.32e+04   |
| time/              |             |
|    total_timesteps | 1463500     |
------------------------------------
Eval num_timesteps=1464000, episode_reward=-51762.85 +/- 4463.72
Episode length: 59.40 +/- 10.07
------------------------------------
| eval/              |             |
|    mean action     | 0.037795294 |
|    mean velocity x | 1.73        |
|    mean velocity y | 0.755       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 59.4        |
|    mean_reward     | -5.18e+04   |
| time/              |             |
|    total_timesteps | 1464000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.1      |
|    ep_rew_mean     | -8.27e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 715       |
|    time_elapsed    | 9059      |
|    total_timesteps | 1464320   |
----------------------------------
Eval num_timesteps=1464500, episode_reward=-78575.42 +/- 33911.39
Episode length: 57.40 +/- 6.53
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.7839512     |
|    mean velocity x      | -1.15         |
|    mean velocity y      | -4.21         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 57.4          |
|    mean_reward          | -7.86e+04     |
| time/                   |               |
|    total_timesteps      | 1464500       |
| train/                  |               |
|    approx_kl            | 1.9907748e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.381         |
|    learning_rate        | 0.001         |
|    loss                 | 7.39e+07      |
|    n_updates            | 7150          |
|    policy_gradient_loss | -0.000103     |
|    std                  | 0.895         |
|    value_loss           | 1.62e+08      |
-------------------------------------------
Eval num_timesteps=1465000, episode_reward=-80132.12 +/- 31775.05
Episode length: 69.20 +/- 20.32
----------------------------------
| eval/              |           |
|    mean action     | 0.4348261 |
|    mean velocity x | -1.81     |
|    mean velocity y | -3.62     |
|    mean velocity z | 17.2      |
|    mean_ep_length  | 69.2      |
|    mean_reward     | -8.01e+04 |
| time/              |           |
|    total_timesteps | 1465000   |
----------------------------------
Eval num_timesteps=1465500, episode_reward=-75267.07 +/- 37437.00
Episode length: 55.80 +/- 13.70
-----------------------------------
| eval/              |            |
|    mean action     | 0.25306633 |
|    mean velocity x | -1.82      |
|    mean velocity y | -1.71      |
|    mean velocity z | 19         |
|    mean_ep_length  | 55.8       |
|    mean_reward     | -7.53e+04  |
| time/              |            |
|    total_timesteps | 1465500    |
-----------------------------------
Eval num_timesteps=1466000, episode_reward=-83701.37 +/- 19668.79
Episode length: 67.60 +/- 6.53
------------------------------------
| eval/              |             |
|    mean action     | -0.06951309 |
|    mean velocity x | 1.51        |
|    mean velocity y | 1.28        |
|    mean velocity z | 17.9        |
|    mean_ep_length  | 67.6        |
|    mean_reward     | -8.37e+04   |
| time/              |             |
|    total_timesteps | 1466000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.9      |
|    ep_rew_mean     | -7.95e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 716       |
|    time_elapsed    | 9067      |
|    total_timesteps | 1466368   |
----------------------------------
Eval num_timesteps=1466500, episode_reward=-75885.94 +/- 13236.07
Episode length: 59.80 +/- 5.15
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.7550686    |
|    mean velocity x      | 2.74          |
|    mean velocity y      | 4.67          |
|    mean velocity z      | 21            |
|    mean_ep_length       | 59.8          |
|    mean_reward          | -7.59e+04     |
| time/                   |               |
|    total_timesteps      | 1466500       |
| train/                  |               |
|    approx_kl            | 2.7583534e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.001         |
|    loss                 | 7.53e+07      |
|    n_updates            | 7160          |
|    policy_gradient_loss | -0.0002       |
|    std                  | 0.895         |
|    value_loss           | 2.06e+08      |
-------------------------------------------
Eval num_timesteps=1467000, episode_reward=-81883.56 +/- 26832.02
Episode length: 62.40 +/- 6.05
------------------------------------
| eval/              |             |
|    mean action     | 0.058718193 |
|    mean velocity x | -0.445      |
|    mean velocity y | 0.032       |
|    mean velocity z | 21.1        |
|    mean_ep_length  | 62.4        |
|    mean_reward     | -8.19e+04   |
| time/              |             |
|    total_timesteps | 1467000     |
------------------------------------
Eval num_timesteps=1467500, episode_reward=-85221.64 +/- 29272.79
Episode length: 60.00 +/- 2.90
-----------------------------------
| eval/              |            |
|    mean action     | 0.64147115 |
|    mean velocity x | -2.21      |
|    mean velocity y | -2.95      |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 60         |
|    mean_reward     | -8.52e+04  |
| time/              |            |
|    total_timesteps | 1467500    |
-----------------------------------
Eval num_timesteps=1468000, episode_reward=-81074.27 +/- 36172.42
Episode length: 65.00 +/- 20.27
------------------------------------
| eval/              |             |
|    mean action     | -0.36301422 |
|    mean velocity x | 1.52        |
|    mean velocity y | 1.95        |
|    mean velocity z | 20.5        |
|    mean_ep_length  | 65          |
|    mean_reward     | -8.11e+04   |
| time/              |             |
|    total_timesteps | 1468000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.9      |
|    ep_rew_mean     | -8.26e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 717       |
|    time_elapsed    | 9074      |
|    total_timesteps | 1468416   |
----------------------------------
Eval num_timesteps=1468500, episode_reward=-80439.85 +/- 33979.36
Episode length: 59.80 +/- 13.75
------------------------------------------
| eval/                   |              |
|    mean action          | 0.21043724   |
|    mean velocity x      | 0.597        |
|    mean velocity y      | -0.923       |
|    mean velocity z      | 20.1         |
|    mean_ep_length       | 59.8         |
|    mean_reward          | -8.04e+04    |
| time/                   |              |
|    total_timesteps      | 1468500      |
| train/                  |              |
|    approx_kl            | 0.0029094534 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.001        |
|    loss                 | 5.67e+07     |
|    n_updates            | 7170         |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.895        |
|    value_loss           | 1.82e+08     |
------------------------------------------
Eval num_timesteps=1469000, episode_reward=-64647.60 +/- 35804.23
Episode length: 62.00 +/- 33.35
------------------------------------
| eval/              |             |
|    mean action     | 0.018176781 |
|    mean velocity x | -0.101      |
|    mean velocity y | -1.04       |
|    mean velocity z | 19.4        |
|    mean_ep_length  | 62          |
|    mean_reward     | -6.46e+04   |
| time/              |             |
|    total_timesteps | 1469000     |
------------------------------------
Eval num_timesteps=1469500, episode_reward=-23434.23 +/- 20830.83
Episode length: 39.80 +/- 15.47
------------------------------------
| eval/              |             |
|    mean action     | -0.44674787 |
|    mean velocity x | 1.82        |
|    mean velocity y | 3.07        |
|    mean velocity z | 21          |
|    mean_ep_length  | 39.8        |
|    mean_reward     | -2.34e+04   |
| time/              |             |
|    total_timesteps | 1469500     |
------------------------------------
Eval num_timesteps=1470000, episode_reward=-84657.61 +/- 12621.37
Episode length: 80.00 +/- 15.47
----------------------------------
| eval/              |           |
|    mean action     | 0.6912395 |
|    mean velocity x | -3.09     |
|    mean velocity y | -4.61     |
|    mean velocity z | 19.3      |
|    mean_ep_length  | 80        |
|    mean_reward     | -8.47e+04 |
| time/              |           |
|    total_timesteps | 1470000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 74.9      |
|    ep_rew_mean     | -8.34e+04 |
| time/              |           |
|    fps             | 161       |
|    iterations      | 718       |
|    time_elapsed    | 9081      |
|    total_timesteps | 1470464   |
----------------------------------
Eval num_timesteps=1470500, episode_reward=-77345.63 +/- 39136.72
Episode length: 53.80 +/- 11.62
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.007687561  |
|    mean velocity x      | 2.18          |
|    mean velocity y      | 0.874         |
|    mean velocity z      | 15.9          |
|    mean_ep_length       | 53.8          |
|    mean_reward          | -7.73e+04     |
| time/                   |               |
|    total_timesteps      | 1470500       |
| train/                  |               |
|    approx_kl            | 0.00018976789 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.377         |
|    learning_rate        | 0.001         |
|    loss                 | 6.66e+07      |
|    n_updates            | 7180          |
|    policy_gradient_loss | -0.000441     |
|    std                  | 0.895         |
|    value_loss           | 1.66e+08      |
-------------------------------------------
Eval num_timesteps=1471000, episode_reward=-80811.97 +/- 35938.54
Episode length: 58.00 +/- 8.05
------------------------------------
| eval/              |             |
|    mean action     | -0.17818853 |
|    mean velocity x | 2.41        |
|    mean velocity y | 2.81        |
|    mean velocity z | 18.8        |
|    mean_ep_length  | 58          |
|    mean_reward     | -8.08e+04   |
| time/              |             |
|    total_timesteps | 1471000     |
------------------------------------
Eval num_timesteps=1471500, episode_reward=-62349.66 +/- 34040.97
Episode length: 59.40 +/- 19.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.45607957 |
|    mean velocity x | -0.934     |
|    mean velocity y | -3.08      |
|    mean velocity z | 18.4       |
|    mean_ep_length  | 59.4       |
|    mean_reward     | -6.23e+04  |
| time/              |            |
|    total_timesteps | 1471500    |
-----------------------------------
Eval num_timesteps=1472000, episode_reward=-67450.13 +/- 45496.27
Episode length: 49.00 +/- 17.03
------------------------------------
| eval/              |             |
|    mean action     | -0.33964726 |
|    mean velocity x | 1.69        |
|    mean velocity y | 2.33        |
|    mean velocity z | 21.6        |
|    mean_ep_length  | 49          |
|    mean_reward     | -6.75e+04   |
| time/              |             |
|    total_timesteps | 1472000     |
------------------------------------
Eval num_timesteps=1472500, episode_reward=-60349.63 +/- 22155.16
Episode length: 67.20 +/- 15.68
------------------------------------
| eval/              |             |
|    mean action     | -0.10958909 |
|    mean velocity x | 0.163       |
|    mean velocity y | 0.353       |
|    mean velocity z | 20.7        |
|    mean_ep_length  | 67.2        |
|    mean_reward     | -6.03e+04   |
| time/              |             |
|    total_timesteps | 1472500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 75.7      |
|    ep_rew_mean     | -8.68e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 719       |
|    time_elapsed    | 9089      |
|    total_timesteps | 1472512   |
----------------------------------
Eval num_timesteps=1473000, episode_reward=-77255.92 +/- 33068.39
Episode length: 56.20 +/- 6.91
------------------------------------------
| eval/                   |              |
|    mean action          | -0.024477554 |
|    mean velocity x      | 1.01         |
|    mean velocity y      | -0.116       |
|    mean velocity z      | 16.4         |
|    mean_ep_length       | 56.2         |
|    mean_reward          | -7.73e+04    |
| time/                   |              |
|    total_timesteps      | 1473000      |
| train/                  |              |
|    approx_kl            | 5.468141e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.292        |
|    learning_rate        | 0.001        |
|    loss                 | 1.27e+08     |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000346    |
|    std                  | 0.895        |
|    value_loss           | 2.54e+08     |
------------------------------------------
Eval num_timesteps=1473500, episode_reward=-73032.94 +/- 41440.78
Episode length: 52.80 +/- 13.99
-------------------------------------
| eval/              |              |
|    mean action     | -0.009278396 |
|    mean velocity x | 0.113        |
|    mean velocity y | -0.296       |
|    mean velocity z | 17.3         |
|    mean_ep_length  | 52.8         |
|    mean_reward     | -7.3e+04     |
| time/              |              |
|    total_timesteps | 1473500      |
-------------------------------------
Eval num_timesteps=1474000, episode_reward=-54180.22 +/- 29084.99
Episode length: 58.20 +/- 22.53
------------------------------------
| eval/              |             |
|    mean action     | -0.36461145 |
|    mean velocity x | 1.91        |
|    mean velocity y | 1.58        |
|    mean velocity z | 17.4        |
|    mean_ep_length  | 58.2        |
|    mean_reward     | -5.42e+04   |
| time/              |             |
|    total_timesteps | 1474000     |
------------------------------------
Eval num_timesteps=1474500, episode_reward=-79232.06 +/- 24323.98
Episode length: 72.20 +/- 21.02
-----------------------------------
| eval/              |            |
|    mean action     | 0.45257404 |
|    mean velocity x | -0.682     |
|    mean velocity y | -1.63      |
|    mean velocity z | 21         |
|    mean_ep_length  | 72.2       |
|    mean_reward     | -7.92e+04  |
| time/              |            |
|    total_timesteps | 1474500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.5      |
|    ep_rew_mean     | -8.13e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 720       |
|    time_elapsed    | 9096      |
|    total_timesteps | 1474560   |
----------------------------------
Eval num_timesteps=1475000, episode_reward=-69017.90 +/- 41188.78
Episode length: 56.20 +/- 10.67
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.1582465     |
|    mean velocity x      | 0.468         |
|    mean velocity y      | -0.56         |
|    mean velocity z      | 20.7          |
|    mean_ep_length       | 56.2          |
|    mean_reward          | -6.9e+04      |
| time/                   |               |
|    total_timesteps      | 1475000       |
| train/                  |               |
|    approx_kl            | 0.00087304297 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.337         |
|    learning_rate        | 0.001         |
|    loss                 | 5.62e+07      |
|    n_updates            | 7200          |
|    policy_gradient_loss | -0.00127      |
|    std                  | 0.895         |
|    value_loss           | 1.82e+08      |
-------------------------------------------
Eval num_timesteps=1475500, episode_reward=-23485.55 +/- 27504.98
Episode length: 35.00 +/- 22.95
------------------------------------
| eval/              |             |
|    mean action     | 0.059145723 |
|    mean velocity x | -2.42       |
|    mean velocity y | -0.126      |
|    mean velocity z | 16.7        |
|    mean_ep_length  | 35          |
|    mean_reward     | -2.35e+04   |
| time/              |             |
|    total_timesteps | 1475500     |
------------------------------------
Eval num_timesteps=1476000, episode_reward=-78954.33 +/- 31638.24
Episode length: 58.80 +/- 6.05
------------------------------------
| eval/              |             |
|    mean action     | -0.09533838 |
|    mean velocity x | -0.404      |
|    mean velocity y | 0.211       |
|    mean velocity z | 20.8        |
|    mean_ep_length  | 58.8        |
|    mean_reward     | -7.9e+04    |
| time/              |             |
|    total_timesteps | 1476000     |
------------------------------------
Eval num_timesteps=1476500, episode_reward=-81893.42 +/- 24631.55
Episode length: 64.40 +/- 12.01
-----------------------------------
| eval/              |            |
|    mean action     | 0.37685403 |
|    mean velocity x | -2.64      |
|    mean velocity y | -2.32      |
|    mean velocity z | 21.2       |
|    mean_ep_length  | 64.4       |
|    mean_reward     | -8.19e+04  |
| time/              |            |
|    total_timesteps | 1476500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 73.3      |
|    ep_rew_mean     | -8.33e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 721       |
|    time_elapsed    | 9103      |
|    total_timesteps | 1476608   |
----------------------------------
Eval num_timesteps=1477000, episode_reward=-51501.56 +/- 28972.23
Episode length: 60.80 +/- 28.74
------------------------------------------
| eval/                   |              |
|    mean action          | 0.07057812   |
|    mean velocity x      | -0.896       |
|    mean velocity y      | -1.19        |
|    mean velocity z      | 21.1         |
|    mean_ep_length       | 60.8         |
|    mean_reward          | -5.15e+04    |
| time/                   |              |
|    total_timesteps      | 1477000      |
| train/                  |              |
|    approx_kl            | 9.536679e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.001        |
|    loss                 | 9.56e+07     |
|    n_updates            | 7210         |
|    policy_gradient_loss | -0.000192    |
|    std                  | 0.895        |
|    value_loss           | 2.08e+08     |
------------------------------------------
Eval num_timesteps=1477500, episode_reward=-59248.56 +/- 35439.90
Episode length: 65.00 +/- 37.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.19279613 |
|    mean velocity x | -1.45      |
|    mean velocity y | -2.47      |
|    mean velocity z | 21.5       |
|    mean_ep_length  | 65         |
|    mean_reward     | -5.92e+04  |
| time/              |            |
|    total_timesteps | 1477500    |
-----------------------------------
Eval num_timesteps=1478000, episode_reward=-111844.16 +/- 16765.80
Episode length: 84.80 +/- 41.68
-----------------------------------
| eval/              |            |
|    mean action     | 0.00945221 |
|    mean velocity x | 0.354      |
|    mean velocity y | -0.573     |
|    mean velocity z | 21.5       |
|    mean_ep_length  | 84.8       |
|    mean_reward     | -1.12e+05  |
| time/              |            |
|    total_timesteps | 1478000    |
-----------------------------------
Eval num_timesteps=1478500, episode_reward=-90293.65 +/- 9702.22
Episode length: 78.40 +/- 14.40
------------------------------------
| eval/              |             |
|    mean action     | -0.18528488 |
|    mean velocity x | 1.34        |
|    mean velocity y | 1.71        |
|    mean velocity z | 20.3        |
|    mean_ep_length  | 78.4        |
|    mean_reward     | -9.03e+04   |
| time/              |             |
|    total_timesteps | 1478500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.6      |
|    ep_rew_mean     | -8.39e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 722       |
|    time_elapsed    | 9111      |
|    total_timesteps | 1478656   |
----------------------------------
Eval num_timesteps=1479000, episode_reward=-81547.72 +/- 14476.27
Episode length: 74.80 +/- 20.78
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.29546002    |
|    mean velocity x      | -1.61         |
|    mean velocity y      | -2.71         |
|    mean velocity z      | 18.6          |
|    mean_ep_length       | 74.8          |
|    mean_reward          | -8.15e+04     |
| time/                   |               |
|    total_timesteps      | 1479000       |
| train/                  |               |
|    approx_kl            | 6.4260093e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.298         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+08      |
|    n_updates            | 7220          |
|    policy_gradient_loss | -3.28e-05     |
|    std                  | 0.895         |
|    value_loss           | 2.47e+08      |
-------------------------------------------
Eval num_timesteps=1479500, episode_reward=-66881.45 +/- 18125.13
Episode length: 73.20 +/- 21.40
-----------------------------------
| eval/              |            |
|    mean action     | 0.34036618 |
|    mean velocity x | -1.35      |
|    mean velocity y | -1.61      |
|    mean velocity z | 19.1       |
|    mean_ep_length  | 73.2       |
|    mean_reward     | -6.69e+04  |
| time/              |            |
|    total_timesteps | 1479500    |
-----------------------------------
Eval num_timesteps=1480000, episode_reward=-53976.01 +/- 28428.42
Episode length: 63.80 +/- 30.46
----------------------------------
| eval/              |           |
|    mean action     | 1.0739965 |
|    mean velocity x | -3.45     |
|    mean velocity y | -6.83     |
|    mean velocity z | 19.6      |
|    mean_ep_length  | 63.8      |
|    mean_reward     | -5.4e+04  |
| time/              |           |
|    total_timesteps | 1480000   |
----------------------------------
Eval num_timesteps=1480500, episode_reward=-65674.24 +/- 20885.09
Episode length: 67.40 +/- 11.93
-------------------------------------
| eval/              |              |
|    mean action     | -0.019703431 |
|    mean velocity x | -0.31        |
|    mean velocity y | 0.0878       |
|    mean velocity z | 21.1         |
|    mean_ep_length  | 67.4         |
|    mean_reward     | -6.57e+04    |
| time/              |              |
|    total_timesteps | 1480500      |
-------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 71.9      |
|    ep_rew_mean     | -8.62e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 723       |
|    time_elapsed    | 9118      |
|    total_timesteps | 1480704   |
----------------------------------
Eval num_timesteps=1481000, episode_reward=-61983.73 +/- 39422.30
Episode length: 53.60 +/- 18.01
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.24968588   |
|    mean velocity x      | -0.333        |
|    mean velocity y      | 0.692         |
|    mean velocity z      | 18.7          |
|    mean_ep_length       | 53.6          |
|    mean_reward          | -6.2e+04      |
| time/                   |               |
|    total_timesteps      | 1481000       |
| train/                  |               |
|    approx_kl            | 0.00076909864 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.314         |
|    learning_rate        | 0.001         |
|    loss                 | 4.82e+07      |
|    n_updates            | 7230          |
|    policy_gradient_loss | -0.00134      |
|    std                  | 0.895         |
|    value_loss           | 2.08e+08      |
-------------------------------------------
Eval num_timesteps=1481500, episode_reward=-80883.94 +/- 10791.44
Episode length: 70.20 +/- 5.08
-----------------------------------
| eval/              |            |
|    mean action     | 0.18996552 |
|    mean velocity x | -0.975     |
|    mean velocity y | -0.946     |
|    mean velocity z | 22.1       |
|    mean_ep_length  | 70.2       |
|    mean_reward     | -8.09e+04  |
| time/              |            |
|    total_timesteps | 1481500    |
-----------------------------------
Eval num_timesteps=1482000, episode_reward=-60298.72 +/- 26692.24
Episode length: 67.80 +/- 21.76
-----------------------------------
| eval/              |            |
|    mean action     | 0.30220142 |
|    mean velocity x | -0.175     |
|    mean velocity y | -1.4       |
|    mean velocity z | 18.3       |
|    mean_ep_length  | 67.8       |
|    mean_reward     | -6.03e+04  |
| time/              |            |
|    total_timesteps | 1482000    |
-----------------------------------
Eval num_timesteps=1482500, episode_reward=-84460.89 +/- 50547.97
Episode length: 72.60 +/- 54.25
------------------------------------
| eval/              |             |
|    mean action     | -0.15809694 |
|    mean velocity x | 0.737       |
|    mean velocity y | 0.142       |
|    mean velocity z | 17.3        |
|    mean_ep_length  | 72.6        |
|    mean_reward     | -8.45e+04   |
| time/              |             |
|    total_timesteps | 1482500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.9      |
|    ep_rew_mean     | -8.18e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 724       |
|    time_elapsed    | 9126      |
|    total_timesteps | 1482752   |
----------------------------------
Eval num_timesteps=1483000, episode_reward=-47140.96 +/- 35913.92
Episode length: 48.00 +/- 22.24
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.37400272    |
|    mean velocity x      | -1.09         |
|    mean velocity y      | -1.54         |
|    mean velocity z      | 17.8          |
|    mean_ep_length       | 48            |
|    mean_reward          | -4.71e+04     |
| time/                   |               |
|    total_timesteps      | 1483000       |
| train/                  |               |
|    approx_kl            | 0.00022558711 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.353         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+08      |
|    n_updates            | 7240          |
|    policy_gradient_loss | -0.000558     |
|    std                  | 0.895         |
|    value_loss           | 1.8e+08       |
-------------------------------------------
Eval num_timesteps=1483500, episode_reward=-72490.28 +/- 28678.14
Episode length: 64.00 +/- 15.92
------------------------------------
| eval/              |             |
|    mean action     | -0.30107638 |
|    mean velocity x | 0.0995      |
|    mean velocity y | 2.19        |
|    mean velocity z | 19.1        |
|    mean_ep_length  | 64          |
|    mean_reward     | -7.25e+04   |
| time/              |             |
|    total_timesteps | 1483500     |
------------------------------------
Eval num_timesteps=1484000, episode_reward=-78864.48 +/- 25189.56
Episode length: 71.80 +/- 20.77
-----------------------------------
| eval/              |            |
|    mean action     | 0.20373337 |
|    mean velocity x | 0.0917     |
|    mean velocity y | 0.127      |
|    mean velocity z | 19.3       |
|    mean_ep_length  | 71.8       |
|    mean_reward     | -7.89e+04  |
| time/              |            |
|    total_timesteps | 1484000    |
-----------------------------------
Eval num_timesteps=1484500, episode_reward=-77893.11 +/- 36605.58
Episode length: 59.80 +/- 6.79
------------------------------------
| eval/              |             |
|    mean action     | -0.06577554 |
|    mean velocity x | 0.383       |
|    mean velocity y | 0.169       |
|    mean velocity z | 19.6        |
|    mean_ep_length  | 59.8        |
|    mean_reward     | -7.79e+04   |
| time/              |             |
|    total_timesteps | 1484500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.8      |
|    ep_rew_mean     | -7.95e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 725       |
|    time_elapsed    | 9133      |
|    total_timesteps | 1484800   |
----------------------------------
Eval num_timesteps=1485000, episode_reward=-76078.85 +/- 35426.45
Episode length: 72.80 +/- 28.48
-----------------------------------------
| eval/                   |             |
|    mean action          | 0.4813133   |
|    mean velocity x      | -2.71       |
|    mean velocity y      | -4.69       |
|    mean velocity z      | 20.7        |
|    mean_ep_length       | 72.8        |
|    mean_reward          | -7.61e+04   |
| time/                   |             |
|    total_timesteps      | 1485000     |
| train/                  |             |
|    approx_kl            | 2.64236e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.001       |
|    loss                 | 1.45e+08    |
|    n_updates            | 7250        |
|    policy_gradient_loss | -0.000184   |
|    std                  | 0.895       |
|    value_loss           | 2.14e+08    |
-----------------------------------------
Eval num_timesteps=1485500, episode_reward=-48302.38 +/- 31420.82
Episode length: 62.60 +/- 33.97
-----------------------------------
| eval/              |            |
|    mean action     | 0.22441898 |
|    mean velocity x | 1.76       |
|    mean velocity y | 0.054      |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 62.6       |
|    mean_reward     | -4.83e+04  |
| time/              |            |
|    total_timesteps | 1485500    |
-----------------------------------
Eval num_timesteps=1486000, episode_reward=-73214.33 +/- 19606.66
Episode length: 62.40 +/- 6.28
-----------------------------------
| eval/              |            |
|    mean action     | 0.58453506 |
|    mean velocity x | -1.12      |
|    mean velocity y | -3.19      |
|    mean velocity z | 21.1       |
|    mean_ep_length  | 62.4       |
|    mean_reward     | -7.32e+04  |
| time/              |            |
|    total_timesteps | 1486000    |
-----------------------------------
Eval num_timesteps=1486500, episode_reward=-34495.13 +/- 28781.15
Episode length: 47.60 +/- 18.39
-----------------------------------
| eval/              |            |
|    mean action     | 0.41263992 |
|    mean velocity x | -0.923     |
|    mean velocity y | -2.79      |
|    mean velocity z | 17.6       |
|    mean_ep_length  | 47.6       |
|    mean_reward     | -3.45e+04  |
| time/              |            |
|    total_timesteps | 1486500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 65.7      |
|    ep_rew_mean     | -7.58e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 726       |
|    time_elapsed    | 9140      |
|    total_timesteps | 1486848   |
----------------------------------
Eval num_timesteps=1487000, episode_reward=-95647.29 +/- 22737.21
Episode length: 97.60 +/- 36.69
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.25923797   |
|    mean velocity x      | -0.366        |
|    mean velocity y      | 0.911         |
|    mean velocity z      | 20.5          |
|    mean_ep_length       | 97.6          |
|    mean_reward          | -9.56e+04     |
| time/                   |               |
|    total_timesteps      | 1487000       |
| train/                  |               |
|    approx_kl            | 0.00059763686 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.344         |
|    learning_rate        | 0.001         |
|    loss                 | 5.73e+07      |
|    n_updates            | 7260          |
|    policy_gradient_loss | -0.0012       |
|    std                  | 0.895         |
|    value_loss           | 1.95e+08      |
-------------------------------------------
Eval num_timesteps=1487500, episode_reward=-55282.34 +/- 42416.75
Episode length: 48.60 +/- 18.99
------------------------------------
| eval/              |             |
|    mean action     | -0.08803679 |
|    mean velocity x | 1.22        |
|    mean velocity y | 0.476       |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 48.6        |
|    mean_reward     | -5.53e+04   |
| time/              |             |
|    total_timesteps | 1487500     |
------------------------------------
Eval num_timesteps=1488000, episode_reward=-54662.58 +/- 26265.38
Episode length: 55.40 +/- 14.31
------------------------------------
| eval/              |             |
|    mean action     | 0.018667083 |
|    mean velocity x | -0.322      |
|    mean velocity y | -1.27       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 55.4        |
|    mean_reward     | -5.47e+04   |
| time/              |             |
|    total_timesteps | 1488000     |
------------------------------------
Eval num_timesteps=1488500, episode_reward=-66778.13 +/- 10675.98
Episode length: 76.20 +/- 21.76
------------------------------------
| eval/              |             |
|    mean action     | -0.45707685 |
|    mean velocity x | 1.54        |
|    mean velocity y | 3.13        |
|    mean velocity z | 18          |
|    mean_ep_length  | 76.2        |
|    mean_reward     | -6.68e+04   |
| time/              |             |
|    total_timesteps | 1488500     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 68.7      |
|    ep_rew_mean     | -7.83e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 727       |
|    time_elapsed    | 9148      |
|    total_timesteps | 1488896   |
----------------------------------
Eval num_timesteps=1489000, episode_reward=-60029.52 +/- 31811.23
Episode length: 58.00 +/- 19.75
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.28801113    |
|    mean velocity x      | 0.738         |
|    mean velocity y      | -0.48         |
|    mean velocity z      | 18.2          |
|    mean_ep_length       | 58            |
|    mean_reward          | -6e+04        |
| time/                   |               |
|    total_timesteps      | 1489000       |
| train/                  |               |
|    approx_kl            | 0.00010066733 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.355         |
|    learning_rate        | 0.001         |
|    loss                 | 9.23e+07      |
|    n_updates            | 7270          |
|    policy_gradient_loss | -0.000382     |
|    std                  | 0.895         |
|    value_loss           | 1.98e+08      |
-------------------------------------------
Eval num_timesteps=1489500, episode_reward=-65414.04 +/- 34391.56
Episode length: 56.60 +/- 12.11
-----------------------------------
| eval/              |            |
|    mean action     | -0.5935827 |
|    mean velocity x | 1.94       |
|    mean velocity y | 3.56       |
|    mean velocity z | 20.1       |
|    mean_ep_length  | 56.6       |
|    mean_reward     | -6.54e+04  |
| time/              |            |
|    total_timesteps | 1489500    |
-----------------------------------
Eval num_timesteps=1490000, episode_reward=-71952.99 +/- 24057.75
Episode length: 60.20 +/- 11.87
------------------------------------
| eval/              |             |
|    mean action     | -0.21593355 |
|    mean velocity x | 1.02        |
|    mean velocity y | 1.28        |
|    mean velocity z | 20.2        |
|    mean_ep_length  | 60.2        |
|    mean_reward     | -7.2e+04    |
| time/              |             |
|    total_timesteps | 1490000     |
------------------------------------
Eval num_timesteps=1490500, episode_reward=-67865.97 +/- 29228.22
Episode length: 61.00 +/- 16.53
----------------------------------
| eval/              |           |
|    mean action     | 0.2448566 |
|    mean velocity x | -2.33     |
|    mean velocity y | -2.45     |
|    mean velocity z | 18.6      |
|    mean_ep_length  | 61        |
|    mean_reward     | -6.79e+04 |
| time/              |           |
|    total_timesteps | 1490500   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 70.7      |
|    ep_rew_mean     | -8.09e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 728       |
|    time_elapsed    | 9155      |
|    total_timesteps | 1490944   |
----------------------------------
Eval num_timesteps=1491000, episode_reward=-46418.78 +/- 35250.37
Episode length: 51.60 +/- 22.47
-------------------------------------------
| eval/                   |               |
|    mean action          | 0.31634143    |
|    mean velocity x      | -0.27         |
|    mean velocity y      | -0.728        |
|    mean velocity z      | 16.5          |
|    mean_ep_length       | 51.6          |
|    mean_reward          | -4.64e+04     |
| time/                   |               |
|    total_timesteps      | 1491000       |
| train/                  |               |
|    approx_kl            | 1.8714403e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.38          |
|    learning_rate        | 0.001         |
|    loss                 | 9.82e+07      |
|    n_updates            | 7280          |
|    policy_gradient_loss | -0.00015      |
|    std                  | 0.895         |
|    value_loss           | 1.73e+08      |
-------------------------------------------
Eval num_timesteps=1491500, episode_reward=-59746.38 +/- 45741.91
Episode length: 46.80 +/- 20.28
------------------------------------
| eval/              |             |
|    mean action     | 0.023972355 |
|    mean velocity x | -0.758      |
|    mean velocity y | -1.63       |
|    mean velocity z | 23.5        |
|    mean_ep_length  | 46.8        |
|    mean_reward     | -5.97e+04   |
| time/              |             |
|    total_timesteps | 1491500     |
------------------------------------
Eval num_timesteps=1492000, episode_reward=-40145.36 +/- 19796.83
Episode length: 53.00 +/- 11.95
-----------------------------------
| eval/              |            |
|    mean action     | 0.20960975 |
|    mean velocity x | 1.99       |
|    mean velocity y | -0.532     |
|    mean velocity z | 17.9       |
|    mean_ep_length  | 53         |
|    mean_reward     | -4.01e+04  |
| time/              |            |
|    total_timesteps | 1492000    |
-----------------------------------
Eval num_timesteps=1492500, episode_reward=-87927.06 +/- 17484.70
Episode length: 71.60 +/- 7.58
-----------------------------------
| eval/              |            |
|    mean action     | 0.38311985 |
|    mean velocity x | -0.174     |
|    mean velocity y | -3.09      |
|    mean velocity z | 16.9       |
|    mean_ep_length  | 71.6       |
|    mean_reward     | -8.79e+04  |
| time/              |            |
|    total_timesteps | 1492500    |
-----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 69        |
|    ep_rew_mean     | -7.77e+04 |
| time/              |           |
|    fps             | 162       |
|    iterations      | 729       |
|    time_elapsed    | 9162      |
|    total_timesteps | 1492992   |
----------------------------------
Eval num_timesteps=1493000, episode_reward=-68004.58 +/- 36952.11
Episode length: 50.20 +/- 20.59
------------------------------------------
| eval/                   |              |
|    mean action          | -0.91685045  |
|    mean velocity x      | 2.88         |
|    mean velocity y      | 5.37         |
|    mean velocity z      | 18.3         |
|    mean_ep_length       | 50.2         |
|    mean_reward          | -6.8e+04     |
| time/                   |              |
|    total_timesteps      | 1493000      |
| train/                  |              |
|    approx_kl            | 0.0010123634 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.001        |
|    loss                 | 5.52e+07     |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 0.894        |
|    value_loss           | 1.6e+08      |
------------------------------------------
Eval num_timesteps=1493500, episode_reward=-66661.66 +/- 35298.21
Episode length: 62.60 +/- 19.62
------------------------------------
| eval/              |             |
|    mean action     | -0.39575848 |
|    mean velocity x | 1.57        |
|    mean velocity y | 2.25        |
|    mean velocity z | 18.7        |
|    mean_ep_length  | 62.6        |
|    mean_reward     | -6.67e+04   |
| time/              |             |
|    total_timesteps | 1493500     |
------------------------------------
Eval num_timesteps=1494000, episode_reward=-77090.65 +/- 38890.52
Episode length: 58.60 +/- 23.36
-----------------------------------
| eval/              |            |
|    mean action     | -0.9492954 |
|    mean velocity x | 3.26       |
|    mean velocity y | 5.89       |
|    mean velocity z | 17.3       |
|    mean_ep_length  | 58.6       |
|    mean_reward     | -7.71e+04  |
| time/              |            |
|    total_timesteps | 1494000    |
-----------------------------------
Eval num_timesteps=1494500, episode_reward=-60097.66 +/- 31178.54
Episode length: 52.00 +/- 9.98
-----------------------------------
| eval/              |            |
|    mean action     | 0.17138992 |
|    mean velocity x | 0.488      |
|    mean velocity y | -1.52      |
|    mean velocity z | 20.7       |
|    mean_ep_length  | 52         |
|    mean_reward     | -6.01e+04  |
| time/              |            |
|    total_timesteps | 1494500    |
-----------------------------------
Eval num_timesteps=1495000, episode_reward=-81925.59 +/- 25036.95
Episode length: 60.40 +/- 4.18
----------------------------------
| eval/              |           |
|    mean action     | 0.4165837 |
|    mean velocity x | -1.03     |
|    mean velocity y | -2.57     |
|    mean velocity z | 19.4      |
|    mean_ep_length  | 60.4      |
|    mean_reward     | -8.19e+04 |
| time/              |           |
|    total_timesteps | 1495000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 66.4      |
|    ep_rew_mean     | -7.38e+04 |
| time/              |           |
|    fps             | 163       |
|    iterations      | 730       |
|    time_elapsed    | 9170      |
|    total_timesteps | 1495040   |
----------------------------------
Eval num_timesteps=1495500, episode_reward=-54082.77 +/- 18869.40
Episode length: 53.00 +/- 10.75
------------------------------------------
| eval/                   |              |
|    mean action          | -0.49769413  |
|    mean velocity x      | 2.53         |
|    mean velocity y      | 3.7          |
|    mean velocity z      | 19.2         |
|    mean_ep_length       | 53           |
|    mean_reward          | -5.41e+04    |
| time/                   |              |
|    total_timesteps      | 1495500      |
| train/                  |              |
|    approx_kl            | 0.0006717171 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.001        |
|    loss                 | 7.99e+07     |
|    n_updates            | 7300         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.895        |
|    value_loss           | 1.86e+08     |
------------------------------------------
Eval num_timesteps=1496000, episode_reward=-68475.09 +/- 31213.38
Episode length: 67.00 +/- 21.59
-----------------------------------
| eval/              |            |
|    mean action     | 0.12079648 |
|    mean velocity x | 0.67       |
|    mean velocity y | -0.0499    |
|    mean velocity z | 20.5       |
|    mean_ep_length  | 67         |
|    mean_reward     | -6.85e+04  |
| time/              |            |
|    total_timesteps | 1496000    |
-----------------------------------
Eval num_timesteps=1496500, episode_reward=-41290.35 +/- 29375.26
Episode length: 47.20 +/- 10.55
------------------------------------
| eval/              |             |
|    mean action     | 0.024816964 |
|    mean velocity x | 0.786       |
|    mean velocity y | -0.11       |
|    mean velocity z | 18.5        |
|    mean_ep_length  | 47.2        |
|    mean_reward     | -4.13e+04   |
| time/              |             |
|    total_timesteps | 1496500     |
------------------------------------
Eval num_timesteps=1497000, episode_reward=-77752.99 +/- 32539.45
Episode length: 58.00 +/- 9.76
------------------------------------
| eval/              |             |
|    mean action     | -0.23434153 |
|    mean velocity x | -0.656      |
|    mean velocity y | 0.634       |
|    mean velocity z | 19.9        |
|    mean_ep_length  | 58          |
|    mean_reward     | -7.78e+04   |
| time/              |             |
|    total_timesteps | 1497000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 61.6      |
|    ep_rew_mean     | -7.01e+04 |
| time/              |           |
|    fps             | 163       |
|    iterations      | 731       |
|    time_elapsed    | 9177      |
|    total_timesteps | 1497088   |
----------------------------------
Eval num_timesteps=1497500, episode_reward=-95587.58 +/- 12491.85
Episode length: 61.80 +/- 2.56
------------------------------------------
| eval/                   |              |
|    mean action          | 0.20742525   |
|    mean velocity x      | -0.494       |
|    mean velocity y      | -1.49        |
|    mean velocity z      | 22           |
|    mean_ep_length       | 61.8         |
|    mean_reward          | -9.56e+04    |
| time/                   |              |
|    total_timesteps      | 1497500      |
| train/                  |              |
|    approx_kl            | 0.0009526142 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.001        |
|    loss                 | 8.24e+07     |
|    n_updates            | 7310         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.895        |
|    value_loss           | 2.12e+08     |
------------------------------------------
Eval num_timesteps=1498000, episode_reward=-68426.37 +/- 36186.93
Episode length: 54.80 +/- 17.45
------------------------------------
| eval/              |             |
|    mean action     | 0.006276016 |
|    mean velocity x | 1.3         |
|    mean velocity y | 1.24        |
|    mean velocity z | 14.9        |
|    mean_ep_length  | 54.8        |
|    mean_reward     | -6.84e+04   |
| time/              |             |
|    total_timesteps | 1498000     |
------------------------------------
Eval num_timesteps=1498500, episode_reward=-86140.49 +/- 33498.86
Episode length: 77.60 +/- 34.44
------------------------------------
| eval/              |             |
|    mean action     | -0.37665302 |
|    mean velocity x | -0.824      |
|    mean velocity y | 0.842       |
|    mean velocity z | 20          |
|    mean_ep_length  | 77.6        |
|    mean_reward     | -8.61e+04   |
| time/              |             |
|    total_timesteps | 1498500     |
------------------------------------
Eval num_timesteps=1499000, episode_reward=-67122.44 +/- 23985.94
Episode length: 57.60 +/- 10.89
----------------------------------
| eval/              |           |
|    mean action     | 0.4480391 |
|    mean velocity x | 1.04      |
|    mean velocity y | -2.63     |
|    mean velocity z | 18.3      |
|    mean_ep_length  | 57.6      |
|    mean_reward     | -6.71e+04 |
| time/              |           |
|    total_timesteps | 1499000   |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.3      |
|    ep_rew_mean     | -7.59e+04 |
| time/              |           |
|    fps             | 163       |
|    iterations      | 732       |
|    time_elapsed    | 9184      |
|    total_timesteps | 1499136   |
----------------------------------
Eval num_timesteps=1499500, episode_reward=-71125.65 +/- 15543.57
Episode length: 65.20 +/- 9.04
-------------------------------------------
| eval/                   |               |
|    mean action          | -0.016814534  |
|    mean velocity x      | -2.01         |
|    mean velocity y      | -0.94         |
|    mean velocity z      | 20.4          |
|    mean_ep_length       | 65.2          |
|    mean_reward          | -7.11e+04     |
| time/                   |               |
|    total_timesteps      | 1499500       |
| train/                  |               |
|    approx_kl            | 0.00031294912 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92         |
|    explained_variance   | 0.381         |
|    learning_rate        | 0.001         |
|    loss                 | 9.06e+07      |
|    n_updates            | 7320          |
|    policy_gradient_loss | -0.00068      |
|    std                  | 0.895         |
|    value_loss           | 1.78e+08      |
-------------------------------------------
Eval num_timesteps=1500000, episode_reward=-71926.14 +/- 20925.55
Episode length: 66.60 +/- 9.29
-----------------------------------
| eval/              |            |
|    mean action     | 0.43860477 |
|    mean velocity x | -1.39      |
|    mean velocity y | -3.15      |
|    mean velocity z | 21.7       |
|    mean_ep_length  | 66.6       |
|    mean_reward     | -7.19e+04  |
| time/              |            |
|    total_timesteps | 1500000    |
-----------------------------------
Eval num_timesteps=1500500, episode_reward=-97858.62 +/- 12539.57
Episode length: 60.40 +/- 2.58
-----------------------------------
| eval/              |            |
|    mean action     | 0.24702488 |
|    mean velocity x | -1.32      |
|    mean velocity y | -1.91      |
|    mean velocity z | 21.4       |
|    mean_ep_length  | 60.4       |
|    mean_reward     | -9.79e+04  |
| time/              |            |
|    total_timesteps | 1500500    |
-----------------------------------
Eval num_timesteps=1501000, episode_reward=-87091.57 +/- 14975.06
Episode length: 69.60 +/- 9.05
------------------------------------
| eval/              |             |
|    mean action     | -0.10028373 |
|    mean velocity x | 0.576       |
|    mean velocity y | 0.398       |
|    mean velocity z | 19          |
|    mean_ep_length  | 69.6        |
|    mean_reward     | -8.71e+04   |
| time/              |             |
|    total_timesteps | 1501000     |
------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.3      |
|    ep_rew_mean     | -7.91e+04 |
| time/              |           |
|    fps             | 163       |
|    iterations      | 733       |
|    time_elapsed    | 9192      |
|    total_timesteps | 1501184   |
----------------------------------
